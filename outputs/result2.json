[{"title": "MOMENTUM MEETS VIRALITY: A NOVEL METRIC FOR UNMASKING SOCIAL BIAS IN VIRAL TWEETS", "link_suffix": "/forum?id=FYvZCwdb6F", "link": "https://openreview.net/forum?id=FYvZCwdb6F", "pdf_link": "https://openreview.net/pdf?id=FYvZCwdb6F", "keywords": "Social bias, Tweet virality, ViralTweetScore, Hindi Tweets, Tweets", "abstract": "Predicting which social media posts will go viral is a critical but complex task in the field of computational social science. Previous studies have utilized various measures to forecast the virality of tweets or Facebook posts, but these approaches exhibit limitations, particularly in the absence of a virality metric that specifically considers social biases. In this paper, we test existing metrics and introduce a new metric, $\\textbf{ViralTweet Score (VTS)}$, inspired by principles of momentum from physics to better predict a tweet's virality given that it consists of social biases. We compare this new metric with others, highlighting the advantages and disadvantages of each of them as a virality measurement metric. We release the $\\textbf{ViralTweets Dataset}$ with $\\mathbf{88.8k}$ Hindi tweets and corresponding virality labels based on our VTS metric. We also show how social biases in posts can influence their potential to go viral. We test our hypothesis that VTS is a better metric using two methodologies and we show how VTS achieves an F1 score of 0.87 based on pairwise evaluation methodology and an overall F1 score of 0.58 based on our clustering-based verification methodology. Our work offers a novel metric for understanding tweet virality for biased tweets and opens the door for more equitable and effective social media analytics by considering the role of social biases in virality.", "title_embedding_index": 50, "title_abs_embedding_index": 75}, {"title": "KARPA: A Training-free Method of Adapting Knowledge Graph as References for Large Language Model's Reasoning Path Aggregation", "link_suffix": "/forum?id=Hw1tOjCWBZ", "link": "https://openreview.net/forum?id=Hw1tOjCWBZ", "pdf_link": "https://openreview.net/pdf?id=Hw1tOjCWBZ", "keywords": "Knowledge Graph, Large Language Models, Chain-of-Thought, Reasoning", "abstract": "Large language models (LLMs) demonstrate exceptional performance across a variety of tasks, yet they are often affected by hallucinations and the timeliness of knowledge. Leveraging knowledge graphs (KGs) as external knowledge sources has emerged as a viable solution, but existing methods for LLM-based knowledge graph question answering (KGQA) are often limited by step-by-step decision-making on KGs, restricting the global planning and reasoning capabilities of LLMs, or they require fine-tuning or pre-training on specific KGs. To address these challenges, we propose Knowledge graph Assisted Reasoning Path Aggregation (KARPA), a novel framework that harnesses the global planning abilities of LLMs for efficient and accurate KG reasoning on KGs. KARPA operates through a three-step process: pre-planning, retrieving, and reasoning. First, KARPA uses the LLM's global planning ability to pre-plan logically coherent relation paths based on the provided question and relevant relations within the KG. Next, in the retrieving phase, relation paths with high semantic similarity to the pre-planned paths are extracted as candidate paths using a semantic embedding model. Finally, these candidate paths are provided to the LLM for comprehensive reasoning. Unlike existing LLM-based KGQA methods, KARPA fully leverages the global planning and reasoning capabilities of LLMs without requiring stepwise traversal or additional training, and it is compatible with various LLM architectures. Extensive experimental results show that KARPA achieves state-of-the-art performance in KGQA tasks, delivering both high efficiency and accuracy.", "title_embedding_index": 51, "title_abs_embedding_index": 76}, {"title": "Mitigating Privacy Risk of Adversarial Examples with Counterfactual Explanations", "link_suffix": "/forum?id=gaa7gWPZBz", "link": "https://openreview.net/forum?id=gaa7gWPZBz", "pdf_link": "https://openreview.net/pdf?id=gaa7gWPZBz", "keywords": "Adversarial Examples, Privacy, Counterfactual Explanations", "abstract": "Robustness and privacy are two fundamental security properties that \nmachine learning models require. Without the balance between robustness and privacy leads to \nrobust models with high privacy risks. Obtaining machine learning models with high adversarial robustness and \nprivacy performance remains an open problem. In order to enhance the privacy performance of \nrobust models, we employ counterfactual explanations as a method \nto mitigate privacy risks while concurrently maintaining robust model accuracy, reducing the privacy risk of the robust model to the level of \nrandom guessing and using counterfactual explanations to generate adversarial examples for the first time. We analyze the similarities and differences between \nadversarial examples and counterfactual explanations and utilize these properties to design the \ngeneration method. We \nconduct an in-depth analysis of the advantages offered by counterfactual explanations compared \nto traditional adversarial examples. Our study indicates that the correlation between \nrobustness and privacy is strong and the ideal balance state of accuracy, robustness, and privacy is with 95% \nadversarial examples involved in model training.", "title_embedding_index": 52, "title_abs_embedding_index": 77}, {"title": "Sparse autoencoders reveal selective remapping of visual concepts during adaptation", "link_suffix": "/forum?id=imT03YXlG2", "link": "https://openreview.net/forum?id=imT03YXlG2", "pdf_link": "https://openreview.net/pdf?id=imT03YXlG2", "keywords": "interpretability, vision-language models, sparse autoencoder, adaptation", "abstract": "Compared to conventional machine learning models, foundation models excel at adaptation to a variety of downstream tasks with limited examples. Adapting foundation models for specific purposes has become a standard, yet it is an open question which mechanisms are in place during adaptation. Here we propose a variant of sparse auto-encoders to discover 49,000 candidate concepts relevant to the ImageNet reference task. We explore how these concepts influence to the model output and extend it to investigate how recent state-of-the-art adaptation techniques like MaPLe change the association of model inputs to these concepts. While activations of concepts slightly change between adapted and non-adapted models, we find that the majority of gains on common adaptation tasks can be explained with the existing concepts within the foundation model. This work provides a concrete framework to train and use SAEs for vision transformers and provides insights into the existing adaptation techniques.", "title_embedding_index": 53, "title_abs_embedding_index": 78}, {"title": "GenBen:A Genarative Benchmark for LLM-Aided Design", "link_suffix": "/forum?id=gtVo4xcpFI", "link": "https://openreview.net/forum?id=gtVo4xcpFI", "pdf_link": "https://openreview.net/pdf?id=gtVo4xcpFI", "keywords": "GenBen; Benchmark; LLM-Aided Design; LLM; Hardware Design", "abstract": "This paper introduces GenBen, a generative benchmark designed to evaluate the capabilities of large language models (LLMs) in hardware design. With the rapid advancement of LLM-aided design (LAD), it has become crucial to assess the effectiveness of these models in automating hardware design processes.\nExisting benchmarks primarily focus on hardware code generation and often neglect critical aspects such as Quality-of-Result (QoR) metrics, design diversity, modality, and test set contamination. GenBen is the first open-source, generative benchmark tailored for LAD that encompasses a range of tasks, from high-level architecture to low-level circuit optimization, and includes diverse, silicon-proven hardware designs. \nWe have also designed a difficulty tiering mechanism to provide fine-grained insights into enhancements of LLM-aided designs. Through extensive evaluations of several state-of-the-art LLMs using GenBen, we reveal their strengths and weaknesses in hardware design automation. Our findings are based on 10,920 experiments and 2,160 hours of evaluation, underscoring the potential of this work to significantly advance the LAD research community. \nIn addition, both GenBen employs an end-to-end testing infrastructure to ensure consistent and reproducible results across different LLMs. The benchmark is available athttps://anonymous.4open.science/r/GENBEN-2812.", "title_embedding_index": 54, "title_abs_embedding_index": 79}, {"title": "DySpec: Faster Speculative Decoding with Dynamic Token Tree Structure", "link_suffix": "/forum?id=orr5uPZY28", "link": "https://openreview.net/forum?id=orr5uPZY28", "pdf_link": "https://openreview.net/pdf?id=orr5uPZY28", "keywords": "inference methods, efficient inference, speculative decoding", "abstract": "While speculative decoding has recently appeared as a promising direction for accelerating the inference of large language models (LLMs), the speedup and scalability are strongly bounded by the token acceptance rate.\nPrevalent methods usually organize predicted tokens as independent chains or fixed token trees, which fails to generalize to diverse query distributions. \nIn this paper, we propose \\textsc{DySpec}, a faster speculative decoding algorithm with a novel dynamic token tree structure. \nWe begin by bridging the draft distribution and acceptance rate from \nintuitive and empirical clues, and successfully show that the two variables are strongly correlated. Based on this, we employ a greedy strategy to dynamically expand the token tree at run time. Theoretically, we show that our method can achieve optimal results under mild assumptions. Empirically, \\textsc{DySpec} yields a higher acceptance rate and speedup than fixed trees. \\textsc{DySpec} can drastically improve the throughput and reduce the latency of token generation across various data distribution and model sizes, which significantly outperforms strong competitors, including Specinfer and Sequoia. Under low temperature setting, \\textsc{DySpec} can improve the throughput up to 9.10x and reduce the latency up to 9.4x on Llama2-70B. Under high temperature setting, \\textsc{DySpec} can also improve the throughput up to 6.21x, despite the increasing difficulty of speculating more than one token per step for draft model.", "title_embedding_index": 55, "title_abs_embedding_index": 80}, {"title": "Hottel Zone Physics-Constrained Networks for Furnaces", "link_suffix": "/forum?id=hz3NtNpDNv", "link": "https://openreview.net/forum?id=hz3NtNpDNv", "pdf_link": "https://openreview.net/pdf?id=hz3NtNpDNv", "keywords": "Hottel Zone method, Physics-Informed Neural Networks, Radiation Heat Transfer, Furnaces", "abstract": "This paper investigates a novel approach to improve the temperature profile prediction of furnaces in foundation industries, crucial for sustainable manufacturing. While existing methods like the Hottel Zone model are accurate, they lack real-time inference capabilities. Deep learning methods excel in speed and prediction but require careful generalization for real-world applications. We propose a regularization technique that leverages the Hottel Zone method to make deep neural networks physics-aware, improving prediction accuracy for furnace temperature profiles. Our approach demonstrates effectiveness on various neural network architectures, including Multi-Layer Perceptrons (MLP), Long Short-Term Memory (LSTM) and Kolmogorov-Arnold Networks (KANs). We also discussion the data generation involved.", "title_embedding_index": 56, "title_abs_embedding_index": 81}, {"title": "StreamingBench: Assessing the Gap for MLLMs to Achieve Streaming Video Understanding", "link_suffix": "/forum?id=qnAZqlMGTB", "link": "https://openreview.net/forum?id=qnAZqlMGTB", "pdf_link": "https://openreview.net/pdf?id=qnAZqlMGTB", "keywords": "Benchmark, Streaming Video Understanding, Multimodal Large Language Models, Video Benchmark, Evaluation", "abstract": "The rapid development of Multimodal Large Language Models (MLLMs) has expanded their capabilities from image comprehension to video understanding. However, most of these MLLMs focus primarily on of\ufb02ine video comprehension, necessitating extensive processing of all video frames before any queries can be made. This presents a signi\ufb01cant gap compared to the human ability to watch, listen, think, and respond to streaming inputs in real time, highlighting the limitations of current MLLMs. In this paper, we introduce StreamingBench, the \ufb01rst comprehensive benchmark designed to evaluate the streaming video understanding capabilities of MLLMs. StreamingBench assesses three core aspects of streaming video understanding: (1) real-time visual understanding, (2) omni-source understanding and (3) contextual understanding. The benchmark consists of 18 tasks, featuring 900 videos and 4,500 human-curated QA pairs. Each video features \ufb01ve questions presented at different time points to simulate a continuous streaming scenario. We conduct experiments on StreamingBench with 15 open-source and proprietary MLLMs and \ufb01nd that even the most advanced proprietary MLLMs like Gemini 1.5 Pro and GPT-4o perform signi\ufb01cantly below human-level streaming video understanding capabilities. We hope our work can facilitate further advancements for MLLMs, empowering them to approach human-level video comprehension and interaction in more realistic scenarios.", "title_embedding_index": 57, "title_abs_embedding_index": 82}, {"title": "Toward Human-Interpretable Explanations in a Unified Framework for GNNs", "link_suffix": "/forum?id=N0MnPLK6r7", "link": "https://openreview.net/forum?id=N0MnPLK6r7", "pdf_link": "https://openreview.net/pdf?id=N0MnPLK6r7", "keywords": "eXplainable AI, Graph Neural Networks", "abstract": "As Graph Neural Networks (GNNs) are increasingly applied across various domains, explainability has become a critical factor for real-world applications. Existing post-hoc explainability methods primarily focus on estimating the importance of edges, nodes, or subgraphs in the input graph to identify substructures crucial for predictions. However, these methods often lack human interpretability and do not provide a unified framework that incorporates both model-level and instance-level explanations. In this context, we propose leveraging a set of graphlets---small, connected, non-isomorphic induced subgraphs widely used in various scientific fields---and their associated orbits as human-interpretable units to decompose GNN predictions. Domain experts can select the most relevant graphlets as interpretable units and request unified explanations based on these units. To address this problem, we introduce UO-Explainer, the Unified and Orbit-based Explainer for GNNs, which utilizes predefined orbits that are generalizable and universal across graph domains as interpretable units. Our model decomposes GNN weights into orbit units to extract class-specific graph patterns (model-level) and to identify important subgraphs within individual data instances for prediction (instance-level). Extensive experimental results demonstrate that UO-Explainer outperforms existing baselines in providing meaningful and interpretable explanations across both synthetic and real-world datasets.", "title_embedding_index": 58, "title_abs_embedding_index": 83}, {"title": "PIED: Physics-Informed Experimental Design for Inverse Problems", "link_suffix": "/forum?id=w7P92BEsb2", "link": "https://openreview.net/forum?id=w7P92BEsb2", "pdf_link": "https://openreview.net/pdf?id=w7P92BEsb2", "keywords": "Physics-Informed Neural Network, PINNs, Experimental Design, AI For Science", "abstract": "In many science and engineering settings, system dynamics are characterized by governing partial differential equations (PDEs), and a major challenge is to solve inverse problems (IPs) where unknown PDE parameters are inferred based on observational data gathered under limited budget. \nDue to the high costs of setting up and running experiments, experimental design (ED) is often done with the help of PDE simulations to optimize for the most informative design parameters (e.g., sensor placements) to solve such IPs, prior to actual data collection. This process of optimizing design parameters is especially critical when the budget and other practical constraints make it infeasible to adjust the design parameters between trials during the experiments.\nHowever, existing experimental design (ED) methods tend to require sequential and frequent design parameter adjustments between trials. Furthermore, they also have significant computational bottlenecks due to the need for complex numerical simulations for PDEs, and do not exploit the advantages provided by physics informed neural networks (PINNs) in solving IPs for PDE-governed systems, such as its meshless solutions, differentiability, and amortized training. \nThis work presents Physics-Informed Experimental Design (PIED), the first ED framework that makes use of PINNs in a fully differentiable architecture to perform continuous optimization of design parameters for IPs for one-shot deployments. \nPIED overcomes existing methods' computational bottlenecks through parallelized computation and meta-learning of PINN parameter initialization, and proposes novel methods to effectively take into account PINN training dynamics in optimizing the ED parameters. \nThrough experiments based on noisy simulated data and even real world experimental data, we empirically show that given limited observation budget, PIED significantly outperforms existing ED methods in solving IPs, including for challenging settings where the inverse parameters are unknown functions rather than just finite-dimensional.", "title_embedding_index": 59, "title_abs_embedding_index": 84}, {"title": "The Good, the Bad and the Ugly: Watermarks, Transferable Attacks and Adversarial Defenses", "link_suffix": "/forum?id=wE5xp3zBaQ", "link": "https://openreview.net/forum?id=wE5xp3zBaQ", "pdf_link": "https://openreview.net/pdf?id=wE5xp3zBaQ", "keywords": "Watermarks, Adversarial Defenses, Transferable Attacks, Interactive Proof Systems, Cryptography, Backdooring, Game Theory, Learning Theory", "abstract": "We formalize and extend existing definitions of backdoor-based watermarks and adversarial defenses asinteractive protocolsbetween two players. The existence of these schemes is inherently tied to the learning tasks for which they are designed. Our main result shows that foralmost everydiscriminative learning task, at least one of the two \u2014 a watermark or an adversarial defense \u2014 exists. The \"almost\" refers to the fact that we also identify a third, counterintuitive but necessary option, i.e., a scheme we call atransferable attack. By transferable attack, we refer to an efficient algorithm computing queries that look indistinguishable from the data distribution and foolallefficient defenders.To this end, we prove the necessity of a transferable attack via a construction that uses a cryptographic tool called homomorphic encryption. Furthermore, we show that any task that satisfies our notion of a transferable attack implies acryptographic primitive, thus requiring the underlying task to be computationally complex. These two facts imply an \"equivalence\" between the existence of transferable attacks and cryptography. Finally, we show that the class of tasks of bounded VC-dimension has an adversarial defense, and a subclass of them has a watermark.", "title_embedding_index": 60, "title_abs_embedding_index": 85}, {"title": "Common Feature Learning for Zero-shot Image Recognition", "link_suffix": "/forum?id=pcnq7fZs4t", "link": "https://openreview.net/forum?id=pcnq7fZs4t", "pdf_link": "https://openreview.net/pdf?id=pcnq7fZs4t", "keywords": "Zero-shot Image Recognition\uff1bVisual-semantic Relationship\uff1bFine-grained Alignment\uff1bSemantic Vectors Generation\uff1b", "abstract": "The key issue of zero-shot image recognition (ZIR)  is how to infer the relationship between visual space and semantic space from seen classes, and then effectively transfer the  relationship to unseen classes. Recently, most methods have focused on how to use images and class semantic vectors or class names to learn the relationship between visual space and semantic space. The relationship established by these two methods is class-level and coarse-grained. The differences between images of the same class are ignored, which leads to insufficiently tight relationships and affects the accurate recognition of unseen classes.To tackle such problem, we propose Common Feature learning for Zero-shot Image Recognition (CF-ZIR) method to learn fine-grained visual semantic relationships at the image-level. Based on the inter class association information provided by class semantic vectors, guide the extraction of common visual features between classes to obtain image semantic vectors. Experiments on three widely used benchmark datasets show the effectiveness of the proposed approach.", "title_embedding_index": 61, "title_abs_embedding_index": 86}, {"title": "Learning Fused State Representations for Control from Multi-View Observations", "link_suffix": "/forum?id=3g2iyFU8gA", "link": "https://openreview.net/forum?id=3g2iyFU8gA", "pdf_link": "https://openreview.net/pdf?id=3g2iyFU8gA", "keywords": "multi-view learning, reinforcement learning", "abstract": "In visual control tasks, leveraging observations from multiple views enables Reinforcement Learning (RL) agents to perceive the environment more effectively. However, while multi-view observations enrich decision-making information, they also increase the dimension of observation space and introduce more redundant information. Thus, how to learn compact and task-relevant representations from multi-view observations for downstream RL tasks remains a challenge. In this paper, we propose a Multi-view Fusion State for Control (MFSC), which integrates a self-attention mechanism with bisimulation metric learning to fuse task-relevant representations from multi-view observations. To foster more compact fused representations, we also incorporate a mask-based latent reconstruction auxiliary task to learn cross-view information. Additionly, this mechanism of mask and reconstruction can enpower the model with the ability to handle missing views by learning an additional mask tokens. We conducted extensive experiments on the Meta-World and Pybullet benchmarks, and the results demonstrate that our proposed method outperforms other multi-view RL algorithms and effectively aggregates task-relevant details from multi-view observations, coordinating attention across different views.", "title_embedding_index": 62, "title_abs_embedding_index": 87}, {"title": "Flow Matching for One-Step Sampling", "link_suffix": "/forum?id=WxLwXyBJLw", "link": "https://openreview.net/forum?id=WxLwXyBJLw", "pdf_link": "https://openreview.net/pdf?id=WxLwXyBJLw", "keywords": "Flow Matching, Generative Models, Ordinary Differential Equations, One-step generation", "abstract": "Flow-based generative models have rapidly advanced as a method for mapping simple distributions to complex ones for which the distribution function is unknown. By leveraging continuous-time stochastic processes, these models offer a powerful framework for density estimation, i.e. an algorithm that samples new points based only on existing samples. However, their requirement of solving ordinary differential equations (ODEs) during sampling process incurs substantial computational costs, particularly for large amount of data and numerous time points. This paper proposes a novel solution, which is based on a theoretical analysis of Flow Matching (FM), to overcome this bottleneck, namely, we developed an algorithm to find the point prototype for a given point from the target distribution. By eliminating the need for ODE solvers, our method significantly accelerates sampling while preserving model performance. Numerical experiments validate the proposed approach, demonstrating its efficiency.", "title_embedding_index": 63, "title_abs_embedding_index": 88}, {"title": "AgentRefine: Enhancing Agent Generalization through Refinement Tuning", "link_suffix": "/forum?id=FDimWzmcWn", "link": "https://openreview.net/forum?id=FDimWzmcWn", "pdf_link": "https://openreview.net/pdf?id=FDimWzmcWn", "keywords": "agent, self-refine, diversity, generalization, data synthesis", "abstract": "Large Language Model (LLM) based agents have proved their ability to perform complex tasks like humans. However, there is still a large gap between open-sourced LLMs and commercial models like the GPT series. In this paper, we focus on improving the agent generalization capabilities of LLMs via instruction tuning. We first observe that the existing agent training corpus exhibits satisfactory results on held-in evaluation sets but fails to generalize to held-out sets. These agent-tuning works face severe formatting errors and are frequently stuck in the same mistake for a long while. We analyze that the poor generalization ability comes from overfitting to several manual agent environments and a lack of adaptation to new situations. They struggle with the wrong action steps and can not learn from the experience but just memorize existing observation-action relations. Inspired by the insight, we propose a novel AgentRefine framework for agent-tuning. The core idea is to enable the model to learn the correct its mistakes via observation in the trajectory. Specifically, we propose an agent synthesis framework to encompass a diverse array of environments and tasks and prompt a strong LLM to refine its error action according to the environment feedback. AgentRefine significantly outperforms state-of-the-art agent-tuning work in terms of generalization ability on diverse agent tasks. It also has better robustness facing perturbation and can generate diversified thought in inference. Our findings establish the correlation between agent generalization and self-refinement and provide a new paradigm for future research.", "title_embedding_index": 64, "title_abs_embedding_index": 89}, {"title": "StarCraft II Arena: Evaluating LLMs in Strategic Planning, Real-Time Decision Making, and Adaptability", "link_suffix": "/forum?id=o3V7OuPxu4", "link": "https://openreview.net/forum?id=o3V7OuPxu4", "pdf_link": "https://openreview.net/pdf?id=o3V7OuPxu4", "keywords": "benchmark evaluation, large language model, LLM-based agent, strategic reasoning, real-time decision-making.", "abstract": "StarCraft II plays an important role in developing AI agents for real-time strategic reasoning due to its complex nature. However, people usually draw conclusions of how competent their agents are according to the level of the built-in agents in StarCraft II which they can win in terms of the final success rate. Little intermediate quantitative information is considered while human-in-the-loop analysis is time inefficient, which results in inadequate reflection of the true strategic reasoning ability. In this work, we propose StarCraft II Arena, a well-designed benchmark for evaluating the strategic planning, real-time decision-making, and adaptability capabilities of large language models (LLMs) agents. We introduce using fine-grained capability metrics, allowing for targeted capture and analysis of specific capability, and further propose a detailed decision trace to enhance the understanding of LLM behavior. We demonstrate the utility of such a benchmark by evaluating several state-of-the-art LLMs in various setups. Our results reveal distinct performances in long-term strategy development, real-time decision-making, and adapting to environmental changes. Such results show that the StarCraft II Arena offers a deeper insight into the decision-making process of LLMs and has the potential to become a challenging and comprehensive benchmark for strategic reasoning.", "title_embedding_index": 65, "title_abs_embedding_index": 90}, {"title": "Alternating Optimized Stochastic Vector Quantization in Neural Compression", "link_suffix": "/forum?id=4XHyThqt1C", "link": "https://openreview.net/forum?id=4XHyThqt1C", "pdf_link": "https://openreview.net/pdf?id=4XHyThqt1C", "keywords": "vector quantization, neural compression, image compression", "abstract": "In neural compression, vector quantization (VQ) is usually replaced by a differentiable approximation during training for gradient backpropagation. However, prior approximation methods face two main issues: 1) the train-test mismatch between differentiable approximation and actual quantization, and 2) the suboptimal encoder gradients for rate-distortion (RD) optimization. In this paper, we first provide new finds about how approximation methods influence the RD optimization in neural compression, and then propose a new solution based on these finds. Specifically, if a neural compressor is regarded as a source-space VQ, we find that the encoder implicitly determines the quantization boundaries, and the decoder determines the quantization centers.  Suboptimal approximation methods lead to suboptimal gradients for RD optimization of quantization boundaries and centers. Therefore, to address the first issue,  we propose an encode-decoder alternating optimization strategy. The encoder is optimized with differentiable approximation, and the decoder is optimized with actual quantization to avoid the train-test mismatch of quantization centers.  To address the second issue, we propose a sphere-noise based stochastic approximation method. During encoder optimization, VQ is replaced with a uniform sphere noise centered at the input vector. When the input vector is located at the quantization boundary, the encoder gradient is closer to the difference in RD loss between adjacent quantization centers, facilitating better encoder optimization. We name the combination of optimization strategy and approximation method as Alternating Optimized Stochastic Vector Quantization.\nExperimental results on various vector sources and natural images demonstrate the effectiveness of our method.", "title_embedding_index": 66, "title_abs_embedding_index": 91}, {"title": "SEAL: Scaling to Emphasize Attention for Long-Context Retrieval", "link_suffix": "/forum?id=yRd4loGAhJ", "link": "https://openreview.net/forum?id=yRd4loGAhJ", "pdf_link": "https://openreview.net/pdf?id=yRd4loGAhJ", "keywords": "large language models, long context, retrieval, attention, supervised fine-tuning", "abstract": "In this work, we introduce a novel approach called Scaling to Emphasize Attention for Long-context retrieval (SEAL), which enhances the retrieval performance of large language models (LLMs) over extended contexts. Previous studies have shown that each attention head in LLMs has unique functionality and collectively contributes to the overall behavior of the model. Similarly, we observe that specific heads are particularly related to long-context retrieval and are positively or negatively correlated with retrieval scores. Building on this insight, we propose a cost-efficient, learning-based mechanism to emphasize these heads, improving the model's performance in long-context retrieval tasks. By applying SEAL, we achieved significant improvements in in-domain retrieval performance across various tasks and considerable improvement in the cross-domain document QA task of LongBench. Additionally, when combined with existing training-free context extension techniques, SEAL extends the context limits of LLMs while maintaining highly reliable outputs, opening new avenues for research in this field.", "title_embedding_index": 67, "title_abs_embedding_index": 92}, {"title": "Slashed Normal: Parameterize Normal Posterior Distributions with KL Amplitude", "link_suffix": "/forum?id=6ifeGfWxtX", "link": "https://openreview.net/forum?id=6ifeGfWxtX", "pdf_link": "https://openreview.net/pdf?id=6ifeGfWxtX", "keywords": "Variational Inference, Kullback-Leibler Divergence, Posterior Parameterization, Variational Autoencoders, Variational Information Bottleneck", "abstract": "We present Slashed Normal, a novel parameterization for the normal posterior\ndistribution in variational-inference-based latent variable models. Slashed Normal\ntakes a simple form resembling conventional practice, but uses the new stdplus\nactivation function to derive the standard deviation instead of softplus or exp. Although taking this simple form, the Slashed Normal establishes a direct connection between the squared l2-norm of the raw neural network output, termed KL amplitude, and the exact KL divergence value between the prior and the posterior. As a result, this parameterization enables a direct control of the KL divergence value, which is usually interpreted as the rate from the rate-distortion perspective for variational\nautoencoders. We demonstrate the versatility of Slashed Normal through theoretical analysis and experiments, showcasing its ability to provide good insight about the posterior distribution, explicit control over the KL divergence, and mitigate\nposterior collapse.", "title_embedding_index": 68, "title_abs_embedding_index": 93}, {"title": "Defend against Jailbreak Attacks via Debate with Partially Perceptive Agents", "link_suffix": "/forum?id=STpxO1Siaq", "link": "https://openreview.net/forum?id=STpxO1Siaq", "pdf_link": "https://openreview.net/pdf?id=STpxO1Siaq", "keywords": "Multi-agent Debate; Defense; Visual Large Language Models", "abstract": "Recent studies have shown that maliciously injecting or perturbing the input image in Vision Large Language Models (VLMs) can lead to jailbreak attacks, raising significant security concerns. A straightforward defense strategy against such attacks is to crop the input image, thereby disrupting the effectiveness of the injection or perturbation. However, the cropping can significantly distort the semantics of the input image, leading to an adverse impact on the model's output when processing clean input. To mitigate the adverse impact, we propose a defense mechanism against jailbreak attacks based on a multi-agent debate approach. In this method, one agent (\u201cintegrated\u201d agent) accesses the full integrated image, while the other (\u201cpartial\u201d agent) only accesses cropped/partial images, aiming to avoid the attack while preserving the correct semantics in the output as much as possible. Our key insight is that when an integrated agent debates with a partial agent, if the integrated agent receives clean input, it can successfully persuade the partial agent. Conversely, if the integrated agent is given an attacked input, the partial agent can persuade it to rethink the original output, thereby achieving effective defense against the attack. Empirical experiments have demonstrated that our method provides more effective defense compared to the baseline method, successfully reducing the average attack success rate from 100% to 22%. In more advanced experimental setups, our proposed method can even limit the average attack success rate to 18% (debating with GPT-4o) and 14% (with enhanced perspective).", "title_embedding_index": 69, "title_abs_embedding_index": 94}, {"title": "Recovering Knowledge by Hardening Language Models", "link_suffix": "/forum?id=uOnElfFuey", "link": "https://openreview.net/forum?id=uOnElfFuey", "pdf_link": "https://openreview.net/pdf?id=uOnElfFuey", "keywords": "regular language, language model, transformers, knowledge interpretation", "abstract": "Recent neural language models show impressive capabilities on a wide range of tasks. However, it is not fully understood how the knowledge of the language is encoded in these models. In this work, we focus on the simplest case of languages, regular languages, and study language models trained on strings matching certain regular expressions. We propose a method, dubbed LaMFA, to recover the full knowledge of the regular language model by hardening it into a finite automaton. Such hardening is conducted by empirically partition the latent space of language models into finite states, and then recover a deterministic finite automaton by the estimated transition probabilities between these states. Through experiments on regular languages of varying complexity, we demonstrate that LaMFA can effectively extract DFA that consistently replicate the performance of the original language model. Notably, the extracted DFAs exhibit enhanced generalization capabilities, achieving 100% accuracy even in out-of-distribution scenarios", "title_embedding_index": 70, "title_abs_embedding_index": 95}, {"title": "MuLan: Multimodal-LLM Agent for Progressive and Interactive Multi-Object Diffusion", "link_suffix": "/forum?id=Yd5MHVIKLk", "link": "https://openreview.net/forum?id=Yd5MHVIKLk", "pdf_link": "https://openreview.net/pdf?id=Yd5MHVIKLk", "keywords": "Diffusion models, Controllable generation, multi-modal agent", "abstract": "Existing text-to-image models still struggle to generate images of multiple objects, especially in handling their spatial positions, relative sizes, overlapping, and attribute bindings. To efficiently address these challenges, we develop a training-free Multimodal-LLM agent (MuLan), as a human painter, that can progressively generate multi-object with intricate planning and feedback control.\nMuLan harnesses a large language model (LLM) to decompose a prompt to a sequence of sub-tasks, each generating only one object by stable diffusion, conditioned on previously generated objects. Unlike existing LLM-grounded methods, MuLan only produces a high-level plan at the beginning while the exact size and location of each object are determined upon each sub-task by an LLM and attention guidance. Moreover, MuLan adopts a vision-language model (VLM) to provide feedback to the image generated in each sub-task and control the diffusion model to re-generate the image if it violates the original prompt. Hence, each model in every step of MuLan only needs to address an easy sub-task it is specialized for. The multi-step process also allows human users to monitor the generation process and make preferred changes at any intermediate step via text prompts, thereby improving the human-AI collaboration experience. We collect 200 prompts containing multi-objects with spatial relationships and attribute bindings from different benchmarks to evaluate MuLan. The results demonstrate the superiority of MuLan in generating multiple objects over baselines and its creativity when collaborating with human users.", "title_embedding_index": 71, "title_abs_embedding_index": 96}, {"title": "TabM: Advancing tabular deep learning with parameter-efficient ensembling", "link_suffix": "/forum?id=Sd4wYYOhmY", "link": "https://openreview.net/forum?id=Sd4wYYOhmY", "pdf_link": "https://openreview.net/pdf?id=Sd4wYYOhmY", "keywords": "tabular, tabular data, deep learning, architecture", "abstract": "Deep learning architectures for supervised learning on tabular data range from simple multilayer perceptrons (MLP) to sophisticated Transformers and retrieval-augmented methods.\nThis study highlights a major, yet so far overlooked opportunity for substantially improving tabular MLPs; namely, parameter-efficient ensembling -- a paradigm for imitating an ensemble of models with just one model.\nWe start by describing TabM -- a simple model based on MLP and BatchEnsemble (an existing technique), improved with our custom modifications.\nThen, we perform a large scale evaluation of tabular DL architectures on public benchmarks in terms of both task performance and efficiency, which renders the landscape of tabular DL in a new light.\nIn particular, we find that TabM outperforms prior tabular DL models, while the complexity of attention- and retrieval-based methods does not pay off.\nLastly, we conduct a detailed empirical analysis, that sheds some light on the high performance of TabM.\nFor example, we show that parameter-efficient ensembling is not an arbitrary trick, but rather a highly effective way to reduce overfitting and improve optimization dynamics of tabular MLPs.\nOverall, our work brings an impactful technique to tabular DL, analyses its behaviour, and advances the performance-efficiency tradeoff with TabM -- a simple and powerful baseline for researchers and practitioners.", "title_embedding_index": 72, "title_abs_embedding_index": 97}, {"title": "Leveraging Implicit Sentiments: Enhancing Reliability and Validity in Psychological Trait Evaluation of LLMs", "link_suffix": "/forum?id=zrdkQaf48Z", "link": "https://openreview.net/forum?id=zrdkQaf48Z", "pdf_link": "https://openreview.net/pdf?id=zrdkQaf48Z", "keywords": "LLM, Benchmark, Evaluation, Psychometrics", "abstract": "Recent advancements in Large Language Models (LLMs) have led to their increasing integration into human life. Understanding their inherent characteristics, such as personalities, temperaments, and emotions, is essential for responsible AI development. However, current psychometric evaluations of LLMs, often derived from human psychological assessments, encounter significant limitations in terms of reliability and validity. Test results reveal that models frequently refuse to provide anthropomorphic responses and exhibit inconsistent scores across various scenarios. Moreover, human-derived theories may not accurately predict model behavior in practical real-world applications.\nTo address these limitations, we propose Core Sentiment Inventory (CSI), a novel evaluation instrument inspired by the Implicit Association Test (IAT). CSI is built from the ground up with a significantly broader range of stimuli words than traditional assessments. CSI covers both English and Chinese to implicitly evaluate models\u2019 sentiment tendencies, which allows for a much more comprehensive assessment.\nThrough extensive experiments, we demonstrate that CSI effectively quantifies models\u2019 sentiments, revealing nuanced emotional patterns that vary significantly across languages and contexts. CSI significantly improves reliability, yielding more consistent results and a reduced reluctance rate, and enhances predictive power by effectively capturing models\u2019 emotional tendencies. These findings validate CSI as a robust and insightful tool for evaluating the psychological traits of LLMs, offering a more reliable alternative to traditional methods.", "title_embedding_index": 73, "title_abs_embedding_index": 98}, {"title": "CASAK-V: Dynamic Sparse Attention and Adaptive KV-Cache Compression for Memory-Efficient Long-Context LLM Inference", "link_suffix": "/forum?id=n7RqgqbxP7", "link": "https://openreview.net/forum?id=n7RqgqbxP7", "pdf_link": "https://openreview.net/pdf?id=n7RqgqbxP7", "keywords": "Large Language Models, Sparse Attention, KV-cache Compression, Long-context Processing, Meta-learning, Adaptive Algorithms, Memory Efficiency, Inference Optimization, On-device Deployment, Context-aware Models, Dynamic Attention, Transformer Architectures, Efficient Natural Language Processing, Machine Learning Systems, Attention Mechanisms, Sparse Computation, Benchmarking, Model Compression, Resource-constrained Computing, Edge AI, Computational Complexity, Information Retrieval, Self-attention, Transfer Learning, Deep Learning, Artificial Intelligence, Chunk-wise Compression, Pattern Recognition", "abstract": "The emergence of long-context Large Language Models (LLMs) has triggered a rapid expansion of applications across various domains. However, these models remain inaccessible for on-device or on-premises deployments due to significant computational and memory challenges. The quadratic complexity of attention mechanisms and the substantial memory requirements of KV-caches, hinder adoption in resource-constrained environments. Current solutions, such as sparse attention mechanisms and KV-cache compression techniques, often rely on pre-observed patterns or context-independent, head-specific profiling strategies, which can compromise model accuracy, especially in long-context processing. This paper introduces Context-Aware adaptive Sparse Attention with Key-Value cache compression (CASAK-V), an inference-time approach that dynamically generates and applies head-specific sparse attention patterns. CASAK-V leverages a meta-learning framework to fine-tune a compact pre-trained vision-language encoder-decoder transformer for sparse pattern identification from per-layer attention scores. These patterns include fixed local windows, dynamic column stripes, block-sparse, and various other learned hybrid configurations. The technique additionally implements adaptive chunk-wise KV-cache compression using policies adapted from these layer-wise sparse configurations. To retain context-awareness, these configuration are dynamically adjusted during token generation, based on an attention map reconstruction heuristic. Our evaluations show that CASAK-V achieves minimal performance degradation on long-context benchmarks (LongBench), while reducing memory usage by 40% and delivering near-linear runtime complexity compared to full attention and caching. In summary, CASAK-V enables efficient long-context processing in memory-limited environments, extending the applicability of LLMs and facilitating their deployment in on-premises and on-device scenarios.", "title_embedding_index": 74, "title_abs_embedding_index": 99}]