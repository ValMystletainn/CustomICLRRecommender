[{"title": "MV-Adapter: Multi-view Consistent Image Generation Made Easy", "link_suffix": "/forum?id=kcmK2utDhu", "link": "https://openreview.net/forum?id=kcmK2utDhu", "pdf_link": "https://openreview.net/pdf?id=kcmK2utDhu", "keywords": "Multi-view Image Generation, 3D Generation, Diffusion Models", "abstract": "Generating multi-view images of an object has important applications in content creation and perception. Existing methods achieved this by making invasive changes to pre-trained text-to-image (T2I) models and performing full-parameter training, leading to three main limitations: (1) High computational costs, especially for high-resolution outputs; (2) Incompatibility with derivatives and extensions of the base model, such as personalized models, distilled few-step models, and plugins like ControlNets; (3) Limited versatility, as they primarily serve a single purpose and cannot handle diverse conditioning signals such as text, images, and geometry. In this paper, we present MV-Adapter to address all the above limitations. MV-Adapter is designed to be a plug-and-play module working on top of pre-trained T2I models. This enables efficient training for high-resolution synthesis while maintaining full compatibility with all kinds of derivatives of the base T2I model. MV-Adapter provides a unified implementation for generating multi-view images from various conditions, facilitating applications such as text- and image-based 3D generation and texturing. We demonstrate that MV-Adapter sets a new quality standard for multi-view image generation, and opens up new possibilities due to its adaptability and versatility.", "title_embedding_index": 22150, "title_abs_embedding_index": 22175}, {"title": "Spiking GS: Towards High-Accuracy and Low-Cost Surface Reconstruction via Spiking Neuron-based Gaussian Splatting", "link_suffix": "/forum?id=bOhr1iVee5", "link": "https://openreview.net/forum?id=bOhr1iVee5", "pdf_link": "https://openreview.net/pdf?id=bOhr1iVee5", "keywords": "3D Surface Reconstruction, Gaussian Splatting, Spiking Neuron", "abstract": "3D Gaussian Splatting is capable of reconstructing 3D scenes in minutes. Despite recent advances in improving surface reconstruction accuracy, the reconstructed results still exhibit bias and suffer from inefficiency in storage and training. This paper provides a different observation on the cause of the inefficiency and the reconstruction bias, which is attributed to the integration of the low-opacity parts (LOPs) of the generated Gaussians. We show that LOPs consist of Gaussians with overall low-opacity (LOGs) and the low-opacity tails (LOTs) of Gaussians. We propose Spiking GS to reduce such two types of LOPs by integrating spiking neurons into the Gaussian Splatting pipeline. Specifically, we introduce global and local full-precision integrate-and-fire spiking neurons to the opacity and representation function of flattened 3D Gaussians, respectively. Furthermore, we enhance the density control strategy with spiking neurons' thresholds and an new criterion on the scale of Gaussians. Our method can represent more accurate reconstructed surfaces at a lower cost. The code is available athttps://anonymous.4open.science/r/SpikingGS-D721.", "title_embedding_index": 22151, "title_abs_embedding_index": 22176}, {"title": "Shaping a Stabilized Video by Mitigating Unintended Changes for Concept-Augmented Video Editing", "link_suffix": "/forum?id=CxS8mlkOH7", "link": "https://openreview.net/forum?id=CxS8mlkOH7", "pdf_link": "https://openreview.net/pdf?id=CxS8mlkOH7", "keywords": "Text-Guided Video Editing", "abstract": "Text-driven video editing utilizing generative diffusion models has garnered significant attention due to their potential applications. However, existing approaches are constrained by the limited word embeddings provided in pre-training, which hinders nuanced editing targeting open concepts with specific attributes. Directly altering the keywords in target prompts often results in unintended disruptions to the attention mechanisms. To achieve more flexible editing easily, this work proposes an improved concept-augmented video editing approach that generates diverse and stable target videos flexibly by devising abstract conceptual pairs. Specifically, the framework involves concept-augmented textual inversion and a dual prior supervision mechanism. The former enables plug-and-play guidance of stable diffusion for video editing, effectively capturing target attributes for more stylized results. The dual prior supervision mechanism significantly enhances video stability and fidelity. Comprehensive evaluations demonstrate that our approach generates more stable and lifelike videos, outperforming state-of-the-art methods. The anonymous code is available at \\href{https://anonymous.4open.science/w/STIVE-PAGE-B4D4/}{https://anonymous.4open.science/w/STIVE-PAGE-B4D4/}.", "title_embedding_index": 22152, "title_abs_embedding_index": 22177}, {"title": "Qihoo-T2X: An Efficient Proxy-Tokenized Diffusion Transformer for Text-to-Any-Task", "link_suffix": "/forum?id=lTrrnNdkOX", "link": "https://openreview.net/forum?id=lTrrnNdkOX", "pdf_link": "https://openreview.net/pdf?id=lTrrnNdkOX", "keywords": "Diffusion Transformer, Image Generation, Video Generation", "abstract": "The global self-attention mechanism in diffusion transformers involves redundant computation due to the sparse and redundant nature of visual information, and the attention map of tokens within a spatial window shows significant similarity. To address this redundancy, we propose the Proxy-Tokenized Diffusion Transformer (PT-DiT), which employs sparse representative token attention (where the number of representative tokens is much smaller than the total number of tokens) to model global visual information efficiently. Specifically, within each transformer block, we compute an averaging token from each spatial-temporal window to serve as a proxy token for that region. The global semantics are captured through the self-attention of these proxy tokens and then injected into all latent tokens via cross-attention. Simultaneously, we introduce window and shift window attention to address the limitations in detail modeling caused by the sparse attention mechanism. Building on the well-designed PT-DiT, we further develop the Qihoo-T2X family, which includes a variety of models for T2I, T2V, and T2MV tasks. Experimental results show that PT-DiT achieves competitive performance while reducing the computational complexity in both image and video generation tasks (e.g., a 49% reduction compared to DiT and a 34% reduction compared to PixArt-$\\alpha$). The visual exhibition of Qihoo-T2X is available athttps://qihoo-t2x.github.io/.", "title_embedding_index": 22153, "title_abs_embedding_index": 22178}, {"title": "BenchMol: A Multi-Modality Benchmarking Platform for Molecular Representation Learning", "link_suffix": "/forum?id=1JgWwOW3EN", "link": "https://openreview.net/forum?id=1JgWwOW3EN", "pdf_link": "https://openreview.net/pdf?id=1JgWwOW3EN", "keywords": "Multi-Modality Learning, Benchmarks and Datasets, Drug Discovery, Molecular Representation Learning", "abstract": "Molecular representation learning (MRL) plays a vital role in high-precision drug discovery. Currently, people represent molecules in multiple modalities (such as sequences, graphs, and images), and have developed many MRL methods. However, three key challenges hinder further progress in the field of multi-modal MRL: (i) Lack of systematic and unified evaluation on models of different modalities, resulting in unfair comparisons or being affected by randomness; (ii) The specific advantages between different molecular modalities are unclear; (iii) Lacking a unified multi-modal platform to integrate these multi-modal data and a large number of MRL methods. Therefore, we propose the first multi-modality MRL platform, called BenchMol, to integrate a large number of multi-modal MRL methods and evaluate them systematically and fairly. BenchMol has four attractive features: (i) Rich modalities: BenchMol supports 7 major modalities of molecules, such as fingerprint, sequence, graph, geometry, image, geometry image, and video; (ii) Comprehensive methods: BenchMol integrates 23 mainstream MRL methods to process these modalities; (iii) New benchmarks: BenchMol constructs two new benchmarks based on PCQM4Mv2 and ChEMBL 34, called MBANet and StructNet, for a more systematic evaluation. (iv) Comprehensive evaluation: evaluation covers different aspects of molecules, such as basic attributes and molecular types. Through BenchMol, we conduct large-scale research on methods of different modalities and report many insightful findings. We hope that BenchMol can help researchers quickly use multi-modal MRL methods on the one hand; and on the other hand, provide meaningful insights into multi-modal MRL and help researchers choose appropriate representations in downstream tasks. We open-sourced BenchMol in \\href{https://anonymous.4open.science/r/BenchMol}{Github}.", "title_embedding_index": 22154, "title_abs_embedding_index": 22179}, {"title": "Ger: Generation, Evaluation and Reflection Enhanced LLM for Knowledge Graph Question Answering", "link_suffix": "/forum?id=OHZO0Hdfo0", "link": "https://openreview.net/forum?id=OHZO0Hdfo0", "pdf_link": "https://openreview.net/pdf?id=OHZO0Hdfo0", "keywords": "KGQA, Reasoning in Large Language Models, Reflection", "abstract": "Knowledge Graph Question Answering (KGQA) involves answering natural language questions based on information provided by knowledge graphs. Large language models (LLMs), utilizing their exceptional natural language understanding capabilities and factual knowledge from knowledge graphs, have made some progress in KGQA reasoning. However, existing methods overlook the amplification of hallucinations in large language models caused by irrelevant information within vast knowledge graphs. This oversight leads to answers containing seemingly correct but unrelated responses, decreasing reliability. In this paper, we propose $\\textbf{\\textit{Generation-Evaluation-Reflection}} $ (Ger), an LLM-enhanced reflective reasoning framework for KGQA. The Ger mechanism introduces evaluation and reflection steps during the reasoning process, allowing LLMs to leverage the factual information in KGs better and utilize their logical reasoning strengths. This process reduces errors and hallucinations while improving reasoning accuracy. Extensive experiments on multiple KGQA benchmark datasets demonstrate that Ger enhances reasoning performance, providing more reliable and interpretable results, and achieves new state-of-the-art.", "title_embedding_index": 22155, "title_abs_embedding_index": 22180}, {"title": "FusionSAM: Visual Multimodal Learning with Segment Anything Model", "link_suffix": "/forum?id=ATgLNAuync", "link": "https://openreview.net/forum?id=ATgLNAuync", "pdf_link": "https://openreview.net/pdf?id=ATgLNAuync", "keywords": "Multimodal Fusion; Segmentation; Latent Space", "abstract": "Multimodal image fusion and semantic segmentation are critical for autonomous driving.   Despite advancements, current models often struggle with segmenting densely packed elements due to a lack of comprehensive fusion features for guidance during training.   While the Segment Anything Model (SAM) allows precise control during fine-tuning through its flexible prompting encoder, its potential remains largely unexplored in the context of multimodal segmentation for natural images. In this paper, we introduce SAM into multimodal image segmentation for the first time, proposing a novel framework that combines Latent Space Token Generation (LSTG) and Fusion Mask Prompting (FMP) modules. This approach transforms the training methodology for multimodal segmentation from a traditional black-box approach to a controllable, prompt-based mechanism. Specifically, we obtain latent space features for both modalities through vector quantization and embed them into a cross-attention-based inter-domain fusion module to establish long-range dependencies between modalities. We then use these comprehensive fusion features as prompts to guide precise pixel-level segmentation. Extensive experiments on multiple public datasets demonstrate that our method significantly outperforms SAM and SAM2 in multimodal autonomous driving scenarios, achieving at least a 3.9$%$ improvement in segmentation mIoU over state-of-the-art methods.", "title_embedding_index": 22156, "title_abs_embedding_index": 22181}, {"title": "Tool Calling: Enhancing Medication Consultation via Retrieval-Augmented Large Language Models", "link_suffix": "/forum?id=jtY2OHKj4a", "link": "https://openreview.net/forum?id=jtY2OHKj4a", "pdf_link": "https://openreview.net/pdf?id=jtY2OHKj4a", "keywords": "Large Language Models, Medication Consultation, Retrieval-Augmented Generation, Tool Call", "abstract": "Large-scale language models (LLMs) have achieved remarkable success across various language tasks but suffer from hallucinations and temporal misalignment. To mitigate these shortcomings, Retrieval-augmented generation (RAG) has been utilized to provide external knowledge to facilitate the answer generation. However, applying such models to the medical domain faces several challenges due to the lack of domain-specific knowledge and the intricacy of real-world scenarios.\nIn this study, we explore LLMs with RAG framework for knowledge-intensive tasks in the medical field. To evaluate the capabilities of LLMs, we introduce MedicineQA, a multi-round dialogue benchmark that simulates the real-world medication consultation scenario and requires LLMs to answer with retrieved evidence from the medicine database. MedicineQA contains 300 multi-round question-answering pairs, each embedded within a detailed dialogue history, highlighting the challenge posed by this knowledge-intensive task to current LLMs. We further propose a new \\textit{Distill-Retrieve-Read} framework instead of the previous \\textit{Retrieve-then-Read}. Specifically, the distillation and retrieval process utilizes a tool calling mechanism to formulate search queries that emulate the keyword-based inquiries used by search engines. With experimental results, we show that our framework brings notable performance improvements and surpasses the previous counterparts in the evidence retrieval process in terms of evidence retrieval accuracy. This advancement underscores the framework's potential to effectively address the inherent challenges of applying RAG models to the medical domain.", "title_embedding_index": 22157, "title_abs_embedding_index": 22182}, {"title": "MeshGen: Generating PBR Textured Mesh with Render-Enhanced Auto-Encoder and Generative Data Augmentation", "link_suffix": "/forum?id=0zGvf2yRMQ", "link": "https://openreview.net/forum?id=0zGvf2yRMQ", "pdf_link": "https://openreview.net/pdf?id=0zGvf2yRMQ", "keywords": "3D Generation, Texture Generation", "abstract": "In this paper, we present MeshGen, an advanced image-to-3D pipeline designed to generate high-quality 3D objects with physically based rendering (PBR) textures. Existing methods struggle with issues such as poor auto-encoder performance, limited training datasets, misalignment between input images and 3D shapes, and inconsistent image-based PBR texturing. MeshGen addresses these limitations through several key innovations. First, we introduce a render-enhanced point-to-shape auto-encoder that compresses 3D shapes into a compact latent space, guided by perceptual loss. A 3D-native diffusion model is then established to directly learn the distribution of 3D shapes within this latent space. To mitigate data scarcity and image-shape misalignment, we propose geometric alignment augmentation and generative rendering augmentation, enhancing the diffusion model's controllability and generalization ability. Following shape generation, MeshGen applies a reference attention-based multi-view ControlNet for image-consistent appearance synthesis, complemented by a PBR decomposer to separate PBR channels. Extensive experiments demonstrate that MeshGen significantly enhances both shape and texture generation compared to previous methods.", "title_embedding_index": 22158, "title_abs_embedding_index": 22183}, {"title": "ZeroTS: Zero-shot time series prediction via multi-party data-model interaction", "link_suffix": "/forum?id=Lz221VLWrO", "link": "https://openreview.net/forum?id=Lz221VLWrO", "pdf_link": "https://openreview.net/pdf?id=Lz221VLWrO", "keywords": "Time-series forecasting; Retrieval Augmented Generation; Large Language Model; Zero-shot prediction", "abstract": "Time series forecasting (TS) is a fundamental task in artificial intelligence, with applications ranging from weather prediction, stock market analysis to electricity demand forecasting. While existing models, particularly large language models (LLMs) tailored for TS, primarily focus on improving accuracy and generalization through pre-training and fine-tuning, zero-shot prediction without task-specific fine-tuning, still remains underexplored. This limitation arises from the restricted scalability and flexibility of current LLMs for TS, which struggle to fully capture the interactions between  data and model. In this work, we introduce ZeroTS, a novel approach that bridges open-world knowledge with inherent data regularities by constructing multi-party interactions between data and models. On the data side, we propose a TS-RAG (Retrieval-Augmented Generation for Time Series), which efficiently retrieves both meta and series information, enabling diverse domain-specific time series to be used as prompts. On the model side, we develop a reinforcement learning framework that treats ground-truth as environments, providing error feedback to optimize a smaller model and harnessing the capabilities of LLMs. This allows ZeroTS to incrementally approach inherent data regularities while iteratively refining its outputs. We validate ZeroTS via extensive experiments on zero-shot and long-horizon forecasting. ZeroTS achieves best or second best results with comparative parameters, \n1/4 memory and 1/7 inference speed, demonstrating its efficiency and effectiveness. Our results highlight the potential of Data-LLM interactions for zero-shot learning with acceptable parameters, opening new avenues on research of this underexplored area.", "title_embedding_index": 22159, "title_abs_embedding_index": 22184}, {"title": "Fast Feedforward 3D Gaussian Splatting Compression", "link_suffix": "/forum?id=DCandSZ2F1", "link": "https://openreview.net/forum?id=DCandSZ2F1", "pdf_link": "https://openreview.net/pdf?id=DCandSZ2F1", "keywords": "3DGS, compression, optimization-free", "abstract": "With 3D Gaussian Splatting (3DGS) advancing real-time and high-fidelity rendering for novel view synthesis, storage requirements pose challenges for their widespread adoption. Although various compression techniques have been proposed, previous art suffers from a common limitation: for any existing 3DGS, per-scene optimization is needed to achieve compression, making the compression sluggish and slow. To address this issue, we introduce Fast Compression of 3D Gaussian Splatting (FCGS), an optimization-free model that can compress 3DGS representations rapidly in a single feed-forward pass, which significantly reduces compression time from minutes to seconds. To enhance compression efficiency, we propose a multi-path entropy module that assigns Gaussian attributes to different entropy constraint paths for balance between size and fidelity. We also carefully design both inter- and intra-Gaussian context models to remove redundancies among the unstructured Gaussian blobs. Overall, FCGS achieves a compression ratio of over 20X while maintaining fidelity, surpassing most per-scene SOTA optimization-based methods. Our code will be made publicly available.", "title_embedding_index": 22160, "title_abs_embedding_index": 22185}, {"title": "On the Out-of-Distribution Generalization of Self-Supervised Learning", "link_suffix": "/forum?id=22ywev7zMt", "link": "https://openreview.net/forum?id=22ywev7zMt", "pdf_link": "https://openreview.net/pdf?id=22ywev7zMt", "keywords": "Self-Supervised Learning, Representation Learning, Out-of-Distribution", "abstract": "In this paper, we focus on the out-of-distribution (OOD) generalization of self-supervised learning (SSL). By analyzing the mini-batch construction during SSL training phase, we first give one plausible explanation for SSL having OOD generalization. Then, from the perspective of data generation and causal inference, we analyze and conclude that SSL learns spurious correlations during the training process, which leads to a reduction in OOD generalization. To address this issue, we propose a post-intervention distribution (PID) grounded in the Structural Causal Model. PID offers a scenario where the relationships between variables are free from the influence of spurious correlations. Besides, we demonstrate that if each mini-batch during SSL training satisfies PID, the resulting SSL model can achieve optimal worst-case OOD performance. This motivates us to develop a batch sampling strategy that enforces PID constraints through the learning of a latent variable model. Through theoretical analysis, we demonstrate the identifiability of the latent variable model and validate the effectiveness of the proposed sampling strategy. Experiments conducted on various downstream OOD tasks demonstrate the effectiveness of the proposed sampling strategy.", "title_embedding_index": 22161, "title_abs_embedding_index": 22186}, {"title": "FedBiP: Heterogeneous One-Shot Federated Learning with Personalized Latent Diffusion Models", "link_suffix": "/forum?id=30saKMFyHt", "link": "https://openreview.net/forum?id=30saKMFyHt", "pdf_link": "https://openreview.net/pdf?id=30saKMFyHt", "keywords": "One-Shot Federated Learning, Latent Diffusion Models, Data Heterogeneity", "abstract": "One-Shot Federated Learning (OSFL), a special decentralized machine learning paradigm, has recently gained significant attention. OSFL requires only a single round of client data or model upload, which reduces communication costs and mitigates privacy threats compared to traditional FL. Despite these promising prospects, existing methods face challenges due to client data heterogeneity and limited data quantity when applied to real-world OSFL systems. Recently, Latent Diffusion Models (LDM) have shown remarkable advancements in synthesizing high-quality images through pretraining on large-scale datasets, thereby presenting a potential solution to overcome these issues. However, directly applying pretrained LDM to heterogeneous OSFL results in significant distribution shifts in synthetic data, leading to performance degradation in classification models trained on such data. This issue is particularly pronounced in rare domains, such as medical imaging, which are underrepresented in LDM's pretraining data. To address this challenge, we propose Federated Bi-Level Personalization (FedBiP), which personalizes the pretrained LDM at both instance-level and concept-level. Hereby, FedBiP synthesizes images following the client's local data distribution without compromising the privacy regulations. FedBiP is also the first approach to simultaneously address feature space heterogeneity and client data scarcity in OSFL. Our method is validated through extensive experiments on three OSFL benchmarks with feature space heterogeneity, as well as on challenging medical and satellite image datasets with label heterogeneity. The results demonstrate the effectiveness of FedBiP, which substantially outperforms other OSFL methods.", "title_embedding_index": 22162, "title_abs_embedding_index": 22187}, {"title": "Bi-Share LoRA: Enhancing the Parameter Efficiency of LoRA with Intra-Layer and Inter-Layer Sharing", "link_suffix": "/forum?id=Thv66GmqZS", "link": "https://openreview.net/forum?id=Thv66GmqZS", "pdf_link": "https://openreview.net/pdf?id=Thv66GmqZS", "keywords": "Parameter-efficient fine-tuning, parameter-sharing", "abstract": "Low-Rank Adaptation (LoRA) is a widely adopted parameter-efficient fine-tuning method for large language models (LLMs) to adapt to downstream tasks. However, in scenarios where multiple LoRA models are deployed simultaneously, standard LoRA introduces substantial trainable parameters, resulting in significant memory overhead and inference latency, particularly when supporting thousands of downstream tasks on a single server. While existing methods reduce stored parameters via parameter sharing, they fail to capture both local and global information simultaneously. To address this issue, we propose Bi-Share LoRA, which integrates local parameters with intra-layer and inter-layer shared parameters to more effectively capture information at both local and global levels. By sharing parameters both within and across layers, our method significantly reduces the number of trainable parameters while preserving or improving model performance. Additionally, we set a local LoRA to capture local parameters, enabling more precise and fine-grained information extraction at the local level. The final implementation introduces three parallel sub-LoRAs and designs transformation techniques to adapt shared parameters of varying shapes, ensuring compatibility and efficient sharing. Experiments on the 7B, 8B, and 13B versions of Llama show\nthat Bi-Share LoRA, with only 44.59% of the parameters of standard LoRA, outperforms LoRA by approximately 0.33% on commonsense reasoning and 2.08% on MMLU benchmarks.", "title_embedding_index": 22163, "title_abs_embedding_index": 22188}, {"title": "Scalable Benchmarking and Robust Learning for Noise-Free Ego-Motion and 3D Reconstruction from Noisy Video", "link_suffix": "/forum?id=Pz9zFea4MQ", "link": "https://openreview.net/forum?id=Pz9zFea4MQ", "pdf_link": "https://openreview.net/pdf?id=Pz9zFea4MQ", "keywords": "Benchmarking, Robustness, Neural 3D Reconstruction, Ego-Motion Estimation, Noisy Sensing", "abstract": "We aim to advance the reconstruction of noise-free ego-motion and photorealistic 3D scenes from noise-free to noisy video inputs. To achieve this, we address three core challenges: scalable data generation, comprehensive  benchmarking, and  model robustness enhancement.\nFirst, we introduce a scalable noisy data synthesis pipeline capable of generating large-scale, customizable datasets that simulate complex motion, RGB-D sensor corruptions, and synchronization perturbations. Second, we use the  synthesis pipeline to fuel Robust-Ego3D, a large-scale benchmark designed to rigorously evaluate model robustness under a wide range of perturbations, providing a challenging testbed for advancing model performance.\nOur extensive experimental evaluations and theoretical analyses reveal a critical limitation of learning-based models: while they perform remarkably well on clean, noise-free benchmarks, they exhibit significant degradation in ego-motion accuracy and 3D reconstruction quality under dynamic motion and imaging noise. Third, to address these challenges, we propose Correspondence-guided Gaussian Splatting (CorrGS). CorrGS employs a progressively updated, clean internal 3D Gaussian Splatting representation, which is rendered into RGB-D frames to establish correspondences with external noisy observations. These correspondences enhance geometric alignment and facilitates appearance restoration learning, enabling robust pose learning and noise-free, photorealistic 3D reconstruction.\nBoth synthetic and real-world experiments demonstrate that CorrGS consistently outperforms prior state-of-the-art methods, particularly in scenarios involving rapid motion. This work establishes a new standard  for stable ego-motion estimation and high-fidelity 3D reconstruction from sparse-view, noisy videos under dynamic illumination. We will release our code and benchmark to foster future advancements.", "title_embedding_index": 22164, "title_abs_embedding_index": 22189}, {"title": "On Evaluation of Generative Robotic Simulations", "link_suffix": "/forum?id=s3sJenvY5H", "link": "https://openreview.net/forum?id=s3sJenvY5H", "pdf_link": "https://openreview.net/pdf?id=s3sJenvY5H", "keywords": "Robotics; Embodied AI; Foundation Models; Generative Simulations;", "abstract": "Due to the difficulty of acquiring extensive real-world data, robot simulation has become crucial for parallel training and sim-to-real transfer, highlighting the importance of scalable simulated robotic tasks. \nFoundation models have demonstrated impressive capacities in autonomously generating feasible robotic tasks. However, this new paradigm underscores the challenge of adequately evaluating these autonomously generated tasks. \nTo address this, we propose a comprehensive evaluation framework tailored to generative simulations. \nOur framework segments evaluation into three core aspects:quality,diversity, andgeneralization.\nFor single-task quality, we evaluate the realism of the generated task and the completeness of the generated trajectories using large language models and vision-language models. In terms of diversity, we measure both task and data diversity through text similarity of task descriptions and world model loss trained on collected task trajectories. For task-level generalization, we assess the zero-shot generalization ability on unseen tasks of a policy trained with multiple generated tasks.\nExperiments conducted on three representative task generation pipelines demonstrate that the results from our framework are highly consistent with human evaluations, confirming the feasibility and validity of our approach. \nThe findings reveal that while metrics of quality and diversity can be achieved through certain methods, no single approach excels across all metrics, suggesting a need for greater focus on balancing these different metrics. Additionally, our analysis further highlights the common challenge of low generalization capability faced by current works.\nOur anonymous website:https://sites.google.com/view/evaltasks.", "title_embedding_index": 22165, "title_abs_embedding_index": 22190}, {"title": "LaMP: Language-Motion Pretraining for Motion Generation, Retrieval, and Captioning", "link_suffix": "/forum?id=LYawG8YkPa", "link": "https://openreview.net/forum?id=LYawG8YkPa", "pdf_link": "https://openreview.net/pdf?id=LYawG8YkPa", "keywords": "motion generation; motion-language aliment; multimodal", "abstract": "Language plays a vital role in the realm of human motion. Existing methods have largely depended on CLIP text embeddings for motion generation, yet they fall short in effectively aligning language and motion due to CLIP\u2019s pretraining on static image-text pairs. This work introduces LaMP, a novel Language-Motion Pretraining model, which transitions from a language-vision to a more suitable language-motion latent space. It addresses key limitations by generating motion-informative text embeddings, significantly enhancing the relevance and semantics of generated motion sequences. With LaMP, we advance three key tasks: text-to-motion generation, motion-text retrieval, and motion captioning through aligned language-motion representation learning. For generation, LaMP instead of CLIP provides the text condition, and an autoregressive masked prediction is designed to achieve mask modeling without rank collapse in transformers. For retrieval, motion features from LaMP\u2019s motion transformer interact with query tokens to retrieve text features from the text transformer, and vice versa. For captioning, we finetune a large language model with the language-informative motion features to develop a strong motion captioning model. In addition, we introduce the LaMP-BertScore metric to assess the alignment of generated motions with textual descriptions. Extensive experimental results on multiple datasets demonstrate substantial improvements over previous methods across all three tasks. The code of our method will be made public.", "title_embedding_index": 22166, "title_abs_embedding_index": 22191}, {"title": "FLEXOUNDIT: VARIABLE-LENGTH DIFFUSION TRANSFORMER FOR TEXT-TO-AUDIO GENERATION", "link_suffix": "/forum?id=6Tyo0yCCez", "link": "https://openreview.net/forum?id=6Tyo0yCCez", "pdf_link": "https://openreview.net/pdf?id=6Tyo0yCCez", "keywords": "text-to-audio, diffusion, generalization", "abstract": "In the real world, sounds inherently vary in length, spanning a broad spectrum\nof durations. We particularly aim to address the challenge of generating variable-length audio in text-to-audio (TTA) diffusion models. Extending audio length\nbeyond what was covered during training, also known as extrapolation, is specifically challenging for audio generative models. The existing TTA diffusion model\ndesign notoriously suffers from the problem of generation with such flexibility.\nTherein, the design of prior TTA diffusion models do not accommodate to the\nchange of positional information. In this work, we introduce a novel framework\nbased on relative position embeddings, which is specifically designed to support\nthe flexibility without fundamental changes to the current diffusion pipeline. Our\nproposed method allows tuning-free audio length extrapolation, thereby enhancing\nefficiency for generating audio with unseen lengths. Moreover, our approach enables a training strategy with shorter audio durations, enjoying reduced training\ncosts while maintaining performance levels comparable to those achieved with\nlonger durations. Empirically, we demonstrate exceptional performance against\nthe existing state-of-the-arts on audio generation benchmarks with a significantly\nlower model size compared to the counterparts. In variable audio length generation,\nour approach consistently outperforms existing methods by a large margin. Our\ndemonstration page is available athttps://flexoundit.github.io/.", "title_embedding_index": 22167, "title_abs_embedding_index": 22192}, {"title": "Bootstrap3D: Improving Multi-view Diffusion Model with Synthetic Data", "link_suffix": "/forum?id=U862lgKUgj", "link": "https://openreview.net/forum?id=U862lgKUgj", "pdf_link": "https://openreview.net/pdf?id=U862lgKUgj", "keywords": "3D content creation; Multi-view Diffusion; MLLM", "abstract": "Recent years have witnessed remarkable progress in multi-view diffusion models for 3D content creation. However, there remains a significant gap in image quality and prompt-following ability compared to 2D diffusion models. A critical bottleneck is the scarcity of high-quality 3D data with detailed captions. To address this challenge, we propose Bootstrap3D, a novel framework that automatically generates an arbitrary quantity of multi-view images to assist in training multi-view diffusion models. Specifically, we introduce a data generation pipeline that employs (1) 2D and video diffusion models to generate multi-view images based on constructed text prompts, and (2) our fine-tuned 3D-aware MV-LLaVA for filtering high-quality data and rewriting inaccurate captions. Leveraging this pipeline, we have generated 1 million high-quality synthetic multi-view images with dense descriptive captions to address the shortage of high-quality 3D data. Furthermore, we present a Training Timestep Reschedule (TTR) strategy that leverages the denoising process to learn multi-view consistency while maintaining the original 2D diffusion prior. Extensive experiments demonstrate that Bootstrap3D can generate high-quality multi-view images with superior aesthetic quality, image-text alignment, and maintained view consistency.", "title_embedding_index": 22168, "title_abs_embedding_index": 22193}, {"title": "LEASE: Offline Preference-based Reinforcement Learning with High Sample Efficiency", "link_suffix": "/forum?id=38kLrJNwaM", "link": "https://openreview.net/forum?id=38kLrJNwaM", "pdf_link": "https://openreview.net/pdf?id=38kLrJNwaM", "keywords": "preference-based reinforcement learning, sample efficiency", "abstract": "Offline preference-based reinforcement learning (PbRL) provides an effective way to overcome the challenges of designing reward and the high costs of online interaction. However, since labeling preference needs real-time human feedback, acquiring sufficient preference labels is challenging. To solve this, this paper proposes a offLine prEference-bAsed RL with high Sample Efficient (LEASE) algorithm, where a learned transition model is leveraged to generate unlabeled preference data. Considering the pretrained reward model may generate incorrect labels for unlabeled data, we design an uncertainty-aware mechanism to ensure the performance of reward model, where only high confidence and low variance data are selected. Moreover, we provide the generalization bound of reward model to analyze the factors influencing reward accuracy, and demonstrate that the policy learned by LEASE has theoretical improvement guarantee. The developed theory is based on state-action pair, which can be easily combined with other offline algorithms. The experimental results show that LEASE can achieve comparable performance to baseline under fewer preference data without online interaction.", "title_embedding_index": 22169, "title_abs_embedding_index": 22194}, {"title": "RuleRAG: Rule-Guided Retrieval-Augmented Generation with Language Models for Question Answering", "link_suffix": "/forum?id=zl3nFqY8l1", "link": "https://openreview.net/forum?id=zl3nFqY8l1", "pdf_link": "https://openreview.net/pdf?id=zl3nFqY8l1", "keywords": "Rule-Guided Retrieval, Rule-Guided Generation, RAG, Question Answering", "abstract": "Retrieval-augmented generation (RAG) framework has shown promising potential in knowledge-intensive question answering (QA) by retrieving external corpus and generating based on augmented context. However, existing approaches only consider the query itself, neither specifying the retrieval preferences for the retrievers nor informing the generators of how to refer to the retrieved documents for the answers, which poses a significant challenge to the QA performance. To address these issues, we propose Rule-Guided Retrieval-Augmented Generation with LMs, which explicitly introduces symbolic rules as demonstrations for in-context learning (RuleRAG-ICL) to guide retrievers to retrieve logically related documents in the directions of rules and uniformly guide generators to generate answers attributed by the guidance of the same set of rules. Moreover, the combination of queries and rules can be further used as supervised fine-tuning data to update retrievers and generators (RuleRAG-FT) to achieve better rule-based instruction following capability, leading to retrieve more supportive results and generate more acceptable answers. To emphasize the attribution of rules, we construct five rule-aware QA benchmarks, including three temporal and two static scenarios, and equip RuleRAG with several kinds of retrievers and generators. Experiments demonstrate that training-free RuleRAG-ICL effectively improves the retrieval quality of +89.2% in Recall@10 scores and generation accuracy of +103.1% in exact match scores over standard RAG on average across the five benchmarks, and further fine-tuned RuleRAG-FT consistently yields more significant performance enhancement. Extensive analyses indicate that RuleRAG scales well with increasing numbers of retrieved documents and exhibits generalization ability for untrained rules. Our code and benchmarks are available athttps://anonymous.4open.science/r/ICLR2025_RuleRAG_ICL_FT.", "title_embedding_index": 22170, "title_abs_embedding_index": 22195}, {"title": "Generating Physical Dynamics under Priors", "link_suffix": "/forum?id=eNjXcP6C0H", "link": "https://openreview.net/forum?id=eNjXcP6C0H", "pdf_link": "https://openreview.net/pdf?id=eNjXcP6C0H", "keywords": "diffusion models, generative models, physical dynamics, priors", "abstract": "Generating physically feasible dynamics in a data-driven context is challenging, especially when adhering to physical priors expressed in specific equations or formulas. Existing methodologies often overlook the integration of ''physical priors'', resulting in violation of basic physical laws and suboptimal performance. In this paper, we introduce a novel framework that seamlessly incorporates physical priors into diffusion-based generative models to address this limitation. Our approach leverages two categories of priors: 1) distributional priors, such as roto-translational invariance, and 2) physical feasibility priors, including energy and momentum conservation laws and PDE constraints. By embedding these priors into the generative process, our method can efficiently generate physically realistic dynamics, encompassing trajectories and flows. Empirical evaluations demonstrate that our method produces high-quality dynamics across a diverse array of physical phenomena with remarkable robustness, underscoring its potential to advance data-driven studies in AI4Physics. Our contributions signify a substantial advancement in the field of generative modeling, offering a robust solution to generate accurate and physically consistent dynamics.", "title_embedding_index": 22171, "title_abs_embedding_index": 22196}, {"title": "Decoupled Data Augmentation for Improving Image Classification", "link_suffix": "/forum?id=w2qzdlvPMK", "link": "https://openreview.net/forum?id=w2qzdlvPMK", "pdf_link": "https://openreview.net/pdf?id=w2qzdlvPMK", "keywords": "Data augmentation, Diffusion, Image classification", "abstract": "Recent advancements in image mixing and generative data augmentation have shown promise in enhancing image classification. However, these techniques face the challenge of balancing semantic fidelity with diversity. Specifically, image mixing involves interpolating two images to create a new one, but this pixel-level interpolation can compromise fidelity. Generative augmentation uses text-to-image generative models to synthesize or modify images, often limiting diversity to avoid generating out-of-distribution data that potentially affects accuracy. We propose that this fidelity-diversity dilemma partially stems from the whole-image paradigm of existing methods. Since an image comprises the class-dependent part (CDP) and the class-independent part (CIP), where each part has fundamentally different impacts on the image's fidelity, treating different parts uniformly can therefore be misleading. To address this fidelity-diversity dilemma, we introduce Decoupled Data Augmentation (De-DA), which resolves the dilemma by separating images into CDPs and CIPs and handling them adaptively. To maintain fidelity, we use generative models to modify real CDPs under controlled conditions, preserving semantic consistency. To enhance diversity, we replace the image's CIP with inter-class variants, creating diverse CDP-CIP combinations. Additionally, we implement an online randomized combination strategy during training to generate numerous distinct CDP-CIP combinations cost-effectively. Comprehensive empirical evaluations validate the effectiveness of our method.", "title_embedding_index": 22172, "title_abs_embedding_index": 22197}, {"title": "A Framework of Distilling Multimodal Large Language Models", "link_suffix": "/forum?id=MFySy0DWAH", "link": "https://openreview.net/forum?id=MFySy0DWAH", "pdf_link": "https://openreview.net/pdf?id=MFySy0DWAH", "keywords": "MLLMs, Knowledge distillation", "abstract": "The success of Large Language Models (LLM) has led researchers to explore Multimodal Large Language Models (MLLM) for unified visual and linguistic understanding. However, the increasing model size and computational complexity of MLLM limit their use in resource-constrained environments. Small-scale MLLM ($s$-MLLM) aims to retain the capabilities of the large-scale model ($l$-MLLM) while reducing computational demands, but resulting in a significant decline in performance. To address the aforementioned issues, we propose a novel LLaVA-KD framework to transfer knowledge from $l$-MLLM to $s$-MLLM. Specifically, we introduce Multimodal Distillation (MDist) to minimize the divergence between the visual-textual output distributions of $l$-MLLM and $s$-MLLM, and Relation Distillation (RDist) to transfer $l$-MLLM\u2019s ability to model correlations between visual features. Additionally, we propose a three-stage training scheme to fully exploit the potential of $s$-MLLM: 1) Distilled Pre-Training to align visual-textual representations, 2) Supervised Fine-Tuning to equip the model with multimodal understanding, and 3) Distilled Fine-Tuning to further transfer $l$-MLLM capabilities. Our approach significantly improves performance without altering the small model's architecture. Extensive experiments and ablation studies validate the effectiveness of each proposed component. Code will be available.", "title_embedding_index": 22173, "title_abs_embedding_index": 22198}, {"title": "FTP: A Fine-grained Token-wise Pruner for Large Language Models via Token Routing", "link_suffix": "/forum?id=gcEhF4nuYI", "link": "https://openreview.net/forum?id=gcEhF4nuYI", "pdf_link": "https://openreview.net/pdf?id=gcEhF4nuYI", "keywords": "LLMs; Pruning, Router", "abstract": "Recently, large language models (LLMs) have demonstrated superior performance across various tasks by adhering to scaling laws, which significantly increase model size. However, the huge computation overhead during inference hinders the deployment in industrial applications. Many works leverage traditional compression approaches to boost model inference, but these always introduce additional training costs to restore the performance and the pruning results typically show noticeable performance drops compared to the original model when aiming for a specific level of acceleration. To address these issues, we propose a fine-grained token-wise pruning approach for the LLMs, which presents a learnable router to adaptively identify the less important tokens and skip them across model blocks to reduce computational cost during inference. To construct the router efficiently, we present a search-based sparsity scheduler for pruning sparsity allocation, a trainable router combined with our proposed four low-dimensional factors as input and three proposed losses. We conduct extensive experiments across different benchmarks on different LLMs to demonstrate the superiority of our method. Our approach achieves state-of-the-art (SOTA) pruning results, surpassing other existing pruning methods. For instance, our method outperforms  BlockPruner and ShortGPT by approximately 10 points on both LLaMA2-7B and Qwen1.5-7B in accuracy retention at comparable token sparsity levels.", "title_embedding_index": 22174, "title_abs_embedding_index": 22199}]