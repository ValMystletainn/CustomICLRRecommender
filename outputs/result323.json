[
    {
        "title": "ScreenWriter: Automatic Screenplay Generation and Movie Summarisation",
        "link_suffix": "/forum?id=cgxqD6tr4E",
        "link": "https://openreview.net/forum?id=cgxqD6tr4E",
        "pdf_link": "https://openreview.net/pdf?id=cgxqD6tr4E",
        "keywords": "video understanding; summarisation; scene segmentation",
        "abstract": "The proliferation of creative video content has driven demand for textual descriptions or summaries that allow users to recall key plot points or get an overview without watching. The volume of movie content and speed of turnover motivates automatic summarisation, which is nevertheless challenging, requiring identifying character intentions and very long-range temporal dependencies. The few existing methods attempting this task rely heavily on textual screenplays as input, greatly limiting their applicability. In this work, we propose the task of automatic screenplay generation, and a method, ScreenWriter, that operates only on video input and produces output which includes dialogue, speaker names, scene breaks and visual descriptions. ScreenWriter introduces a novel algorithm to segment the video into scenes based on the sequence of visual vectors, and a novel method for the challenging problem of determining character names, based on a database of actors\u2019 faces. We further demonstrate how these automatic screenplays can be used to generate plot synopses with a hierarchical summarisation method based on scene breaks. We test the quality of the final summaries on the recent Moviesumm dataset, which we augment with videos, and show that they are superior to a num- ber of comparison models which assume access to goldstandard screenplays."
    },
    {
        "title": "A Quantum Circuit-Based Compression Perspective for Parameter-Efficient Learning",
        "link_suffix": "/forum?id=bB0OKNpznp",
        "link": "https://openreview.net/forum?id=bB0OKNpznp",
        "pdf_link": "https://openreview.net/pdf?id=bB0OKNpznp",
        "keywords": "quantum machine learning, quantum computing, parameter-efficient fine-tuning, large language model, variational quantum circuits",
        "abstract": "Quantum-centric supercomputing presents a compelling framework for large-scale hybrid quantum-classical tasks. Although quantum machine learning (QML) offers theoretical benefits in various applications, challenges such as large-size data encoding in the input stage and the reliance on quantum resources in the inference stage limit its practicality for tasks like fine-tuning large language models (LLMs). Quantum parameter generation, a novel approach of QML, addresses these limitations by using quantum neural networks (QNNs) to generate classical model weights (parameters) exclusively during training, thereby decoupling inference from quantum hardware. In this work, we introduce Quantum Parameter Adaptation (QPA) in the framework of quantum parameter generation, which integrates QNNs with a classical multi-layer perceptron mapping model to generate parameters for fine-tuning methods. Using Gemma-2 and GPT-2 as case studies, QPA demonstrates significant parameter reduction for parameter-efficient fine-tuning methods, such as Low-Rank Adaptation (LoRA), while maintaining comparable or improved performance in text generation tasks. Specifically, QPA reduces the number of  parameters to $52.06%$ of the original LoRA for GPT-2 with a slight performance gain of $0.75%$, and to $16.84%$ for Gemma-2, with a marginal performance improvement of $0.07%$. These results highlight QPA\u2019s ability to achieve efficient parameter reduction without sacrificing performance in the quantum parameter generation framework. This work showcases the potential of quantum-enhanced parameter reduction, offering a scalable quantum-classical solution for fine-tuning LLMs while preserving the feasibility of inference on classical hardware."
    },
    {
        "title": "Consistent Iterative Denoising for Robot Manipulation",
        "link_suffix": "/forum?id=07ZaA3MiL0",
        "link": "https://openreview.net/forum?id=07ZaA3MiL0",
        "pdf_link": "https://openreview.net/pdf?id=07ZaA3MiL0",
        "keywords": "robot manipulation, consistent iterative denoising, diffusion model, imitation learning",
        "abstract": "Robot manipulation in complex scenarios usually involves multiple successful actions, which requires generative models to estimate the distribution of various successful actions. \nIn recent years, the diffusion model has been widely studied in many robot manipulation tasks.\nHowever, the diffusion model experiences inconsistent noise supervision across various action labels and denoising timesteps, which compromises accurate action prediction.\nOn the one hand, CIDM designs new noise supervision to avoid interference between different successful actions, leading to consistent denoising directions.\nOn the other hand, CIDM unifies all denoising timesteps, avoiding inconsistent predictions of the diffusion model over different timesteps.\nMoreover, we also designed a novel radial loss to make the model focus on denoising results rather than iterative process routes.\nOur method achieves a new state-of-the-art performance on RLBench with the highest success rate of 82.3% on a multi-view setup and 83.9% on a single-view setup."
    },
    {
        "title": "Leave-One-Out Stable Conformal Prediction",
        "link_suffix": "/forum?id=Bt1vnCnAVS",
        "link": "https://openreview.net/forum?id=Bt1vnCnAVS",
        "pdf_link": "https://openreview.net/pdf?id=Bt1vnCnAVS",
        "keywords": "Conformal Prediction, Algorithmic Stability, Regularized Loss Minimization, Stochastic Gradient Descent",
        "abstract": "Conformal prediction (CP) is an important tool for distribution-free predictive uncertainty quantification.\nYet, a major challenge is to balance computational efficiency and prediction accuracy, particularly for multiple predictions.\nWe propose {\\bf L}eave-{\\bf O}ne-{\\bf O}ut {\\bf Stab}le {\\bf C}onformal {\\bf P}rediction (\\texttt{LOO-StabCP}), a novel method to speed up full conformal using algorithmic stability without sample splitting.\nBy leveraging \\emph{leave-one-out} stability, our method is much faster in handling a large number of prediction requests compared to existing method {\\tt RO-StabCP} based on \\emph{replace-one} stability.\nWe derived stability bounds for two popular machine learning tools: regularized risk minimization (RLM) and stochastic gradient descent (SGD).\nOur method is theoretically justified and demonstrates superior numerical performance on synthetic and real-world data.\nWe applied our method to a screening problem, where its effective exploitation of training data led to improved test power compared to state-of-the-art method based on split conformal."
    },
    {
        "title": "DDRL: A DIFFUSION-DRIVEN REINFORCEMENT LEARNING APPROACH FOR ENHANCED TSP SOLUTIONS",
        "link_suffix": "/forum?id=XigBo6nWzL",
        "link": "https://openreview.net/forum?id=XigBo6nWzL",
        "pdf_link": "https://openreview.net/pdf?id=XigBo6nWzL",
        "keywords": "TSP, Diffusion model, Reinforcement Learning",
        "abstract": "The Traveling Salesman Problem (TSP) is a fundamental challenge in combinatorial optimization, known for its NP-hard complexity. Reinforcement Learning (RL) has proven to be effective in managing larger and more complex TSP instances, yet it encounters challenges such as training instability and necessity for a substantial amount of training resources. Diffusion models, known for iteratively refining noisy inputs to generate high-quality solutions, offer scalability and exploration capabilities for TSP but may struggle with optimality in complex cases and require large, resource-intensive training datasets. To address these limitations, we propose DDRL (Diffusion-Driven Reinforcement Learning), which integrates diffusion models with RL. DDRL employs a latent vector to generate an adjacency matrix, merging image and graph learning within a unified RL framework. By utilizing a pre-trained diffusion model as a prior, DDRL exhibits strong scalability and enhanced convergence stability. We also provide theoretical analysis that training DDRL aligns with the diffusion policy gradient in the process of solving the TSP, demonstrating its effectiveness. Additionally, we introduce novel constraint datasets\u2014obstacle, path, and cluster constraints\u2014to evaluate DDRL's generalization capabilities. We demonstrate that DDRL offers a robust solution that outperforms existing methods in both basic and constrained TSP problems."
    },
    {
        "title": "Encryption-Friendly LLM Architecture",
        "link_suffix": "/forum?id=pbre0HKsfE",
        "link": "https://openreview.net/forum?id=pbre0HKsfE",
        "pdf_link": "https://openreview.net/pdf?id=pbre0HKsfE",
        "keywords": "Homomorphic Encryption, PPML, Encrypted Fine-tuning, Transformer Architecture",
        "abstract": "Large language models (LLMs) offer personalized responses based on user interactions, but this use case raises serious privacy concerns. Homomorphic encryption (HE) is a cryptographic protocol supporting arithmetic computations in encrypted states and provides a potential solution for privacy-preserving machine learning (PPML). However, the computational intensity of transformers poses challenges for applying HE to LLMs. In this work, we propose a modified HE-friendly transformer architecture with an emphasis on inference following personalized (private) fine-tuning. Utilizing LoRA fine-tuning and Gaussian kernels, we achieve significant computational speedups---6.94$\\times$ for fine-tuning and 2.3$\\times$ for inference---while maintaining performance comparable to plaintext models. Our findings provide a viable proof of concept for offering privacy-preserving LLM services in areas where data protection is crucial."
    },
    {
        "title": "Improved Training Technique for Latent Consistency Models",
        "link_suffix": "/forum?id=PQjZes6vFV",
        "link": "https://openreview.net/forum?id=PQjZes6vFV",
        "pdf_link": "https://openreview.net/pdf?id=PQjZes6vFV",
        "keywords": "Consistency Model, Diffusion Model",
        "abstract": "Consistency models are a new family of generative models capable of producing high-quality samples in either a single step or multiple steps. Recently, consistency models have demonstrated impressive performance, achieving results on par with diffusion models in the pixel space. However, the success of scaling consistency training to large-scale datasets, particularly for text-to-image and video generation tasks, is determined by performance in the latent space. In this work, we analyze the statistical differences between pixel and latent spaces, discovering that latent data often contains highly impulsive outliers, which significantly degrade the performance of iCT \\citep{song2023improved} in the latent space. To address this, we replace Pseudo-Huber losses with Cauchy losses, effectively mitigating the impact of outliers. Additionally, we introduce a diffusion loss at early timesteps and employ optimal transport (OT) coupling to further enhance performance. Lastly, we introduce the adaptive scaling-$c$ scheduler to manage the robust training process and adopt Non-scaling LayerNorm in the architecture to better capture the statistics of the features and reduce outlier impact. With these strategies, we successfully train latent consistency models capable of high-quality sampling with one or two steps, significantly narrowing the performance gap between latent consistency and diffusion models."
    },
    {
        "title": "Suppressing recency bias through implicit task in task-agnostic continual adaptation for foundation language models",
        "link_suffix": "/forum?id=BWMZKHTA9M",
        "link": "https://openreview.net/forum?id=BWMZKHTA9M",
        "pdf_link": "https://openreview.net/pdf?id=BWMZKHTA9M",
        "keywords": "continual learning, lifelong learning, transfer learning, foundation language models",
        "abstract": "Foundation language models have significantly advanced natural language processing but face challenges such as catastrophic forgetting when adapting to dynamic environments with diverse tasks. Recently, among the continual learning (CL) methods for these models, model architecture expansion methods have been spotlighted due to the growth of parameter-efficient fine-tuning (PEFT) methods. However, these methods need to store past PEFT adapters for each task and require task identifiers (task IDs) to distinguish each task, thus limiting their applicability in task-agnostic settings. They also overlook recency bias, where models focus overly on current tasks at the expense of past knowledge. \nTo address these issues, we propose suppressing recency bias (SRB) by using the concept of implicit tasks. SRB assigns a fixed-size adapter to an implicit task, recursively storing historical knowledge through arithmetic operations with current adapters at every time step instead of task IDs. This arithmetic mitigates recency bias by integrating non-overlapping information between historical and current adapters. \nOur approach requires only simple arithmetic operations without backpropagation, minimizing additional computation, and allocates a fixed-size adapter to the implicit task, resulting in low memory requirements. We evaluate SRB on CL benchmarks for foundational LMs. Experimental results demonstrate that SRB outperforms state-of-the-art methods, achieving superior generalization performance across various task sequences and models by effectively mitigating recency bias."
    },
    {
        "title": "Aligning Anything: Hierarchical Motion Estimation for Video Frame Interpolation",
        "link_suffix": "/forum?id=TKAzF69Fcv",
        "link": "https://openreview.net/forum?id=TKAzF69Fcv",
        "pdf_link": "https://openreview.net/pdf?id=TKAzF69Fcv",
        "keywords": "Video frame interpolation;  hierarchical motion estimation; pixel-level; target-level",
        "abstract": "Existing advanced video frame interpolation (VFI) methods struggle to learn accurate per-pixel motion or target-level motion. The reasons lie in that pixel-level motion estimation allows for infinite possibilities, making it challenging to guarantee fitting accuracy and global motion consistency, especially for rigid objects. Conversely, target-level motion consistency from the same moving target also breaks\ndown when the assumption of object rigidity no longer holds. Therefore, a hierarchical motion learn scheme is imperative to promote the accuracy and stability of motion prediction. Specifically, we marry the target-level motion to the pixel-level motion to form the hierarchical motion estimation. It elaborately introduces specific semantics priors from open-world knowledge models such as the Recognize Anything Model (RAM), Grounding DIDO, and the High-Quality Segment Anything Model (HQ-SAM) to facilitate the latent target-level motion learning. In particular, a hybrid contextual feature extraction module (HCE) is employed to aggregate both pixel-wise and semantic representations, followed by the hierarchical motion and feature interactive refinement module (HIR) to simulate the current motion patterns. When integrating these adaptions to existing SOTA VFI methods, more consistent motion estimation and interpolation are predicted. Extensive experiments show that advanced VFI networks plugged with our adaptions can achieve more superior performances on various benchmark datasets"
    },
    {
        "title": "On the Crucial Role of Initialization for Matrix Factorization",
        "link_suffix": "/forum?id=YTEwJaBdh0",
        "link": "https://openreview.net/forum?id=YTEwJaBdh0",
        "pdf_link": "https://openreview.net/pdf?id=YTEwJaBdh0",
        "keywords": "nonconvex optimization, initialization, quadratic rate, low rank adapter, lora",
        "abstract": "This work revisits the classical low-rank matrix factorization problem and unveils the critical role of initialization in shaping convergence rates for such nonconvex and nonsmooth optimization. We introduce Nystrom initialization, which significantly improves the global convergence of Scaled Gradient Descent (ScaledGD) in both symmetric and asymmetric matrix factorization tasks. Specifically, we prove that ScaledGD with Nystrom initialization achieves quadratic convergence in cases where only linear rates were previously known. Furthermore, we extend this initialization to low-rank adapters (LoRA) commonly used for finetuning foundation models. Our approach, NoRA, i.e., LoRA with Nystrom initialization, demonstrates superior performance across various downstream tasks and model scales, from 1B to 7B parameters, in large language and diffusion models."
    },
    {
        "title": "FaceGPT: Self-supervised Learning to Chat about 3D Human Faces",
        "link_suffix": "/forum?id=2vMGPrk0SW",
        "link": "https://openreview.net/forum?id=2vMGPrk0SW",
        "pdf_link": "https://openreview.net/pdf?id=2vMGPrk0SW",
        "keywords": "face reconstruction, vision language model, unsupervised learning",
        "abstract": "We introduce FaceGPT, a self-supervised learning framework for large vision-language models (VLMs) to reason about 3D human faces from images and text. Typical 3D face analysis algorithms are specialized and lack semantic reasoning capabilities. FaceGPT overcomes this limitation by embedding the parameters of a 3D morphable face model (3DMM) into the token space of a VLM, enabling the generation of 3D faces from both textual and visual inputs. FaceGPT is trained as a model-based autoencoder in a self-supervised manner from in-the-wild images. In particular, a dedicated face token is projected to 3DMM parameters and then rendered as a 2D face image to guide the self-supervised learning process through image-based reconstruction. Without relying on expensive 3D annotations, FaceGPT learns to generate 3D faces based on visual or textual inputs, achieving a competitive performance compared to methods that are specialized to each of these tasks. Most importantly, FaceGPT  is able to leverage the world knowledge in VLMs to achieve semantic reasoning capabilities, allowing the model to perform speculative generation of 3D faces purely from subtle textual prompts that do not explicitly describe facial features. This opens a new way of generating 3D faces from subtle descriptions of emotions or general everyday situations."
    },
    {
        "title": "CypST: Improving Cytochrome P450 Substrates Prediction with Fine-Tuned Protein Language Model and Graph Attention Network",
        "link_suffix": "/forum?id=ZyAwBqJ9aP",
        "link": "https://openreview.net/forum?id=ZyAwBqJ9aP",
        "pdf_link": "https://openreview.net/pdf?id=ZyAwBqJ9aP",
        "keywords": "Molecular graph attention networks, Protein language model, Deep learning, Enzyme substrate prediction",
        "abstract": "Cytochrome P450s (CYP450s) are key enzymes involved in human xenobiotics metabolism. So it is critical to make accurate CYP450s substrate predictions for drug discovery and chemical toxicology study. Recent deep learning-based approaches indicated that directly leverage extensive information from proteins and chemicals in biological and chemical databases to predict enzyme-substrate interactions, have achieved remarkable performance. Here, we present CypST, a deep learning-based model that enhances these methods by pre-trained ESM-2 Transformer model to extract detailed CYP450 protein representations and by incorporating our fine-tuned graph attention networks (GATs) for more effective learning on molecular graphs. GATs regard molecular graphs as sets of nodes or edges, with connectivity enforced by masking the attention weight matrix, creating custom attention patterns for each graph. This approach captures key molecular interactions, improving prediction ability for substrates. CypST effectively recognizes substructural interactions, constructing a comprehensive molecular representation through multi-substructural feature extraction.  By pre-training on a large-scale experimental enzyme-substrate pair database and fine-tuning on 51,753 CYP450s enzyme-substrate and 27,857 CYP450s enzyme-non-substrate pairs, CypST focuses on five major human CYP450 isforms, achieving 0.861 accuracy and 0.909 AUROC and demonstrating strong generalizability to novel compounds for different CYP450 isoforms."
    },
    {
        "title": "CALF: Benchmarking Evaluation of LFQA Using Chinese Examinations",
        "link_suffix": "/forum?id=R7pR4dzgAV",
        "link": "https://openreview.net/forum?id=R7pR4dzgAV",
        "pdf_link": "https://openreview.net/pdf?id=R7pR4dzgAV",
        "keywords": "Benchmark, Evaluation, Long-form QA, Large Language Models",
        "abstract": "Long-Form Question Answering (LFQA) refers to generating in-depth,paragraph-level responses to open-ended questions. Although lots of LFQA methods are developed, evaluating LFQA effectively and efficiently remains challenging due to its high complexity and cost. Therefore, there is no standard benchmark for LFQA evaluation till now. To address this gap, we make the first attempt by proposing a well-constructed, reference-based benchmark named Chinese exAmination for LFQA Evaluation (CALF), aiming to rigorously assess the performance of automatic evaluation metrics for LFQA. The CALF benchmark is derived from Chinese examination questions that have been translated into English. It includes up to 1476 examples consisting of knowledge-intensive and nuanced responses. Our evaluation comprises three different settings to analyze the behavior of automatic metrics comprehensively. We conducted extensive experiments on 7 traditional evaluation metrics, 3 prompt-based metrics, and 3 trained evaluation metrics, and tested on agent systems for the LFQA evaluation. The results reveal that none of the current automatic evaluation metrics shows comparable performances with humans, indicating that they cannot capture dense information contained in long-form responses well. In addition, we provide a detailed analysis of the reasons why automatic evaluation metrics fail when evaluating LFQA, offering valuable insights to advance LFQA evaluation systems."
    },
    {
        "title": "Boosting Latent Diffusion with Perceptual Objectives",
        "link_suffix": "/forum?id=y4DtzADzd1",
        "link": "https://openreview.net/forum?id=y4DtzADzd1",
        "pdf_link": "https://openreview.net/pdf?id=y4DtzADzd1",
        "keywords": "diffusion, flows, latent diffusion, LDM, latent generative models, T2I, image generation, generative models.",
        "abstract": "Latent diffusion models (LDMs) power state-of-the-art high-resolution generative image models. LDMs learn the data distribution in the latent space of an autoencoder (AE) and produce images by mapping the generated latents into RGB image space using the AE decoder. While this approach allows for efficient model training and sampling, it induces a disconnect between the training of the diffusion model and the decoder, resulting in a loss of detail in the generated images. To remediate this disconnect, we propose to leverage the internal features of the decoder to define a latent perceptual loss (LPL). This loss encourages the models to create sharper and more realistic images. Our loss can be seamlessly integrated with common autoencoders used in latent diffusion models, and can be applied to different generative modeling paradigms such as DDPM with epsilon and velocity prediction, as well as flow matching. Extensive experiments with models trained on three datasets at 256 and 512 resolution show improved quantitative -- with boosts between 6% and 20% in  FID -- and qualitative results when using our perceptual loss."
    },
    {
        "title": "Dynamic Negative Guidance of Diffusion Models",
        "link_suffix": "/forum?id=6p74UyAdLa",
        "link": "https://openreview.net/forum?id=6p74UyAdLa",
        "pdf_link": "https://openreview.net/pdf?id=6p74UyAdLa",
        "keywords": "Classifier-free guidance, Negative prompting, Diffusion model guidance",
        "abstract": "Negative Prompting (NP) is widely utilized in diffusion models, particularly in text-to-image applications, to prevent the generation of undesired features. In this paper, we show that conventional NP is limited by the assumption of a constant guidance scale, which may lead to highly suboptimal results, or even complete failure, due to the non-stationarity and state-dependence of the reverse process. Based on this analysis, we derive a principled technique calledDynamicNegativeGuidance, which relies on a near-optimal time and state dependent modulation of the guidance without requiring additional training. Unlike NP, negative guidance requires estimating the posterior class probability during the denoising process, which is achieved with limited additional computational overhead by tracking the discrete Markov Chain during the generative process. We evaluate the performance of DNG class-removal on MNIST and CIFAR10, where we show that DNG leads to higher safety, preservation of class balance and image quality when compared with baseline methods. Furthermore, we show that it is possible to use DNG with Stable Diffusion to obtain more accurate and less invasive guidance than NP."
    },
    {
        "title": "Solving Differential Equations with Constrained Learning",
        "link_suffix": "/forum?id=5KqveQdXiZ",
        "link": "https://openreview.net/forum?id=5KqveQdXiZ",
        "pdf_link": "https://openreview.net/pdf?id=5KqveQdXiZ",
        "keywords": "Constrained learning, partial differential equations, neural operators, physics-informed neural networks",
        "abstract": "(Partial) differential equations (PDEs) are fundamental tools for describing natural phenomena, making their solution crucial in science and engineering. While traditional methods, such as the finite element method, provide reliable solutions, their accuracy is often tied to the use of computationally intensive fine meshes. Moreover, they do not naturally account for measurements or prior solutions, and any change in the problem parameters requires results to be fully recomputed. Neural network-based approaches, such as physics-informed neural networks and neural operators, offer a mesh-free alternative by directly fitting those models to the PDE solution. They can also integrate prior knowledge and tackle entire families of PDEs by simply aggregating additional training losses. Nevertheless, they are highly sensitive to hyperparameters such as collocation points and the weights associated with each loss. This paper addresses these challenges by developing a science-constrained learning (SCL) framework. It demonstrates that finding a (weak) solution of a PDE is equivalent to solving a constrained learning problem with worst-case losses. This explains the limitations of previous methods that minimize the expected value of aggregated losses. SCL also organically integrates structural constraints (e.g., invariances) and (partial) measurements or known solutions. The resulting constrained learning problems can be tackled using a practical algorithm that yields accurate solutions across a variety of PDEs, neural network architectures, and prior knowledge levels without extensive hyperparameter tuning and sometimes even at a lower computational cost."
    },
    {
        "title": "Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data",
        "link_suffix": "/forum?id=daByonGVyo",
        "link": "https://openreview.net/forum?id=daByonGVyo",
        "pdf_link": "https://openreview.net/pdf?id=daByonGVyo",
        "keywords": "lossless compression, transformers, multi-modality",
        "abstract": "Foundation models have recently been shown to be strong data compressors. However, when accounting for their excessive parameter count, their compression ratios are actually inferior to standard compression algorithms. Moreover, naively reducing the number of parameters may not necessarily help as it leads to worse predictions and thus weaker compression. In this paper, we conduct a large-scale empirical study to investigate whether there is a sweet spot where competitive compression ratios with pre-trained vanilla transformers are possible. To this end, we train families of models on 165GB of raw byte sequences of either text, image, or audio data (and all possible combinations of the three) and then compress 1GB of out-of-distribution (OOD) data from each modality. We find that relatively small models (i.e., millions of parameters) can outperform standard general-purpose compression algorithms (gzip, LZMA2) and even domain-specific compressors (PNG, JPEG~2000, FLAC) \u2014 even when factoring in parameter count. We achieve, e.g., the lowest compression ratio of 0.49 on OOD audio data (vs. 0.54 for FLAC). To study the impact of model- and dataset scale, we conduct extensive ablations and hyperparameter sweeps, and we investigate the effect of unimodal versus multimodal training. We find that even small models can be trained to perform well on multiple modalities, but, in contrast to previously reported results with large-scale foundation models, transfer to unseen modalities is generally weak."
    },
    {
        "title": "KambaAD: Enhancing State Space Models with Kolmogorov\u2013Arnold for time series Anomaly Detection",
        "link_suffix": "/forum?id=XmnTfSX5Az",
        "link": "https://openreview.net/forum?id=XmnTfSX5Az",
        "pdf_link": "https://openreview.net/pdf?id=XmnTfSX5Az",
        "keywords": "Anomaly Detection, Mamba, KAN, Time series",
        "abstract": "Time series anomaly detection is critical in numerous practical applications, yet existing deep learning methods often fall short of real-world demands. These models fail to swiftly filter out physically implausible anomalies, insufficiently address distributional shifts, and lack a comprehensive approach that integrates both global and local perspectives for anomaly detection. Moreover, most successful models rely on channel-dependent methods that tend to treat all features at the same timestamp as a single token and then focus on finding relationships between these tokens. This approach overlooks the unique periodicities, trends, and lagged relationships between different features, leading to suboptimal performance. To address these limitations, we propose KambaAD, a model comprised of an Encoder and Reconstructor. The Encoder integrates the strengths of the Kolmogorov-Arnold Network (KAN), the attention mechanism, and the Selective Structured State Space Model (MAMBA). Specifically, KAN is employed to swiftly enforce data consistency, enabling rapid detection of anomalies that violate physical laws. The attention mechanism ensures balanced processing of global information while enhancing the representation of key data characteristics. We leverage MAMBA's capability as a sequence model to capture anomalies caused by local variations. Additionally, its internal selection mechanism allows the model to effectively handle distribution shifts, ensuring robustness and adaptability in the presence of changing data distributions. Additionally, the framework incorporates a time-series-specific Reconstructor, which reduces computational complexity through patch-based operations that exploit local consistency in time series data. It also employs channel-independent linear reconstruction to prevent interference between different features. Through extensive experiments on multiple multivariate datasets, KambaAD consistently outperforms state-of-the-art models, demonstrating its superior performance in anomaly detection."
    },
    {
        "title": "ClassDiffusion: More Aligned Personalization Tuning with Explicit Class Guidance",
        "link_suffix": "/forum?id=iTm4H6N4aG",
        "link": "https://openreview.net/forum?id=iTm4H6N4aG",
        "pdf_link": "https://openreview.net/pdf?id=iTm4H6N4aG",
        "keywords": "Diffusion, Subject-Driven Personalization",
        "abstract": "Recent text-to-image customization works have proven successful in generating images of given concepts by fine-tuning diffusion models on a few examples. However, tuning-based methods inherently tend to overfit the concepts, resulting in failure to create the concept under multiple conditions (e.g., headphone is missing when generating \"a <sks>`dog wearing a headphone\"). Interestingly, we notice that the base model before fine-tuning exhibits the capability to compose the base concept with other elements (e.g., \"a dog wearing a headphone\"), implying that the compositional ability only disappears after personalization tuning. We observe a semantic shift in the customized concept after fine-tuning, indicating that the personalized concept is not aligned with the original concept, and further show through theoretical analyses that this semantic shift leads to increased difficulty in sampling the joint conditional probability distribution, resulting in the loss of the compositional ability. Inspired by this finding, we presentClassDiffusion, a technique that leverages asemantic preservation lossto explicitly regulate the concept space when learning a new concept. Although simple, this approach effectively prevents semantic drift during the fine-tuning process of the target concepts. Extensive qualitative and quantitative experiments demonstrate that the use of semantic preservation loss effectively improves the compositional abilities of fine-tuning models. Lastly, we also extend our ClassDiffusion to personalized video generation, demonstrating its flexibility."
    },
    {
        "title": "Procedural Fairness Through Addressing Social Determinants of Opportunity",
        "link_suffix": "/forum?id=4jBJ6JphYM",
        "link": "https://openreview.net/forum?id=4jBJ6JphYM",
        "pdf_link": "https://openreview.net/pdf?id=4jBJ6JphYM",
        "keywords": "Procedural Fairness, Social Determinants of Opportunity, Causal Fairness, Structural Justice",
        "abstract": "Social determinants of opportunityare variables that, while not directly pertaining to any specific individual, capture key aspects of contexts and environments that have direct causal influences on certain attributes of an individual, e.g., environmental pollution in an area affects individual's health condition, and educational resources in an neighborhood influence individual's academic preparedness. Previous algorithmic fairness literature often overlookssocial determinants of opportunity, leading to implications for procedural fairness and structural justice that are incomplete and potentially even inaccurate. We propose a modeling framework that explicitly incorporatessocial determinants of opportunityand their causal influences on individual-level attributes of interest. To demonstrate theoretical perspectives and practical applicability of our framework, we consider college admissions as a running example. Specifically, for three mainstream admission procedures that have historically been implemented or are still in use today, we distinguish and draw connections between the outcome of admission decision-making and the underlying distribution of academic preparedness in the applicant population. Our findings suggest that mitigation strategies centering solely around protected features may introduce new procedural unfairness when addressing existing discrimination. Considering both individual-level attributes andsocial determinants of opportunityfacilitates a more comprehensive explication of benefits and burdens experienced by individuals from diverse demographic backgrounds as well as contextual environments, which is essential for understanding and achieving procedural fairness effectively and transparently."
    },
    {
        "title": "Protecting Copyrighted Material with Unique Identifiers in Large Language Model Training",
        "link_suffix": "/forum?id=5xbKFaaqkS",
        "link": "https://openreview.net/forum?id=5xbKFaaqkS",
        "pdf_link": "https://openreview.net/pdf?id=5xbKFaaqkS",
        "keywords": "LLM, Copyright",
        "abstract": "A primary concern regarding training large language models (LLMs) is whether they abuse copyrighted online text.\nWith the increasing training data scale and the prevalence of LLMs in daily lives, two problems arise:\n(1) false positive membership inference results misled by similar examples;\n(2) membership inference methods are usually too complex for general users to understand and use.\nTo address these issues, we propose an alternative \\textit{insert-and-detect} methodology, advocating that web users and content platforms employ \\textbf{\\textit{unique identifiers}} for reliable and independent membership inference.\nUsers and platforms can create their identifiers, embed them in copyrighted text, and independently detect them in future LLMs.\nAs an initial demonstration, we introduce \\textit{\\textbf{ghost sentences}} and a user-friendly last-$k$ words test, allowing general users to chat with LLMs for membership inference.\nGhost sentences consist primarily of unique passphrases of random natural words, which can come with customized elements to bypass possible filter rules.\nThe last-$k$ words test requires a significant repetition time of ghost sentences~($\\ge10$).\nFor cases with fewer repetitions, we designed an extra perplexity test, as LLMs exhibit high perplexity when encountering unnatural passphrases.\nWe also conduct a comprehensive study on the memorization and membership inference of ghost sentences, examining factors such as training data scales, model sizes, repetition times, insertion positions, wordlist of passphrases, alignment, \\textit{etc}.\nOur study shows the possibility of applying ghost sentences in real scenarios and providing instructions for the potential application."
    },
    {
        "title": "ART-FR: masked Auto-Regressive Transformer for Face Restoration",
        "link_suffix": "/forum?id=M1mL9tneGL",
        "link": "https://openreview.net/forum?id=M1mL9tneGL",
        "pdf_link": "https://openreview.net/pdf?id=M1mL9tneGL",
        "keywords": "Auto-regressive model; Face restoration",
        "abstract": "Restoring authentic facial features from low-quality images presents an extremely challenging task, due to the intricate real-world degradations and the inherently ill-posed nature of the problem. Existing methods, which utilize a codebook prior, help alleviate the complexity of the restoration process and produce visually plausible outcomes. However, these methods struggle to accurately capture the mapping between low-quality (LQ) and high-quality (HQ) images in the discrete latent space, leading to suboptimal results. \nInspired by the success of auto-regressive generation paradigm in discrete modeling problems (e.g.  large language models), we propose an Auto-Regressive Transformer based Face Restoration (ART-FR) method to mitigate this mapping challenge. Specifically, with the aid of a visual tokenizer, we reformulate the face restoration task as a conditional generation problem within the discrete latent space. Furthermore, a masked generative image transformer is employed to model the distribution of this latent space, conditioned on LQ features. Face restoration is subsequently performed in the latent space through iterative sampling, with the HQ image reconstructed using a pretrained decoder. Extensive experimental validation demonstrates ART-FR exhibits superior performance across various benchmark datasets."
    },
    {
        "title": "Language-conditioned Multi-Style Policies with Reinforcement Learning",
        "link_suffix": "/forum?id=KohdorhwHt",
        "link": "https://openreview.net/forum?id=KohdorhwHt",
        "pdf_link": "https://openreview.net/pdf?id=KohdorhwHt",
        "keywords": "language-conditioned reinforcement learning, multi-style policy, large language model, policy control",
        "abstract": "Recent studies have explored the application of large language models (LLMs) in language-conditioned reinforcement learning (LC-RL). These studies typically involve training RL agents to follow straightforward human instructions in domains such as object manipulation, navigation, or text-based environments. To extend these capabilities for following high-level and abstract language instructions with diverse style policies in complex environments, we propose a novel method called LCMSP, which can generate language-conditioned multi-style policies. LCMSP first trains a multi-style RL policy capable of achieving different meta-behaviors, which can be controlled by corresponding style parameters. Subsequently, LCMSP leverages the reasoning capabilities and common knowledge of LLMs to align language instructions with style parameters, thereby realizing language-controlled multi-style policies. Experiments conducted in various environments and with different types of instructions demonstrate that the proposed LCMSP is capable of understanding high-level abstract instructions and executing corresponding behavioral styles in complex environments."
    },
    {
        "title": "ReGRAF: Training free Prompt Refinement via Gradient Flow for Segmentation",
        "link_suffix": "/forum?id=ZvPPLeVuhT",
        "link": "https://openreview.net/forum?id=ZvPPLeVuhT",
        "pdf_link": "https://openreview.net/pdf?id=ZvPPLeVuhT",
        "keywords": "Visual Foundation model, Segmentation, Computer Vision, Prompt engineering",
        "abstract": "Visual Foundation Models (VFMs) such as the Segment Anything Model (SAM) have significantly advanced segmentation, object detection, and image classification tasks. \nHowever, SAM and its fine-tuned variants necessitate substantial manual effort for prompt generation and additional training for specific applications. \nRecent methods have addressed these limitations by integrating SAM into one-shot and few-shot segmentation, enabling auto-prompting through semantic alignment between query and support images. \nDespite these advancements, they still generate inadequate prompts that degrade segmentation quality. \nTo tackle this limitation, we introduce ReGRAF (Refinement via GRAdient Flow), a training-free method that refines prompts through gradient flow derived from SAM's mask decoder. ReGRAF seamlessly integrates into SAM-based auto-prompting frameworks and is theoretically proven to refine segmentation masks with high efficiency and precision. Extensive evaluations demonstrate that ReGRAF consistently improves segmentation quality across various benchmarks, effectively mitigating false positives without requiring additional training or architectural modifications."
    },
    {
        "title": "AtomSurf: Surface Representation for Learning on Protein Structures",
        "link_suffix": "/forum?id=ARQIJXFcTH",
        "link": "https://openreview.net/forum?id=ARQIJXFcTH",
        "pdf_link": "https://openreview.net/pdf?id=ARQIJXFcTH",
        "keywords": "deep learning, protein representation learning; surface methods; geometric deep learning",
        "abstract": "While there has been significant progress in evaluating and comparing different representations for learning on protein data, the role of surface-based learning approaches remains not well-understood. In particular, there is a lack of direct and fair benchmark comparison between the best available surface-based learning methods against alternative representations such as graphs. Moreover, the few existing surface-based approaches either use surface information in isolation or, at best, perform global pooling between surface and graph-based architectures.In this work, we fill this gap by first adapting a state-of-the-art surface encoder for protein learning tasks. We then perform a direct and fair comparison of the resulting method against alternative approaches within the Atom3D benchmark, highlighting the limitations of pure surface-based learning. Finally, we propose an integrated approach, which allows learned feature sharing between graphs and surface representations on the level of nodes and vertices \\textit{across all layers}.We demonstrate that the resulting architecture achieves state-of-the-art results on all tasks in the Atom3D benchmark, while adhering to the strict benchmark protocol, as well as more broadly on binding site identification and binding pocket classification. Furthermore, we use coarsened surfaces and optimize our approach for efficiency, making our tool competitive in training and inference time with existing techniques."
    }
]