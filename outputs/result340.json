[
    {
        "title": "Coordinate In and Value Out: Training Flow Transformers in Ambient Space",
        "link_suffix": "/forum?id=AHnj6YbNbj",
        "link": "https://openreview.net/forum?id=AHnj6YbNbj",
        "pdf_link": "https://openreview.net/pdf?id=AHnj6YbNbj",
        "keywords": "Generative Model, Flow Matching, Domain Agnostic",
        "abstract": "Flow matching models have emerged as a powerful method for generative modeling on domains like images or videos, and even on unstructured data like 3D point clouds. These models are commonly trained in two stages: first, a data compressor (\\ie a variational auto-encoder) is trained, and in a subsequent training stage a flow matching generative model is trained in the low-dimensional latent space of the data compressor. This two stage paradigm adds complexity to the overall training recipe and sets obstacles for unifying models across data domains, as specific data compressors are used for different data modalities. To this end, we introduce Ambient Space Flow Transformers (ASFT), a domain-agnostic approach to learn flow matching transformers in ambient space, sidestepping the requirement of training compressors and simplifying the training process. We introduce a conditionally independent point-wise training objective that enables ASFT to make predictions continuously in coordinate space. Our empirical results demonstrate that using general purpose transformer blocks, ASFT effectively handles different data modalities such as images and 3D point clouds, achieving strong performance in both domains and outperforming comparable approaches. ASFT is a promising step towards domain-agnostic flow matching generative models that can be trivially adopted in different data domains."
    },
    {
        "title": "Do LLMs estimate uncertainty well in instruction-following?",
        "link_suffix": "/forum?id=IHp3vOVQO2",
        "link": "https://openreview.net/forum?id=IHp3vOVQO2",
        "pdf_link": "https://openreview.net/pdf?id=IHp3vOVQO2",
        "keywords": "Instruction-following, Uncertainty, Large language models, Evaluation, Benchmark dataset",
        "abstract": "Large language models (LLMs) could be valuable personal AI agents across various domains, provided they can precisely follow user instructions. However, recent studies have shown significant limitations in LLMs' instruction-following capabilities, raising concerns about their reliability in high-stakes applications. \nAccurately estimating LLMs' uncertainty in adhering to instructions is critical to mitigating deployment risks. We present, to our knowledge, the first systematic evaluation of the uncertainty estimation abilities of LLMs in the context of instruction-following. \nOur study identifies key challenges with existing instruction-following benchmarks, where multiple factors are entangled with uncertainty stems from instruction-following, complicating the isolation and comparison across methods and models.\nTo address these issues, we introduce a controlled evaluation setup with two benchmark versions of data, enabling a comprehensive comparison of uncertainty estimation methods under various conditions.\nOur findings show that existing uncertainty methods struggle, particularly when models make subtle errors in instruction following. While internal model states provide some improvement, they remain inadequate in more complex scenarios. \nThe insights from our controlled evaluation setups provide a crucial understanding of LLMs' limitations and potential for uncertainty estimation in instruction-following tasks, paving the way for more trustworthy AI agents."
    },
    {
        "title": "TopoDiffusionNet: A Topology-aware Diffusion Model",
        "link_suffix": "/forum?id=ZK1LoTo10R",
        "link": "https://openreview.net/forum?id=ZK1LoTo10R",
        "pdf_link": "https://openreview.net/pdf?id=ZK1LoTo10R",
        "keywords": "Topology, Diffusion Models, Persistent Homology",
        "abstract": "Diffusion models excel at creating visually impressive images but often struggle to generate images with a specified topology. The Betti number, which represents the number of structures in an image, is a fundamental measure in topology. Yet, diffusion models fail to satisfy even this basic constraint. This limitation restricts their utility in applications requiring exact control, like robotics and environmental modeling. To address this, we propose TopoDiffusionNet (TDN), a novel approach that enforces diffusion models to maintain the desired topology. We leverage tools from topological data analysis, particularly persistent homology, to extract the topological structures within an image. We then design a topology-based objective function to guide the denoising process, preserving intended structures while suppressing noisy ones. Our experiments across four datasets demonstrate significant improvements in topological accuracy. TDN is the first to integrate topology with diffusion models, opening new avenues of research in this area."
    },
    {
        "title": "Analyzing the Language of Visual Tokens",
        "link_suffix": "/forum?id=qPTFzmXVLd",
        "link": "https://openreview.net/forum?id=qPTFzmXVLd",
        "pdf_link": "https://openreview.net/pdf?id=qPTFzmXVLd",
        "keywords": "Tokenization, Analysis, Statistical Methods, Vision and Language",
        "abstract": "With the introduction of transformer-based models for vision and language tasks, such as LLaVA and Chameleon, there has been renewed interest in the discrete tokenized representation of images. These models often treat image patches as discrete tokens, analogous to words in natural language, learning joint alignments between visual and human languages. However, little is known about the statistical behavior of these visual languages\u2014whether they follow similar frequency distributions, grammatical structures, or topological alignments as natural languages. In this paper, we take a natural-language-centric approach to analyzing discrete visual languages and uncover striking similarities and fundamental differences. We demonstrate that, although visual languages adhere to Zipfian distributions, higher token innovation drives greater entropy and lower compression, with tokens predominantly representing object parts, indicating intermediate granularity. We also show that visual languages lack cohesive grammatical structures, leading to higher perplexity and weaker hierarchical organization compared to natural languages. Finally, we demonstrate that, while vision models align more closely with natural languages than other models, this alignment remains significantly weaker than the cohesion found within natural languages. Through these experiments, we demonstrate how understanding the statistical properties of discrete visual languages can inform the design of more effective computer vision models."
    },
    {
        "title": "DistillHGNN: A Knowledge Distillation Approach for High-Speed Hypergraph Neural Networks",
        "link_suffix": "/forum?id=vzrs42hgb0",
        "link": "https://openreview.net/forum?id=vzrs42hgb0",
        "pdf_link": "https://openreview.net/pdf?id=vzrs42hgb0",
        "keywords": "Knowledge Distillation, Hypergraph Neural Networks, Contrastive Learning.",
        "abstract": "In this paper, we propose a novel framework to significantly enhance the inference speed and memory efficiency of Hypergraph Neural Networks (HGNNs) while preserving their high accuracy. Our approach utilizes an advanced teacher-student knowledge distillation strategy. The teacher model, consisting of an HGNN and a Multi-Layer Perceptron (MLP), not only produces soft labels but also transfers structural and high-order information to a lightweight Graph Convolutional Network (GCN) known as TinyGCN. This dual transfer mechanism enables the student model to effectively capture complex dependencies while benefiting from the faster inference and lower computational cost of the lightweight GCN. The student model is trained using both labeled data and soft labels provided by the teacher, with contrastive learning further ensuring that the student retains high-order relationships. This makes the proposed method efficient and suitable for real-time applications, achieving performance comparable to traditional HGNNs but with significantly reduced resource requirements."
    },
    {
        "title": "The Geometry of Attention: Ricci Curvature and Transformers Training and  Robustness",
        "link_suffix": "/forum?id=eeyhnqYbxw",
        "link": "https://openreview.net/forum?id=eeyhnqYbxw",
        "pdf_link": "https://openreview.net/pdf?id=eeyhnqYbxw",
        "keywords": "Transformers, Attention, Geometry, Robustness",
        "abstract": "Transformer models have revolutionized machine learning, and the theoretical underpinnings behind their success are only now starting to be explored. \nIn this work, we analyze the performance and robustness of transformers by considering the attention mechanism as a graph operator, focusing on the geometry of attention maps viewed as weighted graphs.\nSpecifically, we investigate the role of Ricci curvature, a metric closely tied to graph spectral properties and system robustness, in shaping the training dynamics and robustness of transformers. \nOur theoretical analysis establishes a link between Ricci curvature and the convergence behavior of gradient descent, demonstrating that lower curvature, reflecting lower system robustness, leads to faster training. \nFurthermore, we show that a higher frequency of more positive values in the Ricci curvature distribution of attention graphs, therefore more system robustness, leads to more robust transformers, highlighting the impact of curvature on the robustness of transformers and uncovering a trade-off between their performance and robustness.\nLeveraging these insights, we propose an efficient regularization method to train curvature-adjusted transformers. \nSupporting our theoretical findings, experiments show how the proposed attention curvature manipulation improves the learning speed, performance, and generalizability of vision and language transformers.\nThis work demonstrates that the geometry of the attention map provides a theoretically elegant and computationally versatile framework for analyzing and manipulating transformer training, generalization, performance, and robustness, opening new avenues for designing models using geometric concepts."
    },
    {
        "title": "Learning Transformer-based World Models with Contrastive Predictive Coding",
        "link_suffix": "/forum?id=YK9G4Htdew",
        "link": "https://openreview.net/forum?id=YK9G4Htdew",
        "pdf_link": "https://openreview.net/pdf?id=YK9G4Htdew",
        "keywords": "model-based reinforcement learning, transformer network, contrastive predictive coding",
        "abstract": "The DreamerV3 algorithm recently obtained remarkable performance across diverse environment domains by learning an accurate world model based on Recurrent Neural Networks (RNNs). Following the success of model-based reinforcement learning algorithms and the rapid adoption of the Transformer architecture for its superior training efficiency and favorable scaling properties, recent works such as STORM have proposed replacing RNN-based world models with Transformer-based world models using masked self-attention. However, despite the improved training efficiency of these methods, their impact on performance remains limited compared to the Dreamer algorithm, struggling to learn competitive Transformer-based world models. In this work, we show that the next state prediction objective adopted in previous approaches is insufficient to fully exploit the representation capabilities of Transformers. We propose to extend world model predictions to longer time horizons by introducing TWISTER (Transformer-based World model wIth contraSTivE Representations), a world model using action-conditioned Contrastive Predictive Coding to learn high-level temporal feature representations and improve the agent performance. TWISTER achieves a human-normalized mean score of 162% on the Atari 100k benchmark, setting a new record among state-of-the-art methods that do not employ look-ahead search."
    },
    {
        "title": "Covariate-informed continuous-time gray-box modeling to identify responsiveness of post-surgical pain to opioid therapy",
        "link_suffix": "/forum?id=3X6QlkWfHH",
        "link": "https://openreview.net/forum?id=3X6QlkWfHH",
        "pdf_link": "https://openreview.net/pdf?id=3X6QlkWfHH",
        "keywords": "state space model, gray box, hybrid model, time series, treatment effects",
        "abstract": "Quantifying responsiveness of pain to opioid administration is a clinically important, yet technically challenging problem.\nPain is a subjective phenomenon that is difficult to assess by means other than infrequent and low-resolution patient self-reporting.\nWe tackle this problem using a continuous-time state space modeling approach that incorporates mechanistic models of opioid effect site concentration as well as information from covariates using black-box models iteratively trained to predict the distributions of partially observed variables.\nWe evaluated our method in simulation, and applied it in a real-world observational study of 21,652 surgical cases, where our method is able to recapitulate the known potencies of different opioids, and stratify patients by pain and opioid use related outcomes."
    },
    {
        "title": "Logical Consistency of Large Language Models in Fact-Checking",
        "link_suffix": "/forum?id=SimlDuN0YT",
        "link": "https://openreview.net/forum?id=SimlDuN0YT",
        "pdf_link": "https://openreview.net/pdf?id=SimlDuN0YT",
        "keywords": "LLM, Logical consistency, Fact-checking in Knowledge graphs",
        "abstract": "In recent years, large language model (LLM) has demonstrated significant success in performing varied natural language tasks such as language translation, question-answering, summarising, fact-checking, etc. Despite LLM\u2019s impressive ability to generate human-like texts, LLMs are infamous for their inconsistent responses \u2013 a meaning-preserving change in the input query results in an inconsistent response and attributes to vulnerabilities of LLMs such as hallucination, jail breaking, etc. Consequently, existing research focuses on simple paraphrasing-based consistency assessment of LLMs, and ignores complex queries that necessitates an even better understanding of logical reasoning by an LLM. Our work therefore addresses the logical inconsistency of LLMs under complex logical queries with primitive logical operators, e.g., negation, conjunction, and disjunction. As a test bed, we consider retrieval-augmented LLMs on a fact-checking task involving propositional logic queries from real-world knowledge graphs (KG). Our contributions are three-fold. Benchmark: We introduce three logical fact-checking datasets over KGs for community development towards logically consistent LLMs. Assessment: We propose consistency measures of LLMs on propositional logic queries as input and demonstrate that existing LLMs lack in logical consistency, specially on complex queries. Improvement: We employ supervised fine-tuning to improve the logical consistency of LLMs on the complex\nfact-checking task with KG contexts."
    },
    {
        "title": "GLIMO: Grounding Large Language Models With Imperfect World Models",
        "link_suffix": "/forum?id=ZNsWJkFrqQ",
        "link": "https://openreview.net/forum?id=ZNsWJkFrqQ",
        "pdf_link": "https://openreview.net/pdf?id=ZNsWJkFrqQ",
        "keywords": "Robotics, instruction tuning, LLM",
        "abstract": "Despite a widespread success in various applications, large language models (LLMs) often stumble when tackling basic physical reasoning or executing robotics tasks, due to a lack of\ndirect experience with the physical nuances of the real world. \nTo address these issues, we propose a Grounding Large language model with Imperfect world MOdel (GLIMO), which \nutilizes proxy world models such as simulators to collect and synthesize trining data. \nGLIMO incorporates an LLM agent-based data generator to automatically create high-quality and diverse instruction datasets. The generator includes an iterative self-refining module for temporally consistent experience sampling, a diverse set of question-answering instruction seeds, and a retrieval-augmented generation module for reflecting on prior experiences.\nComprehensive experiments \nshow that our approach improve the performance of strong open-source LLMs like LLaMA-3  with a performance boost of 2.04 $\\times$, 1.54 $\\times$, and 1.82 $\\times$ across three different benchmarks, respectively.\nThe performance is able to compete with or surpass their larger counterparts such as GPT-4."
    },
    {
        "title": "An Image is WorthKSlots: Data-efficient Scaling of Self-supervised Visual Pre-training",
        "link_suffix": "/forum?id=hPq9weqiwp",
        "link": "https://openreview.net/forum?id=hPq9weqiwp",
        "pdf_link": "https://openreview.net/pdf?id=hPq9weqiwp",
        "keywords": "non-object-centric data, self-supervised learning, representation learning, visual pre-training, object discovery",
        "abstract": "Scaling up data and computing has become the norm for pre-training powerful visual encoders. Current algorithms, when scaled up, often require training on large-scale datasets that are unlikely to be object-centric. However, these algorithms were typically developed and validated on the object-centric ImageNet. This discrepancy may suggest sub-optimal scalability and underutilized data potential. Non-object-centric (NOC) data, with its multiple objects and complex layouts, tends to be more information-dense. To better leverage this underlying structure, we introduce a semantic bottleneck to MIM, which reduces the number of prototypes to encourage the emergence of objectness at patch-level token representation. Further, cross-view consistency regularization is applied to encourage multiview invariance. Together, this induces semantic object discovery and allows instance discrimination to be applied between object-level features (slots). Our experiments encompass pre-training on object-centric, scene-centric, web-crawled, and ego-centric data. Across all settings, our approach learns transferrable representations and achieves significant improvements over prior work in image recognition, scene understanding, and robot learning evaluations. When scaled up with million-scale datasets, our method also demonstrates superior data efficiency and scalability. We will make our code and model artifacts publicly available."
    },
    {
        "title": "TimeDiT: General-purpose Diffusion Transformers for Time Series Foundation Model",
        "link_suffix": "/forum?id=FvBTy5Dz9C",
        "link": "https://openreview.net/forum?id=FvBTy5Dz9C",
        "pdf_link": "https://openreview.net/pdf?id=FvBTy5Dz9C",
        "keywords": "Time Series; Foundation model; Diffusion model",
        "abstract": "With recent advances in building foundation models for text and video data, there is a surge of interest in foundation modeling for time series.  Many families of models have been developed utilizing a temporal autoregressive Transformer architecture, whose effectiveness has been proven in Large Language Models (LLMs). However, real-world time series exhibit unique challenges, such as variable channel sizes across domains, missing values, and varying signal sampling intervals due to the multi-resolution nature of real-world data. Additionally, the unidirectional nature of temporally autoregressive decoding typically learns a deterministic mapping relationship and limits the incorporation of domain knowledge, such as physical laws. To address these challenges, we introduce the Time Diffusion Transformer (TimeDiT), a general foundation model for time series that jointly leverages the transformer inductive bias to capture temporal dependencies and the diffusion processes to generate high-quality candidate samples. The proposed mask unit for task-agnostic pretraining and task-specific sampling enables direct processing of multivariate inputs even with missing values or multi-resolution. Furthermore, we introduce a theoretically justified finetuning-free model editing strategy that allows the flexible integration of external knowledge during the sampling process. Extensive experiments conducted on a variety of tasks, such as forecasting, imputation, and anomaly detection highlight TimeDiT's adaptability as a foundation model, addressing diverse time series challenges and advancing analysis in various fields."
    },
    {
        "title": "INS-MMBench: A Comprehensive Benchmark for Evaluating LVLMs' Performance in Insurance",
        "link_suffix": "/forum?id=yIN4yDCcmo",
        "link": "https://openreview.net/forum?id=yIN4yDCcmo",
        "pdf_link": "https://openreview.net/pdf?id=yIN4yDCcmo",
        "keywords": "large vision-language model, insurance, multimodal",
        "abstract": "Large Vision-Language Models (LVLMs) have demonstrated outstanding performance in various general multimodal applications such as image recognition and visual reasoning, and have also shown promising potential in specialized domains. However, the application potential of LVLMs in the insurance domain\u2014characterized by rich application scenarios and abundant multimodal data\u2014has not been effectively explored. There is no systematic review of multimodal tasks in the insurance domain, nor a benchmark specifically designed to evaluate the capabilities of LVLMs in insurance. This gap hinders the development of LVLMs within the insurance domain. In this paper, we systematically review and distill multimodal tasks for four representative types of insurance: auto insurance, property insurance, health insurance, and agricultural insurance. We propose INS-MMBench, the first comprehensive LVLMs benchmark tailored for the insurance domain. INS-MMBench comprises a total of 2.2K thoroughly designed multiple-choice questions, covering 12 meta-tasks and 22 fundamental tasks. Furthermore, we evaluate multiple representative LVLMs, including closed-source models such as GPT-4o and open-source models like BLIP-2. This evaluation not only validates the effectiveness of our benchmark but also provides an in-depth performance analysis of current LVLMs on various multimodal tasks in the insurance domain. We hope that INS-MMBench will facilitate the further application of LVLMs in the insurance domain and inspire interdisciplinary development."
    },
    {
        "title": "Cauchy-Schwarz Fairness Regularizer",
        "link_suffix": "/forum?id=JwoQZ9NKtH",
        "link": "https://openreview.net/forum?id=JwoQZ9NKtH",
        "pdf_link": "https://openreview.net/pdf?id=JwoQZ9NKtH",
        "keywords": "Fairness, Machine Learning, Cauchy-Schwarz Divergence",
        "abstract": "In this paper, we propose a novel approach to fair machine learning, the Cauchy-Schwarz fairness regularizer, which minimizes the Cauchy-Schwarz divergence between the prediction distribution and sensitive attributes. While existing methods effectively reduce bias as indicated by low values on specific fairness metrics, they frequently struggle to achieve a balanced performance across various fairness definitions. For example, many approaches may successfully attain low demographic parity yet still demonstrate significant disparities in equal opportunity. Theoretical studies have shown that the Cauchy-Schwarz divergence provides a tighter bound compared to the Kullback-Leibler divergence and gap parity, suggesting its potential to improve fairness in machine learning models. Our empirical evaluation, conducted on four tabular datasets and one image dataset, demonstrates that the Cauchy-Schwarz fairness regularizer achieves a more balanced performance across fairness metrics while maintaining satisfactory utility. It outperforms existing fairness approaches, providing a superior trade-off between fairness and utility. In addition, the Cauchy-Schwarz fairness regularizer is a versatile, plug-and-play fairness regularizer that can be easily integrated into various machine learning models to promote fairness."
    },
    {
        "title": "PIN: Prolate Spheroidal Wave Function-based Implicit Neural Representations",
        "link_suffix": "/forum?id=Eh1QM3OK51",
        "link": "https://openreview.net/forum?id=Eh1QM3OK51",
        "pdf_link": "https://openreview.net/pdf?id=Eh1QM3OK51",
        "keywords": "Prolate Spheroidal Wave Functions, Implicit Neural Representations, MLPs",
        "abstract": "Implicit Neural Representations (INRs) provide a continuous mapping between the coordinates of a signal and the corresponding values. As the performance of INRs heavily depends on the choice of nonlinear-activation functions, there has been a significant focus on encoding explicit signals within INRs using diverse activation functions. Despite recent advancements, existing INRs often encounter significant challenges, particularly at fine scales where they often introduce noise-like artifacts over smoother areas compromising the quality of the output. Moreover, they frequently struggle to generalize to unseen coordinates. These drawbacks highlight a critical area for further research and development to enhance the robustness and applicability of INRs across diverse scenarios. To address this challenge, we introduce the Prolate Spheroidal Wave Function-based Implicit Neural Representations (PIN), which exploits the optimal space-frequency domain concentration of Prolate Spheroidal Wave Functions (PSWFs) as the nonlinear mechanism in INRs. Our experimental results reveal that PIN excels not only in representing images and 3D shapes but also significantly outperforms existing methods in various vision tasks that require INR generalization, including image inpainting, novel view synthesis, edge detection, and image denoising."
    },
    {
        "title": "Sparse Attention Decomposition Applied to Circuit Tracing",
        "link_suffix": "/forum?id=A2rfALKFBg",
        "link": "https://openreview.net/forum?id=A2rfALKFBg",
        "pdf_link": "https://openreview.net/pdf?id=A2rfALKFBg",
        "keywords": "Mechanistic Interpretability, Transformers, Large Language Models, Interpretability, Singular Value Decomposition",
        "abstract": "Many papers have shown that attention heads work in conjunction with each other to perform complex tasks. It's frequently assumed that communication between attention heads is via the addition of specific features to token residuals. \nIn this work we seek to isolate and identify the features used to effect communication and coordination among attention heads in GPT-2 small.  Our key leverage on the problem is to show that these features are very often sparsely coded in the singular vectors of attention head matrices.  We characterize the dimensionality and occurrence of these signals across the attention heads in GPT-2 small when used for the Indirect Object Identification (IOI) task. The sparse encoding of signals, as provided by attention head singular vectors, allows for efficient separation of signals from the residual background and straightforward identification of communication paths between attention heads. We explore the effectiveness of this approach by tracing portions of the circuits used in the IOI task.  Our traces reveal considerable detail not present in previous studies, shedding light on the nature of redundant paths present in GPT-2. And our traces go beyond previous work by identifying features used to communicate between attention heads when performing IOI."
    },
    {
        "title": "Orthogonal Representation Learning for Estimating Causal Quantities",
        "link_suffix": "/forum?id=mwYkVSddzx",
        "link": "https://openreview.net/forum?id=mwYkVSddzx",
        "pdf_link": "https://openreview.net/pdf?id=mwYkVSddzx",
        "keywords": "treatment effect estimation, counterfactual outcomes estimation, representation learning",
        "abstract": "Representation learning is widely used for estimating causal quantities (e.g., the conditional average treatment effect) from observational data. While existing representation learning methods have the benefit of allowing for end-to-end learning, they do not have favorable theoretical properties of Neyman-orthogonal learners, such as double robustness and quasi-oracle efficiency. Also, such representation learning methods often employ additional constraints, like balancing, which may even lead to inconsistent estimation. In this paper, we propose a novel class of Neyman-orthogonal learners for causal quantities defined at the representation level, which we call OR-learners. Our OR-learners have several practical advantages: they allow for consistent estimation of causal quantities based on any learned representation, while offering favorable theoretical properties including double robustness and quasi-oracle efficiency. In numerous experiments, we show that, under certain regularity conditions, our OR-learners improve existing representation learning methods and achieve state-of-the-art performance. To the best of our knowledge, our OR-learners are the first work to unify representation learning methods and Neyman-orthogonal learners."
    },
    {
        "title": "Deep Linear Hawkes Processes",
        "link_suffix": "/forum?id=aaePIQsAdq",
        "link": "https://openreview.net/forum?id=aaePIQsAdq",
        "pdf_link": "https://openreview.net/pdf?id=aaePIQsAdq",
        "keywords": "Marked temporal point processes, state-space models, Hawkes processes",
        "abstract": "Marked temporal point processes (MTPPs) are used to model sequences of different types of events with irregular arrival times, with broad applications ranging from healthcare and social networks to finance. We address shortcomings in existing point process models by drawing connections between modern deep state-space models (SSMs) and linear Hawkes processes (LHPs), culminating in an MTPP we call thedeep linear Hawkes process(DLHP). The DLHP modifies the linear differential equations in deep SSMs to be stochastic jump differential equations, akin to LHPs. After discretizing, the resulting recurrence can be implemented efficiently using a parallel scan. This brings both linear scaling and parallelism to MTPP models. This contrasts with attention-based MTPPs, which scale quadratically, and RNN-based MTPPs, which do not parallelize across the sequence length. We show empirically that DLHPs match or outperform existing models across a broad range of metrics on eight real-world datasets. Our proposed DLHP model is the first instance of the unique architectural capabilities of SSMs being leveraged to construct a new class of MTPP models."
    },
    {
        "title": "Sparling: Learning Latent Representations with Extremely Sparse Activations",
        "link_suffix": "/forum?id=ZhXJNUEOr9",
        "link": "https://openreview.net/forum?id=ZhXJNUEOr9",
        "pdf_link": "https://openreview.net/pdf?id=ZhXJNUEOr9",
        "keywords": "machine learning, sparsity, interpretability, optimization, identifiability",
        "abstract": "Real-world processes often contain intermediate state that can be modeled as an extremely sparse activation tensor. In this work, we analyze the identifiability of such sparse and local latent intermediate variables, which we call motifs.\nWe prove our Motif Identifiability Theorem, stating that under certain assumptions it is possible to precisely identify these motifs exclusively by reducing end-to-end error. Additionally, we provide the Sparling algorithm, which uses a new kind of informational bottleneck that enforces levels of activation sparsity unachievable using other techniques. We find that extreme sparsity is necessary to achieve good intermediate state modeling empirically. On our synthetic DigitCircle domain as well as the LaTeXOCR and AudioMNISTSequence domains, we are able to precisely localize the intermediate states up to feature permutation with >90% accuracy, even though we only train end-to-end."
    },
    {
        "title": "Hardware Scaling Trends and Diminishing Returns in Large-Scale Distributed Training",
        "link_suffix": "/forum?id=f0GTSxGi6G",
        "link": "https://openreview.net/forum?id=f0GTSxGi6G",
        "pdf_link": "https://openreview.net/pdf?id=f0GTSxGi6G",
        "keywords": "distributed training, scaling, pretraining, hardware-software, parallelization, efficiency, utilization, training, performance.",
        "abstract": "Dramatic increases in the capabilities of neural network models in recent years\nare driven by scaling model size, training data, and corresponding computational\nresources. To develop the exceedingly large networks required in modern applications, such as large language models (LLMs), model training is distributed\nacross tens of thousands of hardware accelerators (e.g. GPUs), requiring orchestration of computation and communication across large computing clusters. In this\nwork, we demonstrate that careful consideration of hardware configuration and\nparallelization strategy is critical for effective (i.e. compute- and cost-efficient)\nscaling of model size, training data, and total computation. We conduct an extensive empirical study of the performance of large-scale LLM training workloads\nacross model size, hardware configurations, and distributed parallelization strategies. We demonstrate that: (1) beyond certain scales, overhead incurred from\ncertain distributed communication strategies leads parallelization strategies previously thought to be sub-optimal in fact become preferable; and (2) scaling the\ntotal number of accelerators for large model training quickly yields diminishing\nreturns even when hardware and parallelization strategies are properly optimized,\nimplying poor marginal performance per additional unit of power or GPU-hour."
    },
    {
        "title": "AINR: Adaptive Learning of Activations for Implicit Neural Representations",
        "link_suffix": "/forum?id=G4P1q2G0XK",
        "link": "https://openreview.net/forum?id=G4P1q2G0XK",
        "pdf_link": "https://openreview.net/pdf?id=G4P1q2G0XK",
        "keywords": "Implicit Neural Representations, Adaptive Activation Function Learning, MLPs",
        "abstract": "Implicit Neural Representations (INRs) provide a continuous function learning framework for discrete signal representations. Using positional embeddings and / or specialized activation functions, INRs have overcome many limitations of traditional discrete representations. However, existing work primarily focuses on the use of a single activation function throughout the network, which often requires an exhaustive search for optimal activation parameters tailored to each signal and INR application. We hypothesize that this approach may restrict the representation power and generalization capabilities of INRs; limiting their broader applicability. In this paper, we introduce AINR, a method that adaptively learns the most suitable activation functions for INRs from a predefined dictionary. This dictionary includes activation functions such as Raised Cosines (RC), Root Raised Cosines (RRC), Prolate Spheroidal Wave Function (PSWF), Sinc, Gabor Wavelet, Gaussian, and Sinusoidal. Our method identifies the activation atom that is mostly matched for each layer of the INR based on the given signal. Experimental results demonstrate that AINR not only significantly improves INR performance across various tasks, such as image representation, image inpainting, 3D shape representation, novel view synthesis, super resolution, and reliable edge detection, but also eliminates the need for the previously required exhaustive search for activation parameters, which had to be conducted even before INR training could begin."
    },
    {
        "title": "When Can Transformers Count to n?",
        "link_suffix": "/forum?id=WULjblaCoc",
        "link": "https://openreview.net/forum?id=WULjblaCoc",
        "pdf_link": "https://openreview.net/pdf?id=WULjblaCoc",
        "keywords": "Transformers, theory, counting, attention",
        "abstract": "Large language models based on the transformer architectures can solve highly complex tasks. But are there simple tasks that such models cannot solve? Here we focus on very simple counting tasks, that involve counting how many times a token in the vocabulary have appeared in a string. We show that if the dimension of the transformer state is linear in the context length, this task can be solved. However, the solution we propose does not scale beyond this limit, and we provide theoretical arguments for why it is likely impossible for a size limited transformer to implement this task. Our empirical results demonstrate the same phase-transition in performance, as anticipated by the theoretical argument. Our results demonstrate the importance of understanding how transformers can solve simple tasks."
    },
    {
        "title": "Tackling the Abstraction and Reasoning Corpus with Vision Transformers: the Importance of 2D Representation, Positions, and Objects",
        "link_suffix": "/forum?id=0gOQeSHNX1",
        "link": "https://openreview.net/forum?id=0gOQeSHNX1",
        "pdf_link": "https://openreview.net/pdf?id=0gOQeSHNX1",
        "keywords": "Abstraction and Reasoning Corpus, Abstract Visual Reasoning, Transformers, Vision Transformers",
        "abstract": "The Abstraction and Reasoning Corpus (ARC) is a popular benchmark focused onvisual reasoningin the evaluation of Artificial Intelligence systems. In its original framing, an ARC task requires solving a program synthesis problem over small 2D images using a few input-output training pairs. In this work, we adopt the recently populardata-drivenapproach to the ARC and ask whether a Vision Transformer (ViT) can learn the implicit mapping, from input image to output image, that underlies the task.  We show that a ViT\u2014otherwise a state-of-the-art model for images\u2014fails dramatically on most ARC tasks even when trained on one million examples per task. This points to an inherent representational deficiency of the ViT architecture that makes it incapable of uncovering the simple structured mappings underlying the ARC tasks. Building on these insights, we propose ViTARC, a ViT-style architecture that unlocks some of the visual reasoning capabilities required by the ARC.  Specifically, we use a pixel-level input representation, design a spatially-aware tokenization scheme, and introduce a novel object-based positional encoding that leverages automatic segmentation, among other enhancements. Our task-specific ViTARC models achieve a test solve rate close to 100% on more than half of the 400 public ARC tasks strictly through supervised learning from input-output grids. This calls attention to the importance of imbuing the powerful (Vision) Transformer with the correct inductive biases for abstract visual reasoning that are critical even when the training data is plentiful and the mapping is noise-free. Hence, ViTARC provides a strong foundation for future research in visual reasoning using transformer-based architectures."
    },
    {
        "title": "Benchmarking Survival Models: Treatment Effects, Bias, and Equity",
        "link_suffix": "/forum?id=aoW5Sm8Op8",
        "link": "https://openreview.net/forum?id=aoW5Sm8Op8",
        "pdf_link": "https://openreview.net/pdf?id=aoW5Sm8Op8",
        "keywords": "benchmarks, survival analysis, time to event, fairness, health equity, responsible ai, heterogeneous treatment effects",
        "abstract": "Survival models are widely used to model time-to-event or survival data, which represents the duration until an event of interest occurs. In clinical research, survival analysis is used for estimating the effects of treatments on patient health outcomes. Recent advancements in machine learning (ML) have aimed to improve survival analysis methods, but current evaluation practices largely focus on predictive performance, often neglecting critical factors such as the ability to accurately estimate treatment effects and possible consequences on health equity. Estimating treatment effects from time-to-event data presents unique challenges due to the complex problem setting, the extensive assumptions required for causal inference, biased observational data, and the ethical consequences of using model outcomes in real-world health decisions. In this work, we introduce a comprehensive benchmarking framework designed to evaluate survival models on their ability to estimate treatment effects under realistic conditions and in the presence of potential inequalities. We formalize the discussion of bias in survival modelling, identifying key sources of inequity, and outline practical desiderata for methods that model time-to-event treatment effects. We clarify common assumptions in survival analysis, discuss critical shortcomings in current evaluation practices, and propose a new benchmarking metric that can be used to better evaluate model calibration. Using this framework, we systematically compare traditional and modern survival models across multiple synthetic and real world datasets, investigating, among other challenges, model performance under mis-specification and observational biases. Through this benchmark, we provide actionable insights for researchers to develop more robust and equitable survival models."
    },
    {
        "title": "Sort-free Gaussian Splatting via Weighted Sum Rendering",
        "link_suffix": "/forum?id=y8uPsxR8PN",
        "link": "https://openreview.net/forum?id=y8uPsxR8PN",
        "pdf_link": "https://openreview.net/pdf?id=y8uPsxR8PN",
        "keywords": "Sort-free, Gaussian Splatting, Weighted Sum Rendering",
        "abstract": "Recently, 3D Gaussian Splatting (3DGS) has emerged as a significant advancement in 3D scene reconstruction, attracting considerable attention due to its ability to recover high-fidelity details while maintaining low complexity. Despite the promising results achieved by 3DGS, its rendering performance is constrained by its dependence on costly non-commutative alpha-blending operations. These operations mandate complex view dependent sorting operations that introduce computational overhead, especially on the resource-constrained platforms such as mobile phones. In this paper, we propose Weighted Sum Rendering, which approximates alpha blending with weighted sums, thereby removing the need for sorting. This simplifies implementation, delivers superior performance, and eliminates the ``popping'' artifacts caused by sorting. Experimental results show that optimizing a generalized Gaussian splatting formulation to the new differentiable rendering yields competitive image quality. The method was implemented and tested in a mobile device GPU, achieving on average $1.23\\times$ faster rendering."
    }
]