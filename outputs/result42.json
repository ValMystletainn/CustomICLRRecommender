[
    {
        "title": "Unmasking Trees for Tabular Data",
        "link_suffix": "/forum?id=TWmETQLZIC",
        "link": "https://openreview.net/forum?id=TWmETQLZIC",
        "pdf_link": "https://openreview.net/pdf?id=TWmETQLZIC",
        "keywords": "tabular data, imputation, missing, generative modeling, tabular, probabilistic prediction, tabular ML",
        "abstract": "Despite much work on advanced deep learning and generative modeling techniques for tabular data generation and imputation, traditional methods have continued to win on imputation benchmarks. We herein present UnmaskingTrees, a simple method for tabular imputation (and generation) employing gradient-boosted decision trees which are used to incrementally unmask individual features. This approach offers state-of-the-art performance on imputation, and on generation given training data with missingness; and it has competitive performance on vanilla generation. To solve the conditional generation subproblem, we propose a tabular probabilistic prediction method, BaltoBot, which fits a balanced tree of boosted tree classifiers. Unlike older methods, it requires no parametric assumption on the conditional distribution, accommodating features with multimodal distributions; unlike newer diffusion methods, it offers fast sampling, closed-form density estimation, and flexible handling of discrete variables. We finally consider our two approaches as meta-algorithms, demonstrating in-context learning-based generative modeling with TabPFN."
    },
    {
        "title": "Token-Supervised Value Models for Enhancing Mathematical Problem-Solving Capabilities of Large Language Models",
        "link_suffix": "/forum?id=6HcnC3pPkp",
        "link": "https://openreview.net/forum?id=6HcnC3pPkp",
        "pdf_link": "https://openreview.net/pdf?id=6HcnC3pPkp",
        "keywords": "Large Language Models, Mathematical Problem-Solving, Verifiers",
        "abstract": "With the rapid advancement of test-time compute search strategies to improve the mathematical problem-solving capabilities of large language models (LLMs), the need for building robust verifiers has become increasingly important. However, all these inference strategies rely on existing verifiers originally designed for Best-of-N search, which makes them sub-optimal for tree search techniques at test time. During tree search, existing verifiers can only offer indirect and implicit assessments of partial solutions or under-value prospective intermediate steps, thus resulting in the premature pruning of promising intermediate steps. To overcome these limitations, we propose token-supervised value models (TVMs) -- a new class of verifiers that assign each token a probability that reflects the likelihood of reaching the correct final answer. This new token-level supervision enables TVMs to directly and explicitly evaluate partial solutions, effectively distinguishing between promising and incorrect intermediate steps during tree search at test time. Experimental results demonstrate that combining tree-search-based inference strategies with TVMs significantly improves the accuracy of LLMs in mathematical problem-solving tasks, surpassing the performance of existing verifiers."
    },
    {
        "title": "Few-shot Style-Conditioned LLM Text Generation via Latent Interpolation",
        "link_suffix": "/forum?id=kVcEiWtld9",
        "link": "https://openreview.net/forum?id=kVcEiWtld9",
        "pdf_link": "https://openreview.net/pdf?id=kVcEiWtld9",
        "keywords": "style-conditioned text generation, few-shot, transfer learning, style representation",
        "abstract": "We propose a novel, model-agnostic approach for adapting large language models (LLMs)  in a few-shot manner to arbitrary styles using text samples from a given author. Rather than use predefined features, our method defines style in terms of LLM model weights and uses a variational autoencoder (VAE) to construct a latent space of these weights, allowing for a generic style representation. Our approach leverages interpolation in this latent embedding space of model weights to generate novel fine-tuned models for low-resource authors. We evaluate this approach compared to reported results, finetuning, and prompting across three datasets. Results indicate that our method outperforms our baselines in low-resource settings."
    },
    {
        "title": "ImProver: Agent-Based Automated Proof Optimization",
        "link_suffix": "/forum?id=dWsdJAXjQD",
        "link": "https://openreview.net/forum?id=dWsdJAXjQD",
        "pdf_link": "https://openreview.net/pdf?id=dWsdJAXjQD",
        "keywords": "Automated Proof Optimization, Neural Theorem Proving, Formal Mathematics, Lean Theorem Prover, Proof Generation, Large Language Models, Symbolic Reasoning, Interactive Theorem Proving",
        "abstract": "Large language models (LLMs) have been used to generate formal proofs of mathematical theorems in proofs assistants such as Lean. However, we often want to optimize a formal proof with respect to various criteria, depending on its downstream use. For example, we may want a proof to adhere to a certain style, or to be readable, concise, or modularly structured. Having suitably optimized proofs is also important for learning tasks, especially since human-written proofs may not optimal for that purpose. To this end, we study a new problem of automated proof optimization: rewriting a proof so that it is correct and optimizes for an arbitrary criterion, such as length or readability. As a first method for automated proof optimization, we present ImProver, a large-language-model agent that rewrites proofs to optimize arbitrary user-defined metrics in Lean. We find that naively applying LLMs to proof optimization falls short, and we incorporate various improvements into ImProver, such as the use of symbolic Lean context in a novel Chain-of-States technique, as well as error-correction and retrieval. We test ImProver on rewriting real-world undergraduate, competition, and research-level mathematics theorems, finding that ImProver is capable of rewriting proofs so that they are substantially shorter, more modular, and more readable."
    },
    {
        "title": "Angle-DFQ: Angle aware data free quantization",
        "link_suffix": "/forum?id=orG37FHN4b",
        "link": "https://openreview.net/forum?id=orG37FHN4b",
        "pdf_link": "https://openreview.net/pdf?id=orG37FHN4b",
        "keywords": "Data free quantization, Computer vision",
        "abstract": "Data free quantization of neural networks is a practical necessity as access to training data in many situations is restricted due to privacy, proprietary concerns, or memory issues. We present a data free weight rounding algorithm for Deep Neural Networks (DNNs) that does not require any training data, synthetic data generation, fine-tuning, or even batch norm statistics. Instead, our approach focuses on preserving the direction of weight vectors during quantization. We demonstrate that traditional weight rounding techniques, that round weights to the nearest quantized level, can result in large angles between the full-precision weight vectors and the quantized weight vectors, particularly under coarse quantization regimes. For a large class of high-dimensional weight vectors in DNNs, this angle error can approach 90 degrees. By minimizing this angle error, we significantly improve top-1 accuracy in quantized DNNs. We analytically derive the angle-minimizing rounding boundaries for ternary quantization under the assumption of Gaussian weights. Building on this, we propose a greedy data-free quantization method based on the cosine similarity between the full-precision weight vectors and the quantized weight vectors. Our approach consistently outperforms existing state-of-the-art data-free quantization techniques and, in several cases, surpasses even data-dependent methods on well-established models such as ResNet-18, VGG-16, and AlexNet with aggressive quantization levels of 3 to 6 bits on the ImageNet dataset."
    },
    {
        "title": "MTEEG: A Multi-Task Learning Framework for Enhanced Electroencephalography Analysis Using Low-Rank Adaptation",
        "link_suffix": "/forum?id=V5lBNcD65H",
        "link": "https://openreview.net/forum?id=V5lBNcD65H",
        "pdf_link": "https://openreview.net/pdf?id=V5lBNcD65H",
        "keywords": "EEG, brain-computer interface, multi-task learning",
        "abstract": "Electroencephalography (EEG) analysis using deep learning has traditionally placed a strong emphasis on models that are custom-built and optimized for specific datasets. Several recent research utilize self-supervised learning to extract generic representations from massive amounts of unlabeled EEG data. The pre-trained models are then fine-tuned on each downstream dataset independently, demonstrating promising results. However, in practical applications involving multiple tasks, utilizing a separate model for each is not ideal regarding computational and spatial cost. In this study, we go one step further and explore the simultaneous adaptation of a pre-trained model to multiple different tasks. The EEG signals exhibit significant heterogeneity due to their collection from various subjects using diverse devices and experimental setups, resulting in potential conflicts among different tasks that impede joint optimization. To tackle this challenge, we propose MTEEG, a multi-task EEG recognition framework which incorporates a task-agnostic temporal encoder and task-specific low-rank adaptation modules to disentangle the parameter space, facilitating both task interaction and specification. Experiments show that MTEEG surpasses other multi-task methods and performs on par with state-of-the-art single-task methods on abnormal detection, event type classification, emotion recognition, seizure detection, sleep stage classification and motor imagery classification after being tuned jointly on six publicly available datasets. MTEEG shows the potential of multi-task EEG recognition and promotes the development of general-purpose brain-computer interfaces in the future. The source code will be released."
    },
    {
        "title": "Learning-Guided Rolling Horizon Optimization for Long-Horizon Flexible Job-Shop Scheduling",
        "link_suffix": "/forum?id=Aly68Y5Es0",
        "link": "https://openreview.net/forum?id=Aly68Y5Es0",
        "pdf_link": "https://openreview.net/pdf?id=Aly68Y5Es0",
        "keywords": "Learning-Guided Optimization, Rolling Horizon Optimization, Flexible Job Shop Scheduling",
        "abstract": "Long-horizon combinatorial optimization problems, such as the Flexible Job-Shop Scheduling Problem (FJSP), often involve complex, interdependent decisions over extended time frames, posing significant challenges for existing solvers. While Rolling Horizon Optimization (RHO) addresses this by decomposing problems into overlapping shorter-horizon subproblems, such overlap often leads to redundant computations. In this paper, we present L-RHO, the first learning-guided RHO framework for long-horizon FJSP. L-RHO employs a customized attention-based model to intelligently fix variables that in hindsight did not need to be re-optimized, resulting in smaller and thus easier-to-solve subproblems. For FJSP, this means identifying operations with unchanged machine assignments between two consecutive subproblems. Empirically, L-RHO accelerates RHO by up to 54% while showing significantly improved solution quality, enabling it to outperform other heuristic and learning-based baselines. We also provide in-depth discussions and verify the desirable adaptability and generalization of L-RHO across various FJSP settings, distributions, and online scenarios. Moreover, we provide a theoretical analysis to elucidate the conditions under which learning is beneficial."
    },
    {
        "title": "PtychoFormer: A Transformer-based Model for Ptychographic Phase Retrieval",
        "link_suffix": "/forum?id=9Qptgv0Eyw",
        "link": "https://openreview.net/forum?id=9Qptgv0Eyw",
        "pdf_link": "https://openreview.net/pdf?id=9Qptgv0Eyw",
        "keywords": "Deep Learning, Transformer, Ptychography, Diffractive Imaging",
        "abstract": "Ptychography is a computational method of microscopy that recovers high-resolution transmission images of samples from a series of diffraction patterns. While conventional phase retrieval algorithms can iteratively recover the images, they require oversampled diffraction patterns, incur significant computational costs, and struggle to recover the absolute phase of the sample's transmission function. Deep learning algorithms for ptychography are a promising approach to resolving the limitations of iterative algorithms. We present PtychoFormer, a hierarchical transformer-based model for data-driven single-shot ptychographic phase retrieval. PtychoFormer processes subsets of diffraction patterns, generating local inferences that are seamlessly stitched together to produce a high-quality reconstruction. Our model exhibits tolerance to sparsely scanned diffraction patterns and achieves up to 3600 times faster imaging speed than the extended ptychographic iterative engine (ePIE). We also propose the extended-PtychoFormer (ePF), a hybrid approach that combines the benefits of PtychoFormer with the ePIE. ePF minimizes global phase shifts and significantly enhances reconstruction quality, achieving state-of-the-art phase retrieval in ptychography."
    },
    {
        "title": "Plug-and-Play Controllable Generation for Discrete Masked Models",
        "link_suffix": "/forum?id=4hFT4rfG40",
        "link": "https://openreview.net/forum?id=4hFT4rfG40",
        "pdf_link": "https://openreview.net/pdf?id=4hFT4rfG40",
        "keywords": "Discrete Masked Models, Controllable Generation, Plug-and-play",
        "abstract": "This article makes discrete masked models for the generative modeling of discrete data controllable. The goal is to generate samples of a discrete random variable that adheres to a posterior distribution, satisfies specific constraints, or optimizes a reward function. This methodological development enables broad applications across downstream tasks such as class-specific image generation and protein design. Existing approaches for controllable generation of masked models typically rely on task-specific fine-tuning or additional modifications, which can be inefficient and resource-intensive. To overcome these limitations, we propose a novel plug-and-play framework based on importance sampling that bypasses the need for training a conditional score. Our framework is agnostic to the choice of control criteria, requires no gradient information, and is well-suited for tasks such as posterior sampling, Bayesian inverse problems, and constrained generation. We demonstrate the effectiveness of our approach through extensive experiments, showcasing its versatility across multiple domains, including protein design."
    },
    {
        "title": "Latent-Predictive Empowerment: Measuring Empowerment without a Simulator",
        "link_suffix": "/forum?id=dVrYcscgLu",
        "link": "https://openreview.net/forum?id=dVrYcscgLu",
        "pdf_link": "https://openreview.net/pdf?id=dVrYcscgLu",
        "keywords": "Empowerment, Unsupervised Skill Learning, Unsupervised Reinforcement Learning, Self-supervised Reinforcement Learning",
        "abstract": "Empowerment has the potential to help agents learn large skillsets, but is not yet a scalable solution for training general-purpose agents.  Recent empowerment methods learn large skillsets by maximizing the mutual information between skills and states, but these approaches require a model of the transition dynamics, which can be challenging to learn in realistic settings with high-dimensional and stochastic observations.  We present an algorithm, Latent-Predictive Empowerment (LPE), that can compute empowerment in a more scalable manner.   LPE learns large skillsets by maximizing an objective that under certain conditions has the same optimal skillset as the mutual information between skills and states, but our objective is more tractable to optimize because it only requires learning a simpler latent-predictive model rather than a full simulator of the environment.   We show empirically in a variety of settings, includes ones with high-dimensional observations and highly stochastic transition dynamics, that our empowerment objective learns similar-sized skillsets as the leading empowerment algorithm, which assumes access to a model of the transition dynamics, and outperforms other model-based approaches to empowerment."
    },
    {
        "title": "Improving Language Model Self-Correction Capability with Meta-Feedback",
        "link_suffix": "/forum?id=jJvJqgPZCD",
        "link": "https://openreview.net/forum?id=jJvJqgPZCD",
        "pdf_link": "https://openreview.net/pdf?id=jJvJqgPZCD",
        "keywords": "Self-Correction, Meta-Feedback, Iterative Refinement, Feedback-on-Feedback (FoF), Natural Language Processing (NLP), Machine Learning, Zero-Shot Learning, Self-Refine, Model Performance Enhancement, Feedback Quality, GSM8K Dataset, MBPP Dataset, CSMT Dataset",
        "abstract": "Large language models (LLMs) are capable of self-correcting their responses by generating feedback and refining the initial output. However, their performance may sometimes decline following self-correction, either because the feedback contains errors or due to unnecessarily attempting to refine an already accurate response. To address these limitations, we investigate whether the same LLM can generate meta-feedback that pinpoints errors in the feedback rather than the response, an ability that remains under-explored despite extensive research on LLMs' self-feedback generation. We design a novel self-correction prompting framework, Feedback-on-Feedback (FoF), which leverages meta-feedback to improve the feedback before refining the response. Our framework first samples multiple pieces of feedback for the initial response, and prompts the LLM to generate meta-feedback that analyzes the inconsistency between these feedback pieces. Based on the meta-feedback, the LLM generates refined feedback that subsequently guides the revision of the response. Our FoF framework consistently outperforms competitive baselines across two LLMs on three datasets, covering arithmetic reasoning, machine translation, and programming tasks. Specifically, FoF improves performance on GSM8K by 3.6 points (45.2% vs. 41.6% for the initial answer) and on MBPP by 6.4 points (51.7% vs. 45.3%) using the LLaMA-3-8B model."
    },
    {
        "title": "Improving Influence-based Instruction Tuning Data Selection for Balanced Learning of Diverse Capabilities",
        "link_suffix": "/forum?id=dCTGFl3lN2",
        "link": "https://openreview.net/forum?id=dCTGFl3lN2",
        "pdf_link": "https://openreview.net/pdf?id=dCTGFl3lN2",
        "keywords": "Instruction Tuning, Data Selection, Influence Estimation",
        "abstract": "Selecting appropriate training data is crucial for successful supervised instruction fine-tuning (SFT), which aims to (1) elicit strong capabilities from pretrained large language models (LLMs), and (2) achieve balanced performance across a diverse range of tasks. Algorithms based on influence estimation have shown promise in achieving (1) through estimating the contribution of each training example to model's prediction on a downstream task, but often struggle with (2). Through systematic experiments, we attribute their underperformance to an inherent bias---certain tasks intrinsically have greater influence than others. Directly comparing influence scores across different tasks would thus bias the selected data towards these tasks, hurting the LM's performance not only on other capabilities, but also, surprisingly, on the tasks for which the selected data has high influence.We propose BIDS, a novel Data Selection algorithm that targets Influential data in a Balanced way, to address this issue. Aiming to address the biased influence, BIDS first normalizes influence scores of the training data with respect to each downstream task at an instance level. BIDS then applies an iterative optimization process to further balance the selection of influential training data. At each step, BIDS selects the training example that bears the highest influence on the most underrepresented capability by the currently selected data. Experimental results demonstrate that BIDS consistently outperforms state-of-the-art influence-based data selection algorithms under various budgets. Remarkably, training on a 15% subset by BIDS can even outperform full-dataset training with a much more balanced distribution of downstream performance. Our analysis further highlights the importance of both instance-level normalization and iterative optimization of selected data for balanced learning of diverse capabilities."
    },
    {
        "title": "Evolving Alignment via Asymmetric Self-Play",
        "link_suffix": "/forum?id=TMYe4rUuTc",
        "link": "https://openreview.net/forum?id=TMYe4rUuTc",
        "pdf_link": "https://openreview.net/pdf?id=TMYe4rUuTc",
        "keywords": "large language model, RLHF, open-ended learning, alignment",
        "abstract": "Current RLHF approaches for aligning large language models (LLMs) typically assume a fixed prompt distribution, which is sub-optimal and limits the generalization capabilities for language models. To address this issue, we introduce a general framework that casts alignment as an asymmetric game between two players:  (i) a creator, which strategically generates informative prompt distributions using reward signals, and (ii) a solver, which learns to produce preferred responses on prompts produced by the creator.This framework of Evolving Alignment via Asymmetric Self-Play (eva), results in a simple and efficient approach that can utilize any existing RLHF algorithm.evaachieves a new state of the art in widely adopted alignment benchmarks, without the need of any additional human crafted prompts, e.g., it can improve the win rate of finetuned gemma-2-9b-it on Arena-Hard from 51.6% to 60.1% with DPO, from 55.7% to 58.9% with SPPO, from 52.3% to 60.7% with SimPO, and from 54.8% to 60.3% with ORPO, surpassing its 27B version and matching Claude-3-opus. Finally, we showevais effective and robust under various ablation settings.We hopeevacan serve as a scalable methodology for the research community to build open-ended, robust, and self-improving language agents, that align with human values."
    },
    {
        "title": "OTTC: A differentiable alignment approach to automatic speech recognition",
        "link_suffix": "/forum?id=EMpvfnzQqD",
        "link": "https://openreview.net/forum?id=EMpvfnzQqD",
        "pdf_link": "https://openreview.net/pdf?id=EMpvfnzQqD",
        "keywords": "ASR, Optimal Transport, Sequence to Sequence, Alignment",
        "abstract": "The Connectionist Temporal Classification (CTC) and transducer-based models are widely used for end-to-end (E2E) automatic speech recognition (ASR). These methods maximize the marginal probability over all valid alignments within the probability lattice over the vocabulary during training. However, research has shown that most alignments are highly improbable, with the model often concentrating on a limited set, undermining the purpose of considering all possible alignments. In this paper, we propose a novel differentiable alignment framework based on a one-dimensional optimal transport formulation, enabling the model to learn a single alignment and perform ASR in an E2E manner.\nWe define a pseudo-metric, called Sequence Optimal Transport Distance (SOTD), over the sequence space and highlight its theoretical properties.\nBased on the SOTD, we propose Optimal Temporal Transport Classification (OTTC) loss for ASR and contrast its behavior with that of CTC.\nExperimental results on the English Librispeech and AMI datasets demonstrate that our method achieves competitive performance compared to CTC in ASR.\nWe believe this work opens up a potential new direction for research in ASR, offering a foundation for the community to further explore and build upon."
    },
    {
        "title": "Higher Order Transformers: Efficient Attention Mechanism for Tensor Structured Data",
        "link_suffix": "/forum?id=MxGGdhDmv5",
        "link": "https://openreview.net/forum?id=MxGGdhDmv5",
        "pdf_link": "https://openreview.net/pdf?id=MxGGdhDmv5",
        "keywords": "transformers, multihead attention, high order tensor, kronecker decomposition, multivariate timeseries forecasting, 3D medical image classification",
        "abstract": "Transformers are now ubiquitous for sequence modeling tasks, but their extension to multi-dimensional data remains a challenge due to the quadratic cost of the attention mechanism.  In this paper, we propose Higher-Order Transformers (HOT), a novel architecture designed to efficiently process data with more than two axes, i.e. higher-order tensors. \nTo address the computational challenges associated with high-order tensor attention, we introduce a novel Kronecker factorized attention mechanism that reduces the attention cost to quadratic in each axis' dimension, rather than quadratic in the total size of the input tensor. To further enhance efficiency, HOT leverages kernelized attention, reducing the complexity to linear. This strategy maintains the model's expressiveness while enabling scalable attention computation.\nWe validate the effectiveness of HOT on two high-dimensional tasks, including multivariate time series forecasting, and 3D medical image classification. Experimental results demonstrate that HOT achieves competitive performance while significantly improving computational efficiency, showcasing its potential for tackling a wide range of complex, multi-dimensional data."
    },
    {
        "title": "Bias-Augmented Consistency Training Reduces Biased Reasoning in Chain-of-Thought",
        "link_suffix": "/forum?id=zGvwENuzPU",
        "link": "https://openreview.net/forum?id=zGvwENuzPU",
        "pdf_link": "https://openreview.net/pdf?id=zGvwENuzPU",
        "keywords": "Chain-of-thought prompting, Explainability, Generalization, Reasoning, Bias",
        "abstract": "While chain-of-thought prompting (CoT) has the potential to improve the explainability of language model reasoning, it can systematically misrepresent the factors influencing models' behavior--for example, rationalizing answers in line with a user's opinion without mentioning this bias. To mitigate this biased reasoning problem, we introduce bias-augmented consistency training (BCT), an unsupervised fine-tuning scheme that trains models to give consistent reasoning across prompts with and without biasing features. We construct a suite testing nine forms of biased reasoning on seven question-answering tasks, and find that applying BCT to GPT-3.5-Turbo with one bias reduces the rate of biased reasoning by 86% on held-out tasks. Moreover, this model generalizes to other forms of bias, reducing biased reasoning on held-out biases by an average of 37%. As BCT generalizes to held-out biases and does not require gold labels, this method may hold promise for reducing biased reasoning from as-of-yet unknown biases and on tasks where ground truth reasoning is unavailable."
    },
    {
        "title": "Humanizing the Machine: Proxy Attacks to Mislead LLM Detectors",
        "link_suffix": "/forum?id=PIpGN5Ko3v",
        "link": "https://openreview.net/forum?id=PIpGN5Ko3v",
        "pdf_link": "https://openreview.net/pdf?id=PIpGN5Ko3v",
        "keywords": "machine-generted text detection; evade detection; fine-tuning",
        "abstract": "The advent of large language models (LLMs) has revolutionized the field of text generation, producing outputs that closely mimic human-like writing. Although academic and industrial institutions have developed detectors to prevent the malicious usage of LLM-generated texts, other research has doubt about the robustness of these systems. To stress test these detectors, we introduce a proxy-attack strategy that effortlessly compromises LLMs, causing them to produce outputs that align with human-written text and mislead detection systems. Our method attacks the source model by leveraging a reinforcement learning (RL) fine-tuned humanized small language model (SLM) in the decoding phase. Through an in-depth analysis, we demonstrate that our attack strategy is capable of generating responses that are indistinguishable to detectors, preventing them from differentiating between machine-generated and human-written text. We conduct systematic evaluations on extensive datasets using proxy-attacked open-source models, including Llama2-13B, Llama3-70B, and Mixtral-8*7B in both white- and black-box settings. Our findings show that the proxy-attack strategy effectively deceives the leading detectors, resulting in an average AUROC drop of 70.4% across multiple datasets, with a maximum drop of 90.3% on a single dataset. Furthermore, in cross-discipline scenarios, our strategy also bypasses these detectors, leading to a significant relative decrease of up to 90.9%, while in cross-language scenario, the drop reaches 91.3%. Despite our proxy-attack strategy successfully bypassing the detectors with such significant relative drops, we find that the generation quality of the attacked models remains preserved, even within a modest utility budget, when compared to the text produced by the original, unattacked source model."
    },
    {
        "title": "Online Detecting LLM-Generated Texts via Sequential Hypothesis Testing by Betting",
        "link_suffix": "/forum?id=WbqBj2aC5k",
        "link": "https://openreview.net/forum?id=WbqBj2aC5k",
        "pdf_link": "https://openreview.net/pdf?id=WbqBj2aC5k",
        "keywords": "Large Language Models (LLMs), Machine-Generated Text, Text Generation Detection, Sequential Hypothesis Testing, Online Optimization",
        "abstract": "Developing algorithms to differentiate between machine-generated texts and human-written texts has garnered substantial attention in recent years. Existing methods in this direction typically concern an offline setting where a dataset containing a mix of real and machine-generated texts is given upfront, and the task is to determine whether each sample in the dataset is from a large language model (LLM) or a human. However, in many practical scenarios, sources such as news websites, social media accounts, or on other forums publish content in a streaming fashion. Therefore, in this online scenario, how to quickly and accurately determine whether the source is an LLM with strong statistical guarantees is crucial for these media or platforms to function effectively and prevent the spread of misinformation and other potential misuse of LLMs. To tackle the problem of online detection, we develop an algorithm based on the techniques of sequential hypothesis testing by betting that not only builds upon and complements existing offline detection techniques but also enjoys statistical guarantees, which include a controlled false positive rate and the expected time to correctly identify a source as an LLM. Experiments were conducted to demonstrate the effectiveness of our method."
    },
    {
        "title": "Large Language Models as Realistic Microservice Trace Generators",
        "link_suffix": "/forum?id=f9GURUHZQo",
        "link": "https://openreview.net/forum?id=f9GURUHZQo",
        "pdf_link": "https://openreview.net/pdf?id=f9GURUHZQo",
        "keywords": "synthetic data, synthetic trace, microservice, large language model, machine learning for systems",
        "abstract": "Computer system workload traces, which record hardware or software events during application execution, are essential for understanding the behavior of complex systems and managing their processing and memory resources. However, obtaining real-world traces can be challenging due to the significant collection overheads in performance and privacy concerns that arise in proprietary systems. As a result, synthetic trace generation is considered a promising alternative to using traces collected in real-world production deployments. This paper proposes to train a large language model (LLM) to generate synthetic workload traces, specifically microservice call graphs. To capture complex and arbitrary hierarchical structures and implicit constraints in such traces, we fine-tune LLMs to generate each layer recursively, making call graph generation a sequence of easier steps. To further enforce learning constraints in traces and generate uncommon situations, we apply additional instruction tuning steps to align our model with the desired trace features. Our evaluation results show that our model can generate diverse realistic traces under various conditions and outperform existing methods in accuracy and validity. We show that our synthetically generated traces can effectively substitute real-world data in optimizing or tuning systems management tasks. We also show that our model can be adapted to perform key downstream trace-related tasks, specifically, predicting key trace features and infilling missing data given partial traces."
    },
    {
        "title": "Probe Pruning: Accelerating LLMs through Dynamic Pruning via Model-Probing",
        "link_suffix": "/forum?id=WOt1owGfuN",
        "link": "https://openreview.net/forum?id=WOt1owGfuN",
        "pdf_link": "https://openreview.net/pdf?id=WOt1owGfuN",
        "keywords": "Large Lanuage Model Pruning, Probe Pruning",
        "abstract": "We introduce Probe Pruning (PP), a novel framework for online, dynamic, structured pruning of Large Language Models (LLMs) applied in a batch-wise manner. PP leverages the insight that not all samples and tokens contribute equally to the model's output, and probing a small portion of each batch effectively identifies crucial weights, enabling tailored dynamic pruning for different batches. It comprises three main stages: probing, history-informed pruning, and full inference. In the probing stage, PP selects a small yet crucial set of hidden states, based on residual importance, to run a few model layers ahead. During the history-informed pruning stage, PP strategically integrates the probing states with historical states. Subsequently, it structurally prunes weights based on the integrated states and the PP importance score, a metric developed specifically to assess the importance of each weight channel in maintaining performance. In the final stage, full inference is conducted on the remaining weights. A major advantage of PP is its compatibility with existing models, as it operates without requiring additional neural network modules or fine-tuning. Comprehensive evaluations of PP on LLaMA-2/3 and OPT models reveal that even minimal probing—using just 1.5% of FLOPs—can substantially enhance the efficiency of structured pruning of LLMs. For instance, when evaluated on LLaMA-2-7B with WikiText2, PP achieves a 2.56 times lower ratio of performance degradation per unit of latency reduction compared to the state-of-the-art method at a 40% pruning ratio."
    },
    {
        "title": "Bias Learning: Quantifying and Mitigating Position Sensitivity in Text Embeddings",
        "link_suffix": "/forum?id=4GD7a9Bo9A",
        "link": "https://openreview.net/forum?id=4GD7a9Bo9A",
        "pdf_link": "https://openreview.net/pdf?id=4GD7a9Bo9A",
        "keywords": "Deep Learning or Neural Networks, Similarity and Distance Learning, (Application) Information Retrieval Regression, (Cognitive/Neuroscience) Language, (Other) Statistics",
        "abstract": "Embedding models are crucial for tasks in Information Retrieval (IR) and semantic similarity measurement, yet their handling of longer texts and associated positional biases remains underexplored. In this study, we investigate the impact of content position and input size on text embeddings. Our experiments reveal that embedding models, particularly APE- and RoPE-based models, disproportionately prioritize the initial portion of the input. Ablation studies demonstrate that insertion of irrelevant text or removal at the start of a document reduces cosine similarity between altered and original embeddings by up to 12.3% more than ablations at the end. Regression analysis further confirms this bias, with sentence importance declining as position moves further from the start, even with with content-agnosticity. We hypothesize that this effect arises from pre-processing strategies and chosen positional encoding techniques.  To address this, we introduce a novel data augmentation scheme called Position-Aware Data Sampling (PADS), which mitigates positional bias and improves embedding robustness across varying input lengths. These findings quantify the sensitivity of retrieval systems and suggest a new lens towards long-context embedding models."
    },
    {
        "title": "MaestroMotif: Skill Design from Artificial Intelligence Feedback",
        "link_suffix": "/forum?id=or8mMhmyRV",
        "link": "https://openreview.net/forum?id=or8mMhmyRV",
        "pdf_link": "https://openreview.net/pdf?id=or8mMhmyRV",
        "keywords": "Hierarchical RL, Reinforcement Learning, LLMs",
        "abstract": "Describing skills in natural language has the potential to provide an accessible way to inject human knowledge about decision-making into an AI system. We present MaestroMotif, a method for AI-assisted skill design, which yields high-performing and adaptable agents. MaestroMotif leverages the capabilities of Large Language Models (LLMs) to effectively create and reuse skills. It first uses an LLM's feedback to automatically design rewards corresponding to each skill, starting from their natural language description. Then, it employs an LLM's code generation abilities, together with reinforcement learning, for training the skills and combining them to implement complex behaviors specified in language. We evaluate MaestroMotif using a suite of complex tasks in the NetHack Learning Environment (NLE), demonstrating that it surpasses existing approaches in both performance and usability."
    },
    {
        "title": "Influencing Humans to Conform to Preference Models for RLHF",
        "link_suffix": "/forum?id=mDEYl0Ucgr",
        "link": "https://openreview.net/forum?id=mDEYl0Ucgr",
        "pdf_link": "https://openreview.net/pdf?id=mDEYl0Ucgr",
        "keywords": "reinforcement learning from human feedback, reinforcement learning, reward functions, preferences, regret, alignment",
        "abstract": "Designing a reinforcement learning from human feedback (RLHF) algorithm for learning from preferences requires assuming a preference model, sometimes implicitly.  A preference model that poorly describes how humans generate preferences risks learning a poor approximation of the human’s unobservable reward function. In this paper, we conduct three human studies to assess whether one can influence the expression of real human preferences to more closely conform to a desired preference model. Importantly, our approach does not seek to alter the human's unobserved reward function. Rather, we change how humans use this reward function to generate preferences, such that they better match whatever preference model is assumed by a particular RLHF algorithm. We introduce three interventions: showing humans the quantities that underlie a preference model, which is normally unobservable information derived from the reward function; training people to follow a specific preference model; and modifying the preference elicitation question. All intervention types show significant effects, providing practical tools to improve preference data quality and the resultant alignment of learned reward functions.Overall we establish a novel research direction in model alignment: training humans and designing interfaces to increase human conformance with the assumptions of the algorithm that will learn from their input."
    },
    {
        "title": "Combating the Generalization-Forgetting Trade-off in Continual Learning: A Cautious Passive Low-Rank Approach",
        "link_suffix": "/forum?id=gV0Moskp7k",
        "link": "https://openreview.net/forum?id=gV0Moskp7k",
        "pdf_link": "https://openreview.net/pdf?id=gV0Moskp7k",
        "keywords": "continual learning, LLMs",
        "abstract": "Large Language Models (LLMs) have shown remarkable capabilities through wide-scale pre-training on a wide range of domains. However, they often suffer from catastrophic forgetting when learning sequential tasks. In this paper, we propose a novel parameter-efficient approach for continual learning in LLMs, which empirically explores the role of different effective layerwise ranks, leveraging lower ranks to mitigate catastrophic forgetting of previous tasks and higher ranks to enhance generalization on new tasks. By employing a subspace similarity metric that evaluates the orthogonality of low-rank subspaces between tasks, we gradually increase the rank of layerwise matrices for each new task, minimizing interference with previously learned tasks while enhancing generalization. Experimental results on standard continual learning benchmarks and challenging math benchmarks demonstrate that our method outperforms existing state-of-the-art approaches, effectively mitigating forgetting, improving task performance, and maintaining strong generalization to unseen tasks in a memory-efficient manner."
    },
    {
        "title": "AgentOccam: A Simple Yet Strong Baseline for LLM-Based Web Agents",
        "link_suffix": "/forum?id=oWdzUpOlkX",
        "link": "https://openreview.net/forum?id=oWdzUpOlkX",
        "pdf_link": "https://openreview.net/pdf?id=oWdzUpOlkX",
        "keywords": "LLM, Agent, LLM-based Agent, Web Agent, Web Navigation",
        "abstract": "Autonomy via agents based on large language models (LLMs) that can carry out personalized yet standardized tasks presents a significant opportunity to drive human efficiency. There is an emerging need and interest in automating web tasks  (e.g., booking a hotel for a given date within a budget). Being a practical use case itself, the web agent also serves as an important proof-of-concept example for various agent grounding scenarios, with its success promising advancements in many future applications. Meanwhile, much prior research focuses on handcrafting their web agent strategies (e.g. agent's prompting templates, reflective workflow, role-play and multi-agent systems, search or sampling methods, etc.) and the corresponding in-context examples. However, these custom strategies often struggle with generalizability across all potential real-world applications. On the other hand, there has been limited study on the misalignment between a web agent's observation and action representation, and the data on which the agent's underlying LLM has been pre-trained. This is especially notable when LLMs are primarily trained for language completion rather than tasks involving embodied navigation actions and symbolic web elements. In our study, we enhance an LLM-based web agent by simply refining its observation and action space, aligning these more closely with the LLM's capabilities. This approach enables our base agent to significantly outperform previous methods on a wide variety of web tasks. Specifically, on WebArena, a benchmark featuring general-purpose web interaction tasks, our agent AgentOccam surpasses the previous state-of-the-art and concurrent work by 9.8 (+29.4%) and 5.9 (+15.8%) absolute points respectively, and boosts the success rate by 26.6 points (+161%) over similar plain web agents with its observation and action space alignment. AgentOccam's simple design highlights the LLMs' impressive zero-shot performance in web tasks, and underlines the critical role of carefully tuning observation and action spaces for LLM-based agents."
    }
]