[
    {
        "title": "GITD: Enhancing Medical Classification on Tabular Data with Missing Values via Graph Modeling",
        "link_suffix": "/forum?id=GSrs4vIqiF",
        "link": "https://openreview.net/forum?id=GSrs4vIqiF",
        "pdf_link": "https://openreview.net/pdf?id=GSrs4vIqiF",
        "keywords": "medical classification, tabular data, imputation",
        "abstract": "With the advancement of machine learning, various techniques have been developed to classify patients for disease diagnosis using medical tabular data. Due to the presence of missing values in the medical tabular data, these techniques commonly impute the missing values before applying classifiers. However, most existing techniques classify patients solely based on each patient's individual features despite the advantages of leveraging patients with similar features that can enhance both imputation and classification. To address this issue, we introduce graph data imputation for tabular data (GITD), a novel approach that constructs feature-attentive k-nearest neighbor (kNN) graphs to enable the use of graph data imputation methods on medical tabular data. The key idea of GITD is constructing a kNN graph among patients by prioritizing important features for classification. Our extensive experimental results demonstrate that GITD successfully bridges graph data imputation methods and medical tabular classification, achieving state-of-the-art performance across various medical tabular datasets."
    },
    {
        "title": "Multi-Head RAG: Solving Multi-Aspect Problems with LLMs",
        "link_suffix": "/forum?id=UJOaE4l3og",
        "link": "https://openreview.net/forum?id=UJOaE4l3og",
        "pdf_link": "https://openreview.net/pdf?id=UJOaE4l3og",
        "keywords": "embedding models, vectors space, retrieval augmented generation",
        "abstract": "Retrieval Augmented Generation (RAG) enhances the abilities of Large Language Models (LLMs) by enabling the retrieval of documents into the LLM context to provide more accurate and relevant responses. Existing RAG solutions do not focus on queries that may require fetching multiple documents with substantially different contents. Such queries occur frequently, but are challenging because the embeddings of these documents may be distant in the embedding space, making it hard to retrieve them all. This paper introduces Multi-Head RAG (MRAG), a novel scheme designed to address this gap with a simple yet powerful idea: leveraging activations of Transformer\u2019s multi-head attention layer, instead of the decoder layer, as keys for fetching multi-aspect documents. The driving motivation is that different attention heads can learn to capture different data aspects. Harnessing the corresponding activations results in embeddings that represent various facets of data items and queries, improving the retrieval accuracy for complex queries. We provide an evaluation methodology and metrics, multi-aspect datasets that we release online, and real-world use cases to demonstrate MRAG\u2019s effectiveness, showing improvements of up to 20% in relevance over standard RAG baselines. MRAG can be seamlessly integrated with existing RAG frameworks and benchmarking tools like RAGAS as well as different classes of data stores."
    },
    {
        "title": "DICE: Data Influence Cascade in Decentralized Learning",
        "link_suffix": "/forum?id=2TIYkqieKw",
        "link": "https://openreview.net/forum?id=2TIYkqieKw",
        "pdf_link": "https://openreview.net/pdf?id=2TIYkqieKw",
        "keywords": "Decentralized Learning, Data Influence",
        "abstract": "Decentralized learning offers a promising approach to crowdsource computational workloads across geographically distributed compute interconnected through peer-to-peer networks, accommodating the exponentially increasing compute demands in the era of large models. However, the absence of proper incentives in locally connected decentralized networks poses significant risks of free riding and malicious behaviors. Data influence, which ensures fair attribution of data source contributions, holds great potential for establishing effective incentive mechanisms. Despite the importance, little effort has been made to analyze data influence in decentralized scenarios, due to non-trivial challenges arising from the distributed nature and the localized connections inherent in decentralized networks. To overcome this fundamental incentive problem, we propose DICE, the first comprehensive framework for analyzing Data Influence CascadEs in decentralized environments. Our framework characterizes how data influence cascades across the communication network and highlights the interplay between original data and network structure in shaping data influence in decentralized learning. We anticipate that DICE can open new avenues for incentive mechanism design and enable impactful applications of influence in decentralized learning, including anomaly detection, collaborator selection and machine unlearning."
    },
    {
        "title": "SHIKI: Self-Supervised Heuristic for Improving MLPs' Knowledge by Integrating GNNs",
        "link_suffix": "/forum?id=kZvkcc4mXi",
        "link": "https://openreview.net/forum?id=kZvkcc4mXi",
        "pdf_link": "https://openreview.net/pdf?id=kZvkcc4mXi",
        "keywords": "GNNs, Self-Supervised Learning, Deep Learning",
        "abstract": "Graph Neural Networks (GNNs) are widely recognized as leading architectures for addressing classification problems involving graphical data. In this thesis, we formally define the challenge of effectively constructing edges within a dataset and training a GNN over this graph and introduce SHIKI - a novel method to tackle this task. We provide a comprehensive theoretical analysis demonstrating how graph convolutions can improve expected performance by leveraging edges. Our study focuses on the node classification problem within a non-linearly separable Gaussian mixture model, combined with a stochastic block model, and we visually demonstrate its applicability to real-world datasets. Specifically, we show that a single graph convolution in the second layer can reduce the expected loss when applying a heuristic for edge creation. We validate our findings through extensive experiments on both synthetic and real-world datasets, including those related to the entity matching problem and textual review classification. For the synthetic data, we conduct experiments based on the dataset's difficulty and various hyperparameters in our method, drawing connections between the two. Additionally, we perform an ablation study by systematically removing components of our method and testing the resulting degraded approach, which highlights the necessity of our full method. We employ several GNN architectures in the experiments, including GCN, GraphSAGE, and GAT."
    },
    {
        "title": "Gene Regulatory Network Inference in the Presence of Selection Bias and Latent Confounders",
        "link_suffix": "/forum?id=G5KbDVAlI6",
        "link": "https://openreview.net/forum?id=G5KbDVAlI6",
        "pdf_link": "https://openreview.net/pdf?id=G5KbDVAlI6",
        "keywords": "gene regulatory network inference, selection bias, latent confounders, causal discovery",
        "abstract": "The study of gene regulatory network inference (GRNI), with a focus on uncovering causal relations among genes, holds significant potential to explain fundamental biological processes, such as how cellular identity is established or disrupted in disease. Unfortunately, current methods fail to adequately interpret the widespread phenomena of differential gene expression. The limitation can largely be attributed to the overlook of the selection process (e.g., survival bias), which is ubiquitous and fundamental in biology. Furthermore, recent studies have shown that gene expression is regulated by latent confounders (e.g., non-coding RNAs). Both of which can lead to spurious dependencies, thereby distorting GRNI results. To mitigate these challenges, we propose a novel algorithm, called  Gene Regulatory Network Inference in the presence of Selection bias and Latent confounders (GISL). It is designed to uncover the causal structure by leveraging data across multiple distributions obtained via gene perturbation. Surprisingly, we find that the qualitative structure information, selection process, and latent confounders are partially identifiable without any parametric assumption under mild graphical conditions. Experimental results on both synthetic and real-world single-cell gene expression datasets demonstrate the superiority of GISL over existing strong baseline methods."
    },
    {
        "title": "SpikeLLM: Scaling up Spiking Neural Network to Large Language Models via Saliency-based Spiking",
        "link_suffix": "/forum?id=ZadnlOHsHv",
        "link": "https://openreview.net/forum?id=ZadnlOHsHv",
        "pdf_link": "https://openreview.net/pdf?id=ZadnlOHsHv",
        "keywords": "spiking neural network, energy efficient language model, brain-inspired computing",
        "abstract": "The recent advancements in large language models (LLMs) with billions of parameters have significantly boosted their performance across various real-world applications. However, the inference processes for these models require substantial energy and computational resources, presenting considerable deployment challenges. In contrast, human brains, which contain approximately 86 billion biological neurons, exhibit significantly greater energy efficiency compared to LLMs with a similar number of parameters. Inspired by this, we redesign 7~70 billion parameter LLMs using bio-plausible spiking mechanisms, emulating the efficient behavior of the human brain. We propose the first spiking large language model termed SpikeLLM. Coupled with the proposed model, two essential approaches are proposed to improve spiking efficiency: Generalized Integrate-and-Fire (GIF) neurons to compress spike length from $T$ to $\\frac{T}{L} \\log_2 L$ bits, and an Optimal Brain Spiking framework to divide outlier channels and allocate different $T$ for GIF neurons, which further compresses spike length to approximate $log_2T$ bits. The necessity of spike-driven LLM is proved by comparison with quantized LLMs with similar operations. In the OmniQuant pipeline, SpikeLLM reduces 24.85% WikiText2 perplexity and improves 2.01% accuracy of common scene reasoning on a LLAMA2-7B 4A4W model. In the GPTQ pipeline, SpikeLLM achieves direct additive in linear layers, significantly exceeding PB-LLMs. In the LLAMA-2-7B, SpikeLLM saves $\\times 12.68$ and $\\times 13.13$ operations with general matrix multiply and event-driven implementations respectively. We will release our code on GitHub."
    },
    {
        "title": "State & Image Guidance: Teaching Old Text-to-Video Diffusion Models New Tricks",
        "link_suffix": "/forum?id=zkGxROm7D3",
        "link": "https://openreview.net/forum?id=zkGxROm7D3",
        "pdf_link": "https://openreview.net/pdf?id=zkGxROm7D3",
        "keywords": "Text-to-Video Generation, Diffusion Models, Diffusion Guidance, Zero-shot Image-to-Video Generation",
        "abstract": "Current text-to-video (T2V) models have made significant progress in generating high-quality video. However, these models are limited when it comes to generating dynamic video scenes where the description per frame can vary dramatically. Changing the color, shape, position and state of objects in the scene is a challenge that current video models cannot handle. In addition, the lack of a cheap image-based conditioning mechanism limits their creative application. To address these challenges and extend the applicability of T2V models, we propose two innovative approaches:State GuidanceandImage Guidance.State Guidanceuses advanced guidance mechanisms to control motion dynamics and scene transformation smoothness by navigating the diffusion process between a state triplet <initial state, transition state, final state>. This mechanism enables the generation of dynamic video scenes (Dynamic Scene T2V) and allows to control the speed and the expressiveness of the scene transformation by introducing temporal dynamics via a guidance weight schedule across video frames.Image Guidanceenables Zero-Shot Image-to-Video generation (Zero-Shot I2V) by injecting reference image into the initial diffusion steps noise predictions. Furthermore, the combination ofState GuidanceandImage Guidanceallows for zero-shot transitions between two input reference frames of a video (Zero-Shot II2V). Finally, we introduce the novelDynamic Scene Benchmarkto evaluate the ability of the models to generate dynamic video scenes. Extensive experiments show thatState GuidanceandImage Guidancesuccessfully address the aforementioned challenges and significantly improve the generation capabilities of existing T2V architectures."
    },
    {
        "title": "UNSURE: Unknown Noise level Stein's Unbiased Risk Estimator",
        "link_suffix": "/forum?id=ScVnYBaSEw",
        "link": "https://openreview.net/forum?id=ScVnYBaSEw",
        "pdf_link": "https://openreview.net/pdf?id=ScVnYBaSEw",
        "keywords": "self-supervised learning, imaging inverse problems",
        "abstract": "Recently, many self-supervised learning methods for image reconstruction have been proposed that can learn from noisy data alone, bypassing the need for ground-truth references.  Most existing methods cluster around two classes: i) Noise2Self and similar cross-validation methods that require very mild knowledge about the noise distribution, and ii) Stein's Unbiased Risk Estimator (SURE) and similar approaches that assume full knowledge of the distribution. The first class of methods is often suboptimal compared to supervised learning, and the second class tends to be impractical, as the noise level is often unknown in real-world applications.\nIn this paper, we provide a theoretical framework that characterizes this expressivity-robustness trade-off and propose a new approach based on SURE, but unlike the standard SURE, does not require knowledge about the noise level. Throughout a series of experiments, we show that the proposed estimator outperforms other existing self-supervised methods on various imaging inverse problems."
    },
    {
        "title": "ALBAR: Adversarial Learning approach to mitigate Biases in Action Recognition",
        "link_suffix": "/forum?id=9KiE3t6CsL",
        "link": "https://openreview.net/forum?id=9KiE3t6CsL",
        "pdf_link": "https://openreview.net/pdf?id=9KiE3t6CsL",
        "keywords": "Bias Mitigation, Action Recognition",
        "abstract": "Bias in machine learning models can lead to unfair decision making, and while it has been well-studied in the image and text domains, it remains underexplored in action recognition. Action recognition models often suffer from background bias (i.e., inferring actions based on background cues) and foreground bias (i.e., relying on subject appearance), which can be detrimental to real-life applications such as autonomous vehicles or assisted living monitoring. While prior approaches have mainly focused on mitigating background bias using specialized augmentations, we thoroughly study both biases. We propose \\approachname, a novel adversarial training method that mitigates foreground and background biases without requiring specialized knowledge of the bias attributes. Our framework applies an adversarial cross-entropy loss to the sampled static clip (where all the frames are the same) and aims to make its class probabilities uniform using a proposed \\textit{entropy maximization} loss. Additionally, we introduce a \\textit{gradient penalty} loss for regularization against the debiasing process. We evaluate our method on established background and foreground bias protocols, setting a new state-of-the-art and strongly improving combined debiasing performance by over \\textbf{12%} on HMDB51. \nFurthermore, we identify an issue of background leakage in the existing UCF101 protocol for bias evaluation which provides a shortcut to predict actions and does not provide an accurate measure of the debiasing capability of a model. We address this issue by proposing more fine-grained segmentation boundaries for the actor, where our method also outperforms existing approaches."
    },
    {
        "title": "Efficient Exploration and Discriminative World Model Learning with an Object-Centric Abstraction",
        "link_suffix": "/forum?id=hgwGi81ndj",
        "link": "https://openreview.net/forum?id=hgwGi81ndj",
        "pdf_link": "https://openreview.net/pdf?id=hgwGi81ndj",
        "keywords": "reinforcement learning, model based reinforcement learning, world model, exploration, hierarchy",
        "abstract": "In the face of difficult exploration problems in reinforcement learning, we study whether giving an agent an object-centric mapping (describing a set of items and their attributes) allow for more efficient learning. We found this problem is best solved hierarchically by modelling items at a higher level of state abstraction to pixels, and attribute change at a higher level of temporal abstraction to primitive actions. This abstraction simplifies the transition dynamic by making specific future states easier to predict. We make use of this to propose a fully model-based algorithm that learns a discriminative world model, plans to explore efficiently with only a count-based intrinsic reward, and can subsequently plan to reach any discovered (abstract) states.We demonstrate the model's ability to (i) efficiently solve single tasks, (ii) transfer zero-shot and few-shot across item types and environments, and (iii) plan across long horizons. Across a suite of 2D crafting and MiniHack environments, we empirically show our model significantly out-performs state-of-the-art low-level methods (without abstraction), as well as performant model-free and model-based methods using the same abstraction. Finally, we show how to reinforce learn low level object-perturbing policies, and supervise learn the object mapping itself."
    },
    {
        "title": "Drama: Mamba-Enabled Model-Based Reinforcement Learning Is Sample and Parameter Efficient",
        "link_suffix": "/forum?id=7XIkRgYjK3",
        "link": "https://openreview.net/forum?id=7XIkRgYjK3",
        "pdf_link": "https://openreview.net/pdf?id=7XIkRgYjK3",
        "keywords": "Mamba, Model based reinforcement learning, Atari100k, Mamba-2",
        "abstract": "Model-based reinforcement learning (RL) offers a solution to the data inefficiency that plagues most model-free RL algorithms. However, learning a robust world model often demands complex and deep architectures, which are expensive to compute and train. Within the world model, dynamics models are particularly crucial for accurate predictions, and various dynamics-model architectures have been explored, each with its own set of challenges. Currently, recurrent neural network (RNN) based world models face issues such as vanishing gradients and difficulty in capturing long-term dependencies effectively. In contrast, use of transformers suffers from the well-known issues of self-attention mechanisms, where both memory and computational complexity scale as $O(n^2)$, with $n$ representing the sequence length.To address these challenges we propose a state space model (SSM) based world model, specifically based on Mamba, that achieves $O(n)$ memory and computational complexity while effectively capturing long-term dependencies and facilitating the use of longer training sequences efficiently. We also introduce a novel sampling method to mitigate the suboptimality caused by an incorrect world model in the early stages of training, combining it with the aforementioned technique to achieve a normalised score comparable to other state-of-the-art model-based RL algorithms using only a 7 million trainable parameter world model. This model is accessible and can be trained on an off-the-shelf laptop."
    },
    {
        "title": "Visual Instruction Tuning with 500x Fewer Parameters through Modality Linear Representation-Steering",
        "link_suffix": "/forum?id=uV9KFBVaFI",
        "link": "https://openreview.net/forum?id=uV9KFBVaFI",
        "pdf_link": "https://openreview.net/pdf?id=uV9KFBVaFI",
        "keywords": "MLLMs; PEFTs; Representation Steering",
        "abstract": "Multimodal Large Language Models (MLLMs) have significantly advanced visual tasks by integrating visual representations into large language models (LLMs). The textual modality, inherited from LLMs, equips MLLMs with abilities like instruction following and in-context learning. In contrast, the visual modality enhances performance in downstream tasks by leveraging rich semantic content, spatial information, and grounding capabilities. These intrinsic modalities work synergistically across various visual tasks.\nOur research initially reveals a persistent imbalance between these modalities, with text often dominating output generation during visual instruction tuning. This imbalance occurs when using both full fine-tuning and parameter-efficient fine-tuning (PEFT) methods. We then found that re-balancing these modalities can significantly reduce the number of trainable parameters required, inspiring a direction for further optimizing visual instruction tuning. Hence, in this paper, we introduce Modality Linear Representation-Steering (MoReS) to achieve the goal. MoReS effectively re-balances the intrinsic modalities throughout the model, where the key idea is to steer visual representations through linear transformations in the visual subspace across each model layer. \nTo validate our solution, we composed LLaVA Steering, a suite of models integrated with the proposed MoReS method. Evaluation results show that the composed LLaVA Steering models require, on average, 500 times fewer trainable parameters than LoRA needs while still achieving comparable performance across three visual benchmarks and eight visual question-answering tasks.\nLast, we present the LLaVA Steering Factory, an in-house developed platform that enables researchers to quickly customize various MLLMs with component-based architecture for seamlessly integrating state-of-the-art models, and evaluate their intrinsic modality imbalance. This open-source project enriches the research community to gain a deeper understanding of MLLMs."
    },
    {
        "title": "LongViTU: Instruction Tuning for Long-Form Video Understanding",
        "link_suffix": "/forum?id=4j9plQoOH1",
        "link": "https://openreview.net/forum?id=4j9plQoOH1",
        "pdf_link": "https://openreview.net/pdf?id=4j9plQoOH1",
        "keywords": "vision language models, instruction-tuning, long-form video understanding",
        "abstract": "This paper presents LongViTU, a large-scale (~121k QA pairs, ~900h videos), automatically generated dataset for long-form video understanding. Our key idea is inspired by the success of Large Language Models (LLMs) and Multimodal Language Models (MLMs) that are fueled by machine-generated instruction-following data (e.g., InstructGPT, LLaVA). We developed asystematicapproach to produce massive question-answeringing pairs tailored to virtually unbounded long videos by organizing them into ahierarchical tree, incorporatingself-revisionmechanisms to guarantee high quality. We curate LongViTU for each QA pair: 1) involves a long context (averagecertificate lengthof 4.6 minutes); 2) requires rich knowledge and condensed reasoning (commonsense, causality, planning,etc.); 3) explicit labels the timestamps of relevant events throughout the entire video. Furthermore, LongViTU provides a benchmark to facilitate future research in instruction-following for long-form videos. Our experiments first reveal the performance gap between open-source video MLMs and their commercial counterparts (e.g., Gemini-1.5-Pro) on this benchmark. Supervised Fine-Tuning (SFT) on open-source models led to Video-LLaVA achieving the best performance, with a GPT-4 score of $50.7$, closely following $52.3$ by the leading closed-source model Gemini-1.5-Pro, underscoring the substantial challenge posed by our benchmark. Further SFT on LongViTU with Video-LLaVA resulted in improvements of $30.7$% on the In-Distribution (ID) benchmark EgoSchema; $12.9$% and $0.6$% on the Out-of-Distribution (OOD) benchmarks WorldQA and VideoMME, respectively. These outcomes demonstrate the effectiveness and robust OOD generalizability of our proposed instruction-tuning scheme for long-form video understanding. The dataset, SFT models, and code are publicly available on the anonymous pageLongViTU."
    },
    {
        "title": "Next Block Prediction: Video Generation via Semi-Auto-Regressive Modeling",
        "link_suffix": "/forum?id=JUYBEmwSJK",
        "link": "https://openreview.net/forum?id=JUYBEmwSJK",
        "pdf_link": "https://openreview.net/pdf?id=JUYBEmwSJK",
        "keywords": "video generation, auto-regressive model, semi-auto-regressive model",
        "abstract": "Next-Token Prediction (NTP) is a de facto approach for autoregressive (AR) video generation, but it suffers from suboptimal unidirectional dependencies and slow inference speed.  In this work, we propose a semi-autoregressive (semi-AR) framework, called Next-Block Prediction (NBP), for video generation. By uniformly decomposing video content into equal-sized blocks (e.g., rows or frames), we shift the generation unit from individual tokens to blocks, allowing each token in the current block to simultaneously predict the corresponding token in the next block. Unlike traditional AR modeling, our framework employs bidirectional attention within each block, enabling tokens to capture more robust spatial dependencies. By predicting multiple tokens in parallel, NBP models significantly reduce the number of generation steps, leading to faster and more efficient inference. Our model achieves FVD scores of 55.0 on UCF101 and 25.5 on K600, outperforming the vanilla NTP model by an average of 4.4. Furthermore, thanks to the reduced number of inference steps, the NBP model generates 8.89 frames (128x128 resolution) per second, achieving an 11\u00d7 speedup in inference. We also explored model scales ranging from 700M to 3B parameters, observing significant improvements in generation quality, with FVD scores dropping from 25.5 to 19.5 on K600, demonstrating the scalability of our approach."
    },
    {
        "title": "Time-dependent Development of Scientific Discourse: A Novel Approach Using UMAP and Word Embeddings",
        "link_suffix": "/forum?id=P49gSPmrvN",
        "link": "https://openreview.net/forum?id=P49gSPmrvN",
        "pdf_link": "https://openreview.net/pdf?id=P49gSPmrvN",
        "keywords": "visualization, umap, dimension reduction, history of science, word embeddings",
        "abstract": "This study presents a method for visualizing the time-dependent development of a scientific discipline using UMAP (Uniform Manifold Approximation and Projection) and text embeddings. This study demonstrates how the evolution of research interests and topics in a specific field can be mapped over time by encoding the abstracts of scholarly articles into a high-dimensional space and then projecting them into a 3D space. This computational approach converts the history of discourse into a point-cloud that can be further studied as a manifold and as a time series, which leads to new insights into the dynamics of scholarly discourse and the emergence and disappearance of research themes."
    },
    {
        "title": "PooDLe\ud83d\udc29: Pooled and dense self-supervised learning from naturalistic videos",
        "link_suffix": "/forum?id=dEg5SdGaiq",
        "link": "https://openreview.net/forum?id=dEg5SdGaiq",
        "pdf_link": "https://openreview.net/pdf?id=dEg5SdGaiq",
        "keywords": "computer vision, representation learning, self-supervised learning, egocentric video, visual representation",
        "abstract": "Self-supervised learning has driven significant progress in learning from single-subject,iconicimages.\nHowever, there are still unanswered questions about the use of minimally-curated, naturalistic video data, which containdensescenes with many independent objects, imbalanced class distributions, and varying object sizes.\nIn this paper, we propose PooDLe, a self-supervised learning method that combines an invariance-based objective on pooled representations with a dense SSL objective that enforces equivariance to optical flow warping.\nOur results show that a unified objective applied at multiple feature scales is essential for learning effective image representations from naturalistic videos.\nWe validate our method with experiments on the BDD100K driving video dataset and the Walking Tours first-person video dataset, demonstrating its ability to capture spatial understanding from a dense objective and semantic understanding via a pooled representation objective."
    },
    {
        "title": "Apollo-MILP: An Alternating Prediction-Correction Neural Solving Framework for Mixed-Integer Linear Programming",
        "link_suffix": "/forum?id=mFY0tPDWK8",
        "link": "https://openreview.net/forum?id=mFY0tPDWK8",
        "pdf_link": "https://openreview.net/pdf?id=mFY0tPDWK8",
        "keywords": "Mixed-integer Linear Programming, Learning to Optimize",
        "abstract": "Leveraging machine learning (ML) to predict an initial solution for mixed-integer linear programming (MILP) has gained considerable popularity in recent years. These methods predict a solution and fix a subset of variables to reduce the problem dimension. Then, they solve the reduced problem to obtain the final solutions. However, directly fixing variable values can lead to low-quality solutions or even infeasible reduced problems if the predicted solution is not accurate enough. To address this challenge, we propose an Alternating prediction-correction neural solving framework (Apollo-MILP) that can identify and select accurate and reliable predicted values to fix. In each iteration, Apollo-MILP conducts a prediction step for the unfixed variables, followed by a correction step to obtain an improved solution (called reference solution) through a trust-region search. By incorporating the predicted and reference solutions, we introduce a novel Uncertainty-based Error upper BOund (UEBO) to evaluate the uncertainty of the predicted values and fix those with high confidence. A notable feature of Apollo-MILP is the superior ability for problem reduction while preserving optimality, leading to high-quality final solutions. Experiments on commonly used benchmarks demonstrate that our proposed Apollo-MILP significantly outperforms other ML-based approaches in terms of solution quality, achieving over a 50% reduction in the solution gap."
    },
    {
        "title": "Diffusion Models Need Visual Priors for Image Generation",
        "link_suffix": "/forum?id=WNb4P8aG66",
        "link": "https://openreview.net/forum?id=WNb4P8aG66",
        "pdf_link": "https://openreview.net/pdf?id=WNb4P8aG66",
        "keywords": "Diffusion Model, Image Generation, Generative Model",
        "abstract": "Conventional class-guided diffusion models generally succeed in generating images with correct semantic content, but often struggle with texture details. This limitation stems from the usage of class priors, which only provide coarse and limited conditional information. To address this issue, we propose Diffusion on Diffusion (DoD), an innovative multi-stage generation framework that first extracts visual priors from previously generated samples, then provides rich guidance for the diffusion model leveraging visual priors from the early stages of diffusion sampling. Specifically, we introduce a latent embedding module that employs a compression-reconstruction approach to discard redundant detail information from the conditional samples in each stage, retaining only the semantic information for guidance. We evaluate DoD on the popular ImageNet-$256 \\times 256$ dataset, reducing 7$\\times$ training cost compared to SiT and DiT with even better performance in terms of the FID-50K score. Our largest model DoD-XL achieves an FID-50K score of 1.83 with only 1 million training steps, which surpasses other state-of-the-art methods without bells and whistles during inference."
    },
    {
        "title": "Uncovering Intersectional Stereotypes in Humans and Large Language Models",
        "link_suffix": "/forum?id=J6nKxekCCo",
        "link": "https://openreview.net/forum?id=J6nKxekCCo",
        "pdf_link": "https://openreview.net/pdf?id=J6nKxekCCo",
        "keywords": "fairness, uncertainty quantification, intersectionality, stereotypes, social psychology, cognitive psychology",
        "abstract": "Recent work has shown that Large Language Models (LLMs) learn and reproduce pre-existing biases in their training corpora, such as preferences for socially privileged identities (e.g., men or White people) and prejudices against socially marginalized identities (e.g., women or Black people). Current evaluations largely focus on single-attribute discrimination (e.g., gender stereotypes). By contrast, we investigate intersectional stereotypical bias (e.g., against Black women) as these social groups face unique challenges that cannot be explained by any single aspect of their identity alone. Our contributions in this work are two-fold: First, we design and release a new fairness benchmark for intersectional stereotypes in LLMs by augmenting the WinoBias corpus using 25 demographic markers including gender identity, body type, and disability.\nWe use this benchmark to evaluate the fairness of five causal LLMs through the lens of uncertainty, and find that they are disparately uncertain for intersectional identities on the pronoun-occupation co-reference resolution task, indicating systematic intersectional stereotypical bias. Second, we build on cognitive psychology research on stereotypes in human society, by using LLMs to detect stereotypes against intersectional identities that have previously not been studied in the social sciences. Drawing from the seminal warmth-competence stereotype content model, we compare stereotypes in LLMs to stereotypes produced by human annotators and report statistically significant alignment between the two. Our findings underscore the potential for LLMs to be used to conduct social psychology research that could otherwise be harmful to conduct with human subjects."
    },
    {
        "title": "Theory, Analysis, and Best Practices for Sigmoid Self-Attention",
        "link_suffix": "/forum?id=Zhdhg6n2OG",
        "link": "https://openreview.net/forum?id=Zhdhg6n2OG",
        "pdf_link": "https://openreview.net/pdf?id=Zhdhg6n2OG",
        "keywords": "Sigmoid Attention, Pointwise Attention, Universal Function Approximators",
        "abstract": "Attention is a key part of the transformer architecture. It is a sequence-to-sequence mapping that transforms each sequence element into a weighted sum of values. The weights are typically obtained as the softmax of dot products between keys and queries. Recent work has explored alternatives to softmax attention in transformers, such as ReLU and sigmoid activations. In this work, we revisit sigmoid attention and conduct an in-depth theoretical and empirical analysis. Theoretically, we prove that transformers with sigmoid attention are universal function approximators and benefit from improved regularity compared to softmax attention. Through detailed empirical analysis, we identify stabilization of large initial attention norms during the early stages of training as a crucial factor for the successful training of models with sigmoid attention, outperforming prior attempts. We also introduce FLASHSIGMOID, a hardware-aware and memory-efficient implementation of sigmoid attention yielding a 17% inference kernel speed-up over FLASHATTENTION2 on H100 GPUs. Experiments across language, vision, and speech show that properly normalized sigmoid attention matches the strong performance of softmax attention on a wide range of domains and scales, which previous attempts at sigmoid attention were unable to fully achieve. Our work unifies prior art and establishes best practices for sigmoid attention as a drop-in softmax replacement in transformers."
    },
    {
        "title": "Towards Meta-Models for Automated Interpretability",
        "link_suffix": "/forum?id=1zDOkoZAtl",
        "link": "https://openreview.net/forum?id=1zDOkoZAtl",
        "pdf_link": "https://openreview.net/pdf?id=1zDOkoZAtl",
        "keywords": "interpretability, safety, automated interpretability, ai safety, explainability, extraction, tracr, rasp",
        "abstract": "Previous work has demonstrated that in some settings, the mechanisms implemented by small neural networks can be reverse-engineered. \nHowever, these efforts rely on human labor that does not easily scale. \nTo investigate a potential avenue towards scalable interpretability, we show it is possible to use \\emph{meta-models}, neural networks that take another network's parameters as input, to learn a mapping from transformer weights to human-readable code.\nWe build on RASP and Tracr to synthetically generate transformer weights that implement known programs, then train a transformer to extract RASP programs from weights. \nOur trained compiler effectively extracts algorithms from model weights, reconstructing a fully correct algorithm 60% of the time."
    },
    {
        "title": "Can Knowledge Graphs Make Large Language Models More Trustworthy? An Empirical Study over Open-ended Question Answering",
        "link_suffix": "/forum?id=DOA1WSPZSi",
        "link": "https://openreview.net/forum?id=DOA1WSPZSi",
        "pdf_link": "https://openreview.net/pdf?id=DOA1WSPZSi",
        "keywords": "Large Language Models, Hallucination, Open-ended Question Answering",
        "abstract": "Recent works integrating Knowledge Graphs (KGs) have led to promising improvements in enhancing reasoning accuracy of Large Language Models (LLMs).\nHowever, current benchmarks mainly focus on closed tasks, leaving a gap in the assessment of more complex, real-world scenarios. This gap has also obscured the evaluation of KGs' potential to mitigate the problem of hallucination in LLMs.\nTo fill the gap, we introduce OKGQA, a new benchmark specifically designed to assess LLMs enhanced with KGs under open-ended, real-world question answering scenarios.\nOKGQA is designed to closely reflect the complexities of practical applications using questions from different types, and incorporates specific metrics to measure both the reduction in hallucinations and the enhancement in reasoning capabilities.\nTo consider the scenario in which KGs may have varying levels of mistakes, we further propose another experiment setting OKGQA-P to assess model performance when the semantics and structure of KGs are deliberately perturbed and contaminated.\nOKGQA aims to (1) explore whether KGs can make LLMs more trustworthy in an open-ended setting, and (2) conduct a comparative analysis to shed light on methods and future directions for leveraging KGs to reduce LLMs' hallucination.\nWe believe that this study can facilitate a more complete performance comparison and encourage continuous improvement in integrating KGs with LLMs. The code of this paper is released athttps://anonymous.4open.science/r/OKGQA-CBB0."
    },
    {
        "title": "Synergy Learning with Small Models promotes LLM Zero-Shot Tabular Prediction",
        "link_suffix": "/forum?id=WoPovNkM5h",
        "link": "https://openreview.net/forum?id=WoPovNkM5h",
        "pdf_link": "https://openreview.net/pdf?id=WoPovNkM5h",
        "keywords": "tabular data, prompt learning, classification",
        "abstract": "Recent development in large language models (LLMs) has demonstrated impressive zero-shot proficiency on unstructured textual or multi-modal tasks across various domains. However, despite with inherent world knowledge, their application on structured tabular data prediction still lags behind, primarily due to the numerical insensitivity and modality discrepancy that brings a gap between LLM reasoning and statistical machine learning. Unlike textual or vision data (e.g., electronic health records, medical images), tabular data is often presented in heterogeneous numerical values (e.g., blood test reports). This ubiquitous data format requires intensive expert annotation, and its numerical nature limits LLMs' ability to effectively transfer untapped domain expertise. In this paper, we propose SERSAL, a general loop of thought prompting method by synergy learning with small models to unconditionally enhance zero-shot tabular prediction for LLMs. Specifically, SERSAL utilizes the LLM's zero-shot outcomes as original soft annotations, which are dynamically leveraged to teach a better small student model in a semi-supervised manner. Reversely, the outcomes from the trained small model are used to teach the LLM to further refine its real capability. Such mutual process can be repeatedly applied for continuous progress. Comprehensive experiments on widely used domain tabular datasets show that, without access to gold labels, applying SERSAL to OpenAI GPT reasoning process attains substantial improvement compared to linguistic prompting methods, which serves as an orthogonal direction for tabular LLM, and increasing prompting bonus is observed as more powerful LLMs appear."
    },
    {
        "title": "Hardware Simulation for Analog Ultrasonic 2D Convolution",
        "link_suffix": "/forum?id=ETokBVXrbC",
        "link": "https://openreview.net/forum?id=ETokBVXrbC",
        "pdf_link": "https://openreview.net/pdf?id=ETokBVXrbC",
        "keywords": "simulation, convolution, ultrasonic, hardware, accelerator, Fourier transform, acoustic, wave",
        "abstract": "As its name suggests, the convolution operator is the basis and an essential component in Convolutional Neural Networks (CNNs). At the moment, modern CNN architectures rely heavily on parallel computation using GPUs and CPUs to perform many convolutions as fast as possible. However, the performance of computing CNNs is reaching its limit as the scaling of transistors approaches its size limits. The convolutional theorem suggests the possibility of using acoustic waves to efficiently perform the convolution operations through Fourier transforms in analog. This promises hardware that would be several orders of magnitude faster than existing silicon-based approaches. However, to date, nobody has shown the practical feasibility of such an approach. In this paper, we describe the first physics-based simulator for Ultrasonic Fourier Transform Convolutions (UFTC). By exploiting the diffraction nature of the waves, the Fourier transforms can be computed in the time it takes to propagate an ultrasonic wavefront. Our results show that ultrasonic computation could drastically improve the performance of CNNs by 12-458x FLOPS reduction and 1.3-4x computation speedup without loss of prediction accuracy."
    },
    {
        "title": "Learning the Hamiltonian of Disordered Materials with Equivariant Graph Networks",
        "link_suffix": "/forum?id=t2f7sD9M7n",
        "link": "https://openreview.net/forum?id=t2f7sD9M7n",
        "pdf_link": "https://openreview.net/pdf?id=t2f7sD9M7n",
        "keywords": "Materials modeling, atomic structure, electronic structure, density functional theory, graph neural networks, hamiltonian, amorphous systems",
        "abstract": "Graph neural networks (GNNs) have shown promise in learning the ground-state electronic properties of molecules and crystalline materials, subverting computationally intensive density functional theory (DFT) calculations. Materials with structural disorder, however, are more challenging to learn as they exhibit higher complexity and a more extensive palette of local atomic environments, all of which require large (10+ Angstrom) cells to be accurately captured. In this work, we adapt efficient equivariant GNN approaches to learn disordered materials' electronic properties, represented by the Hamiltonian matrix ($\\mathbf{H}$). Since creating a large graph corresponding to the whole structure of interest would be computationally prohibitive, we introduce an 'augmented partitioning' approach in which the graph is sliced into multiple partitions, each augmented with masked virtual nodes and edges. This method maintains correct atomic neighborhoods within a single message passing layer, allowing for the network to learn the electronic properties of amorphous HfO$_2$ materials with 3,000 nodes (atoms), 500,000+ edges, and $\\sim$28 million orbital interactions (non-zero entries of $\\mathbf{H}$)."
    }
]