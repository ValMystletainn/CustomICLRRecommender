[{"title": "MOREL: Enhancing Adversarial Robustness through Multi-Objective Representation Learning", "link_suffix": "/forum?id=WypSbOf9S9", "link": "https://openreview.net/forum?id=WypSbOf9S9", "pdf_link": "https://openreview.net/pdf?id=WypSbOf9S9", "keywords": "Adversarial robustness, Representation learning, Multi-objective optimization, Deep neural networks", "abstract": "Extensive research has shown that deep neural networks (DNNs) are vulnerable to slight adversarial perturbations\u2014small changes to the input data that appear insignificant but cause the model to produce drastically different outputs. In addition to augmenting training data with adversarial examples generated from a specific attack method, most of the current defense strategies necessitate modifying the original model architecture components to improve robustness or performing test-time data purification to handle adversarial attacks. In this work, we demonstrate that strong feature representation learning during training can significantly enhance the original model's robustness. We propose MOREL, a multi-objective feature representation learning approach, encouraging classification models to produce similar features for inputs within the same class, despite perturbations. Our training method involves an embedding space where cosine similarity loss and multi-positive contrastive loss are used to align natural and adversarial features from the model encoder and ensure tight clustering. Concurrently, the classifier is motivated to achieve accurate predictions. Through extensive experiments, we demonstrate that our approach significantly enhances the robustness of DNNs against white-box and black-box adversarial attacks, outperforming other methods that similarly require no architectural changes or test-time data purification.", "title_embedding_index": 3000, "title_abs_embedding_index": 3025}, {"title": "CELL-Diff: Unified Diffusion Modeling for Protein Sequences and Microscopy Images", "link_suffix": "/forum?id=tAkdzjHnkP", "link": "https://openreview.net/forum?id=tAkdzjHnkP", "pdf_link": "https://openreview.net/pdf?id=tAkdzjHnkP", "keywords": "AI for biology, Image generation, Diffusion model, MultiModal learning, Cell biology", "abstract": "Fluorescence microscopy is ubiquitously used in cell biology research to characterize the cellular role of a protein. To help elucidate the relationship between the amino acid sequence of a protein and its cellular function, we introduce CELL-Diff, a unified diffusion model facilitating bidirectional transformations between protein sequences and their corresponding microscopy images. Utilizing reference cell morphology images and a protein sequence, CELL-Diff efficiently generates corresponding protein images. Conversely, given a protein image, the model outputs protein sequences. CELL-Diff integrates continuous and diffusion models within a unified framework and is implemented using a transformer-based network. We train CELL-Diff on the Human Protein Atlas (HPA) dataset and fine-tune it on the OpenCell dataset. Experimental results demonstrate that CELL-Diff outperforms existing methods in generating high-fidelity protein images, making it a practical tool for investigating subcellular protein localization and interactions.", "title_embedding_index": 3001, "title_abs_embedding_index": 3026}, {"title": "Do Mice Grok? Glimpses of Hidden Progress in Sensory Cortex", "link_suffix": "/forum?id=oYemKnlIrO", "link": "https://openreview.net/forum?id=oYemKnlIrO", "pdf_link": "https://openreview.net/pdf?id=oYemKnlIrO", "keywords": "neuroscience; representation learning; grokking; overtraining; cortex", "abstract": "Does learning of task-relevant representations stop when behavior stops changing? Motivated by recent work in machine learning and the intuitive observation that human experts continue to learn after mastery, we hypothesize that task-specific representation learning in cortex can continue, even when behavior saturates. In a novel reanalysis of recently published neural data, we find evidence for such learning in posterior piriform cortex of mice following continued training on a task, long after behavior saturates at near-ceiling performance (\"overtraining\"). We demonstrate that class representations in cortex continue to separate during overtraining, so that examples that were incorrectly classified at the beginning of overtraining can abruptly be correctly classified later on, despite no changes in behavior during that time. We hypothesize this hidden learning takes the form of approximate margin maximization; we validate this and other predictions in the neural data, as well as build and interpret a simple synthetic model that recapitulates these phenomena. We conclude by demonstrating how this model of late-time feature learning implies an explanation for the empirical puzzle of overtraining reversal in animal learning, where task-specific representations are more robust to particular task changes because the learned features can be reused.", "title_embedding_index": 3002, "title_abs_embedding_index": 3027}, {"title": "Test-Time RAG: Enhancing Long Context Understanding in LLMs with Retrieval-Augmented Mechanisms", "link_suffix": "/forum?id=zgs450VzkU", "link": "https://openreview.net/forum?id=zgs450VzkU", "pdf_link": "https://openreview.net/pdf?id=zgs450VzkU", "keywords": "Retreival Augmented Generation (RAG), Personalization, LLM", "abstract": "Large Language Models (LLMs) are becoming increasingly pivotal in applications that depend on extensive personalized context, such as conversational agents and specialized task-oriented systems. In these scenarios, effective long-context handling is essential to support agentic tasks and enhance in-context learning capabilities.  To address this challenge, we propose a novel integration of Retrieval-Augmented Generation (RAG) techniques with LLMs, designed to enhance their ability to effectively manage and utilize large contextual information. Our methodology, Test-time RAG, enriches LLMs by dynamically generating contextual embeddings and utilizing semantic search to retrieve the most relevant document chunks at test time. This process preserves the context's meaning and enhances the model\u2019s responsiveness and accuracy in knowledge-intensive Question Answering tasks. We evaluate our approach using three benchmarks that capture our system's ability synthesize and retrieve information across extensive texts: HotpotQA (+9.87%), QASPER (+3.15%), and Natural Questions (+7.29%). The results indicate a substantial improvement in handling complex queries: demonstrating the effectiveness of Test-time RAG in maintaining high performance across varied document lengths and complexities.", "title_embedding_index": 3003, "title_abs_embedding_index": 3028}, {"title": "WaveDiffusion: Exploring Full Waveform Inversion via Joint Diffusion in the Latent Space", "link_suffix": "/forum?id=0nJt9aVGtl", "link": "https://openreview.net/forum?id=0nJt9aVGtl", "pdf_link": "https://openreview.net/pdf?id=0nJt9aVGtl", "keywords": "Full waveform inversion, Diffusion model, Partial differential equation", "abstract": "Full Waveform Inversion (FWI) is a vital technique for reconstructing high-resolution subsurface velocity maps from seismic waveform data, governed by partial differential equations (PDEs) that model wave propagation. Traditional machine learning approaches typically map seismic data to velocity maps by encoding seismic waveforms into latent embeddings and decoding them into velocity maps. In this paper, we introduce a novel framework that reframes FWI as a joint diffusion process in a shared latent space, bridging seismic waveform data and velocity maps. Our approach has two key components: first, we merge the bottlenecks of two separate autoencoders\u2014one for seismic data and one for velocity maps\u2014into a unified latent space using vector quantization to establish a shared codebook. Second, we train a diffusion model in this latent space, enabling the simultaneous generation of seismic and velocity map pairs by sampling and denoising the latent representations, followed by decoding each modality with its respective decoder. Remarkably, our jointly generated seismic-velocity pairs approximately satisfy the governing PDE without any additional constraint, offering a new geometric interpretation of FWI. The diffusion process learns to score the latent space according to its deviation from the PDE, with higher scores representing smaller deviations from the true solutions. By following this diffusion process, the model traces a path from random initialization to a valid solution of the governing PDE. Our experiments on the OpenFWI dataset demonstrate that the generated seismic and velocity map pairs not only exhibit high fidelity and diversity but also adhere to the physical constraints imposed by the governing PDE.", "title_embedding_index": 3004, "title_abs_embedding_index": 3029}, {"title": "Predictive Differential Training Guided by Training Dynamics", "link_suffix": "/forum?id=eVsSjNRuAp", "link": "https://openreview.net/forum?id=eVsSjNRuAp", "pdf_link": "https://openreview.net/pdf?id=eVsSjNRuAp", "keywords": "Training Dynamics, Koopman Operator Theory, Predictive Training, Deep Neural Networks", "abstract": "This paper centers around a novel concept proposed recently by researchers from the control community where the training process of a deep neural network can be considered a nonlinear dynamical system acting upon the high-dimensional weight space. Koopman operator theory, a data-driven dynamical system analysis framework, can then be deployed to discover the otherwise non-intuitive training dynamics. Taking advantage of the predictive power of the Koopman operator theory, the time-consuming Stochastic Gradient Descent ( SGD) iterations can be bypassed by directly predicting network weights a few epochs later. This novel predictive training framework, however, often suffers from gradient explosion especially for more extensive and complex models. In this paper, we incorporate the idea of differential learning, where different parts of the network can undergo different learning rates during training, into the predictive training framework and propose the so-called \"predictive differential training'' (PDT) to sustain robust performance for accelerated learning even for complex network structures. The key contribution is the design of an effective masking strategy based on Koopman analysis of training dynamics of each parameter in order to select the subset of parameters that exhibits \"good'' prediction performance. PDT also includes the design of an acceleration scheduler to keep track of the prediction error so that the training process can roll back to the traditional GD-based approaches to \"correct'' deviations  from off-predictions. We demonstrate that PDT can be seamlessly integrated as a plug-in with existing optimizers, including, for example, SGD, momentum, and Adam. The experimental results have shown consistent performance improvement in terms of faster convergence, lower training/testing loss, and fewer number of epochs to achieve the best loss of Baseline.", "title_embedding_index": 3005, "title_abs_embedding_index": 3030}, {"title": "Leveraging Skills from Unlabeled Prior Data for Efficient Online Exploration", "link_suffix": "/forum?id=Bff9RniI03", "link": "https://openreview.net/forum?id=Bff9RniI03", "pdf_link": "https://openreview.net/pdf?id=Bff9RniI03", "keywords": "reinforcement learning, exploration, skills, unsupervised pretraining, offline to online rl", "abstract": "Unsupervised pretraining has been transformative in many supervised domains. However, applying such ideas to reinforcement learning (RL) presents a unique challenge in that fine-tuning does not involve mimicking task-specific data, but rather exploring and locating the solution through iterative self-improvement. In this work, we showcase how unlabeled prior trajectory data can be leveraged to learn efficient exploration strategies. The key insight is to use unlabelled trajectories twice, 1) to extract a set of low-level skills offline, and 2) as additional data for a high-level policy that composes these skills to explore. We utilize a simple strategy of learning an optimistic reward model from online samples, and relabeling past trajectories into high-level, task-relevant examples. We instantiate these insights as SUPE (Skills from Unlabeled Prior data for Exploration), and empirically show that SUPE reliably outperforms prior strategies, successfully solving a suite of long-horizon, sparse-reward tasks.", "title_embedding_index": 3006, "title_abs_embedding_index": 3031}, {"title": "TreeX: Generating Global Graphical GNN Explanations via Critical Subtree Extraction", "link_suffix": "/forum?id=zSUXo1nkqR", "link": "https://openreview.net/forum?id=zSUXo1nkqR", "pdf_link": "https://openreview.net/pdf?id=zSUXo1nkqR", "keywords": "GNN Explainability, global-level, XAI, Explainable AI", "abstract": "The growing demand for transparency and interpretability in critical domains has driven increased interests in comprehending the explainability of Message-Passing (MP) Graph Neural Networks (GNNs). Although substantial research efforts have been made to generate explanations for individual graph instances, identifying global explaining concepts for a GNN still poses great challenges, especially when concepts are desired in a graphical form on the dataset level. While most prior works treat GNNs as black boxes, in this paper, we propose to unbox GNNs by analyzing and extracting critical subtrees incurred by the inner workings of message passing, which correspond to critical subgraphs in the datasets. By aggregating subtrees in an embedding space with an efficient algorithm, which does not require complex subgraph matching or search, we can make intuitive graphical explanations for Message-Passing GNNs on local, class and global levels. We empirically show that our proposed approach not only generates clean subgraph concepts on a dataset level in contrast to existing global explaining methods which generate non-graphical rules (e.g., language or embeddings) as explanations, but it is also capable of providing explanations for individual instances with a comparable or even superior performance as compared to leading local-level GNN explainers.", "title_embedding_index": 3007, "title_abs_embedding_index": 3032}, {"title": "Proof Search Augmented Language Models", "link_suffix": "/forum?id=zIPFFhowuM", "link": "https://openreview.net/forum?id=zIPFFhowuM", "pdf_link": "https://openreview.net/pdf?id=zIPFFhowuM", "keywords": "reasoning, transformers, neural theorem proving, neural network architectures, differentiable algorithms", "abstract": "Transformer language models (TLMs) exhibit an impressively general range of capabilities. A growing body of work aims to harness these models for complex reasoning problems expressed in natural language. However, recent theoretical and empirical results have revealed limits to the algorithmic generalization of TLM reasoning. Transformers trained to solve deduction problems from one distribution fail to solve instances of the same problem class drawn from other distributions. We propose to improve the systematic reasoning capabilities of TLMs via a differentiable proof search module, yielding proof-search augmented language models (PSALMs).\nIn a PSALM, a Transformer is responsible for predicting rule and statement representations for a neural theorem prover (NTP). The NTP performs a backward-chaining search over proofs, scoring them based on a soft unification operation. Our principal challenge is to train models to reason without also learning spurious features.\nOur results show that rule-level supervision allows PSALMs to successfully generalize across problem distributions in deduction tasks where vanilla transformers fail to learn systematic behavior. We also find we only need label supervision to adapt PSALMs to more natural text.", "title_embedding_index": 3008, "title_abs_embedding_index": 3033}, {"title": "It Takes Two: On the Seamlessness between Reward and Policy Model in RLHF", "link_suffix": "/forum?id=M8xtZuxqC5", "link": "https://openreview.net/forum?id=M8xtZuxqC5", "pdf_link": "https://openreview.net/pdf?id=M8xtZuxqC5", "keywords": "Large Language Model, RLHF, Data-centric methods", "abstract": "Reinforcement Learning from Human Feedback (RLHF) involves training policy models (PMs) and reward models (RMs) to align language models with human preferences. Instead of focusing solely on PMs and RMs independently, we propose to examine their interactions during fine-tuning, introducing the concept of \\textbf{seamlessness}. Our study starts with observing the saturation phenomenon, where continual improvements in RM and PM do not translate into RLHF progress. Our analysis shows that RMs fail to assign proper scores to PM responses, resulting in a 35% mismatch rate with human preferences, highlighting a significant discrepancy between PM and RM. To measure seamlessness between PM and RM without human effort, we propose an automatic metric, SEAM. SEAM quantifies the discrepancies between PM and RM judgments induced by data samples. We validate the effectiveness of SEAM in data selection and model augmentation. Our experiments demonstrate that (1) using SEAM-filtered data for RL training improves RLHF performance by 4.5%, and (2) SEAM-guided model augmentation results in a 4% performance improvement over standard augmentation methods.", "title_embedding_index": 3009, "title_abs_embedding_index": 3034}, {"title": "Discovering Global Minima of High-Dimensional Energy Landscapes", "link_suffix": "/forum?id=OcTUquFXfx", "link": "https://openreview.net/forum?id=OcTUquFXfx", "pdf_link": "https://openreview.net/pdf?id=OcTUquFXfx", "keywords": "non-convex optimization, structural biology, statistical mechanics", "abstract": "Identifying global minima of high-dimensional non-convex functions is a fundamental problem in fields such as structural biology and materials modeling. Existing solutions (e.g. AlphaFold) often rely on generalizing from data. In contrast, we address the challenging domain where no existing data is available, and only the ground-truth energy function is provided. Utilizing the action functional, we formulate a novel loss function that transforms the input's rough loss landscape into a benign one for the neural network parameters. This allows minimizing the loss to align with finding the global minimum of the energy landscape. We validate our method on high-dimensional global optimization tasks, demonstrating its ability to approximate global minima for energy landscapes with thousand-dimensional inputs.", "title_embedding_index": 3010, "title_abs_embedding_index": 3035}, {"title": "TiC-LM: A Multi-Year Benchmark for Continual Pretraining of Language Models", "link_suffix": "/forum?id=MB53uAZKSc", "link": "https://openreview.net/forum?id=MB53uAZKSc", "pdf_link": "https://openreview.net/pdf?id=MB53uAZKSc", "keywords": "language models, continual learning, benchmark, temporal adaptation", "abstract": "Large language models (LLMs) are trained on data crawled over many years from the web. We investigate how quickly LLMs become outdated as the world evolves with time and how to best update them with newer data. Specifically, we simulate a world where the latest dump of Common Crawl (CC), the most prominent public source of pre-training data, is used every month tocontinuallytrain an LLM. We design various dynamic evaluations from the CC data, Wikipedia, StackExchange, and code documentations to measure continual learning metrics such as forgetting and forward transfer. Notably, our TiC-CC training data is more than 100 times larger compared with prior continual learning benchmarks for language modeling. We discover that recent DataComp-LM models trained on data before 2023 have already become outdated, incurring up to 45% larger noun-perplexity on 2024 Wikipedia articles compared to pre-2023 articles. Further, we use our setup to evaluate the effectiveness of several large-scale continual learning methods and find that replaying older data is most effective for combating forgetting: for previously seen CC dumps, it can reduce the regret on held-out loss by 60% compared to other optimizer and loss-based interventions. However, some domains evolve more quickly than others, favoring different trade-offs between mixing old and new data.", "title_embedding_index": 3011, "title_abs_embedding_index": 3036}, {"title": "CardBench: A Benchmark for Learned Cardinality  Estimation in Relational Databases", "link_suffix": "/forum?id=Nu8b9C1xcr", "link": "https://openreview.net/forum?id=Nu8b9C1xcr", "pdf_link": "https://openreview.net/pdf?id=Nu8b9C1xcr", "keywords": "cardinality estimation, zero-shot, fine tuning, databases", "abstract": "Cardinality estimation is crucial for enabling high query performance in relational\ndatabases. Recently learned cardinality estimation models have been proposed\nto improve accuracy but there is no systematic benchmark or datasets which\nallows researchers to evaluate the progress made by new learned approaches\nand even systematically develop new learned approaches. In this paper, we are\nreleasing a benchmark, containing thousands of queries over 20 distinct real-world\ndatabases for learned cardinality estimation. In contrast to other initial benchmarks,\nour benchmark is much more diverse and can be used for training and testing\nlearned models systematically. Using this benchmark, we explored whether learned\ncardinality estimation can be transferred to an unseen dataset in a zero-shot manner.\nWe trained GNN-based and transformer-based models to study the problem in three\nsetups: 1-) instance-based, 2-) zero-shot, and 3-) fine-tuned.\nOur results show that while we get promising results for zero-shot cardinality estimation on simple single table queries; as soon as we add joins, the accuracy drops.\nHowever, we show that with fine-tuning, we can still utilize pre-trained models\nfor cardinality estimation, significantly reducing training overheads compared to\ninstance specific models. We are open sourcing our scripts to collect statistics,\ngenerate queries and training datasets to foster more extensive research, also from\nthe ML community on the important problem of cardinality estimation and in\nparticular improve on recent directions such as pre-trained cardinality estimation.", "title_embedding_index": 3012, "title_abs_embedding_index": 3037}, {"title": "Task-Unaware Lifelong Robot Learning with Retrieval-based Weighted Local Adaptation", "link_suffix": "/forum?id=YR79EyejsG", "link": "https://openreview.net/forum?id=YR79EyejsG", "pdf_link": "https://openreview.net/pdf?id=YR79EyejsG", "keywords": "Robotic Lifelong Learning, Task-Unaware Continual Learning, Episodic Memory Retrieval, Visuomotor Behavior Cloning, Error-driven Policy Adaptation", "abstract": "Real-world environments require robots to continuously acquire new skills while retaining previously learned abilities, all without the need for clearly defined task boundaries. Storing all past data to prevent forgetting is impractical due to storage and privacy concerns. To address this, we propose a method that efficiently restores a robot's proficiency in previously learned tasks over its lifespan. Using an Episodic Memory (EM), our approach enables experience replay during training and retrieval during testing for local fine-tuning, allowing rapid adaptation to previously encountered problems without explicit task identifiers. Additionally, we introduce a selective weighting mechanism that emphasizes the most challenging segments of retrieved demonstrations, focusing local adaptation where it is most needed. This framework offers a scalable solution for lifelong learning in dynamic, task-unaware environments, combining retrieval-based adaptation with selective weighting to enhance robot performance in open-ended scenarios.", "title_embedding_index": 3013, "title_abs_embedding_index": 3038}, {"title": "Walk the Talk? Measuring the Faithfulness of Large Language Model Explanations", "link_suffix": "/forum?id=4ub9gpx9xw", "link": "https://openreview.net/forum?id=4ub9gpx9xw", "pdf_link": "https://openreview.net/pdf?id=4ub9gpx9xw", "keywords": "large language models, faithful explanations, explainability, safety, counterfactual reasoning", "abstract": "Large language models (LLMs) are capable of generatingplausibleexplanations of how they arrived at an answer to a question. However, these explanations can misrepresent the model's \"reasoning\" process, i.e., they can beunfaithful. This, in turn, can lead to over-trust and misuse. We introduce a new approach for measuring the faithfulness of LLM explanations. First, we provide a rigorous definition of faithfulness. Since LLM explanations mimic human explanations, they often reference high-levelconceptsin the input question that purportedly influenced the model. We define faithfulness in terms of the difference between the set of concepts that the LLM'sexplanations implyare influential and the set thattrulyare. Second, we present a novel method for estimating faithfulness that is based on: (1) using an auxiliary LLM to modify the values of concepts within model inputs to create realistic counterfactuals, and (2) using a hierarchical Bayesian model to quantify the causal effects of concepts at both the example- and dataset-level. Our experiments show that our method can be used to quantify and discover interpretable patterns of unfaithfulness. On a social bias task, we uncover cases where LLM explanations hide the influence of social bias. On a medical question answering task, we uncover cases where LLMs provide false claims about which pieces of evidence influenced its decisions.", "title_embedding_index": 3014, "title_abs_embedding_index": 3039}, {"title": "Direct Judgement Preference Optimization", "link_suffix": "/forum?id=ToWKyjwDqO", "link": "https://openreview.net/forum?id=ToWKyjwDqO", "pdf_link": "https://openreview.net/pdf?id=ToWKyjwDqO", "keywords": "LLM-as-judge, generative judge, auto-evaluation", "abstract": "Auto-evaluation is crucial for assessing response quality and offering feedback for model development. Recent studies have explored training large language models (LLMs) as generative judges to both evaluate model responses and generate natural language critiques. However, existing models have been trained almost exclusively with supervised fine-tuning (SFT), often only on a small number of datasets, resulting in poor generalization across different evaluation settings and tasks. In this paper, we investigate how learning from both positive and negative data with direct preference optimization (DPO) enhances the evaluation capabilities of LLM judges across three evaluation tasks: pairwise, single ratings, and binary classification. We achieve this by creating three forms of DPO data from a diverse collection of human and synthetic judgements on contemporary model outputs, with the goal of training our model to generate meaningful critiques, make accurate judgements, and understand what constitutes good and bad responses for a given user input. To demonstrate the effectiveness of our method, we train judge models of three sizes: 8B parameters, 12B, and 70B, and conduct a comprehensive study over 13 benchmarks (7 pairwise, 4 single rating, and 2 classification), measuring agreement with human and GPT-4 annotations. Our models exhibit the best aggregate performance, with even our 8B model outperforming strong baselines like GPT-4o and specialized judge models, such as OffsetBias-8B, Auto-J-13B, Prometheus-2-8x7B, and Skywork-Critic-70B, in pairwise benchmarks. Further analysis shows that our judge model robustly counters biases such as position and length bias, flexibly adapts to practitioner-specified evaluation protocols, and provides helpful language feedback for improving downstream generator models.", "title_embedding_index": 3015, "title_abs_embedding_index": 3040}, {"title": "The Probability Simplex is Compatible", "link_suffix": "/forum?id=pPmQvd1NUp", "link": "https://openreview.net/forum?id=pPmQvd1NUp", "pdf_link": "https://openreview.net/pdf?id=pPmQvd1NUp", "keywords": "Deep Learning, Representation Learning, Compatible Learning, Neural Collapse", "abstract": "In retrieval systems, updating the base model involves re-extracting feature vectors for all gallery data due to changes in internal feature representations. This process can be computationally expensive and time-consuming, especially for large-scale gallery sets. To address this issue, backward compatible learning was introduced, allowing direct comparison between the representations of the old model and those obtained by the newly trained model. Existing backward compatible methods introduce additional losses or specific network architecture changes, which require the availability of base models, thereby limiting compatibility with models trained independently. In this paper, we show that any independently trained model can be made compatible with any other by simply using features derived from softmax outputs. \nWe leverage the geometric properties of the softmax function, which projects vectors into the Probability Simplex, preserving the alignment of softmax vectors across model updates and verifying the definition of compatibility. A similar property is observed when using logits as a feature representation. They distribute during training in a simplex configuration, but with a wider spread in the feature distribution than softmax outputs, leading to a more robust and transferable representation. Our framework achieves state-of-the-art performance on standard benchmarks, where either the number of training classes extends across multiple steps or the base model is updated with advanced network architectures. This demonstrates that any publicly available pretrained model can be made compatible without requiring any additional training or adaptation. Our code will be made available upon acceptance.", "title_embedding_index": 3016, "title_abs_embedding_index": 3041}, {"title": "Approximating Two-Layer ReLU Networks for Hidden State Analysis in Differential Privacy", "link_suffix": "/forum?id=MyMrDTiFdk", "link": "https://openreview.net/forum?id=MyMrDTiFdk", "pdf_link": "https://openreview.net/pdf?id=MyMrDTiFdk", "keywords": "Differential privacy, machine learning, hidden state threat model, privacy amplification by iteration, DP-SGD", "abstract": "The hidden state threat model of differential privacy (DP) assumes that the adversary has access only to the final trained machine learning (ML) model, without seeing intermediate states during training. Current privacy analyses under this model, however, are limited to convex optimization problems, reducing their applicability to multi-layer neural networks, which are essential in modern deep learning applications. Additionally, the most successful applications of the hidden state privacy analyses in classification tasks have been for logistic regression models. We demonstrate that it is possible to privately train convex problems with privacy-utility trade-offs comparable to those of one hidden-layer ReLU networks trained with DP stochastic gradient descent (DP-SGD). We achieve this through a stochastic approximation of a dual formulation of the ReLU minimization problem which results in a strongly convex problem. This enables the use of existing hidden state privacy analyses, providing accurate privacy bounds also for the noisy cyclic mini-batch gradient descent (NoisyCGD) method with fixed disjoint mini-batches. Our experiments on benchmark classification tasks show that NoisyCGD can achieve privacy-utility trade-offs comparable to DP-SGD applied to one-hidden-layer ReLU networks. Additionally, we provide theoretical utility bounds that highlight the speed-ups gained through the convex approximation.", "title_embedding_index": 3017, "title_abs_embedding_index": 3042}, {"title": "Stuffed Mamba: State Collapse and State Capacity of RNN-Based Long-Context Modeling", "link_suffix": "/forum?id=cu2CT2VAvs", "link": "https://openreview.net/forum?id=cu2CT2VAvs", "pdf_link": "https://openreview.net/pdf?id=cu2CT2VAvs", "keywords": "RNN, foundation models, long-context", "abstract": "One essential advantage of recurrent neural networks (RNNs) over transformer-based language models is their linear computational complexity concerning the sequence length, which makes them much faster in handling long sequences during inference. However, most publicly available RNNs (e.g., Mamba and RWKV) are trained on sequences with less than 10K tokens, and their effectiveness in longer contexts remains largely unsatisfying so far. In this paper, we study the cause of the inability to process long context for RNNs and suggest critical mitigations. We examine two practical concerns when applying state-of-the-art RNNs to long contexts: (1) the inability to extrapolate to inputs longer than the training length and (2) the upper bound of memory capacity. Addressing the first concern, we first investigatestate collapse(SC), a phenomenon that causes severe performance degradation on sequence lengths not encountered during training. With controlled experiments, we attribute this to overfitting due to the recurrent state being overparameterized for the training length. For the second concern, we train a series of Mamba-2 models on long documents to empirically estimate the recurrent state capacity in language modeling and passkey retrieval. Then, three SC mitigation methods are proposed to improve Mamba-2's length generalizability, allowing the model to process more than 1M tokens without state collapse. We also find that the recurrent state capacity in passkey retrieval scales exponentially to the state size, and we empirically train a Mamba-2 370M with near-perfect passkey retrieval accuracy on 256K context length. This suggests a promising future for RNN-based long-context modeling. Code and model checkpoints will be publicly released.", "title_embedding_index": 3018, "title_abs_embedding_index": 3043}, {"title": "Initialization Matters: Unraveling the Impact of Pre-Training on Federated Learning", "link_suffix": "/forum?id=GYik1jT3gE", "link": "https://openreview.net/forum?id=GYik1jT3gE", "pdf_link": "https://openreview.net/pdf?id=GYik1jT3gE", "keywords": "Federated Learning, Initialization, Two-Layer CNN, Pre-training, Generalization", "abstract": "Initializing with pre-trained models when learning on downstream tasks is now standard practice in machine learning. Several recent works explore the benefits of pre-trained initialization in a federated learning (FL) setting, where the downstream training is performed at the edge clients with heterogeneous data distribution. These works show that starting from a pre-trained model can substantially reduce the adverse impact of data heterogeneity on the test performance of a model trained in a federated setting, with no changes to the standard FedAvg training algorithm. In this work, we provide a deeper theoretical understanding of this phenomenon. To do so, we study the class of two-layer convolutional neural networks (CNNs) and provide bounds on the training error convergence and test error of such a network trained with FedAvg. We introduce the notion of aligned and misaligned filters at initialization and show that the data heterogeneity only affects learning on misaligned filters. Starting with a pre-trained model typically results in fewer misaligned filters at initialization, thus producing a lower test error even when the model is trained in a federated setting with data heterogeneity. Experiments in synthetic settings and practical FL training on CNNs verify our theoretical findings.", "title_embedding_index": 3019, "title_abs_embedding_index": 3044}, {"title": "SymmCD: Symmetry-Preserving Crystal Generation with Diffusion Models", "link_suffix": "/forum?id=xnssGv9rpW", "link": "https://openreview.net/forum?id=xnssGv9rpW", "pdf_link": "https://openreview.net/pdf?id=xnssGv9rpW", "keywords": "Crystals, Symmetry, Materials, Diffusion, Generative Models, Equivariance", "abstract": "Generating novel crystalline materials has potential to lead to advancements in fields such as electronics, energy storage, and catalysis. The defining characteristic of crystals is their symmetry, which plays a central role in determining their physical properties. However, existing crystal generation methods either fail to generate materials that display the symmetries of real-world crystals, or simply replicate the symmetry information from examples in a database.  To address this limitation, we propose SymmCD, a novel diffusion-based generative model that explicitly incorporates crystallographic symmetry into the generative process. We decompose crystals into two components and learn their joint distribution through diffusion: 1) the asymmetric unit, the smallest subset of the crystal  which can generate the whole crystal through symmetry transformations, and; 2) the symmetry transformations needed to be applied to each atom in the asymmetric unit. We also use a novel and interpretable representation for these transformations, enabling generalization across different crystallographic symmetry groups. We showcase the competitive performance of SymmCD on a subset of the Materials Project, obtaining diverse and valid crystals with realistic symmetries and predicted properties.", "title_embedding_index": 3020, "title_abs_embedding_index": 3045}, {"title": "On Exact Bit-level Reversible Transformers Without Changing Architectures", "link_suffix": "/forum?id=EUe0yA2pAw", "link": "https://openreview.net/forum?id=EUe0yA2pAw", "pdf_link": "https://openreview.net/pdf?id=EUe0yA2pAw", "keywords": "transformer, ViT, BDIA, reversibility, ODE solvers", "abstract": "Various reversible deep neural networks (DNN) models have been proposed to reduce memory consumption in the training process. However, almost all existing reversible DNNs either require special non-standard architectures or are constructed by modifying existing DNN architectures considerably to enable reversibility. In this work we present the BDIA-transformer, which is an exact bit-level reversible transformer that uses an unchanged standard architecture for inference. The basic idea is to first treat each transformer block as the Euler integration approximation for solving an ordinary differential equation (ODE) and then incorporate the technique of bidirectional integration approximation (BDIA) (originally designed for diffusion inversion) into the neural architecture, together with activation quantization to make it exactly bit-level reversible. In the training process, we let a hyper-parameter $\\gamma$ in BDIA-transformer randomly take one of the two values ${0.5, -0.5}$ per training sample per transformer block for averaging every two consecutive integration approximations. As a result, BDIA-transformer can be viewed as training an ensemble of ODE solvers parameterized by a set of binary random variables,  which regularizes the model and results in improved validation accuracy. Lightweight side information per transformer block is required to be stored in the forward process to account for binary quantization loss to enable exact bit-level reversibility.  In the inference procedure, the expectation $\\mathbb{E}(\\gamma)=0$ is taken to make the resulting architectures of BDIA-transformer identical to transformers up to activation quantization. Our experiments in both image classification and language translation show that BDIA-transformers outperform their conventional counterparts significantly in terms of validation performance due to the regularization effect of the set of $\\gamma$ random variables while also requiring considerably less training memory.", "title_embedding_index": 3021, "title_abs_embedding_index": 3046}, {"title": "Implicit Bias of Mirror Descent for Shallow Neural Networks in Univariate Regression", "link_suffix": "/forum?id=IF0Q9KY3p2", "link": "https://openreview.net/forum?id=IF0Q9KY3p2", "pdf_link": "https://openreview.net/pdf?id=IF0Q9KY3p2", "keywords": "implicit bias, overparametrized neural network, mirror descent, univariate regression, lazy training", "abstract": "We examine the implicit bias of mirror flow in univariate least squares error regression with wide and shallow neural networks. For a broad class of potential functions, we show that mirror flow exhibits lazy training and has the same implicit bias as ordinary gradient flow when the network width tends to infinity. For ReLU networks, we characterize this bias through a variational problem in function space. Our analysis includes prior results for ordinary gradient flow as a special case and lifts limitations which required either an intractable adjustment of the training data or networks with skip connections. We further introducescaled potentialsand show that for these, mirror flow still exhibits lazy training but is not in the kernel regime. For networks with absolute value activations, we show that mirror flow with scaled potentials induces a rich class of biases, which generally cannot be captured by an RKHS norm. A takeaway is that whereas the parameter initialization determines how strongly the curvature of the learned function is penalized at different locations of the input space, the scaled potential determines how the different magnitudes of the curvature are penalized.", "title_embedding_index": 3022, "title_abs_embedding_index": 3047}, {"title": "Geometry of the Loss Landscape in Invariant Deep Linear Neural Networks", "link_suffix": "/forum?id=3Pn24GOcQ1", "link": "https://openreview.net/forum?id=3Pn24GOcQ1", "pdf_link": "https://openreview.net/pdf?id=3Pn24GOcQ1", "keywords": "Invariant Models, Data Augmentation, Deep Linear Networks, Low Rank Approximation, Regularization", "abstract": "Equivariant and invariant machine learning models seek to take advantage of symmetries and other structures present in the data to reduce the sample complexity of learning. Empirical work has suggested that data-driven methods, such as regularization and data augmentation, may achieve a comparable performance as genuinely invariant models, but theoretical results are still limited. In this work, we conduct a theoretical comparison of three different approaches to achieve invariance: data augmentation, regularization, and hard-wiring. We focus on mean squared error regression with deep linear networks, which parametrize rank-bounded linear maps and can be hard-wired to be invariant to specific group actions. We show that the optimization problems resulting from hard-wiring and data augmentation have the same critical points, all of which are saddles except for the global optimum. In contrast, regularization leads to a larger number of critical points, again all of which are saddles except for the global optimum. The regularization path is continuous and converges to the hard-wired optimum.", "title_embedding_index": 3023, "title_abs_embedding_index": 3048}, {"title": "Benchmarking LLMs' Judgments with No Gold Standard", "link_suffix": "/forum?id=uE84MGbKD7", "link": "https://openreview.net/forum?id=uE84MGbKD7", "pdf_link": "https://openreview.net/pdf?id=uE84MGbKD7", "keywords": "Benchmarking, Peer Review, Mutual Information, Data Contamination, Large Language Models", "abstract": "We introduce the GEM (Generative Estimator for Mutual Information), an evaluation metric for assessing language generation by large language models (LLMs), particularly in generating informative judgments, without the need for a gold standard reference. GEM broadens the scenarios where we can benchmark LLM generation performance-from traditional ones, like machine translation and summarization, where gold standard references are readily available, to subjective tasks without clear gold standards, such as academic peer review.GEM uses a generative model to estimate mutual information between candidate and reference responses, without requiring the reference to be a gold standard. In experiments on two human-annotated datasets, GEM demonstrates competitive correlations with human scores compared to the state-of-the-art GPT-4o Examiner, and outperforms all other baselines. Additionally, GEM is more robust against strategic manipulation, such as rephrasing or elongation, which can artificially inflate scores under a GPT-4o Examiner.We also present GRE-bench (Generating Review Evaluation Benchmark) which evaluates LLMs based on how well they can generate high-quality peer reviews for academic research papers.  Because GRE-bench is based upon GEM, it inherits its robustness properties.  Additionally, GRE-bench circumvents data contamination problems (or data leakage) by using the continuous influx of new open-access research papers and peer reviews each year. We show GRE-bench results of various popular LLMs on their peer review capabilities using the ICLR2023 dataset.", "title_embedding_index": 3024, "title_abs_embedding_index": 3049}]