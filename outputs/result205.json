[
    {
        "title": "Advancing Out-of-Distribution Detection via Local Neuroplasticity",
        "link_suffix": "/forum?id=1F8xTfv6ah",
        "link": "https://openreview.net/forum?id=1F8xTfv6ah",
        "pdf_link": "https://openreview.net/pdf?id=1F8xTfv6ah",
        "keywords": "Out-of-Distribution Detection, Local Neuroplasticity, Kolmogorov-Arnold Networks",
        "abstract": "In the domain of machine learning, the assumption that training and test data share the same distribution is often violated in real-world scenarios, requiring effective out-of-distribution (OOD) detection. \nThis paper presents a novel OOD detection method that leverages the unique local neuroplasticity property of Kolmogorov-Arnold Networks (KANs). \nUnlike traditional multilayer perceptrons, KANs exhibit local plasticity, allowing them to preserve learned information while adapting to new tasks. \nOur method compares the activation patterns of a trained KAN against its untrained counterpart to detect OOD samples. \nWe validate our approach on benchmarks from image and medical domains, demonstrating superior performance and robustness compared to state-of-the-art techniques. \nThese results underscore the potential of KANs in enhancing the reliability of machine learning systems in diverse environments."
    },
    {
        "title": "On the Language of Thoughts in Large Language Models",
        "link_suffix": "/forum?id=3wrMRYuLlQ",
        "link": "https://openreview.net/forum?id=3wrMRYuLlQ",
        "pdf_link": "https://openreview.net/pdf?id=3wrMRYuLlQ",
        "keywords": "Language models, system 2 reasoning, language of thoughts",
        "abstract": "System 2 reasoning is one of the defining characteristics of intelligence, which requires slow and logical thinking. Human conducts System 2 reasoning via the language of thoughts that organizes the reasoning process asa causal sequence of mental language, or thoughts. Recently, it has been observed that System 2 reasoning can be elicited from Large Language Models (LLMs) pre-trained on large-scale natural languages. However, in this work, we show that there is a significant gap between the modeling of languages and thoughts. As language is primarily a tool for humans to share knowledge and thinking,modeling human language can easily integrate into language biasesthat are not related to thoughts. Furthermore, we show that the biases may mislead the eliciting of \u201cthoughts\u201d in LLMs to focus only on a given part of the premise. To this end, we propose a new prompt technique termedCall-of-Thoughts ( CaT ) to alleviate the issue. Instead of directly eliciting the chain of thoughts from the potentially biased information, CaT instructs LLMs to focus and expand based on all the relevant information. We show that the simple strategy significantly reduces the language modeling biases in LLMs and improves the performance of LLMs across a variety of reasoning tasks."
    },
    {
        "title": "Revisiting Convergence: A Study on Shuffling-Type Gradient Methods",
        "link_suffix": "/forum?id=2ev44Srmt9",
        "link": "https://openreview.net/forum?id=2ev44Srmt9",
        "pdf_link": "https://openreview.net/pdf?id=2ev44Srmt9",
        "keywords": "shuffling-type gradient methods, convergence analysis, relaxed smoothness assumptions",
        "abstract": "Shuffling-type gradient methods are favored in practice for their simplicity and rapid empirical performance. Despite extensive development of convergence guarantees under various assumptions in recent years, most require the Lipschitz smoothness condition, which is often not met in common machine learning models. We highlight this issue with specific counterexamples. To address this gap, we revisit the convergence rates of shuffling-type gradient methods without assuming Lipschitz smoothness. Using our stepsize strategy, the shuffling-type gradient algorithm not only converges under weaker assumptions but also match the current best-known convergence rates, thereby broadening its applicability. We prove the convergence rates for nonconvex, strongly convex, and non-strongly convex cases, each under both random reshuffling and arbitrary shuffling schemes, and under bounded or sub-Gaussian gradient noise. Numerical experiments further validate the performance of our shuffling-type gradient algorithm, underscoring its practical efficacy."
    },
    {
        "title": "Large Learning Rates without the Agonizing Pain: Dispelling the Curse of Singularities in Deep Neural Networks",
        "link_suffix": "/forum?id=LNYL96VIsD",
        "link": "https://openreview.net/forum?id=LNYL96VIsD",
        "pdf_link": "https://openreview.net/pdf?id=LNYL96VIsD",
        "keywords": "learning rate, training stability, parametric singularity",
        "abstract": "Employing large learning rates (LRs) in deep learning can accelerate convergence and improve generalization, but it can also cause training instability and loss explosion: determining an appropriate LR is an often laborious and painful art. Our study into the fine-grained behaviors of parametric singularities, specifically the stable ranks of weight matrices of network components, reveals a strong connection between these singularities and training instability. As training progresses, parametric singularities trend upward, a phenomenon that is directly aggravated by large LRs. Crucially, several training steps before prominent instabilities such as gradient explosions, we observe unusually high parametric singularities across the network components, leading to rank-deficient representations. These representations, in turn, amplify parametric singularities during backpropagation, creating a vicious cycle that eventually results in loss explosions. We refer to this phenomenon as \\textit{the curse of singularities}. \nBuilding on this understanding, we propose a lightweight and robust stabilization method called Parametric Singularity Smoothing (PSS), which allows for early intervention and mitigates impending instability by smoothing the singular spectra of weight matrices, thereby preventing the curse of singularities.\nThis approach is easy to implement, works at any stage of training by restoring stable training even after instability, \nhas neglectable computational overhead, and, most importantly, frees us from the painful LR fine-tunings to avoid instabilities. Experimental results across various datasets, networks, and optimizers demonstrate that our approach allows a 5-10$\\times$ increase in LR without producing instability, attaining better training efficiency and generalization. We release our code for everyone to use our methods and reproduce the experiments, available athttps://anonymous.4open.science/r/ICLR_stability-C69C."
    },
    {
        "title": "TimeInf: Time Series Data Contribution via Influence Functions",
        "link_suffix": "/forum?id=Vz0CWFMPUe",
        "link": "https://openreview.net/forum?id=Vz0CWFMPUe",
        "pdf_link": "https://openreview.net/pdf?id=Vz0CWFMPUe",
        "keywords": "Time Series Data Contribution, Time Series Anomaly Detection",
        "abstract": "Evaluating the contribution of individual data points to a model's prediction is critical for interpreting model predictions and improving model performance. Existing data contribution methods have been applied to various data types, including tabular data, images, and text; however, their primary focus has been on i.i.d. settings. Despite the pressing need for principled approaches tailored to time series datasets, the problem of estimating data contribution in such settings remains under-explored, possibly due to challenges associated with handling inherent temporal dependencies. This paper introduces TimeInf, a model-agnostic data contribution estimation method for time-series datasets. By leveraging influence scores, TimeInf attributes model predictions to individual time points while preserving temporal structures between the time points. Our empirical results show that TimeInf effectively detects time series anomalies and outperforms existing data attribution techniques as well as state-of-the-art anomaly detection methods. Moreover, TimeInf offers interpretable attributions of data values, allowing us to distinguish diverse anomalous patterns through visualizations. We also showcase a potential application of TimeInf in identifying mislabeled anomalies in the ground truth annotations."
    },
    {
        "title": "DistRL: An Asynchronous Distributed Reinforcement Learning Framework for On-Device Control Agent",
        "link_suffix": "/forum?id=LPG8pPSfQD",
        "link": "https://openreview.net/forum?id=LPG8pPSfQD",
        "pdf_link": "https://openreview.net/pdf?id=LPG8pPSfQD",
        "keywords": "Mobile Agent, LLM, Reinforcement Learning, Fine Tuning, Distributed Training",
        "abstract": "On-device control agents, especially on mobile devices, are responsible for operating mobile devices to fulfill users' requests, enabling seamless and intuitive interactions. Integrating Multimodal Large Language Models (MLLMs) into these agents enhances their ability to understand and execute complex commands, thereby improving user experience. However, fine-tuning MLLMs for on-device control presents significant challenges due to limited data availability and inefficient online training processes. This paper introduces DistRL, a novel framework designed to enhance the efficiency of online RL fine-tuning for mobile device control agents. DistRL employs centralized training and decentralized data acquisition to ensure efficient fine-tuning in the context of dynamic online interactions. Additionally, the framework is backed by our tailor-made RL algorithm, which effectively balances exploration with the prioritized utilization of collected data to ensure stable and robust training. Our experiments show that, on average, DistRL delivers a 3$\\times$ improvement in training efficiency and enables training data collection 2.4$\\times$ faster than the leading synchronous multi-machine methods. Notably, after training, DistRL achieves a 20% relative improvement in success rate compared to state-of-the-art methods on general Android tasks from an open benchmark, significantly outperforming existing approaches while maintaining the same training time. These results validate DistRL as a scalable and efficient solution, offering substantial improvements in both training efficiency and agent performance for real-world, in-the-wild device control tasks."
    },
    {
        "title": "JoPA: Explaining Large Language Model's Generation via Joint Prompt Attribution",
        "link_suffix": "/forum?id=T01rY5kQoo",
        "link": "https://openreview.net/forum?id=T01rY5kQoo",
        "pdf_link": "https://openreview.net/pdf?id=T01rY5kQoo",
        "keywords": "Large Language Model, Explainability",
        "abstract": "Large Language Models (LLMs) have demonstrated impressive performances in complex text generation tasks. However, the contribution of the input prompt to the generated content still remains obscure to humans, underscoring the necessity of elucidating and explaining the causality between input and output pairs. Existing works for providing prompt-specific explanation often confine model output to be classification or next-word prediction. Few initial attempts aiming to explain the entire language generation often treat input prompt texts independently, ignoring their combinatorial effects on the follow-up generation. In this study, we introduce a counterfactual explanation framework based on joint prompt attribution, JoPA, which aims to explain how a few prompt texts collaboratively influences the LLM's complete generation. Particularly, we formulate the task of prompt attribution for generation interpretation as a combinatorial optimization problem, and introduce a probabilistic algorithm to search for the casual input combination in the discrete space. We define and utilize multiple metrics to evaluate the produced explanations, demonstrating both faithfulness and efficiency of our framework."
    },
    {
        "title": "DiffVAS: Diffusion-Guided Visual Active Search in Partially Observable Environments",
        "link_suffix": "/forum?id=kCDQwiwlvH",
        "link": "https://openreview.net/forum?id=kCDQwiwlvH",
        "pdf_link": "https://openreview.net/pdf?id=kCDQwiwlvH",
        "keywords": "Visual Active Search, Reinforcement Learning",
        "abstract": "Visual active search (VAS) has been introduced as a modeling framework that leverages visual cues to direct aerial (e.g., UAV-based) exploration and pinpoint areas of interest within extensive geospatial regions. Potential applications of VAS include detecting hotspots for rare wildlife poaching, aiding in search-and-rescue missions, and uncovering illegal trafficking of weapons, among other uses. Previous VAS approaches assume that the entire search space is known upfront, which is often unrealistic due to constraints such as a restricted field of view and high acquisition costs, and they typically learn policies tailored to specific target objects, which limits their ability to search for multiple target categories simultaneously. In this work, we propose DiffVAS, a target-conditioned policy that searches for diverse objects simultaneously according to task requirements in partially observable environments, which advances the deployment of visual active search policies in real-world applications. DiffVAS uses a diffusion model to reconstruct the entire geospatial area from sequentially observed partial glimpses, which enables a target-conditioned reinforcement learning-based planning module to effectively reason and guide subsequent search steps. Our extensive experiments demonstrate that DiffVAS excels in searching diverse objects in partially observable environments, significantly surpassing state-of-the-art methods across datasets."
    },
    {
        "title": "DC3DO: Diffusion Classifier for 3D Objects",
        "link_suffix": "/forum?id=MqvQUP7ZuZ",
        "link": "https://openreview.net/forum?id=MqvQUP7ZuZ",
        "pdf_link": "https://openreview.net/pdf?id=MqvQUP7ZuZ",
        "keywords": "diffusion, classifier, 3D, deep generative models, classification",
        "abstract": "Inspired by Geoffrey Hinton\u2019s emphasis on generative modeling \"To recognize shapes, first learn to generate them\", we explore the use of 3D diffusion models for 3D object classification. Leveraging the density estimates from these models, our approach DC3DO, Diffusion Classifier for 3D Objects enables zero-shot classification of 3D shapes without additional training. Our method achieves on average 12.5% improvement compared with its multi-view counterparts, demonstrating superior performance compared to discriminative approaches. In DC3DO we use a class-conditional diffusion model trained on ShapeNet and validate our findings on two classes: chairs and cars. DC3DO has been tested for out-of-distribution (OOD) with IFCNet and ModelNet datasets. This work underscores the potential of generative models in 3D object classification."
    },
    {
        "title": "Learning mirror maps in policy mirror descent",
        "link_suffix": "/forum?id=n4wcdct43X",
        "link": "https://openreview.net/forum?id=n4wcdct43X",
        "pdf_link": "https://openreview.net/pdf?id=n4wcdct43X",
        "keywords": "Policy optimization, mirror descent",
        "abstract": "Policy Mirror Descent (PMD) is a popular framework in reinforcement learning, serving as a unifying perspective that encompasses numerous algorithms. These algorithms are derived through the selection of a mirror map and enjoy finite-time convergence guarantees. Despite its popularity, the exploration of PMD's full potential is limited, with the majority of research focusing on a particular mirror map---namely, the negative entropy---which gives rise to the renowned Natural Policy Gradient (NPG) method. It remains uncertain from existing theoretical studies whether the choice of mirror map significantly influences PMD's efficacy. In our work, we conduct empirical investigations to show that the conventional mirror map choice (NPG) often yields less-than-optimal outcomes across several standard benchmark environments. Using evolutionary strategies, we identify more efficient mirror maps that enhance the performance of PMD. We first focus on a tabular environment, i.e.\\ Grid-World, where we relate existing theoretical bounds with the performance of PMD for a few standard mirror maps and the learned one. We then show that it is possible to learn a mirror map that outperforms the negative entropy in more complex environments, such as the MinAtar suite. Additionally, we demonstrate that the learned mirror maps generalize effectively to different tasks by testing each map across various other environments."
    },
    {
        "title": "Collapse or Thrive? Perils and Promises of Synthetic Data in a Self-Generating World",
        "link_suffix": "/forum?id=Xr5iINA3zU",
        "link": "https://openreview.net/forum?id=Xr5iINA3zU",
        "pdf_link": "https://openreview.net/pdf?id=Xr5iINA3zU",
        "keywords": "model collapse, model-data feedback loops, synthetic data, sampling bias, deep generative models, model misbehavior",
        "abstract": "The increasing prevalence of AI-generated content on the internet raises a critical and timely question: What happens when generative machine learning models are pretrained on web-scale datasets containing data created by earlier models? Previous work prophesiedmodel collapse, a phenomenon whereby model-generated synthetic data degrades performance with each additional model-fitting and sampling iteration, with newer models becoming more and more useless. In this work, we clarify and unify the fractured literature on the perils and promises of synthetic data in model-data feedback loops to better forecast likely futures of large-scale deep generative models.\nFirst, previous work claimed that model collapse is caused by replacing all past data with fresh synthetic data at each model-fitting iteration and that collapse is avoided by instead accumulating data across model-fitting iterations; we test this claim on three prominent generative modeling settings, and find both claims hold in all three settings. Second, we study a middle-ground in which the available data pool contains increasing amounts of synthetic data, but each model is fit using a fixed compute budget; we demonstrate that model test loss on real data increases more quickly, but still plateaus unlike when data are replaced en masse. Third, we investigate whether the cardinality or proportion of real data matters more for avoiding model collapse. Surprisingly, we find a non-trivial interaction between real and synthetic data, where the value of synthetic data for reducing model test loss depends on the absolute quantity of real data. Our insights are particularly important when forecasting whether future deep generative models will collapse or thrive, and our results open avenues for empirically and mathematically studying the context-dependent value of synthetic data."
    },
    {
        "title": "Certified Defense on the Fairness of Graph Neural Networks",
        "link_suffix": "/forum?id=UdGwotKVQI",
        "link": "https://openreview.net/forum?id=UdGwotKVQI",
        "pdf_link": "https://openreview.net/pdf?id=UdGwotKVQI",
        "keywords": "Algorithmic Fairness, Graph Neural Networks, Attack and Defense",
        "abstract": "Graph Neural Networks (GNNs) have emerged as a prominent graph learning model in various graph-based tasks over the years. Nevertheless, due to the vulnerabilities of GNNs, it has been empirically proved that malicious attackers could easily corrupt the fairness level of their predictions by adding perturbations to the input graph data. In this paper, we take crucial steps to study a novel problem of certifiable defense on the fairness level of GNNs. Specifically, we propose a principled framework named ELEGANT and present a detailed theoretical certification analysis for the fairness of GNNs. ELEGANT takes any GNNs as its backbone, and the fairness level of such a backbone is theoretically impossible to be corrupted under certain perturbation budgets for attackers. Notably, ELEGANT does not have any assumption over the GNN structure or parameters, and does not require re-training the GNNs to realize certification. Hence it can serve as a plug-and-play framework for any optimized GNNs ready to be deployed. We verify the satisfactory effectiveness of ELEGANT in practice through extensive experiments on real-world datasets across different backbones of GNNs, where ELEGANT is also demonstrated to be beneficial for GNN debiasing."
    },
    {
        "title": "Dual-Stream Adapters for Anomaly Segmentation",
        "link_suffix": "/forum?id=D1gKsJagiq",
        "link": "https://openreview.net/forum?id=D1gKsJagiq",
        "pdf_link": "https://openreview.net/pdf?id=D1gKsJagiq",
        "keywords": "Adapters, Anomaly Segmentation",
        "abstract": "Anomaly segmentation aims to identify pixels of objects not present during the model\u2019s training. Recent approaches address this task using mask-based architectures, but these methods have high training costs due to the large transformer backbones involved. While vision adapters can help reduce training costs, they are not specialized for this task, leading to inferior performance. In this work, we propose Dual-Stream Adapters (DSA), a vision adapter tailored for anomaly segmentation. DSA extracts both in-distribution and out-of-distribution features via (i) an anomaly prior module that produces separate initial embeddings for the two streams; and (ii) a dual-stream feature refinement that implicitly guides the separation of in-distribution from out-of-distribution features. We train DSA using a novel hyperbolic loss function that provides supervised guidance for differentiating in-distribution and out-of-distribution features. Experiments on various benchmarks show that dual-stream adapters achieve the best results while reducing training parameters by 38% w.r.t. the previous state-of-the-art."
    },
    {
        "title": "Understanding Optimization in Deep Learning with Central Flows",
        "link_suffix": "/forum?id=sIE2rI3ZPs",
        "link": "https://openreview.net/forum?id=sIE2rI3ZPs",
        "pdf_link": "https://openreview.net/pdf?id=sIE2rI3ZPs",
        "keywords": "Edge of Stability, Optimization Dynamics, Adaptive Optimizers, RMSProp",
        "abstract": "Optimization in deep learning remains poorly understood, even in the simple setting of deterministic (i.e. full-batch) training. A key difficulty is that much of an optimizer's behavior isimplicitlydetermined by complex oscillatory dynamics, referred to as the \"edge of stability.\" The main contribution of this paper is to show that an optimizer's implicit behavior can be explicitly captured by acentral flow: a differential equation which models the time-averaged optimization trajectory. These flows can empirically predict long-term optimization trajectories of generic neural networks with an unprecedentedly high degree of numerical accuracy. By interpreting these flows, we reveal for the first time 1) the precise sense in which RMSProp adapts to the local loss landscape, and 2) anacceleration via regularizationmechanism, wherein adaptive optimizers implicitly navigate towards low-curvature regions in which they can take larger steps. This mechanism is key to the efficacy of these adaptive optimizers. Overall, we believe that central flows constitute a promising tool for reasoning about optimization in deep learning."
    },
    {
        "title": "MazeNet: An Accurate, Fast, & Scalable Deep Learning Solution for Steiner Minimum Trees",
        "link_suffix": "/forum?id=1ctV3yry3B",
        "link": "https://openreview.net/forum?id=1ctV3yry3B",
        "pdf_link": "https://openreview.net/pdf?id=1ctV3yry3B",
        "keywords": "Recurrent Convolutional Neural Networks (RCNNs), Obstacle-Avoiding Rectilinear Steiner Minimum Tree (OARSMT), Deep learning for maze-solving, Search algorithm for termination condition, Graph-to-image transformation",
        "abstract": "The Obstacle Avoiding Rectilinear Steiner Minimum Tree (OARSMT) problem, which seeks the shortest interconnection of a given number of terminals in a rectilinear plane while avoiding obstacles, is a critical task in integrated circuit design, network optimization, and robot path planning. Since OARSMT is NP-hard, exact algorithms scale poorly with the number of terminals, leading practical solvers to sacrifice accuracy for large problems. We propose and study MazeNet, a deep learning-based method that learns to solve the OARSMT from data. MazeNet reframes OARSMT as a maze-solving task that can be addressed with a recurrent convolutional neural network (RCNN). A key hallmark of MazeNet is its scalability: we only need to train the RCNN blocks on mazes with a small number of terminals; mazes with a larger number of terminals can be solved simply by replicating the same pre-trained blocks to create a larger network. Across a wide range of experiments, MazeNet achieves perfect OARSMT-solving accuracy, with significantly reduced runtime compared to classical exact algorithms, and with the ability to handle larger numbers of terminals than state-of-the-art approximate algorithms."
    },
    {
        "title": "Memory-efficient Training of Large Language Models with Larger Mini-batches",
        "link_suffix": "/forum?id=bAFVlpFQvT",
        "link": "https://openreview.net/forum?id=bAFVlpFQvT",
        "pdf_link": "https://openreview.net/pdf?id=bAFVlpFQvT",
        "keywords": "Data selection, Memory efficient, Large Language Models",
        "abstract": "Training with larger mini-batches improves the convergence rate and can yield superior performance. However, training with large mini-batches becomes prohibitive for Large Language Models (LLMs), due to the large GPU memory requirement. To address this problem, an effective approach is finding small mini-batch coresets that closely match the gradient of larger mini-batches. However, this approach becomes infeasible and ineffective for LLMs, due to the highly imbalanced nature of the sources in language data, use of the Adam optimizer, and the very large gradient dimensionality of LLMs. In this work, we address the above challenges by proposingCoresets for Training LLMs(CoLM). First, we show that mini-batch coresets found by gradient matching do not contain representative examples of the small sources w.h.p., and thus including all examples of the small sources in the mini-batch coresets is crucial for optimal performance. Second, we normalize the gradients by their historical exponential to find mini-batch coresets for training with Adam. Finally, we leverage zeroth-order methods to find smooth gradient of the lastV-projection matrix and sparsify it to keep the dimensions with the largest normalized gradient magnitude. We apply CoLM to fine-tuning Phi-2, Phi-3, and Zephyr with LoRA on MathInstruct and SuperGLUE benchmark. Remarkably, CoLM reduces the memory requirement of fine-tuning by 2x and even outperforms training with 4x larger mini-batches. Notably, CoLM easily stack with existing memory-efficient training methods, such as LoRA."
    },
    {
        "title": "MalTrans: Unsupervised Binary Code Translation with Application to Malware Detection",
        "link_suffix": "/forum?id=jmmk5xjYhd",
        "link": "https://openreview.net/forum?id=jmmk5xjYhd",
        "pdf_link": "https://openreview.net/pdf?id=jmmk5xjYhd",
        "keywords": "binary code analysis, malware detection, neural machine translation",
        "abstract": "Applying deep learning to malware detection has drawn great attention due to its notable performance. With the increasing prevalence of cyberattacks targeting IoT devices, there is a parallel rise in the development of malware across various Instruction Set Architectures (ISAs). It is thus important to extend malware detection capacity to multiple ISAs. However, training a deep learning-based malware detection model usually requires a large number of labeled malware samples.\nThe process of collecting and labeling sufficient malware samples to build datasets for each ISA is labor-intensive and time-consuming.\nTo reduce the burden of data collection, we propose to leverage the ideas and techniques in Neural Machine Translation (NMT) for malware detection. Specifically, when dealing with malware in a certain ISA, we translate it to an ISA with sufficient malware samples (such as X86-64). This allows us to apply a model trained on one ISA to analyze malware from another ISA. Our approach reduces the data collection effort by enabling malware detection across multiple ISAs using a model trained on a single ISA. We have implemented and evaluated the model on five ISAs, including X86-64, i386, ARM64, ARM32, and s390x. The results demonstrate its high translation capability, thereby enabling superior malware detection across ISAs."
    },
    {
        "title": "Why Has Predicting Downstream Capabilities of Frontier AI Models with Scale Remained Elusive?",
        "link_suffix": "/forum?id=zpBamnxyPm",
        "link": "https://openreview.net/forum?id=zpBamnxyPm",
        "pdf_link": "https://openreview.net/pdf?id=zpBamnxyPm",
        "keywords": "evaluations, benchmarks, scaling laws, emergent abilities, capabilities, frontier models, foundation models",
        "abstract": "Predictable behavior from scaling advanced AI systems is an extremely desirable property for engineers, companies, economists and governments alike, and while a well-established literature exists on how pretraining performance scales, predictable scaling behavior on downstream capabilities remains elusive. While many factors are certainly responsible, this paper shines a light on a significant factor that makes predicting scaling behavior on widely used multiple-choice question answering benchmarks challenging and illuminates a path towards making such downstream evaluations predictable with scale. Using five model families and twelve well-established multiple-choice benchmarks, we show that downstream performance is computed from negative log likelihoods via a sequence of transformations that progressively degrades the statistical relationship between performance and scale. We then reveal the mechanism causing this degradation: downstream metrics require comparing the correct choice against a small number of specific incorrect choices, meaning accurately predicting downstream capabilities requires predicting not just how probability mass concentrates on the correct choice with scale, but also how probability mass fluctuates on specific incorrect choices with scale. We empirically study how probability mass on the correct choice co-varies with probability mass on incorrect choices with increasing compute, suggesting that scaling laws for \\textit{incorrect} choices might be achievable. Our work also explains why pretraining scaling laws are commonly regarded as more predictable than downstream capabilities and contributes towards establishing scaling-predictable evaluations of frontier AI models."
    },
    {
        "title": "SafeText: Safe Text-to-image Models via Aligning the Text Encoder",
        "link_suffix": "/forum?id=T7kThJhl02",
        "link": "https://openreview.net/forum?id=T7kThJhl02",
        "pdf_link": "https://openreview.net/pdf?id=T7kThJhl02",
        "keywords": "Text-to-image Model, Alignment, Safeguard",
        "abstract": "Text-to-image models can generate harmful images when presented with unsafe prompts, posing significant safety and societal risks. Alignment methods aim to modify these models to ensure they generate only non-harmful images, even when exposed to unsafe prompts. A typical text-to-image model comprises two main components: 1) a text encoder and 2) a diffusion module. Existing alignment methods mainly focus on modifying the diffusion module to prevent harmful image generation. However, this often significantly impacts the model\u2019s behavior for safe prompts, causing substantial quality degradation of generated images. In this work, we propose SafeText, a novel alignment method that fine-tunes the text encoder rather than the diffusion module. By adjusting the text encoder, SafeText significantly alters the embedding vectors for unsafe prompts, while minimally affecting those for safe prompts. As a result, the diffusion module generates non-harmful images for unsafe prompts while preserving the quality of images for safe prompts. We evaluate SafeText on multiple datasets of safe and unsafe prompts, including those generated through jailbreak attacks. Our results show that SafeText effectively prevents harmful image generation with minor impact on the images for safe prompts, and SafeText outperforms six existing alignment methods. We will publish our code and data after paper acceptance."
    },
    {
        "title": "Spherical Tree-Sliced Wasserstein Distance",
        "link_suffix": "/forum?id=FPQzXME9NK",
        "link": "https://openreview.net/forum?id=FPQzXME9NK",
        "pdf_link": "https://openreview.net/pdf?id=FPQzXME9NK",
        "keywords": "tree-sliced wasserstein distance, spherical optimal transport, equivariance",
        "abstract": "Sliced Optimal Transport (OT) simplifies the OT problem in high-dimensional spaces by projecting supports of input measures onto one-dimensional lines, then exploiting the closed-form expression of the univariate OT to reduce the computational burden of OT. Recently, the Tree-Sliced method has been introduced to replace these lines with more intricate structures, known as tree systems. This approach enhances the ability to capture topological information of integration domains in Sliced OT while maintaining low computational cost. Inspired by this approach, in this paper, we present an adaptation of tree systems on OT problem for measures supported on a sphere. As counterpart to the Radon transform variant on tree systems, we propose a novel spherical Radon transform, with a new integration domain called spherical trees. By leveraging this transform and exploiting the spherical tree structures, we derive closed-form expressions for OT problems on the sphere. Consequently, we obtain an efficient metric for measures on the sphere, named Spherical Tree-Sliced Wasserstein (STSW) distance. We provide an extensive theoretical analysis to demonstrate the topology of spherical trees, the well-definedness and injectivity of our Radon transform variant, which leads to an orthogonally invariant distance between spherical measures. Finally, we conduct a wide range of numerical experiments, including gradient flows and self-supervised learning, to assess the performance of our proposed metric, comparing it to recent benchmarks."
    },
    {
        "title": "Repetition Improves Language Model Embeddings",
        "link_suffix": "/forum?id=Ahlrf2HGJR",
        "link": "https://openreview.net/forum?id=Ahlrf2HGJR",
        "pdf_link": "https://openreview.net/pdf?id=Ahlrf2HGJR",
        "keywords": "embeddings",
        "abstract": "Bidirectional models are considered essential for strong text embeddings. Recent approaches to adapt autoregressive language models (LMs) into strong text embedding models have largely had the requirement to modify the LM architecture to be bidirectional. We challenge this premise by introducing \"echo embeddings\" which converts autoregressive LMs into high quality text embedding models without changing the architecture or requiring fine-tuning. By repeating the input and extracting embeddings from the repeated tokens\u2014which have access to all original tokens\u2014echo embeddings improve over classical LM embeddings by over 5% in zero-shot settings. Our zero-shot embeddings nearly match those obtained by bidirectionally-converted LMs that undergo additional masked-language modeling training. Echo embeddings are also compatible with supervised fine-tuning, matching or outperforming bidirectionally-converted LMs in an apples-to-apples comparison, even with an identical compute budget during training and inference. Overall, repetition is a simple and effective strategy to circumvent the need for bidirectional attention in embedding models, paving the way towards a unified architecture for all NLP tasks."
    },
    {
        "title": "TypedThinker: Typed Thinking Improves Large Language Model Reasoning",
        "link_suffix": "/forum?id=VIUisLx8lQ",
        "link": "https://openreview.net/forum?id=VIUisLx8lQ",
        "pdf_link": "https://openreview.net/pdf?id=VIUisLx8lQ",
        "keywords": "Reasoning, Large Language Models, Self-training",
        "abstract": "Despite significant advancements in the reasoning capabilities of Large Language Models (LLMs), the exploration of diverse reasoning solutions remains understudied. In this paper, we propose TypedThinker, a novel framework that enhances LLMs\u2019 problem-solving abilities by incorporating multiple reasoning types (deductive, inductive, abductive, and analogical). Our analysis across four benchmarks reveals that different reasoning types uniquely solve distinct sets of problems, highlighting the importance of diverse thinking approaches. TypedThinker addresses two key challenges: selecting appropriate reasoning types for given problems and effectively implementing specific reasoning types. The framework employs a meta-thinker for reasoning type selection and a reasoner for execution, supported by an explicit memory for experience retrieval. Through self-training on successful experiences, TypedThinker learns an implicit policy for reasoning type selection and application. Experimental results demonstrate significant improvements over baseline models, with accuracy increases of 3.4% for Mistral 7B and 16.7% for LLaMA3 8B across logical and mathematical benchmarks. Notably, TypedThinker shows effective generalization to new benchmarks and can enhance even powerful models like GPT-4o."
    },
    {
        "title": "Investigating Online RL in World Models",
        "link_suffix": "/forum?id=xw4jtToUrf",
        "link": "https://openreview.net/forum?id=xw4jtToUrf",
        "pdf_link": "https://openreview.net/pdf?id=xw4jtToUrf",
        "keywords": "World models, Domain Randomization, Offline RL",
        "abstract": "Over the past decade, online reinforcement learning (RL) has made drastic improvements in a number of settings, such as video games and robotics. However, despite these successes, the impact of RL on manyreal-worldproblems has remained limited. Underlying this fact is that, in many settings, we are unable to learn in an online fashion due to excessive cost and safety requirements or lack of an accurate simulator. \nIn principle, foundation world models trained on large-scale uncurated offline data such as internet videos and other modalities could provide a training paradigm for generalist AI agents which alleviates the need for task specific simulation environments. \nUnfortunately, training inside world models is usually studied in the context of offline RL, where popular datasets have a biased structure. This necessitates short roll-outs or other severely limiting mechanisms to prevent model exploitation. \nHere we probe under what circumstances full roll-out training inside world models is possiblewithoutany penalties.\nWe find that on a non-adversarial offline dataset simply ensembling over a large number of independently trained world models is sufficient to ensure transfer to the real world, even for datasets that are orders of magnitude smaller than is common in offline RL. Interestingly, more sophisticated methods for level selection provide no advantage and standard offline RL methods underperform in this setting."
    },
    {
        "title": "BAYESIAN EXPERIMENTAL DESIGN VIA CONTRASTIVE DIFFUSIONS",
        "link_suffix": "/forum?id=h8yg0hT96f",
        "link": "https://openreview.net/forum?id=h8yg0hT96f",
        "pdf_link": "https://openreview.net/pdf?id=h8yg0hT96f",
        "keywords": "Bayesian Optimal Experimental Design, Conditional Diffusion Models, score based sampling, Bayesian Inverse Problems, Experimental Design, Sampling as Optimization",
        "abstract": "Bayesian Optimal Experimental Design (BOED) is a powerful tool to reduce the cost of running a sequence of experiments.\nWhen based on the Expected Information Gain (EIG), design optimization corresponds to the maximization of some intractable expectedcontrastbetween prior and posterior distributions.\nScaling this maximization to high dimensional and complex settings has been an issue due to BOED inherent computational complexity.\nIn this work, we introduce anexpected posteriordistribution with cost-effective sampling properties and provide a tractable access to the EIG contrast maximization via a new EIG gradient expression. Diffusion-based samplers are used to compute the dynamics of the expected posterior and ideas from bi-level optimization are leveraged to derive an efficient joint sampling-optimization loop, without resorting to lower bound approximations of the EIG. The resulting efficiency gain allows to extend BOED to the well-tested generative capabilities of diffusion models. \nBy incorporating generative models into the BOED framework, we expand its scope and its use in scenarios that were previously impractical. Numerical experiments and comparison with state-of-the-art methods show the potential of the approach."
    },
    {
        "title": "Positive-Unlabeled Diffusion Models for Preventing Sensitive Data Generation",
        "link_suffix": "/forum?id=jKcZ4hF4s5",
        "link": "https://openreview.net/forum?id=jKcZ4hF4s5",
        "pdf_link": "https://openreview.net/pdf?id=jKcZ4hF4s5",
        "keywords": "Diffusion Model, Positive-Unlabeled Learning",
        "abstract": "Diffusion models are powerful generative models but often generate sensitive data that are unwanted by users,\nmainly because the unlabeled training data frequently contain such sensitive data.\nSince labeling all sensitive data in the large-scale unlabeled training data is impractical,\nwe address this problem by using a small amount of labeled sensitive data.\nIn this paper,\nwe propose positive-unlabeled diffusion models,\nwhich prevent the generation of sensitive data using unlabeled and sensitive data.\nOur approach can approximate the evidence lower bound (ELBO) for normal (negative) data using only unlabeled and sensitive (positive) data.\nTherefore, even without labeled normal data,\nwe can maximize the ELBO for normal data and minimize it for labeled sensitive data,\nensuring the generation of only normal data.\nThrough experiments across various datasets and settings,\nwe demonstrated that our approach can prevent the generation of sensitive images without compromising image quality."
    }
]