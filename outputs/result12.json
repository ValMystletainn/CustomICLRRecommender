[{"title": "Weak Bisimulation Metric-based Representations for Sparse-Reward Reinforcement Learning", "link_suffix": "/forum?id=x7Q0uFTH2a", "link": "https://openreview.net/forum?id=x7Q0uFTH2a", "pdf_link": "https://openreview.net/pdf?id=x7Q0uFTH2a", "keywords": "Deep reinforcement learning, Weak bisimulation metric, Representation learning, Sparse reward", "abstract": "Recent studies have shown that bisimulation metrics possess the superiority of essentially extracting the features related to reinforcement learning tasks. However, limited by strict assumptions and the inherent conflict between metrics and sparse rewards, they suffer from serious representation degeneration and even collapse in sparse reward settings. To tackle the problems, we propose a reward-free weak bisimulation metric-based scalable representation learning approach (SRL). Specifically, we first introduce the weak bisimulation metric, which bypasses the intractable reward difference, instead leveraging a trainable Gaussian distribution to relax the traditional bisimulation metrics. Particularly, the Gaussian noise creates a flexible information margin for the metric optimization, which mitigates potential representation collapse caused by sparse rewards. Additionally, due to its pure distribution internally, the metric potentially mitigates representation degeneration resulting from inconsistent computations under strict assumptions. To tighten the metric, we accordingly consider continuous differences over the transition distribution to enhance the accuracy of the initial transition distribution difference, strengthening the extraction of equivalent task features. We evaluate SRL on challenging DeepMind Control Suite, MetaWorld, and Adroit tasks with sparse rewards. Empirical results demonstrate that SRL significantly outperforms state-of-the-art baselines on various tasks. The source code will be available later.", "title_embedding_index": 550, "title_abs_embedding_index": 575}, {"title": "Multivariate Time-series Forecasting with SPACE: Series Prediction Augmented by Causality Estimation", "link_suffix": "/forum?id=v5BouOktUP", "link": "https://openreview.net/forum?id=v5BouOktUP", "pdf_link": "https://openreview.net/pdf?id=v5BouOktUP", "keywords": "Time Series Forecasting, Causal Learning, Transfer Entropy, Graph Based Learning", "abstract": "The analysis of multivariate time series (MTS) presents a complex yet crucial task with substantial applications in areas such as weather forecasting, policy formulation, and stock market prediction. It is important to highlight three key characteristics of MTS that contribute to the challenging and multifaceted nature of their analysis: (i) their interrelationships are represented through causal relationships rather than mere similarities; (ii) they convey information across multiple independent factors; and (iii) their dynamics often arise from inherent temporal dependencies. While conventional time series analysis frameworks often fail to capture one or more of these aspects, resulting in incomplete or even misleading conclusions, we propose an end-to-end trainable $\\textbf{S}$eries $\\textbf{P}$rediction model $\\textbf{A}$ugmented by $\\textbf{C}$ausality $\\textbf{E}$stimation (SPACE) to address these limitations. This model effectively incorporates temporal dependencies and causal relationships, featuring a temporal embedding and a transfer entropy-based Cross-TE module designed to enhance predictions through causality-augmented mechanisms. Experiments demonstrate that SPACE achieves state-of-the-art results on challenging real-world time series prediction tasks, showing its effectiveness and versatility.", "title_embedding_index": 551, "title_abs_embedding_index": 576}, {"title": "Distribution-free Data Uncertainty for Neural Network Regression", "link_suffix": "/forum?id=pDDODPtpx9", "link": "https://openreview.net/forum?id=pDDODPtpx9", "pdf_link": "https://openreview.net/pdf?id=pDDODPtpx9", "keywords": "deep learning, uncertainty quantification, regression uncertainty, aleatoric uncertainty, CRPS", "abstract": "Quantifying uncertainty is an essential part of predictive modeling, especially in the context of high-stakes decision-making. While classification output includes data uncertainty by design in the form of class probabilities, the regression task generally aims only to predict the expected value of the target variable. Probabilistic extensions often assume parametric distributions around the expected value, optimizing the likelihood over the resulting explicit densities. However, using parametric distributions can limit practical applicability, making it difficult for models to capture skewed, multi-modal, or otherwise complex distributions. In this paper, we propose optimizing a novel nondeterministic neural network regression architecture for loss functions derived from a sample-based approximation of the continuous ranked probability score (CRPS), enabling a truly distribution-free approach by learning to sample from the target's aleatoric distribution, rather than predicting explicit densities. Our approach allows the model to learn well-calibrated, arbitrary uni- and multivariate output distributions. We evaluate the method on a variety of synthetic and real-world tasks, including uni- and multivariate problems, function inverse approximation, and standard regression uncertainty benchmarks. Finally, we make all experiment code publicly available.", "title_embedding_index": 552, "title_abs_embedding_index": 577}, {"title": "Learning representations on Lp hyperspheres: The equivalence of loss functions in a MAP approach", "link_suffix": "/forum?id=S7fuHAL89C", "link": "https://openreview.net/forum?id=S7fuHAL89C", "pdf_link": "https://openreview.net/pdf?id=S7fuHAL89C", "keywords": "Representation Learning; Lp norms; Projected Gaussian Distributions; Image Classification", "abstract": "A common practice when training Deep Neural Networks is to force the learned representations to lie on the standard unit hypersphere, with respect to the  $L_2$ norms. Such practice has been shown to improve both the stability and final performances of DNNs in many applications. In this paper, we derive a unified theoretical framework for learning representation on any $L_p$ hyperspheres for classification tasks, based on Maximum A Posteriori (MAP) modeling. Specifically, we give an expression of the probability distribution of multivariate Gaussians projected on any $L_p$ hypersphere and derive the general associated loss function. Additionally, we show that this framework demonstrates the theoretical equivalence of all projections on $L_p$ hyperspheres through the MAP modeling. It also provides a new interpretation of traditional Softmax Cross Entropy with temperature (SCE-$\\tau$) loss functions. Experiments on standard computer vision datasets give an empirical validation of the equivalence of projections on $L_p$ unit hyperspheres when using adequate objectives. It also shows that the SCE-$\\tau$ on projected representations, with optimally chosen temperature, shows comparable performances.  The code is publicly available at \\url{https://anonymous.4open.science/r/map_code-71C7/", "title_embedding_index": 553, "title_abs_embedding_index": 578}, {"title": "Self-Choose: Leveraging Diverse Reasoning Solutions to Self-Correct Multimodal Large Language Models", "link_suffix": "/forum?id=5w51I0XlOP", "link": "https://openreview.net/forum?id=5w51I0XlOP", "pdf_link": "https://openreview.net/pdf?id=5w51I0XlOP", "keywords": "Multimodal Large Language Models, Self-Correct, Reasoning, Prompting", "abstract": "In the past few years, Multimodal Large Language Models (MLLMs) have achieved remarkable advancements in reasoning while still suffering from mistakes. Some existing approaches on LLMs self-correct the answers without external feedback, proven limited in reasoning. We revisit these previous approaches and propose an improved effective strategy dubbed Self-Choose to teach MLLMs to utilize diverse reasoning solutions to self-correct reasoning. Our approach first employs various reasoning methods to generate candidate answers. Then, it evaluates them by comparing the reasoning processes and candidate answers to choose the optimal solution. Finally, it outputs the best candidate or reflects to generate an improved solution if all the answers are deemed inaccurate. We evaluate our method on multiple datasets with mainstream foundation models including LLaVA and Gemini. The extensive experiments show that Self-Choose achieves consistent improvements on different benchmarks and metrics. We hope this study will promote future research on self-correction and its application across various tasks.", "title_embedding_index": 554, "title_abs_embedding_index": 579}, {"title": "SOO-Bench: Benchmarks for Evaluating the Stability of Offline Black-Box Optimization", "link_suffix": "/forum?id=bqf0aCF3Dd", "link": "https://openreview.net/forum?id=bqf0aCF3Dd", "pdf_link": "https://openreview.net/pdf?id=bqf0aCF3Dd", "keywords": "Offline Optimization, Black-Box Optimization, Stability, Benchmarks", "abstract": "Black-box optimization aims to find the optima through building a model close to the black-box objective function based on function value evaluation. However, in many real-world tasks, such as design of molecular formulas and mechanical structures, it is perilous, costly, or even infeasible to evaluate the objective function value of an actively sampled solution. In this situation, optimization can only be conducted via utilizing offline historical data, which yields offline black-box optimization. Different from the traditional goal that is to pursue the optimal solution, this paper at first discloses that the goal of offline optimization is to stably surpass the offline dataset during optimization procedure. Although benchmarks called Design-Bench already exist in this emerging field, it can hardly evaluate the stability of offline optimization, and mainly provides real-world offline tasks and the corresponding offline datasets. To this end, this paper proposes benchmarks named SOO-Bench (i.e., Stable Offline Optimization Benchmarks) for offline black-box optimization algorithms, so as to evaluate the stability of surpassing the offline dataset under different data distributions. Along with SOO-Bench, we also propose a stability indicator to measure the degree of stability. Specifically, SOO-Bench includes various real-world offline optimization tasks and offline datasets under different data distributions, involving the fields of satellites, materials science, structural mechanics and automobile manufacturing. Empirically, baseline and state-of-the-art algorithms are tested and analyzed on SOO-Bench. Hopefully, SOO-Bench is expected to serve as a catalyst for rapid developments of more novel and stable offline optimization methods. The code is available athttps://anonymous.4open.science/r/SOO-Bench-9025.", "title_embedding_index": 555, "title_abs_embedding_index": 580}, {"title": "Identifying and Tuning Safety Neurons in Large Language Models", "link_suffix": "/forum?id=yR47RmND1m", "link": "https://openreview.net/forum?id=yR47RmND1m", "pdf_link": "https://openreview.net/pdf?id=yR47RmND1m", "keywords": "Large Language Models, Alignment, Safety, Interpretability, Neuron Detection", "abstract": "Safety alignment for Large Language Models (LLMs) has become a critical issue due to their rapid progress. However, our understanding of effective safety mechanisms in LLMs remains limited, leading to safety alignment training that mainly focuses on improving optimization, data-level enhancement, or adding extra structures to intentionally block harmful outputs. To address this gap, we develop a neuron detection method to identify safety neurons\u2014those consistently crucial for handling and defending against harmful queries. Our findings reveal that these safety neurons constitute less than $1%$ of all parameters, are language-specific and are predominantly located in self-attention layers. Moreover, safety is collectively managed by these neurons in the first several layers. Based on these observations, we introduce a $\\underline{S}$afety $\\underline{N}$euron $\\underline{Tun}$ing method, named $\\texttt{SN-Tune}$, that exclusively tune safety neurons without compromising models' general capabilities. $\\texttt{SN-Tune}$ significantly enhances the safety of instruction-tuned models, notably reducing the harmful scores of Llama3-8B-Instruction from $65.5$ to $2.0$, Mistral-7B-Instruct-v0.2 from $70.8$ to $4.5$, and Vicuna-13B-1.5 from $93.5$ to $3.0$. Moreover, $\\texttt{SN-Tune}$ can be applied to base models on establishing LLMs' safety mechanism, effectively diminishing models' harmful scores from around $100$ to $5.3$, $13.5$, and $13.8$ for LLama2-7B-Base, LLama3-8B-Base, and Mistral-7B-v0.1, respectively. In addition, we improve the LLMs' safety robustness during downstream tasks fine-tuning by separating the safety neurons from models' foundation neurons.", "title_embedding_index": 556, "title_abs_embedding_index": 581}, {"title": "Long Context Compression with Activation Beacon", "link_suffix": "/forum?id=1eQT9OzfNQ", "link": "https://openreview.net/forum?id=1eQT9OzfNQ", "pdf_link": "https://openreview.net/pdf?id=1eQT9OzfNQ", "keywords": "Context Compression, Long Context LLMs, LLM Memory", "abstract": "Long context compression is a critical research problem due to its significance in reducing the high computational and memory costs associated with LLMs. In this paper, we propose Activation Beacon, a plug-in module for transformer-based LLMs that targets effective, efficient, and flexible compression of long contexts. To achieve this, our method introduces the following technical designs.We directly compress the activations (i.e. keys and values at every layer), rather than leveraging soft prompts to relay information (which constitute a major bottleneck to encapsulate the complex information within long contexts).We tailor the compression workflow, where each fine-grained input unit is progressively compressed, enabling high-quality compression and efficient computation during both training and inference.We train the model through compression-based auto-regression, making full use of plain texts and instructional data to optimize the model's compression performance.During training, we randomly sample a compression ratio at each step, teaching the model to support a wide range of compression configurations.Extensive evaluations are conducted on various long-context tasks whose lengths (e.g., 128K) may far exceed the maximum training length (20K), such as document understanding, few-shot learning, and Needle-in-a-Haystack. Whilst existing methods struggle to handle these challenging tasks, Activation Beacon maintains a comparable performance to the uncompressed baseline across various scenarios, \nachieving a 2x acceleration in inference time and an 8x reduction of memory costs for KV cache.", "title_embedding_index": 557, "title_abs_embedding_index": 582}, {"title": "RouteFinder: Towards Foundation Models for Vehicle Routing Problems", "link_suffix": "/forum?id=du9reSRIo1", "link": "https://openreview.net/forum?id=du9reSRIo1", "pdf_link": "https://openreview.net/pdf?id=du9reSRIo1", "keywords": "Vehicle Routing Problems, VRP, Foundation Models, Neural Combinatorial Optimization", "abstract": "This paper introduces RouteFinder, a comprehensive foundation model framework to tackle different Vehicle Routing Problem (VRP) variants. Our core idea is that a foundation model for VRPs should be able to represent variants by treating each as a subset of a generalized problem equipped with different attributes. We propose a unified VRP environment capable of efficiently handling any attribute combination. The RouteFinder model leverages a modern transformer-based encoder and global attribute embeddings to improve task representation. Additionally, we introduce two reinforcement learning techniques to enhance multi-task performance: mixed batch training, which enables training on different variants at once, and multi-variant reward normalization to balance different reward scales. Finally, we propose efficient adapter layers that enable fine-tuning for new variants with unseen attributes. Extensive experiments on 24 VRP variants show RouteFinder achieves competitive results. Our code is openly available.", "title_embedding_index": 558, "title_abs_embedding_index": 583}, {"title": "Pre-Trained Vision-Language Model Selection and Reuse for Downstream Tasks", "link_suffix": "/forum?id=vG9dVXwXQV", "link": "https://openreview.net/forum?id=vG9dVXwXQV", "pdf_link": "https://openreview.net/pdf?id=vG9dVXwXQV", "keywords": "Vision-Langage Model; Model Selection; Model Reuse", "abstract": "Pre-trained Vision-Language Models (VLMs) are becoming increasingly popular across various visual tasks, and several open-sourced VLM variants have been released. However, selecting the best-performing pre-trained VLM for a specific downstream task is challenging since no single VLM can achieve promising performance on all downstream tasks, and evaluating all available VLMs is impossible due to time and data limitations. To address this problem, this paper proposes a novel paradigm to select and reuse VLM for downstream tasks, called Model Label Learning (MLL). The proposal contains three key modules: \\emph{model labeling}, which assigns labels to each VLM to describe their specialty and utility; \\emph{model selection}, which matches the requirements of the target task with model labels; and \\emph{model reuse}, which applies selected VLMs to the target task in an ensemble manner. The proposal is highly computationally efficient and growable since the model labeling process is completed target task independent and the ability could grow with the number of candidate VLMs. We also introduce a new benchmark for evaluating VLM selection methods, including 49 VLMs and 17 target task datasets. Experimental results clearly demonstrate the effectiveness of the proposed method for selecting and reusing VLMs.", "title_embedding_index": 559, "title_abs_embedding_index": 584}, {"title": "IMPaCT GNN: Imposing invariance with Message Passing in Chronological split Temporal Graphs", "link_suffix": "/forum?id=VU4WuN0zwV", "link": "https://openreview.net/forum?id=VU4WuN0zwV", "pdf_link": "https://openreview.net/pdf?id=VU4WuN0zwV", "keywords": "Graph Neural Networks, Domain Adaptation, Distribution Shift, Temporal Graph, Chronological Split, Semi-Supervised Node Classification, Generalization Bound", "abstract": "This paper addresses domain adaptation challenges in graph data resulting from chronological splits. In a transductive graph learning setting, where each node is associated with a timestamp, we focus on the task of Semi-Supervised Node Classification (SSNC), aiming to classify recent nodes using labels of past nodes. Temporal dependencies in node connections create domain shifts, causing significant performance degradation when applying models trained on historical data into recent data. Given the practical relevance of this scenario, addressing domain adaptation in chronological split data is crucial, yet underexplored. We propose Imposing invariance with Message Passing in Chronological split Temporal Graphs (\\IMPaCT), a method that imposes invariant properties based on realistic assumptions derived from temporal graph structures. Unlike traditional domain adaptation approaches which rely on unverifiable assumptions, \\IMPaCT explicitly accounts for the characteristics of chronological splits. The \\IMPaCT is further supported by rigorous mathematical analysis, including a derivation of an upper bound of the generalization error. Experimentally, \\IMPaCT achieves a 3.8% performance improvement over current SOTA method on the ogbn-mag graph dataset. Additionally, we introduce the Temporal Stochastic Block Model (TSBM), which replicates temporal graphs under varying conditions, demonstrating the applicability of our methods to general spatial GNNs.", "title_embedding_index": 560, "title_abs_embedding_index": 585}, {"title": "A Diagonal Structured State Space Model on Loihi 2 for Efficient Streaming Sequence Processing", "link_suffix": "/forum?id=ZNHGsuMAgX", "link": "https://openreview.net/forum?id=ZNHGsuMAgX", "pdf_link": "https://openreview.net/pdf?id=ZNHGsuMAgX", "keywords": "State Space Models, Neuromorphic Computing, Loihi 2, New Hardware Paradigms", "abstract": "The unsustainable rise in energy cost from increasingly capable deep learning systems spurs computer architecture innovation beyond conventional deep learning accelerators such as GPUs.\nHowever, a novel computer architecture presents a problem: much of deep learning research has been optimized for conventional computer architectures, and the extent to which modern deep learning models can unlock improved efficiency on a novel computer architecture is not well understood. \nIn this work, we demonstrate for the first time that a State Space Model (SSM) can achieve substantial efficiency improvement when mapped to Loihi 2, a state-of-the-art neuromorphic research chip, versus a Jetson Orin Nano GPU (Jetson).\nSpecifically, we benchmark our SSM on sMNIST, psMNIST, and sCIFAR online token-by-token inference and find approximately 1000x increased energy efficiency and 75x improved latency and throughput on Loihi 2 with a decrease in accuracy of less than one to three percentage points compared to the full precision implementation on Jetson.\nWe comprehensively tailor our implementation to Loihi-specific features and constraints, such as the co-location of memory and compute as well as fixed precision arithmetic.\nOur results elucidate how SSMs meaningfully bridge conventional and neuromorphic hardware via their dual nature: SSMs can operate in an offline mode using convolution or scan, which is efficient on a GPU, or in an online mode as a recurrent network, which we show is efficient on Loihi 2.\nThis work provides a foundation for performant sequence models on neuromorphic hardware, potentially unlocking substantial improvements in latency-sensitive or energy-limited online inference applications, such as speech enhancement or vision for robotic control.", "title_embedding_index": 561, "title_abs_embedding_index": 586}, {"title": "LASeR: Towards Diversified and Generalizable Robot Design with Large Language Models", "link_suffix": "/forum?id=7mlvOHL6qJ", "link": "https://openreview.net/forum?id=7mlvOHL6qJ", "pdf_link": "https://openreview.net/pdf?id=7mlvOHL6qJ", "keywords": "Robot Design Automation, Large Language Model", "abstract": "Recent advances in Large Language Models (LLMs) have stimulated a significant paradigm shift in evolutionary optimization, where hand-crafted search heuristics are gradually replaced with LLMs serving as intelligent search operators. However, these studies still bear some notable limitations, including a challenge to balance exploitation with exploration, often leading to inferior solution diversity, as well as poor generalizability of problem solving across different task settings. These unsolved issues render the prowess of LLMs in robot design automation largely untapped. In this work, we present {\\ttfamily{LASeR}} -- \\underline{L}arge Language Model-\\underline{A}ided Evolutionary \\underline{Se}arch for \\underline{R}obot Design Automation. Leveraging a novel reflection mechanism termed {\\ttfamily{DiRect}}, we elicit more knowledgeable exploratory behaviors from LLMs based on past search trajectories, reshaping the exploration-exploitation tradeoff with dual improvements in optimization efficiency and solution diversity. Additionally, with evolution fully grounded in task-related background information, we unprecedentedly uncover the inter-task reasoning capabilities of LLMs, facilitating generalizable design processes that effectively inspire zero-shot robot proposals for new applications. Our simulated experiments on voxel-based soft robots showcase distinct advantages of {\\ttfamily{LASeR}} over competitive baselines.", "title_embedding_index": 562, "title_abs_embedding_index": 587}, {"title": "Transformers Learn Bayesian Networks Autoregressively In-Context", "link_suffix": "/forum?id=4g0PUEAHg0", "link": "https://openreview.net/forum?id=4g0PUEAHg0", "pdf_link": "https://openreview.net/pdf?id=4g0PUEAHg0", "keywords": "tansformer, Bayesian network, in-context learning", "abstract": "Transformers have achieved tremendous successes in various fields, notably excelling in tasks involving sequential data like natural language processing. Despite their achievements, there is limited understanding of the theoretical capabilities of transformers. In this paper, we theoretically investigate the capability of transformers to autoregressively learn Bayesian networks in-context. Specifically, we consider a setting where a set of independent samples generated from a Bayesian network are observed and form a context. We show that, there exists a simple transformer model that can (i) estimate the conditional probabilities of the Bayesian network according to the context, and (ii) autoregressively generate a new sample according to the Bayesian network with estimated conditional probabilities. We further demonstrate in extensive experiments that such a transformer does not only exist in theory, but can also be effectively obtained through training. Our analysis showcases the potential of transformers to effectively learn complicated probabilistic models, and contributes to a better understanding of the success of large language models.", "title_embedding_index": 563, "title_abs_embedding_index": 588}, {"title": "When Prompt Meets Frequency Learning for Efficient Image Restoration", "link_suffix": "/forum?id=b0qxhCaKIY", "link": "https://openreview.net/forum?id=b0qxhCaKIY", "pdf_link": "https://openreview.net/pdf?id=b0qxhCaKIY", "keywords": "Image restoration, prompt learning, frequency learning, all-in-one", "abstract": "Image restoration, as a longstanding task, aims to recover the missing details and remove degradations from a corrupted observation. Inspired by the success of prompt learning in natural language processing, many prompt-based approaches have been developed for various image restoration tasks. However, these algorithms mostly operate in the spatial domain. As frequency learning plays an important role in image restoration by reducing the spectra discrepancy between degraded/sharp image pairs, this study explores the potential of frequency prompts for efficient image restoration by proposing a plug-and-play mechanism, which mainly comprises a prompt generation module and a prompt integration module. Specifically, the former encodes different frequency information by aggregating the pre-defined learnable parameters under the guidance of implicitly decomposed spectra of input features. Subsequently, to dynamically guide reconstruction, the learned prompts are embedded into the spectra of features via dual-dimensional attention for effective frequency learning. To demonstrate the effectiveness of our mechanism, we conduct experiments on general and all-in-one image restoration tasks. By incorporating it into a CNN-based backbone, the model achieves state-of-the-art performance on 15 benchmark datasets for five representative image restoration tasks. Furthermore, equipped with our mechanism, a pure Transformer network performs favorably against state-of-the-art algorithms under two all-in-one settings.", "title_embedding_index": 564, "title_abs_embedding_index": 589}, {"title": "MQFL-FHE: Multimodal Quantum Federated Learning Framework with Fully Homomorphic Encryption", "link_suffix": "/forum?id=wgnMdxS2nZ", "link": "https://openreview.net/forum?id=wgnMdxS2nZ", "pdf_link": "https://openreview.net/pdf?id=wgnMdxS2nZ", "keywords": "Quantum Federated Learning, Fully Homomorphic Encryption, Multimodal Quantum Mixture of Experts", "abstract": "The integration of fully homomorphic encryption (FHE) in federated learning (FL) has led to significant advances in data privacy. However, during the aggregation phase, it often results in performance degradation of the aggregated model, hindering the development of robust representational generalization. In this work, we propose a novel multimodal quantum federated learning framework that utilizes quantum computing to counteract the performance drop resulting from FHE. For the first time in FL, our framework combines a multimodal quantum mixture of experts (MQMoE) model with FHE, incorporating multimodal datasets for enriched representation and task-specific learning. Our MQMoE framework enhances performance on multimodal datasets and combined genomics and brain MRI scans, especially for underrepresented categories. Our results also demonstrate that the quantum-enhanced approach mitigates the performance degradation associated with FHE and improves classification accuracy across diverse datasets, validating the potential of quantum interventions in enhancing privacy in FL.", "title_embedding_index": 565, "title_abs_embedding_index": 590}, {"title": "Be More Diverse than the Most Diverse: Online Selection of Diverse Mixtures of Generative Models", "link_suffix": "/forum?id=2Chkk5Ye2s", "link": "https://openreview.net/forum?id=2Chkk5Ye2s", "pdf_link": "https://openreview.net/pdf?id=2Chkk5Ye2s", "keywords": "multi-armed bandits, evaluation of generative models, kernel-based evaluation scores", "abstract": "The availability of multiple training algorithms and architectures for generative models requires a selection mechanism to form a single model over a group of well-trained generation models. The selection task is commonly addressed by identifying the model that maximizes an evaluation score based on the diversity and quality of the generated data. However, such a best-model identification approach overlooks the possibility that a mixture of available models can outperform each individual model. In this work, we explore the selection of a mixture of multiple generative models and formulate a quadratic optimization problem to find an optimal mixture model achieving the maximum of kernel-based evaluation scores including kernel inception distance (KID) and Renyi kernel entropy (RKE). To identify the optimal mixture of the models using the fewest possible sample queries, we propose an online learning approach calledMixture Upper Confidence Bound (Mixture-UCB). Specifically, our proposed online learning method can be extended to every convex quadratic function of the mixture weights, for which we prove a concentration bound to enable the application of the UCB approach. We prove a regret bound for the proposed Mixture-UCB algorithm and perform several numerical experiments to show the success of the proposed Mixture-UCB method in finding the optimal mixture of text-based and image-based generative models.", "title_embedding_index": 566, "title_abs_embedding_index": 591}, {"title": "Analyzing Pitfalls and Filling the Gaps in Tabular Deep Learning Benchmarks", "link_suffix": "/forum?id=L14sqcrUC3", "link": "https://openreview.net/forum?id=L14sqcrUC3", "pdf_link": "https://openreview.net/pdf?id=L14sqcrUC3", "keywords": "Tabular Data, Benchmarks, Reality Check, Tabular Deep Learning, Applications", "abstract": "Advances in machine learning research drive progress in real-world applications. \nTo ensure this progress, it is important to understand the potential pitfalls on the way from a novel method's success on academic benchmarks to its practical deployment.In this work, we analyze existing tabular benchmarks and find two common characteristics of tabular data in typical industrial applications that are underrepresented in the datasets usually used for evaluation in the literature.\nFirst, in real-world deployment scenarios, distribution of data often changes over time. To account for this distribution drift, time-based train/test splits should be used in evaluation. However, existing academic tabular datasets generally lack timestamp metadata to enable such evaluation.\nSecond, a considerable portion of datasets in production settings stem from extensive data acquisition and feature engineering pipelines. This can have an impact on the absolute and relative number of predictive, uninformative, and correlated features compared to academic datasets.\nIn this work, we aim to understand how recent advances in tabular deep learning, which are evaluated on academic benchmarks, transfer to these underrepresented conditions.\nTo this end, we introduce TabReD -- a collection of eight industry-grade tabular datasets. \nWe reassess a large number of tabular ML models and techniques on TabReD. We demonstrate that evaluation on time-based data splits leads to different methods ranking, compared to evaluation on random splits, which are common in academic benchmarks. Furthermore, simple MLP-like architectures and GBDT show the best results on the TabReD datasets, while other methods are less effective in the new setting.", "title_embedding_index": 567, "title_abs_embedding_index": 592}, {"title": "Linear-Time Sequence Modeling with MLPs", "link_suffix": "/forum?id=dM1wO2OkbO", "link": "https://openreview.net/forum?id=dM1wO2OkbO", "pdf_link": "https://openreview.net/pdf?id=dM1wO2OkbO", "keywords": "All-MLP, Sequence Modeling, Multilayer Perceptron, Transformer", "abstract": "We present Causal Relation Networks (CausalRNs), the first all-MLP sequence modeling architecture with linear-time parallel training.\nTo enable autoregressive modeling, we made Relation Networks (RNs) equivariant and causal through relaxation and masking.\nContrary to the earlier belief that RNs are quadratic-time, we show that when using exp(x) as the activation function, any RN is linear-time, fully parallelizable, and numerically stable.\nOur derivation spontaneously gave rise to familiar design choices adopted by state-of-the-art architectures, e.g. exponential gating and state expansion.\nSuch duality provided a new perspective, from which we not only validated popular design choices, but also discovered new design considerations.\nExperiments on autoregressive language modeling and image classification showed CausalRNs to be comparable to Linear Transformers.\nThe quadratic variant of CausalRNs achieved perfect retrieval on the copying task, which was previously only possible with Transformers.", "title_embedding_index": 568, "title_abs_embedding_index": 593}, {"title": "SaTran: An efficient Transformer exploiting Spatiotemporal Redundancies for Satellite Image Time Series Representation Learning", "link_suffix": "/forum?id=YcbE2K3i2E", "link": "https://openreview.net/forum?id=YcbE2K3i2E", "pdf_link": "https://openreview.net/pdf?id=YcbE2K3i2E", "keywords": "Satellite image time series analytics, Transformer, Earth observation applications, Spatiotemporal redundancy, Representation learning", "abstract": "Earth observation applications like crop yield prediction, solar energy prediction, land cover classification, etc., need large size Satellite Image Time Series (SITS) leading to huge computational requirements. A couple of BERT-based models exist which work at pixel level unable to exploit spatial correlation among pixels and also require ground truth at pixel granularity during fine-tuning, rendering them infeasible for prediction tasks. The  models based on Vision Transformer factorize spatial and time dimensions and first process images and then time series of image embeddings. However, in many cases, SITS require simultaneous analysis of both dimensions. We present a transformer, SaTran, which focuses on non-redundant patch tubes to overcome the limitations listed above. Transformers developed for RGB videos are found lacking when applied to SITS data characterized by the presence of patches with spatiotemporal redundancy persisting throughout the time series. SITS data also has patches where temporal redundancy lasts only for a few timestamps. The salient features of SaTran include: 1) an automatic patch tube selection mechanism which ignores spatiotemporally redundant patches; 2) exploitation of spatial correlation between pixels by the processing of patch tubes and handling of their temporal redundancy using tube masking; 3) two-fold handling of redundancy and distributed application of VideoMAE enables space and time efficient processing of large size SITS; and 4) learning end task agnostic representation of entire time series. Extensive experimentation shows that SaTran outperforms competing models and exhibit state-of-the-art performance for various earth observation applications. The code is available on (.. will be given after acceptance..).", "title_embedding_index": 569, "title_abs_embedding_index": 594}, {"title": "Convergence of Adafactor under Non-Convex Smooth Stochastic Optimization", "link_suffix": "/forum?id=DIAaRdL2Ra", "link": "https://openreview.net/forum?id=DIAaRdL2Ra", "pdf_link": "https://openreview.net/pdf?id=DIAaRdL2Ra", "keywords": "Adafactor, stochastic optimization, non-convex smooth optimization, convergence", "abstract": "Adafactor, a memory-efficient variant of Adam, has emerged as one of the popular choices for training deep learning tasks, particularly large language models. However, despite its practical success, there is limited theoretical analysis on Adafactor's convergence.\n In this paper, we present a comprehensive analysis on Adafactor in a non-convex smooth setting, demonstrating its convergence to find a stationary point at a rate of $\\tilde{\\mathcal{O}}(1/T)$ in full-batch case and $\\tilde{\\mathcal{O}}(1/\\sqrt{T})$ in stochastic case. We also prove that Adafactor equipped with a suitable time-varying clipping threshold could also find a stationary point with the rate of $\\tilde{\\mathcal{O}}(1/\\sqrt{T})$.", "title_embedding_index": 570, "title_abs_embedding_index": 595}, {"title": "Arithmetic Transformers Can Length-Generalize in Both Operand Length and Count", "link_suffix": "/forum?id=eIgGesYKLG", "link": "https://openreview.net/forum?id=eIgGesYKLG", "pdf_link": "https://openreview.net/pdf?id=eIgGesYKLG", "keywords": "Length Generalization, Transformers, Scratchpad, Position Coupling, Positional Encoding, Out-of-distribution Generalization, Arithmetic Tasks", "abstract": "Transformer-based language models often struggle with length generalization, meaning they fail to generalize to sequences longer than those encountered during training. While arithmetic tasks are commonly used to study length generalization, certain tasks are considered notoriously difficult, e.g., multi-operand addition (which requires generalization over both the number of operands and their lengths) and multiplication (which requires generalization over both operand lengths). In this paper, we achieve approximately 2--3$\\times$ length generalization on both tasks, which is the first such achievement in arithmetic Transformers. To this end, we design task-specific scratchpads enabling the model to focus on a fixed number of tokens per each next-token prediction step, and then apply multi-level versions of Position Coupling (Cho et al., 2024; McLeish et al., 2024) to offer Transformers information about the right position to attend to. On the theory side, we prove that a 1-layer Transformer using our method can solve multi-operand addition, up to operand length and operand count that are exponential in embedding dimension.", "title_embedding_index": 571, "title_abs_embedding_index": 596}, {"title": "Training-Like Data Reconstruction", "link_suffix": "/forum?id=0rS9o1uKqu", "link": "https://openreview.net/forum?id=0rS9o1uKqu", "pdf_link": "https://openreview.net/pdf?id=0rS9o1uKqu", "keywords": "Network Inversion, Interpretability, Privacy, Training Data Reconstruction", "abstract": "Machine Learning models are often trained on proprietary and private data that cannot be shared, though the trained models themselves are distributed openly assuming that sharing model weights is privacy preserving, as training data is not expected to be inferred from the model weights. In this paper, we present Training-Like Data Reconstruction (TLDR), a network inversion-based approach to reconstruct training-like data from trained models. To begin with, we introduce a comprehensive network inversion technique that learns the input space corresponding to different classes in the classifier using a single conditioned generator. While inversion may typically return random and arbitrary input images for a given output label, we modify the inversion process to incentivize the generator to reconstruct training-like data by exploiting key properties of the classifier with respect to the training data. Specifically, the classifier is expected to be relatively more confident and robust in classifying training samples, and the gradient of the classifiers output with respect to the classifier\u2019s weights is also expected to be lower for training data than for random inverted samples. Using these insights, along with some prior knowledge about the images, we guide the generator to produce data closely resembling the original training data. To validate our approach, we conduct empirical evaluations on multiple standard vision classification datasets, demonstrating that leveraging these robustness and gradient properties enables the reconstruction of data semantically similar to the original training data, thereby highlighting the potential privacy risks involved in sharing machine learning models.", "title_embedding_index": 572, "title_abs_embedding_index": 597}, {"title": "Stealthy Shield Defense: A Conditional Mutual Information-Based Approach against Black-Box Model Inversion Attacks", "link_suffix": "/forum?id=p0DjhjPXl3", "link": "https://openreview.net/forum?id=p0DjhjPXl3", "pdf_link": "https://openreview.net/pdf?id=p0DjhjPXl3", "keywords": "model inversion attack, model inversion defense, conditional mutual information", "abstract": "Model inversion attacks (MIA) aim to uncover private training data by accessing public models, raising  increasing concerns about privacy breaches. Black-box MIA, where attackers can generate inputs and obtain the model's outputs arbitrarily, has gained more attention due to its closer alignment with real-world scenarios and greater potential threats. Existing defenses primarily focus on white-box attacks, with a lack of specialized defenses to address the latest black-box attacks. To fill this technological gap, we propose a post-processing defense algorithm based on conditional mutual information (CMI). We have theoretically proven that our CMI framework serves as a special information bottleneck, making outputs less dependent on inputs and more dependent on true labels. To further reduce the modifications to outputs, we introduce an adaptive rate-distortion framework and optimize it by water-filling method. Experimental results show that our approach outperforms existing defenses, in terms of both MIA robustness and model utility, across various attack algorithms, training datasets, and model architectures. In particular, on CelebA dataset, our defense lowers the attack accuracy of LOKT to 0% while other defenses remain 50-75%.", "title_embedding_index": 573, "title_abs_embedding_index": 598}, {"title": "A Computation and Communication Efficient Projection-free Algorithm for Decentralized Constrained Optimization", "link_suffix": "/forum?id=17idjbdHVW", "link": "https://openreview.net/forum?id=17idjbdHVW", "pdf_link": "https://openreview.net/pdf?id=17idjbdHVW", "keywords": "Decentralized stochastic optimization, variance reduction, Frank-Wolfe method", "abstract": "Decentralized constrained optimization problems arise in numerous real-world applications, where a major challenge lies in the computational complexity of projecting onto complex sets, especially in large-scale systems. \nThe projection-free method, Frank-Wolfe (FW), is popular for the constrained optimization problem with complex sets due to its efficiency in tackling the projection process. \nHowever, when applying FW methods to decentralized constrained finite-sum optimization problems, previous studies provide suboptimal incremental first-order oracle (IFO) bounds in both convex and non-convex settings. \nIn this paper, we propose a stochastic algorithm named Decentralized Variance Reduction Gradient Tracking Frank-Wolfe ($\\texttt{DVRGTFW}$), which incorporates the techniques of variance reduction, gradient tracking, and multi-consensus in the FW update to obtain tight bounds. \nWe present a novel convergence analysis, diverging from previous decentralized FW methods, and demonstrating $\\tilde{\\mathcal{O}}(n+\\sqrt{\\frac{n}{m}}L\\varepsilon^{-1})$ and $\\mathcal{O}(\\sqrt{\\frac{n}{m}}L^2\\varepsilon^{-2})$ IFO complexity bounds in convex and non-convex settings, respectively. \nTo the best of our knowledge, these bounds are the best achieved in the literature to date. Besides, in the non-convex case, $\\texttt{DVRGTFW}$ achieves $\\mathcal{O}(\\frac{L^2\\varepsilon^{-2}}{\\sqrt{1-\\lambda_2(W)}})$ communication complexity which is closed to the lower bound $\\Omega(\\frac{L\\varepsilon^{-2}}{\\sqrt{1-\\lambda_2(W)}})$. \nEmpirical results validate the convergence properties of $\\texttt{DVRGTFW}$ and highlight its superior performance over other related methods.", "title_embedding_index": 574, "title_abs_embedding_index": 599}]