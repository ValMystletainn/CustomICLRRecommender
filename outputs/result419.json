[
    {
        "title": "Heavy Labels Out! Dataset Distillation with Label Space Lightening",
        "link_suffix": "/forum?id=IK7l0CqZuH",
        "link": "https://openreview.net/forum?id=IK7l0CqZuH",
        "pdf_link": "https://openreview.net/pdf?id=IK7l0CqZuH",
        "keywords": "dataset distillation, efficient training, label space lightening",
        "abstract": "Dataset distillation or condensation aims to condense a large-scale training dataset into a much smaller synthetic one such that the training performance of distilled and original sets on neural networks are similar. Although the number of training samples can be reduced substantially, current state-of-the-art methods heavily rely on enormous soft labels to achieve satisfactory performance. As a result, the required storage can be comparable even to original datasets, especially for large-scale ones. To solve this problem, instead of storing these heavy labels, we propose a novel label-lightening framework termed HeLlO aiming at effective image-to-label projectors, with which synthetic labels can be directly generated online from synthetic images. Specifically, to construct such projectors, we leverage prior knowledge in open-source foundation models, e.g., CLIP, and introduce a LoRA-like fine-tuning strategy to mitigate the gap between pre-trained and target distributions, so that original models for soft-label generation can be distilled into a group of low-rank matrices. Moreover, an effective image optimization method is proposed to further mitigate the potential error between the original and distilled label generators. Extensive experiments demonstrate that with only about 0.003% of the original storage required for a complete set of soft labels, we achieve comparable performance to current state-of-the-art dataset distillation methods on large-scale datasets. Our code will be available."
    },
    {
        "title": "Benchmarking Ethics in Text-to-Image Models: A Holistic Dataset and Evaluator for Fairness, Toxicity, and Privacy",
        "link_suffix": "/forum?id=kIboeK0Wzs",
        "link": "https://openreview.net/forum?id=kIboeK0Wzs",
        "pdf_link": "https://openreview.net/pdf?id=kIboeK0Wzs",
        "keywords": "Ethics, Benchmark, Text to image, Evaluation",
        "abstract": "Text-to-image (T2I) models have rapidly advanced, enabling the generation of high-quality images from text prompts across various domains. However, these models raise significant ethical concerns, including the risk of generating harmful, biased, or private content. Existing safety benchmarks are limited in scope, lacking comprehensive coverage of critical ethical aspects such as detailed categories of toxicity, privacy, and fairness, and often rely on inadequate evaluation techniques. To address these gaps, we introduce T2IEthics, a comprehensive benchmark that rigorously evaluates T2I models across three key ethical dimensions: fairness, toxicity, and privacy. Additionally, we propose ImageGuard, a multimodal large language model-based evaluator designed for more accurate and nuanced ethical assessments. It significantly outperforms existing models including GPT-4o across all ethical dimensions. Using this benchmark, we evaluate 12 diffusion models, including popular models from the Stable Diffusion series. Our results indicate persistent issues with racial fairness, a tendency to generate toxic content, and significant variation in privacy protection among the models even when defense methods like concept erasing are employed."
    },
    {
        "title": "Randomized Feature Squeezing against  Unseen Attacks without Adversarial Training",
        "link_suffix": "/forum?id=kfYM5lBzB6",
        "link": "https://openreview.net/forum?id=kfYM5lBzB6",
        "pdf_link": "https://openreview.net/pdf?id=kfYM5lBzB6",
        "keywords": "Randomized Feature Squeezing, Unseen Attacks, adversarial defense",
        "abstract": "Deep learning has made tremendous progress in the last decades; however, it is not robust to adversarial attacks.Perhaps the most effective approach for this is adversarial training, although it is impractical as it needs prior knowledge about the attackers and incurs high computational costs.\nIn this paper, we propose a novel approach that can train a robust network only through standard training\nwith clean images without awareness of the attacker's strategy. We add a specially designed network input layer,\nwhich accomplishes a randomized feature squeezing to reduce the malicious perturbation. \nIt achieves the state of the art of robustness against unseen ${l_1,l_2}$ and $ {l_\\infty} $ attacks at one time in terms of the computational cost of the attacker versus the defender through just 100/50 epochs of standard training with clean images in CIFAR-10/ImageNet. Both experiments and Rademacher complexity analysis validate the high performance. Moreover, it can also defend against the ``attacks\" on training data, i.e., unlearnable examples, seemingly being the only solution for the One-Pixel Shortcut without any data augmentation."
    },
    {
        "title": "Generalization of Transformers with In-Context Learning: An Empirical Study",
        "link_suffix": "/forum?id=yOhNLIqTEF",
        "link": "https://openreview.net/forum?id=yOhNLIqTEF",
        "pdf_link": "https://openreview.net/pdf?id=yOhNLIqTEF",
        "keywords": "generalization, in-context learning, transformer",
        "abstract": "Large language models (LLMs) like GPT-4 and LLaMA-3 utilize the powerful in-context learning (ICL) capability of Transformer architecture to learn on the fly from limited examples. While ICL underpins many LLM applications, its full potential remains hindered by a limited understanding of its generalization boundaries and vulnerabilities. We present a systematic investigation of transformers' generalization capability with ICL relative to training data coverage by defining a task-centric framework along three dimensions: inter-problem, intra-problem, and intra-task generalization. Through extensive simulation and real-world experiments, encompassing tasks such as function fitting, API calling, and translation, we find that transformers lack inter-problem generalization with ICL, but excel in intra-task and intra-problem generalization. Furthermore, when the training data includes a greater variety of mixed tasks, it significantly enhances the generalization ability of ICL on unseen tasks and even on known simple tasks. This guides us in designing training data to maximize the diversity of tasks covered and to combine different tasks whenever possible, rather than solely focusing on the target task for testing."
    },
    {
        "title": "LinFusion: 1 GPU, 1 Minute, 16K Image",
        "link_suffix": "/forum?id=D2as3jDmRA",
        "link": "https://openreview.net/forum?id=D2as3jDmRA",
        "pdf_link": "https://openreview.net/pdf?id=D2as3jDmRA",
        "keywords": "Linear Attention, Diffusion Models, Image Generation",
        "abstract": "Modern diffusion models, particularly those utilizing a Transformer-based UNet for denoising, rely heavily on self-attention operations to manage complex spatial relationships, thus achieving impressive generation performance. However, this existing paradigm faces significant challenges in generating high-resolution visual content due to its quadratic time and memory complexity with respect to the number of spatial tokens. To address this limitation, we aim at a novel linear attention mechanism as an alternative in this paper. \nSpecifically, we begin our exploration from recently introduced models with linear complexity, e.g., Mamba2, RWKV6, Gated Linear Attention, etc, and identify two key features\u2014attention normalization and non-causal inference\u2014that enhance high-resolution visual generation performance. Building on these insights, we introduce a generalized linear attention paradigm, which serves as a low-rank approximation of a wide spectrum of popular linear token mixers. To save the training cost and better leverage pre-trained models, we initialize our models and distill the knowledge from pre-trained StableDiffusion (SD). We find that the distilled model, termed LinFusion, achieves performance on par with or superior to the original SD after only modest training, while significantly reducing time and memory complexity. Extensive experiments on SD-v1.5, SD-v2.1, and SD-XL demonstrate that LinFusion enables satisfactory and efficient zero-shot cross-resolution generation, accommodating ultra-resolution images like 16K on a single GPU. Moreover, it is highly compatible with pre-trained SD components and pipelines, such as ControlNet, IP-Adapter, DemoFusion, DistriFusion, etc, requiring no adaptation efforts."
    },
    {
        "title": "Progressive Mixed-Precision Decoding for Efficient LLM Inference",
        "link_suffix": "/forum?id=OVxmpus9NA",
        "link": "https://openreview.net/forum?id=OVxmpus9NA",
        "pdf_link": "https://openreview.net/pdf?id=OVxmpus9NA",
        "keywords": "LLM Quantization, Efficient LLM Inference",
        "abstract": "In spite of the great potential of large language models (LLMs) across various tasks, their deployment on resource-constrained devices remains challenging due to their excessive computational and memory demands. Quantization has emerged as an effective solution by storing weights in reduced precision. However, utilizing low precisions (i.e.~2/3-bit) to substantially alleviate the memory-boundedness of LLM decoding, still suffers from prohibitive performance drop. In this work, \nwe argue that existing approaches fail to explore the diversity in computational patterns, redundancy, and sensitivity to approximations of the different phases of LLM inference, resorting to a uniform quantization policy throughout.\nInstead, we propose a novel phase-aware method that selectively allocates precision during different phases of LLM inference, achieving both strong context extraction during prefill and efficient memory bandwidth utilization during decoding. To further address the memory-boundedness of the decoding phase, we introduce Progressive Mixed-Precision Decoding (PMPD), a technique that enables the gradual lowering of precision deeper in the generated sequence, together with a spectrum of precision-switching schedulers that dynamically drive the precision-lowering decisions in either task-adaptive or prompt-adaptive manner. \nExtensive evaluation across diverse language tasks shows that when targeting Nvidia GPUs, PMPD achieves 1.4$-$12.2$\\times$ speedup in matrix-vector multiplications over fp16 models, while when targeting an LLM-optimized NPU, our approach delivers a throughput gain of 3.8$-$8.0$\\times$ over fp16 models and up to 1.54$\\times$ over uniform quantization approaches while preserving the output quality."
    },
    {
        "title": "SAM2Long: Enhancing SAM2 for Long Video Segmentation with a Training-Free Memory Tree",
        "link_suffix": "/forum?id=Ze49bGd4ON",
        "link": "https://openreview.net/forum?id=Ze49bGd4ON",
        "pdf_link": "https://openreview.net/pdf?id=Ze49bGd4ON",
        "keywords": "Segment Anything Model 2, Video Object Segmentation",
        "abstract": "The Segment Anything Model 2 (SAM2) has emerged as a powerful foundation model for object segmentation in both images and videos, paving the way for various downstream video applications. The crucial design of SAM2 for video segmentation is its memory module, which prompts object-aware memories from previous frames for current frame prediction. However, its greedy-selection memory design suffers from the ``error accumulation\" problem, where an errored or missed mask will cascade and influence the segmentation of the subsequent frames, which limits the performance of SAM2 toward complex long-term videos. To this end, we introduce SAM2Long, an improved \\textbf{training-free} video object segmentation strategy, which considers the segmentation uncertainty within each frame and chooses the video-level optimal results from multiple segmentation pathways in a constrained tree search manner. In practice, we maintain a fixed number of segmentation pathways throughout the video. For each frame, multiple masks are proposed based on the existing pathways, creating various candidate branches. We then select the same fixed number of branches with higher cumulative scores as the new pathways for the next frame. After processing the final frame, the pathway with the highest cumulative score is chosen as the final segmentation result. Benefiting from its heuristic search design, SAM2Long is robust toward occlusions and object reappearances, and can effectively segment and track objects for complex long-term videos. Without introducing any additional parameters or further training, SAM2Long significantly outperforms SAM2 on six VOS benchmarks. Notably, it achieves an average improvement of 3.8 points across all model sizes and, in some cases, up to 5 points in $\\mathcal{J}$&$\\mathcal{F}$ on long-term video object segmentation benchmarks SA-V and LVOS."
    },
    {
        "title": "Recursive Abstractive Processing for Retrieval in Dynamic Datasets",
        "link_suffix": "/forum?id=lvhEptUoFF",
        "link": "https://openreview.net/forum?id=lvhEptUoFF",
        "pdf_link": "https://openreview.net/pdf?id=lvhEptUoFF",
        "keywords": "Retrieval Augmented Language Models, Information Retrieval, Dynamic Datasets",
        "abstract": "Recent retrieval-augmented models enhance basic methods by building a hierarchical structure over retrieved text chunks through recursive embedding, clustering, and summarization. The most relevant information is then retrieved from both the original text and generated summaries. However, such approaches face limitations with dynamic datasets, where adding or removing documents over time complicates the updating of hierarchical representations formed through clustering.\nWe propose a new algorithm to efficiently maintain the recursive-abstractive tree structure in dynamic datasets, without compromising performance. Additionally, we introduce a novel post-retrieval method that applies query-focused recursive abstractive processing to substantially improve context quality. Our method overcomes the limitations of other approaches by functioning as a black-box post-retrieval layer compatible with any retrieval algorithm.\nBoth algorithms are validated through extensive experiments on real-world datasets, demonstrating their effectiveness in handling dynamic data and improving retrieval performance."
    },
    {
        "title": "Manifold Constraint Reduces Exposure Bias in Accelerated Diffusion Sampling",
        "link_suffix": "/forum?id=5xmXUwDxep",
        "link": "https://openreview.net/forum?id=5xmXUwDxep",
        "pdf_link": "https://openreview.net/pdf?id=5xmXUwDxep",
        "keywords": "Diffusion Models, Exposure Bias",
        "abstract": "Diffusion models have demonstrated significant potential for generating high-quality images, audio, and videos. However, their iterative inference process entails substantial computational costs, limiting practical applications. Recently, researchers have introduced accelerated sampling methods that enable diffusion models to generate samples with far fewer timesteps than those used during training. Nonetheless, as the number of sampling steps decreases, the prediction errors significantly degrade the quality of generated outputs. Additionally, the inherent exposure bias in diffusion models causes errors to propagate and amplify, further introducing non-negligible inaccuracies in inference. To address these challenges, we leverage a manifold hypothesis to explore the exposure bias problem in depth. Based on this geometric perspective, we propose a manifold constraint that effectively reduces exposure bias during accelerated sampling of diffusion models. Notably, our method involves no additional training and requires only minimal hyperparameter tuning. Extensive experiments on high-resolution datasets demonstrate the effectiveness of our approach, achieving a FID score of 15.60 with 10-step SDXL on MS-COCO, surpassing the baseline by a reduction of 2.57 in FID."
    },
    {
        "title": "Gaussian Head & Shoulders: High Fidelity Neural Upper Body Avatars with Anchor Gaussian Guided Texture Warping",
        "link_suffix": "/forum?id=HtbqsbNw9c",
        "link": "https://openreview.net/forum?id=HtbqsbNw9c",
        "pdf_link": "https://openreview.net/pdf?id=HtbqsbNw9c",
        "keywords": "Neural Radiance Field, Gaussian Splatting, Neural Head Avatar",
        "abstract": "The ability to reconstruct realistic and controllable upper body avatars from casual monocular videos is critical for various applications in communication and entertainment. By equipping the most recent 3D Gaussian Splatting representation with head 3D morphable models (3DMM), existing methods manage to create head avatars with high fidelity. However, most existing methods only reconstruct a head without the body, substantially limiting their application scenarios. We found that naively applying Gaussians to model the clothed chest and shoulders tends to result in blurry reconstruction and noisy floaters under novel poses. This is because of the fundamental limitation of Gaussians and point clouds -- each Gaussian or point can only have a single directional radiance without spatial variance, therefore an unnecessarily large number of them is required to represent complicated spatially varying texture, even for simple geometry. In contrast, we propose to model the body part with a neural texture that consists of coarse and pose-dependent fine colors. To properly render the body texture for each view and pose without accurate geometry nor UV mapping, we optimize another sparse set of Gaussians as anchors that constrain the neural warping field that maps image plane coordinates to the texture space. We demonstrate that Gaussian Head & Shoulders can fit the high-frequency details on the clothed upper body with high fidelity and potentially improve the accuracy and fidelity of the head region. We evaluate our method with casual phone-captured and internet videos and show our method archives superior reconstruction quality and robustness in both self and cross reenactment tasks. To fully utilize the efficient rendering speed of Gaussian splatting, we additionally propose an accelerated inference method of our trained model without Multi-Layer Perceptron (MLP) queries and reach a stable rendering speed of around 130 FPS for any subjects."
    },
    {
        "title": "Bootstrapping Language-Guided Navigation Learning with Self-Refining Data Flywheel",
        "link_suffix": "/forum?id=OUuhwVsk9Z",
        "link": "https://openreview.net/forum?id=OUuhwVsk9Z",
        "pdf_link": "https://openreview.net/pdf?id=OUuhwVsk9Z",
        "keywords": "vision-and-language navigation, data flywheel, dataset curation",
        "abstract": "Creating high-quality data for training robust language-instructed agents is a long-lasting challenge in embodied AI. In this paper, we introduce a Self-Refining Data Flywheel (SRDF) that generates high-quality and large-scale navigational instruction-trajectory pairs by iteratively refining the data pool through the collaboration between two models, the instruction generator and the navigator, without any human-in-the-loop annotation. Specifically, SRDF starts with using a base generator to create an initial data pool for training a base navigator, followed by applying the trained strong navigator to filter the data pool. This leads to higher-fidelity data to train a better generator, which can, in turn, produce higher-quality data for training the next-round navigator. Such a flywheel establishes a data self-refining process, yielding a continuously improved and highly effective dataset for large-scale language-guided navigation learning. Our experiments demonstrate that after several flywheel rounds, the navigator elevates the performance boundary from 70% to 78% SPL on the classic R2R test set, surpassing human performance (76%) for the first time. Meanwhile, this process results in a superior instruction generator, as reflected by the improved SPICE from 23.5 to 25.7, better than all published approaches tailored for VLN instruction generation. Finally, we demonstrate the scalability of our method through increasing environment and instruction diversity, and the generalization ability of our pre-trained navigator across various downstream navigation tasks, surpassing state-of-the-art performance by a large margin in all cases. Code is uploaded as supplementary materials and all our data/code/models will also be publicly released."
    },
    {
        "title": "Multimodal MR Image Synthesis via Learning Adaptive Group-wise Interactions",
        "link_suffix": "/forum?id=te2Q9dThlE",
        "link": "https://openreview.net/forum?id=te2Q9dThlE",
        "pdf_link": "https://openreview.net/pdf?id=te2Q9dThlE",
        "keywords": "Multimodal Image  Synthesis; Group-wise Rolling; Cross Group Attention",
        "abstract": "Multimodal MR image synthesis aims to generate missing modality image by fusing and mapping a few available MRI data. Most existing approaches typically adopt an image-to-image translation scheme. However, these methods often suffer from sub-optimal performance due to the spatial misalignment between different modalities while they are typically treated as input channels. Therefore, in this paper, we propose an \\textit{Adaptive Group-wise Interaction Network} (\\textbf{AGI-Net}) that explores both inter-modality and intra-modality relationships for multimodal MR image synthesis. Specifically, groups are first pre-defined along the channel dimension and then we perform an adaptive rolling for the standard convolutional kernel to capture inter-modality spatial correspondences. At the same time, a cross-group attention module is introduced to fuse information across different channel groups, leading to better feature representation. We evaluated the effectiveness of our model on the publicly available IXI and BraTS2023 datasets, where the AGI-Net achieved state-of-the-art performance for multimodal MR image synthesis. \\textit{Code will be released}."
    },
    {
        "title": "Hydra-SGG: Hybrid Relation Assignment for One-stage Scene Graph Generation",
        "link_suffix": "/forum?id=tpD1rs25Uu",
        "link": "https://openreview.net/forum?id=tpD1rs25Uu",
        "pdf_link": "https://openreview.net/pdf?id=tpD1rs25Uu",
        "keywords": "Scene Graph Generation, Visual Relation Detection",
        "abstract": "DETR introduces a simplified one-stage framework for scene graph generation (SGG) but faces challenges of sparse supervision and false negative samples. The former occurs because each image typically contains fewer than 10 relation annotations, while DETR-based SGG models employ over 100 relation queries. Each ground truth relation is assigned to only one query during training. The latter arises when one ground truth relation may have multiple queries with similar matching scores, leading to suboptimally matched queries being treated as negative samples. To address these, we propose Hydra-SGG, a one-stage SGG method featuring a Hybrid Relation Assignment. This approach combines a One-to-One Relation Assignment with an IoU-based One-to-Many Relation Assignment, increasing positive training samples and mitigating sparse supervision. In addition, we empirically demonstrate that removing self-attention between relation queries leads to duplicate predictions, which actually benefits the proposed One-to-Many Relation Assignment. With this insight, we introduce Hydra Branch, an auxiliary decoder without self-attention layers, to further enhance One-to-Many Relation Assignment by promoting different queries to make the same relation prediction. Hydra-SGG achieves state-of-the-art performance on multiple datasets, including VG150 (16.0 mR@50), Open Images V6 (50.1 weighted score), and GQA (12.7 mR@50). Our code and pre-trained models will be released."
    },
    {
        "title": "SANER: Annotation-free Societal Attribute Neutralizer for Debiasing CLIP",
        "link_suffix": "/forum?id=x5hXkSMOd1",
        "link": "https://openreview.net/forum?id=x5hXkSMOd1",
        "pdf_link": "https://openreview.net/pdf?id=x5hXkSMOd1",
        "keywords": "Societal bias, CLIP, Debiasing, Fairness",
        "abstract": "Large-scale vision-language models, such as CLIP, are known to contain harmful societal bias regarding protected attributes (e.g., gender and age). In this paper, we aim to address the problems of societal bias in CLIP. Although previous studies have proposed to debias societal bias through adversarial learning or test-time projecting, our comprehensive study of these works identifies two critical limitations: 1) loss of attribute information when it is explicitly disclosed in the input and 2) use of the attribute annotations during debiasing process. To mitigate societal bias in CLIP and overcome these limitations simultaneously, we introduce a simple-yet-effective debiasing method called SANER (societal attribute neutralizer) that eliminates attribute information from CLIP text features only of attribute-neutral descriptions. Experimental results show that SANER, which does not require attribute annotations and preserves original information for attribute-specific descriptions, demonstrates superior debiasing ability than the existing methods."
    },
    {
        "title": "Empowering Teachers with Enhanced Knowledge via Variable Scale Distillation Framework",
        "link_suffix": "/forum?id=O6DKGUwv0m",
        "link": "https://openreview.net/forum?id=O6DKGUwv0m",
        "pdf_link": "https://openreview.net/pdf?id=O6DKGUwv0m",
        "keywords": "knowledge distillation, hierarchical distillation, self-supervision, cross-scale image processing.",
        "abstract": "Knowledge distillation, a widely used model compression technique, enables a smaller student network to replicate the performance of a larger teacher network by transferring knowledge, typically in the form of softened class probabilities or feature representations. However, current approaches often fail to maximize the teacher\u2019s feature extraction capabilities, as they treat the semantic information transfer between teacher and student as equal. This paper presents a novel framework that addresses this limitation by enhancing the teacher\u2019s learning process through the Variable Scale Distillation Framework. Central to our approach is the Rescale Block, which preserves scale consistency during hierarchical distillation, allowing the teacher to extract richer, more informative features. In extensive experiments on the CIFAR100 dataset, our method consistently outperforms state-of-the-art distillation techniques, achieving an average accuracy improvement of 2.12%. This demonstrates the effectiveness of our approach in fully leveraging the teacher\u2019s capacity to guide the student, pushing the boundaries of knowledge distillation."
    },
    {
        "title": "Broadening Discovery through Structural Models: Multimodal Combination of Local and Structural Properties for Predicting Chemical Features.",
        "link_suffix": "/forum?id=N4lUNwEn1c",
        "link": "https://openreview.net/forum?id=N4lUNwEn1c",
        "pdf_link": "https://openreview.net/pdf?id=N4lUNwEn1c",
        "keywords": "ECFP, GRAPH, LLMs, TRANSFORMERS",
        "abstract": "In recent years, machine learning (ML) has significantly impacted the field of chemistry, facilitating advancements in diverse applications such as the prediction of molecular properties and the generation of molecular structures. Traditional string representations, such as the Simplified Molecular Input Line Entry System (SMILES), although widely adopted, exhibit limitations in conveying essential physical and chemical properties of compounds. Conversely, vector representations, particularly chemical fingerprints, have demonstrated notable efficacy in various ML tasks. Additionally, graph-based models, which leverage the inherent structural properties of chemical compounds, have shown promise in improving predictive accuracy. This study investigates the potential of language models based on fingerprints within a bimodal architecture that combines both graph-based and language model components. We propose a method that integrates the aforementioned approaches, significantly enhancing predictive performance compared to conventional methodologies while simultaneously capturing more accurate chemical information."
    },
    {
        "title": "FRUGAL: Memory-Efficient Optimization by Reducing State Overhead for Scalable Training",
        "link_suffix": "/forum?id=xSSo8kCA9G",
        "link": "https://openreview.net/forum?id=xSSo8kCA9G",
        "pdf_link": "https://openreview.net/pdf?id=xSSo8kCA9G",
        "keywords": "Memory-efficient training, Optimization, Full-Rank Update, Large Language Models, LLM, Pre-training, Fine-tuning",
        "abstract": "With the increase in the number of parameters in large language models, the process of pre-training and fine-tuning increasingly demands larger volumes of GPU memory. A significant portion of this memory is typically consumed by the optimizer state. To overcome this challenge, recent approaches such as low-rank adaptation (LoRA (Hu et al., 2021)), low-rank gradient projection (GaLore (Zhao\net al., 2024)), and block-wise optimization (BAdam (Luo et al., 2024)) have been proposed. However, in all these algorithms, the effective rank of the weight updates remains low-rank, which can lead to a substantial loss of information from the gradient. This loss can be critically important, especially during the pre-training stage. In this paper, we introduceFRUGAL; (Full-RankUpdates withGrAdient spLitting),, a new memory-efficient optimization framework. The framework leverages gradient splitting to perform low-rank updates using advanced optimization algorithms (such as Adam), while updates along the remaining directions are\nexecuted via state-free methods like SGD or signSGD. Our framework can be integrated with various low-rank update selection techniques, including GaLore and BAdam. We provide theoretical convergence guarantees for our framework when\nusing SGDM for low-rank updates and SGD for state-free updates. Additionally, our method consistently outperforms concurrent approaches across various fixed memory budgets, achieving state-of-the-art results in pre-training and fine-tuning\ntasks while balancing memory efficiency and perplexity targets."
    },
    {
        "title": "EXPLORING FEW-SHOT IMAGE GENERATION WITH MINIMIZED RISK OF OVERFITTING",
        "link_suffix": "/forum?id=FQaZeFGca2",
        "link": "https://openreview.net/forum?id=FQaZeFGca2",
        "pdf_link": "https://openreview.net/pdf?id=FQaZeFGca2",
        "keywords": "few shot learning, generative model, diffusion model",
        "abstract": "Few-shot image generation (FSIG) using deep generative models (DGMs) presents a significant challenge in accurately estimating the distribution of the target domain with extremely limited samples. Recent work has addressed the problem using a transfer learning approach, i.e., fine-tuning, leveraging a DGM that pre-trained on a large-scale source domain dataset, and then adapting it to the target domain with very limited samples. However, despite various proposed regularization techniques, existing frameworks lack a systematic mechanism to analyze the degree of overfitting, relying primarily on empirical validation without rigorous theoretical grounding.\nWe present Few-Shot Diffusion-regularized Representation Learning (FS-DRL), an innovative approach designed to minimize the risk of over-fitting while preserving distribution consistency in target image adaptation. \nOur method is distinct from conventional methods in two aspects: First, instead of fine-tuning, FS-DRL employs a novel scalable Invariant Guidance Matrix (IGM) during the diffusion process, which acts as a regularizer in the feature space of the model. This IGM is designed to have the same dimensionality as the target images, effectively constraining its capacity and encouraging it to learn a low-dimensional manifold that captures the essential structure of the target domain. Second, our method introduces a controllable parameter called sharing degree, which determines how many target images correspond to each IGM, enabling a fine-grained balance between overfitting risk and model flexibility, thus providing a quantifiable mechanism to analyze and mitigate overfitting.\nExtensive experiments demonstrate that our approach effectively mitigates overfitting, enabling efficient and robust few-shot learning across diverse domains."
    },
    {
        "title": "Cross-Modal Few-Shot Learning: a Generative Transfer Learning Framework",
        "link_suffix": "/forum?id=Cb4YXpqBIc",
        "link": "https://openreview.net/forum?id=Cb4YXpqBIc",
        "pdf_link": "https://openreview.net/pdf?id=Cb4YXpqBIc",
        "keywords": "Few-Shot Classification, Corss-modality Recognition, Transfer Learning",
        "abstract": "Most existing studies on few-shot learning focus on unimodal settings, where models are trained to generalize on unseen data using only a small number of labeled examples from the same modality. However, real-world data are inherently multi-modal, and unimodal approaches limit the practical applications of few-shot learning. To address this gap, this paper introduces the Cross-modal Few-Shot Learning (CFSL) task, which aims to recognize instances from multiple modalities when only a few labeled examples are available. \nThis task presents additional challenges compared to classical few-shot learning due to the distinct visual characteristics and structural properties unique to each modality. To tackle these challenges, we propose a Generative Transfer Learning (GTL) framework consisting of two stages: the first stage involves training on abundant unimodal data, and the second stage focuses on transfer learning to adapt to novel data. Our GTL framework jointly estimates the latent shared concept across modalities and in-modality disturbance in both stages, while freezing the generative module during the transfer phase to maintain the stability of the learned representations and prevent overfitting to the limited multi-modal samples. Our finds demonstrate that GTL has superior performance compared to state-of-the-art methods across four distinct multi-modal datasets: Sketchy, TU-Berlin, Mask1K, and SKSF-A. Additionally, the results suggest that the model can estimate latent concepts from vast unimodal data and generalize these concepts to unseen modalities using only a limited number of available samples, much like human cognitive processes."
    },
    {
        "title": "Dynamical Diffusion: Learning Temporal Dynamics with Diffusion Models",
        "link_suffix": "/forum?id=c5JZEPyFUE",
        "link": "https://openreview.net/forum?id=c5JZEPyFUE",
        "pdf_link": "https://openreview.net/pdf?id=c5JZEPyFUE",
        "keywords": "Diffusion Model, Generative Model, Prediction Learning, Dynamics",
        "abstract": "Diffusion models have emerged as powerful generative frameworks by progressively adding noise to data through a forward process and then reversing this process to generate realistic samples. While these models have achieved strong performance across various tasks and modalities, their application to temporal predictive learning remains underexplored. Existing approaches treat predictive learning as a conditional generation problem, but often fail to fully exploit the temporal dynamics inherent in the data, leading to challenges in generating temporally coherent sequences. To address this, we introduce Dynamical Diffusion (DyDiff), a theoretically sound framework that incorporates temporally aware forward and reverse processes. Dynamical Diffusion explicitly models temporal transitions at each diffusion step, establishing dependencies on preceding states to better capture temporal dynamics. Through the reparameterization trick, Dynamical Diffusion achieves efficient training and inference similar to any standard diffusion model. Extensive experiments across scientific spatiotemporal forecasting, video prediction, and time series forecasting demonstrate that Dynamical Diffusion consistently improves performance in temporal predictive tasks, filling a crucial gap in existing methodologies."
    },
    {
        "title": "\u03c8DAG : Projected Stochastic Approximation Iteration for DAG Structure Learning",
        "link_suffix": "/forum?id=iTVKOOZeYW",
        "link": "https://openreview.net/forum?id=iTVKOOZeYW",
        "pdf_link": "https://openreview.net/pdf?id=iTVKOOZeYW",
        "keywords": "Structure Learning, continuous optimization, directed acyclic graphs, stochastic optimization",
        "abstract": "Learning the structure of Directed Acyclic Graphs (DAGs) presents a significant challenge due to the vast combinatorial search space of possible graphs, which scales exponentially with the number of nodes. Recent advancements have redefined this problem as a continuous optimization task by incorporating differentiable acyclicity constraints. These methods commonly rely on algebraic characterizations of DAGs, such as matrix exponentials, to enable the use of gradient-based optimization techniques. Despite these innovations, existing methods often face optimization difficulties due to the highly non-convex nature of DAG constraints and the per-iteration computational complexity. In this work, we present a novel framework for learning DAGs, employing a Stochastic Approximation approach integrated with Stochastic Gradient Descent (SGD)-based optimization techniques. Our framework introduces new projection methods tailored to efficiently enforce DAG constraints, ensuring that the algorithm converges to a feasible local minimum. With its low iteration complexity, the proposed method is well-suited for handling large-scale problems with improved computational efficiency. We demonstrate the effectiveness and scalability of our framework through comprehensive experimental evaluations, which confirm its superior performance across various settings."
    },
    {
        "title": "Fixing Data Augmentations for Out-of-distribution Detection",
        "link_suffix": "/forum?id=1ebgtm7P10",
        "link": "https://openreview.net/forum?id=1ebgtm7P10",
        "pdf_link": "https://openreview.net/pdf?id=1ebgtm7P10",
        "keywords": "OOD Detection; Data Augmentation",
        "abstract": "Out-of-distribution (OOD) detection methods, especially post-hoc methods, rely on off-the-shelf pre-trained models. Existing literature shows how OOD and ID performance are correlated, i.e. stronger models with better ID performance tend to perform better in OOD detection. However, significant performance discrepancies exist between model versions, sometimes exceeding the impact of the OOD detection methods themselves. In this study, we systematically investigated this issue and identified two main factors\u2014label smoothing and mixup\u2014that, while improving in-distribution accuracy, lead to a decline in OOD detection performance. We provide empirical and theoretical explanations for this phenomenon and propose a solution that enhances OOD Detection while maintaining strong in-distribution performance. Code will be released upon acceptance."
    },
    {
        "title": "Correcting the Bias of  Normalizing Flows by Synthetic Outliers for Improving Out-of-Distribution Detection",
        "link_suffix": "/forum?id=yuymgwkjj1",
        "link": "https://openreview.net/forum?id=yuymgwkjj1",
        "pdf_link": "https://openreview.net/pdf?id=yuymgwkjj1",
        "keywords": "OOD Detection, Normalizing Flow",
        "abstract": "Out-of-distribution (OOD) detection is critical for ensuring the reliability and robustness of deep learning models in real-world applications. While normalizing flows have demonstrated impressive performance for various task of image OOD detection, recent findings suggest that they still encounter limitations and severe biases when applied to datasets with different statistics. Specifically, it has been observed that normalizing flow models tend to assign higher likelihoods to OOD samples with low complexity, which undermines the effectiveness of likelihood based OOD detection methods. In this paper, we explore the bias related to data complexity linked to normalizing flow models  in OOD detection. We propose a novel method for bias correction by incorporating synthetic outliers during training, guiding the model to assign lower likelihoods to OOD samples. Additionally, we introduce a specialized training objective that leverages the softplus function for OOD data, ensuring a smooth and effective training process. Extensive experiments on benchmark and high-dimensional real-world datasets, including both images and texts, confirm that our proposed approach significantly enhances OOD detection accuracy, achieving performance comparable to models trained with a limited number of real outliers. Moreover, our method increases the Lipschitz constant, supporting the hypothesis presented in related literature."
    },
    {
        "title": "General Framework for Off-Policy Learning with Partially-Observed Reward",
        "link_suffix": "/forum?id=mUbYof5MKp",
        "link": "https://openreview.net/forum?id=mUbYof5MKp",
        "pdf_link": "https://openreview.net/pdf?id=mUbYof5MKp",
        "keywords": "off-policy learning, partially-observed rewards, contextual bandits",
        "abstract": "Off-policy learning (OPL) in contextual bandits aims to learn a decision-making policy that maximizes the target rewards by using only historical interaction data collected under previously developed policies. Unfortunately, when rewards are only partially observed, the effectiveness of OPL degrades severely. Well-known examples of such partial rewards include explicit ratings in content recommendations, conversion signals on e-commerce platforms that are partial due to delay, and the issue of censoring in medical problems. One possible solution to deal with such partial rewards is to use secondary rewards, such as dwelling time, clicks, and medical indicators, which are more densely observed. However, relying solely on such secondary rewards can also lead to poor policy learning since they may not align with the target reward. Thus, this work studies a new and general problem of OPL where the goal is to learn a policy that maximizes the expected target reward by leveraging densely observed secondary rewards as supplemental data. We then propose a new method called Hybrid Policy Optimization for Partially-Observed Reward (HyPeR), which effectively uses the secondary rewards in addition to the partially observed target reward to achieve effective OPL despite the challenging scenario. We also discuss a case where we aim to optimize not only the expected target reward but also the expected secondary rewards to some extent; counter-intuitively, we will show that leveraging the two objectives is in fact advantageous also for the optimization of only the target reward. Along with statistical analysis of our proposed methods, empirical evaluations on both synthetic and real-world data show that HyPeR outperforms existing methods in various scenarios."
    },
    {
        "title": "Not-So-Optimal Transport Flows for 3D Point Cloud Generation",
        "link_suffix": "/forum?id=62Ff8LDAJZ",
        "link": "https://openreview.net/forum?id=62Ff8LDAJZ",
        "pdf_link": "https://openreview.net/pdf?id=62Ff8LDAJZ",
        "keywords": "Generative models, 3D point cloud generation, flow matching, optimal transport flows",
        "abstract": "Learning generative models of 3D point clouds is one of the fundamental problems in 3D generative learning. One of the key properties of point clouds is their permutation invariance, i.e., changing the order of points in a point cloud does not change the shape they represent. In this paper, we analyze the recently proposed equivariant OT flows that learn permutation invariant generative models for point-based molecular data and we show that these models scale poorly on large point clouds. Also, we observe learning (equivariant) OT flows is generally challenging since straightening flow trajectories makes the learned flow model complex at the beginning of the trajectory. To remedy these, we propose not-so-optimal transport flow models that obtain an approximate OT by an offline OT precomputation, enabling an efficient construction of OT pairs for training. During training, we can additionally construct a hybrid coupling by combining our approximate OT and independent coupling to make the target flow models easier to learn. In an extensive empirical study, we show that our proposed model outperforms prior diffusion- and flow -based approaches on a wide range of unconditional generation and shape completion on the ShapeNet benchmark."
    }
]