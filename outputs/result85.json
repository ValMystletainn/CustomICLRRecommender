[{"title": "Latte: Latent Attention for Linear Time Transformers", "link_suffix": "/forum?id=Cj3B4SoWuT", "link": "https://openreview.net/forum?id=Cj3B4SoWuT", "pdf_link": "https://openreview.net/pdf?id=Cj3B4SoWuT", "keywords": "Sequence Modelling, Long Sequences, Linear Attention, Latent Variable Model", "abstract": "The time complexity of the standard attention mechanism in transformers scales quadratically with sequence length. We propose a probabilistic framework for attention, enabling us to derive a novel low-rank linear re-parameterisation of both bidirectional and causal cases, based on defining a latent variable model. Our method can be seamlessly integrated as a drop-in replacement for the standard attention mechanism. Additionally, this framework provides a natural extension for combining local standard attention with our global linear attention. This approach allows us to extend the context length of existing large pre-trained models with only a few additional training steps. The resulting ``Latte Transformer'' achieves performance comparable to standard attention and other state-of-the-art models, while maintaining linear time and memory complexity, along with constant-time next-token prediction during inference.", "title_embedding_index": 4200, "title_abs_embedding_index": 4225}, {"title": "Knowledge Graph Based Agent For Complex, Knowledge-Intensive QA in Medicine", "link_suffix": "/forum?id=tnB94WQGrn", "link": "https://openreview.net/forum?id=tnB94WQGrn", "pdf_link": "https://openreview.net/pdf?id=tnB94WQGrn", "keywords": "Medical Reasoning; Medical QA, Agent, Knowledge Graph, LLM", "abstract": "Biomedical knowledge is uniquely complex and structured, requiring distinct reasoning strategies compared to other scientific disciplines like physics or chemistry. Biomedical scientists do not rely on a single approach to reasoning; instead, they use various strategies, including rule-based, prototype-based, and case-based reasoning. This diversity calls for flexible approaches that accommodate multiple reasoning strategies while leveraging in-domain knowledge. We introduce KGARevion, a knowledge graph (KG) based agent designed to address the complexity of knowledge-intensive medical queries. Upon receiving a query, KGARevion generates relevant triplets by using the knowledge base of the LLM. These triplets are then verified against a grounded KG to filter out erroneous information and ensure that only accurate, relevant data contribute to the final answer. Unlike RAG-based models, this multi-step process ensures robustness in reasoning while adapting to different models of medical reasoning. Evaluations on four gold-standard medical QA datasets show that KGARevion improves accuracy by over 5.2%, outperforming 15 models in handling complex medical questions. To test its capabilities, we curated three new medical QA datasets with varying levels of semantic complexity, where KGARevion achieved a 10.4% improvement in accuracy.  The source code is provided athttps://anonymous.4open.science/r/KGARevion-B3B6.", "title_embedding_index": 4201, "title_abs_embedding_index": 4226}, {"title": "Stated Causal Language Modeling: Off-the-Shelf Enhancement of Context Memorization", "link_suffix": "/forum?id=HPcpLDJlS6", "link": "https://openreview.net/forum?id=HPcpLDJlS6", "pdf_link": "https://openreview.net/pdf?id=HPcpLDJlS6", "keywords": "Memory-Enhanced Causal Language Modeling, Training-Free Approach, Context Compression, Language Models, Attention", "abstract": "We propose stated causal language modeling (stated-CLM), a novel method to enhance the memory capacity of large language models (LLMs) without modifying their architecture or parameters. Unlike existing context segmentation and sliding methods that discard low-weight tokens, stated-CLM compresses adjacent tokens, significantly reducing context information loss. We utilize the classic network pruning techniques with second-order derivatives to optimize the compressed token in the differentiable key-value space. Experiments on LLaMA, Mistral, and Gemma demonstrate that stated-CLM outperforms baselines on the LongBench benchmark by an average of 6.12% (LLaMA3.1-8B) and 5.97% (Mistral-v0.3-7B). On TopicRet, stated-CLM achieves accuracy levels comparable to full context models, while the baselines' accuracy is close to zero.", "title_embedding_index": 4202, "title_abs_embedding_index": 4227}, {"title": "Incorporating Human Preferences into Interpretable Reinforcement Learning with Tree Policies", "link_suffix": "/forum?id=sTllbUNLz0", "link": "https://openreview.net/forum?id=sTllbUNLz0", "pdf_link": "https://openreview.net/pdf?id=sTllbUNLz0", "keywords": "interpretable reinforcement learning, explainable reinforcement learning, preference learning, alignment", "abstract": "Interpretable reinforcement learning (RL) seeks to create agents that are efficient, transparent, and understandable to the populations that they impact. A significant gap in current approaches is the underutilization of human feedback, which is typically employed only for post-hoc evaluation. We propose to center the needs of end users by incorporating the feedback that would be obtained in a user study directly into the training of interpretable RL algorithms.  Our approach involves preference learning, where we learn preferences over high-level features that are not directly optimizable during the RL training process. We introduce an evolutionary algorithm that leverages user feedback to guide training toward interpretable decision-tree policies that are better-aligned with human preferences. We demonstrate the effectiveness of our method through experiments using synthetic preference data. Our results show an improvement in preference alignment compared to baselines, yielding policies that are more aligned with underlying user preferences but does so with sample efficiency in the number of user queries, thereby decreasing the burden on the user in providing such data.", "title_embedding_index": 4203, "title_abs_embedding_index": 4228}, {"title": "Scaling Sparse Feature Circuits For Studying In-Context Learning", "link_suffix": "/forum?id=Pa1vr1Prww", "link": "https://openreview.net/forum?id=Pa1vr1Prww", "pdf_link": "https://openreview.net/pdf?id=Pa1vr1Prww", "keywords": "SAE, ICL, SFC, Interpretability, Gemma, LLM", "abstract": "Sparse autoencoders (SAEs) are a popular tool for interpreting large language\nmodel activations, but their utility in addressing open questions in interpretability\nremains unclear. In this work, we demonstrate their effectiveness by using SAEs\nto deepen our understanding of the mechanism behind in-context learning (ICL).\nWe identify abstract SAE features that encode the model\u2019s knowledge of which\ntask to execute and whose latent vectors causally induce the task zero-shot. This\naligns with prior work showing that ICL is mediated by task vectors. We further\ndemonstrate that these task vectors are well approximated by a sparse sum of SAE\nlatents, including these task-execution features. To explore the ICL mechanism,\nwe adapt the sparse feature circuits methodology of Marks et al. (2024) to work for\nthe much larger Gemma-1 2B model, with 30 times as many parameters, and to\nthe more complex task of ICL. Through circuit finding, we discover task-detecting\nfeatures with corresponding SAE latents that activate earlier in the prompt, that\ndetect when tasks have been performed. They are causally linked with task-\nexecuting features through attention layer and MLP.", "title_embedding_index": 4204, "title_abs_embedding_index": 4229}, {"title": "Moral Alignment for LLM Agents", "link_suffix": "/forum?id=MeGDmZjUXy", "link": "https://openreview.net/forum?id=MeGDmZjUXy", "pdf_link": "https://openreview.net/pdf?id=MeGDmZjUXy", "keywords": "alignment, LLM fine-tuning, moral decision-making, social dilemmas", "abstract": "Decision-making agents based on pre-trained Large Language Models (LLMs) are increasingly being deployed across various domains of human activity. While their applications are currently rather specialized, several research efforts are under way to develop more generalist agents. As LLM-based systems become more agentic, their influence on human activity will grow and the transparency of this will decrease. Consequently, developing effective methods for aligning them to human values is vital.The prevailing practice in alignment often relies on human preference data (e.g., in RLHF or DPO), in which values are implicit and are essentially deduced from relative preferences over different model outputs. In this work, instead of relying on human feedback, we introduce the design of reward functions that explicitly encode core human values for Reinforcement Learning-based fine-tuning of foundation agent models. Specifically, we use intrinsic rewards for the moral alignment of LLM agents.We evaluate our approach using the traditional philosophical frameworks of Deontological Ethics and Utilitarianism, quantifying moral rewards for agents in terms of actions and consequences on the Iterated Prisoner's Dilemma (IPD) environment. We also show how moral fine-tuning can be deployed to enable an agent to unlearn a previously developed selfish strategy. Finally, we find that certain moral strategies learned on the IPD game generalize to several other matrix game environments. In summary, we demonstrate that fine-tuning with intrinsic rewards is a promising general solution for aligning LLM agents to human values, and it might represent a more transparent and cost-effective alternative to currently predominant alignment techniques.", "title_embedding_index": 4205, "title_abs_embedding_index": 4230}, {"title": "RecLM: Recommendation Instruction Tuning with Large Language Models", "link_suffix": "/forum?id=SXyUF6RVmT", "link": "https://openreview.net/forum?id=SXyUF6RVmT", "pdf_link": "https://openreview.net/pdf?id=SXyUF6RVmT", "keywords": "Large Language Model, Recommendation Systems, Instruction-tuning", "abstract": "Recommender systems aim to deeply understand users' complex preferences based on their past interactions. Deep collaborative filtering paradigms, leveraging advanced neural architectures like Graph Neural Networks (GNNs), excel at capturing collaborative relationships among users. However, limitations emerge when dealing with sparse data or zero-shot learning from unseen datasets, due to the design constraints of ID-based embedding functions in existing solutions. These challenges hinder robust generalization and adaptability. To address this, we propose a model-agnostic recommendation instruction-tuning paradigm that integrates large language models with collaborative filtering. Our Recommendation Language Model (RecLM) is introduced to enhance the capability of capturing user preference diversity. We design a reinforcement learning reward function to facilitate self-augmentation of our language models. Comprehensive evaluations demonstrate significant advantages of our approach across various settings. It can be integrated as a plug-and-play component with state-of-the-art recommender systems, resulting in notable performance enhancements.", "title_embedding_index": 4206, "title_abs_embedding_index": 4231}, {"title": "Constraining embedding learning with Self-Matrix Factorization", "link_suffix": "/forum?id=2hI3o9GHMq", "link": "https://openreview.net/forum?id=2hI3o9GHMq", "pdf_link": "https://openreview.net/pdf?id=2hI3o9GHMq", "keywords": "representation learning, constrained matrix decomposition, link prediction", "abstract": "We focus on the problem of learning object representations from solely association data, that is observed associations between objects of two different types, e.g. movies rated by users. We aim to obtain embeddings encoding object attributes that were not part of the learning process, e.g. movie genres. It has been shown that meaningful representations can be obtained by constraining the learning with manually curated object similarities. Here, we assume that objects lie in multiple linear manifolds embedded in high-dimensional space, and we argue that similarities between objects that correspond to sharing manifolds can be learned from the observed associations. We propose Self-Matrix Factorization (SMF), a method that learns object representations by constraining them with object similarities that are learned together with the representations. In our extensive evaluation across three real-world datasets, we compared SMF with SLIM, HCCF and NMF obtaining better performance at predicting missing associations as measured by RMSE and precision at top-K. We also show that SMF outperforms the competitors at encoding object attributes as measured by the embedding distances between objects divided into attribute-driven groups.", "title_embedding_index": 4207, "title_abs_embedding_index": 4232}, {"title": "Topic-XICL: Demonstration Selection with Topic Inference for Cross-lingual In-context Learning", "link_suffix": "/forum?id=tCmIGJivc7", "link": "https://openreview.net/forum?id=tCmIGJivc7", "pdf_link": "https://openreview.net/pdf?id=tCmIGJivc7", "keywords": "Multilingual and Cross-lingual, In-context Learning, Demonstration Selection for ICL", "abstract": "Cross-lingual in-context learning (XICL) shows promise for adapting large language models (LLMs) to low-resource languages. Previous methods typically rely on off-the-shelf similarity-based approaches or task-specific retrievers trained with LLM feedback for demonstration selection. However, these methods often overlook important factors beyond a single criterion or can be resource-intensive. To address these challenges, we propose a novel approach called Topic-XICL, which leverages a latent topic model for demonstration selection. We assume that latent topic variables encapsulate information that more accurately characterizes demonstrations. By training this topic model on rich-resource language data with a compact LLM, we obtain more relevant demonstrations through topic inference and apply them for in-context learning across various LLMs. We evaluated our method on three multilingual tasks (XNLI, XCOPA, and TyDiQA-GoldP) using three models with 7 to 8 billion parameters (BLOOM, Qwen1.5, and Llama3.1). Our approach outperformed the baselines\u2014random selection, semantic similarity, and clustering-based methods\u2014on TyDiQA-GoldP, XCOPA, and XNLI by 3.32%, 2.47%, and 1.77%, respectively, while requiring only moderate additional resources.", "title_embedding_index": 4208, "title_abs_embedding_index": 4233}, {"title": "Training-free guidance of diffusion models for generalised inpainting", "link_suffix": "/forum?id=AC1QLOJK7l", "link": "https://openreview.net/forum?id=AC1QLOJK7l", "pdf_link": "https://openreview.net/pdf?id=AC1QLOJK7l", "keywords": "generative, diffusion, sampling, guidance, langevin, mcmc, images, inpainting, proteins, t-cells", "abstract": "Diffusion models facilitate powerful control over the generative process. Here we introduce training-free guidance, a method for sampling from a broad class of conditional distributions that can be considered generalisations of inpainting. The method is grounded in annealed Langevin dynamics which ensures convergence to the exact conditional distribution, unlike existing methods for inpainting which rely on heuristics. We demonstrate training-free guidance using pretrained unconditional models for image, protein structure, and protein sequence generation and improve upon state-of-the-art approaches. We show the versatility of training-free guidance by addressing a wide range of tasks, including multi-motif scaffolding and amino acid mutagenesis of T cell receptors.", "title_embedding_index": 4209, "title_abs_embedding_index": 4234}, {"title": "Cometh: A continuous-time discrete-state graph diffusion model", "link_suffix": "/forum?id=AY89HCxunl", "link": "https://openreview.net/forum?id=AY89HCxunl", "pdf_link": "https://openreview.net/pdf?id=AY89HCxunl", "keywords": "Graph generation, discrete diffusion, molecule generation", "abstract": "Discrete-state denoising diffusion models led to state-of-the-art performance in graph generation, especially in the molecular domain. Recently, they have been transposed to continuous time, allowing more flexibility in the reverse process and a better trade-off between sampling efficiency and quality. Here, to leverage the benefits of both approaches, we propose Cometh, a continuous-time discrete-state graph diffusion model, tailored to the specificities of graph data. In addition, we also successfully replaced the set of structural encodings previously used in the discrete graph diffusion model with a single random-walk-based encoding, providing a simple and principled way to boost the model's expressive power. Empirically, we show that integrating continuous time leads to significant improvements across various metrics over state-of-the-art discrete-state diffusion models on a large set of molecular and non-molecular benchmark datasets. In terms of VUN samples, Cometh obtains a near-perfect performance of $99.5$% on the planar graph dataset and outperforms DiGress by $12.6$% on the large GuacaMol dataset.", "title_embedding_index": 4210, "title_abs_embedding_index": 4235}, {"title": "ColPali: Efficient Document Retrieval with Vision Language Models", "link_suffix": "/forum?id=ogjBpZ8uSi", "link": "https://openreview.net/forum?id=ogjBpZ8uSi", "pdf_link": "https://openreview.net/pdf?id=ogjBpZ8uSi", "keywords": "document embeddings, vision language models, late interaction, document retrieval, information retrieval", "abstract": "Documents are visually rich structures that convey information through text, but also figures, page layouts, tables, or even fonts. Since modern retrieval systems mainly rely on the textual information they extract from document pages to index documents -often through lengthy and brittle processes-, they struggle to exploit key visual cues efficiently. This limits their capabilities in many practical document retrieval applications such as Retrieval Augmented Generation (RAG).\nTo benchmark current systems on visually rich document retrieval, we introduce the Visual Document Retrieval Benchmark $\\textit{ViDoRe}$, composed of various page-level retrieval tasks spanning multiple domains, languages, and practical settings. \nThe inherent complexity and performance shortcomings of modern systems motivate a new concept; doing document retrieval by directly embedding the images of the document pages. We release $\\textit{ColPali}$, a Vision Language Model trained to produce high-quality multi-vector embeddings from images of document pages. Combined with a late interaction matching mechanism, $\\textit{ColPali}$ largely outperforms modern document retrieval pipelines while being drastically simpler, faster and end-to-end trainable. \nWe release models, data, code and benchmarks under open licenses athttps://hf.co/anonymous.", "title_embedding_index": 4211, "title_abs_embedding_index": 4236}, {"title": "Navigation with QPHIL: Offline Goal-Conditioned RL in a Learned Discretized Space", "link_suffix": "/forum?id=Uxm7DxPwrZ", "link": "https://openreview.net/forum?id=Uxm7DxPwrZ", "pdf_link": "https://openreview.net/pdf?id=Uxm7DxPwrZ", "keywords": "Reinforcement Learning, Offline Reinforcement Learning, Goal-Conditioned Reinforcement Learning", "abstract": "Offline Reinforcement Learning (RL) has emerged as a powerful alternative to imitation learning for behavior modeling in various domains, particularly in complex navigation tasks. An existing challenge with Offline RL is the signal-to-noise ratio, i.e. how to mitigate incorrect policy updates due to errors in value estimates. Towards this, multiple works have demonstrated the advantage of hierarchical offline RL methods, which decouples high-level path planning from low-level path following. In this work, we present a novel hierarchical transformer-based approach leveraging a learned quantizer of space. This quantization enables the training of a zone-conditioned low-level policy and simplifies planning, which is reduced to discrete autoregressive prediction. Among other benefits, zone-level reasoning in planning enables explicit trajectory stitching rather than implicit stitching based on noisy value function estimates. By combining this transformer-based planner with recent advancements in offline RL, our approach achieves state-of-the-art results in complex long-distance navigation environments.", "title_embedding_index": 4212, "title_abs_embedding_index": 4237}, {"title": "Assessing Large Language Models for Valid and Correct Code Reasoning", "link_suffix": "/forum?id=2umZVWYmVG", "link": "https://openreview.net/forum?id=2umZVWYmVG", "pdf_link": "https://openreview.net/pdf?id=2umZVWYmVG", "keywords": "LLM Reasoning, Code Execution Reasoning", "abstract": "Frontier large language models (LLMs) consider reasoning as first-class citizens: they learn to refine their reasoning process and try different strategies during training. Thereby, when prompted, can think through problems and respond better with proper reasoning. For programming tasks, this makes code reasoning a must. In this paper, we propose the task of Code Execution Simulation (CES) as a proxy for evaluating the code reasoning capabilities of LLMs. CES defines the notions of valid or invalid reasoning process, which enables it to promptly (1) determine where the execution simulation diverges from ground truth for incorrect output predictions (essential to understanding limitations of LLMs in code reasoning) and (2) identify suspiciously correct output predictions (essential to understanding reasoning shortcuts, hallucinations, or potential data leakage). In addition to evaluating LLMs\u2019 execution reasoning on a program with a single test, CES measures their reasoning consistency across tests with the same or different prime path coverage. This enables it to evaluate the code reasoning of LLMs in a spectrum: strong, weak, and random. Our results show that LLMs, to a great extent (82.32%), follow a valid reasoning process (results in 30.79% correct and 51.53% incorrect output predictions). However, their reasoning is mostly random (55.59%) or weak (41.69%), which explains their weakness in programming tasksthat require flow- or path-sensitive program analysis to succeed.", "title_embedding_index": 4213, "title_abs_embedding_index": 4238}, {"title": "Content-Style Learning from Unaligned Domains: Identifiability under Unknown Latent Dimensions", "link_suffix": "/forum?id=p60Y6o85Cj", "link": "https://openreview.net/forum?id=p60Y6o85Cj", "pdf_link": "https://openreview.net/pdf?id=p60Y6o85Cj", "keywords": "unsupervised learning, identifiability, unknown latent dimension", "abstract": "Understanding identifiability of latent content and style variables from unaligned multi-domain data is essential for tasks such as\ndomain translation and data generation. Existing works on content-style identification were often developed under somewhat stringent conditions, e.g., that all latent components are mutually independent and that the dimensions of the content and style variables are known. We introduce a new analytical framework via cross-domainlatent distribution matching(LDM), which establishes content-style identifiability under substantially more relaxed conditions. Specifically, we show that restrictive assumptions such as component-wise independence of the latent variables can be removed. Most notably, we prove that prior knowledge of the content and style dimensions is not necessary for ensuring identifiability, if sparsity constraints are properly imposed onto the learned latent representations. Bypassing the knowledge of the exact latent dimension has been a longstanding aspiration in unsupervised representation learning---our analysis is the first to underpin its theoretical and practical viability. On the implementation side, we recast the LDM formulation into a regularized multi-domain GAN loss with coupled latent variables. We show that the reformulation is equivalent to LDM under mild conditions---yet requiring considerably less computational resource. Experiments corroborate with our theoretical claims.", "title_embedding_index": 4214, "title_abs_embedding_index": 4239}, {"title": "Multi-scale Conditional Generative Modeling for Microscopic Image Restoration", "link_suffix": "/forum?id=19QWQSsbOA", "link": "https://openreview.net/forum?id=19QWQSsbOA", "pdf_link": "https://openreview.net/pdf?id=19QWQSsbOA", "keywords": "Microscopic Image Restoration, Generative Model", "abstract": "The advance of diffusion-based generative models in recent years has revolutionized state-of-the-art (SOTA) techniques in a wide variety of image analysis and synthesis tasks, whereas their adaptation on image restoration, particularly within computational microscopy remains theoretically and empirically underexplored. In this research, we introduce a multi-scale generative model that enhances conditional image restoration through a novel exploitation of the Brownian Bridge process within wavelet domain. By initiating the Brownian Bridge diffusion process specifically at the lowest-frequency subband and applying generative adversarial networks at subsequent multi-scale high-frequency subbands in the wavelet domain, our method provides significant acceleration during training and sampling while sustaining a high image generation quality and diversity on par with SOTA diffusion models. Experimental results on various computational microscopy and imaging tasks confirm our method's robust performance and its considerable reduction in its sampling steps and time. This pioneering technique offers an efficient image restoration framework that harmonizes efficiency with quality, signifying a major stride in incorporating cutting-edge generative models into computational microscopy workflows.", "title_embedding_index": 4215, "title_abs_embedding_index": 4240}, {"title": "LLS: Regulating Neural Network Training via Learnable Label Smoothing", "link_suffix": "/forum?id=VmW7Sf84sj", "link": "https://openreview.net/forum?id=VmW7Sf84sj", "pdf_link": "https://openreview.net/pdf?id=VmW7Sf84sj", "keywords": "Learnable Label Smoothing, LLS, Label Regularization", "abstract": "Training a neural network using one-hot targets often leads to the issue of overconfidence. \nTo address this, Label Smoothing has been introduced, modifying the targets to a mix of one-hot encoding and a uniform probability vector. \nHowever, the uniform probability vector indiscriminately assigns equal weights to all categories, thereby undermining inter-category relationships. To overcome these challenges, we propose a novel solution, Learnable Label Smoothing (LLS) that aims to regulate training by granting networks the ability to assign optimal targets. Unlike conventional methods, Learnable Label Smoothing utilizes probability vectors unique to each category, resulting in diverse targets. The acquired relationships are beneficial for regularization and also prove to be transferable, facilitating knowledge distillation even in the absence of a Teacher model. Our extensive experiments across multiple datasets highlight the advantages of our method in addressing both overconfidence and the preservation of inter-category relationships in neural network training.", "title_embedding_index": 4216, "title_abs_embedding_index": 4241}, {"title": "Worse than Random? An Embarrassingly Simple Probing Evaluation of Large Multimodal Models in Medical VQA", "link_suffix": "/forum?id=QByW8EYEtt", "link": "https://openreview.net/forum?id=QByW8EYEtt", "pdf_link": "https://openreview.net/pdf?id=QByW8EYEtt", "keywords": "Vision and Language, AI for Healthcare, Benchmark", "abstract": "Large Multimodal Models (LMMs) have shown remarkable progress in medical Visual Question Answering (Med-VQA), achieving high accuracy on existing benchmarks. However, their reliability under robust evaluation is questionable. This study reveals that state-of-the-art models perform worse than random guessing on medical diagnosis questions when subjected to simple probing evaluation. To address this critical evaluation problem, we introduce the Probing Evaluation for Medical Diagnosis (ProbMed) dataset to rigorously assess LMM performance in medical imaging through probing evaluation and procedural diagnosis. Particularly, probing evaluation features pairing original questions with negation questions with hallucinated attributes, while procedural diagnosis requires reasoning across various diagnostic dimensions for each image, including modality recognition, organ identification, clinical findings, abnormalities, and positional grounding. Our evaluation reveals that top-performing models like GPT-4o, GPT-4V, and Gemini Pro perform worse than random guessing on specialized diagnostic questions, indicating significant limitations in handling fine-grained medical inquiries. We further investigate the underperformance of open-source models (e.g., LLaVA, LLaVA-Med, and Med-Flamingo) through an ablation study. This study reveals that poor visual understanding is a primary bottleneck, which can be mitigated by adding visual descriptions generated by GPT-4o, leading to an average performance improvement of 9.44%. These findings underscore the urgent need for more robust evaluation methods and domain-specific expertise to ensure LMM reliability in critical medical fields.", "title_embedding_index": 4217, "title_abs_embedding_index": 4242}, {"title": "Generalization Gradient Descent", "link_suffix": "/forum?id=fkKW1PK8Ga", "link": "https://openreview.net/forum?id=fkKW1PK8Ga", "pdf_link": "https://openreview.net/pdf?id=fkKW1PK8Ga", "keywords": "out-of-distribution, generalization problem, gradient descent", "abstract": "We propose a new framework for evaluating the relationship between features\nand generalization via a theoretical analysis of the out-of-distribution (OOD)\ngeneralization problem, in which we simultaneously use two mathematical methods:\na generalization ratio that quantitatively characterizes the degree of generalization,\nand a generalization decision process (GDP) that formalizes the relationship of loss\nbetween seen and unseen domains. By combining the concepts of informativeness\nand variation in the generalization ratio, we intuitively associate them with OOD\nproblems to derive the generalization inequality. We then introduce it to the GDP to\nselect the best loss from seen domains to gradient descent for backpropagation. In\nthe case where the classifier is defined by fully connected neural network, the entire\nsystem is trained with backpropagation. There is no need for any model selection\ncriterion or operating on gradients during training. Experiments demonstrate the\npotential of the framework through qualitative and quantitative evaluation of the\ngeneralization ability.", "title_embedding_index": 4218, "title_abs_embedding_index": 4243}, {"title": "From an LLM Swarm to a PDDL-empowered Hive: Planning Self-executed Instructions in a Multi-modal Jungle", "link_suffix": "/forum?id=QAAsnSRwgu", "link": "https://openreview.net/forum?id=QAAsnSRwgu", "pdf_link": "https://openreview.net/pdf?id=QAAsnSRwgu", "keywords": "Deep Models, Planning, PDDL, Knowledge Graphs, Benchmark, Large Language Models", "abstract": "In response to the call for agent-based solutions that leverage the ever-increasing capabilities of the deep models' ecosystem, we introduce a comprehensive solution for selecting appropriate models and subsequently planning a set of atomic actions to satisfy the end-users' instructions.Our system, Hive, operates over sets of models and, upon receiving natural language instructions, schedules and executes, explainable plans of atomic actions. These actions can involve one or more of the available models to achieve the overall task, while respecting end-users specific constraints. Hive is able to plan complex chains of actions while guaranteeing explainability, using an LLM-based formal logic backbone empowered by PDDL operations. We introduce the MuSE benchmark in order to offer a comprehensive evaluation of the multi-modal capabilities of agent systems. Our findings show that our framework redefines the state-of-the-art for task selection, outperforming other competing systems that plan operations across multiple models while offering transparency guarantees while fully adhering to user constraints.", "title_embedding_index": 4219, "title_abs_embedding_index": 4244}, {"title": "ZooProbe: A Data Engine for Evaluating, Exploring, and Evolving Large-scale Training Data for Multimodal LLMs", "link_suffix": "/forum?id=T4LtGj7us1", "link": "https://openreview.net/forum?id=T4LtGj7us1", "pdf_link": "https://openreview.net/pdf?id=T4LtGj7us1", "keywords": "Multimodal Large Language Model, Training Data Engine, Deep Learning", "abstract": "Multimodal Large Language Models (MLLMs) are thriving through continuous fine-tuning by LLMs. Driven by the law that \"scale is everything\", MLLMs expand their training sets during version iterations. In this paper, we propose a large-scale training data engine built around an evaluating-exploring-evolving (E3) loop. Evaluating the data provides insights into its characteristics. Exploring quality rules helps identify which data enhances training. Together, these processes facilitate the systematic evolution of new, high-quality data. With the E3 loop, we introduce ZooProbe, an efficient data engine for MLLMs. First, the problem of data expansion is formalized as a tree of sampling and growth. ZooProbe introduces a small-scale modelzooto obtain comprehensive evaluations for child datasets. From multiple perspectives, visual, textual, and multimodal models cover over 50 dimensions of intrinsic and meta attributes, such as object and topic distribution, and higher-level properties, like annotation quality and scene complexity. ZooProbe constructs based on A$^\\star$ search, modeling the heuristic function as a quality estimate from data evaluation results. It dynamically explores the rule of data quality based on the model state of theprobedatasets. Additionally, it evolves new targeted data with identified high-quality rules. We also develop an extra heuristic quality ranker with the data utilized and discarded during the expansion. Our experiments show that ZooProbe significantly breaks the scaling law in multimodal instruction fine-tuning at scales of 260$k$ and below.\nZooProbe generates high-quality data that accelerates MLLM training and enhances performance, automating the evolution of large-scale training data.", "title_embedding_index": 4220, "title_abs_embedding_index": 4245}, {"title": "Wicked Oddities: Selectively Poisoning for Effective Clean-Label Backdoor Attacks", "link_suffix": "/forum?id=1Z3C49JQVf", "link": "https://openreview.net/forum?id=1Z3C49JQVf", "pdf_link": "https://openreview.net/pdf?id=1Z3C49JQVf", "keywords": "backdoor attack, data selection", "abstract": "Deep neural networks are vulnerable to backdoor attacks, a  type of adversarial attack that poisons the training data to manipulate the behavior of models trained on such data. \nClean-label attacks are a more stealthy form of backdoor attacks that can perform the attack without changing the labels of poisoned data.\nEarly works on clean-label attacks added triggers to a random subset of the training set, ignoring the fact that samples contribute unequally to the attack's success. This results in high poisoning rates and low attack success rates.\nTo alleviate the problem, several supervised learning-based sample selection strategies have been proposed.\nHowever, these methods assume access to the entire labeled training set and require training, which is expensive and may not always be practical.\nThis work studies a new and more practical (but also more challenging) threat model where the attacker only provides data for the target class (e.g., in face recognition systems) and has no knowledge of the victim model or any other classes in the training set.\nWe study different strategies for selectively poisoning a small set of training samples in the target class to boost the attack success rate in this setting. \nOur threat model poses a serious threat in training machine learning models with third-party datasets, since the attack can be performed effectively with limited information. Experiments on benchmark datasets illustrate the effectiveness of our strategies in improving clean-label backdoor attacks.", "title_embedding_index": 4221, "title_abs_embedding_index": 4246}, {"title": "RotRNN: Modelling Long Sequences with Rotations", "link_suffix": "/forum?id=z6qmomJW91", "link": "https://openreview.net/forum?id=z6qmomJW91", "pdf_link": "https://openreview.net/pdf?id=z6qmomJW91", "keywords": "Sequence Modelling, Recurrent Neural Networks, State Space Models, Long Sequences", "abstract": "Linear recurrent neural networks, such as State Space Models (SSMs) and Linear Recurrent Units (LRUs), have recently shown state-of-the-art performance on long sequence modelling benchmarks. Despite their success, their empirical performance is not well understood and they come with a number of drawbacks, most notably their complex initialisation and normalisation schemes. In this work, we address some of these issues by proposing RotRNN \u2013 a linear recurrent model which utilises the convenient properties of rotation matrices. We show that RotRNN provides a simple and efficient model with a robust normalisation procedure, and a practical implementation that remains faithful to its theoretical derivation. RotRNN also achieves competitive performance to state-of-the-art linear recurrent models on several long sequence modelling datasets.", "title_embedding_index": 4222, "title_abs_embedding_index": 4247}, {"title": "Faster Gradient Descent in Deep Linear Networks: The Advantage of Depth", "link_suffix": "/forum?id=NbbsRnPBoS", "link": "https://openreview.net/forum?id=NbbsRnPBoS", "pdf_link": "https://openreview.net/pdf?id=NbbsRnPBoS", "keywords": "Deep Linear Network; Gradient Descent; Faster Convergence in Finite Time", "abstract": "Gradient descent dynamics in deep linear networks has been studied under a wide range of settings. These studies have reported some negative results on the role of depth, in that, gradient descent in  deep linear networks: (i) can take exponential number of iterations to converge, (ii) can exhibit sigmoidal learning, i.e., almost no learning in initial phase followed by rapid learning, (iii) can delay convergence with increase in depth. Some of these results are also under stronger assumptions such as whitened data and balanced initialisation. These messages from prior works suggest that depth hurts the speed of convergence.In this paper, we argue that the negative role of depth in the prior works is due to certain pitfalls which can be carefully avoided. We give a positive message on the role of depth, i.e., seen as an additional resource, depth can always be used to speed up convergence. For this purpose, we consider scalar regression with quadratic loss. In this setting, we propose a novel aligned gradient descent (AGD) algorithm for which we show that (i) linear convergence is always possible (ii) depth accelerates the speed of convergence. In AGD, feature alignment happens in first layer and the deeper layers accelerate by learning the right scale. We show acceleration in AGD happens in finite time for unwhitened data. We provide insights into the {acceleration} mechanism and also show that acceleration happens in phases. We also demonstrate the acceleration due to AGD on synthetic and benchmark datasets. Our main message is not propose AGD as a new algorithm in itself, but to demonstrate that depth is an advantage in linear networks thereby dispelling some of the past negative results on the role of depth.", "title_embedding_index": 4223, "title_abs_embedding_index": 4248}, {"title": "Evaluating Synthetic Activations composed of SAE Latents in GPT-2", "link_suffix": "/forum?id=U0y32WKeOd", "link": "https://openreview.net/forum?id=U0y32WKeOd", "pdf_link": "https://openreview.net/pdf?id=U0y32WKeOd", "keywords": "Mechanistic Interpretability, SAEs, Activations, SAE Latents", "abstract": "Sparse Auto-Encoders (SAEs) are commonly employed in mechanistic interpretability to decompose the residual stream into monosemantic SAE latents. Recent work demonstrates that perturbing a model's activations at an early layer results in a step-function-like change in the model's final layer activations. Furthermore, the model's sensitivity to this perturbation differs between model-generated (real) activations and random activations. In our study, we assess model sensitivity in order to compare real activations to synthetic activations composed of SAE latents. Our findings indicate that synthetic activations closely resemble real activations when we control for the sparsity and cosine similarity of the constituent SAE latents. This suggests that real activations cannot be explained by a simple \"bag of SAE latents\" lacking internal structure, and instead suggests that SAE latents possess significant geometric and statistical properties. Notably, we observe that our synthetic activations exhibit less pronounced activation plateaus compared to those typically surrounding real activations.", "title_embedding_index": 4224, "title_abs_embedding_index": 4249}]