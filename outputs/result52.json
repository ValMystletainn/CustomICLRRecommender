[
    {
        "title": "The 3D-PC: a benchmark for visual perspective taking in humans and machines",
        "link_suffix": "/forum?id=UIFAJZ22ZF",
        "link": "https://openreview.net/forum?id=UIFAJZ22ZF",
        "pdf_link": "https://openreview.net/pdf?id=UIFAJZ22ZF",
        "keywords": "3D vision, visual cognition, developmental psychology, visual reasonsing",
        "abstract": "Visual perspective taking (VPT) is the ability to perceive and reason about the perspectives of others. It is an essential feature of human intelligence, which develops over the first decade of life and requires an ability to process the 3D structure of visual scenes. A growing number of reports have indicated that deep neural networks (DNNs) become capable of analyzing 3D scenes after training on large image datasets. We investigated if this emergent ability for 3D analysis in DNNs is sufficient for VPT with the 3D perception challenge (3D-PC): a novel benchmark for 3D perception in humans and DNNs. The 3D-PC is comprised of three 3D-analysis tasks posed within natural scene images: (i.) a simple test of object depth order, (ii.) a basic VPT task (VPT-basic), and (iii.) a more challenging version of VPT (VPT-perturb) designed to limit the effectiveness of \"shortcut\" visual strategies. We tested human participants (N=33) and linearly probed or text-prompted over 300 DNNs on the challenge and found that nearly all of the DNNs approached or exceeded human accuracy in analyzing object depth order. Surprisingly, DNN accuracy on this task correlated with their object recognition performance. In contrast, there was an extraordinary gap between DNNs and humans on VPT-basic. Humans were nearly perfect, whereas most DNNs were near chance. Fine-tuning DNNs on VPT-basic brought them close to human performance, but they, unlike humans, dropped back to chance when tested on VPT-perturb. Our challenge demonstrates that the training routines and architectures of today's DNNs are well-suited for learning basic 3D properties of scenes and objects but are ill-suited for reasoning about these properties like humans do. We release our 3D-PC datasets and code to help bridge this gap in 3D perception between humans and machines."
    },
    {
        "title": "SAIL: Self-improving Efficient Online Alignment of Large Language Models",
        "link_suffix": "/forum?id=02kZwCo0C3",
        "link": "https://openreview.net/forum?id=02kZwCo0C3",
        "pdf_link": "https://openreview.net/pdf?id=02kZwCo0C3",
        "keywords": "RLHF, Alignment, Online Alignment, Self-Play",
        "abstract": "Reinforcement Learning from Human Feedback (RLHF) is a critical method for aligning large language models (LLMs) with human preferences. However, existing offline alignment approaches, such as DPO, IPO, and SLiC, rely heavily on static datasets of human preferences, often leading to suboptimal performance. Recent efforts in the literature have moved towards online RLHF methods, but they lack a unified framework and suffer from distribution shift issues. In this work, we formalize online LLM alignment as a bilevel optimization problem. By reducing this formulation to a more computationally efficient single-level first-order method, utilizing reward-policy equivalence, we propose SAIL (Self-improving Efficient Online Alignment).SAIL generates new samples and iteratively refines model alignment through online exploration and regulation of preference labels. This enables continuous, self-improving alignment and generalizes prior online RLHF methods as special cases. Compared to state-of-the-art RLHF methods, SAIL delivers significant performance gains, with up to 11.6% improvement in win rate and a 3.6-point increase in evaluation rewards, while maintaining low computational overhead."
    },
    {
        "title": "HelpSteer2-Preference: Complementing Ratings with Preferences",
        "link_suffix": "/forum?id=MnfHxPP5gs",
        "link": "https://openreview.net/forum?id=MnfHxPP5gs",
        "pdf_link": "https://openreview.net/pdf?id=MnfHxPP5gs",
        "keywords": "reward modelling, rlhf, model alignment",
        "abstract": "Reward models are critical for aligning models to follow instructions, and are typically trained following one of two popular paradigms: Bradley-Terry style or Regression style. However, there is a lack of evidence that either approach is better than the other, when adequately matched for data. This is primarily because these approaches require data collected in different (but incompatible) formats, meaning that adequately matched data is not available in existing public datasets. To tackle this problem, we release preference annotations (designed for Bradley-Terry training) to complement existing ratings (designed for Regression style training) in the HelpSteer2 dataset. To improve data interpretability, preference annotations are accompanied with human-written justifications. Using this data, we conduct the first head-to-head comparison of Bradley-Terry and Regression models when adequately matched for data. Based on insights derived from such a comparison, we propose a novel approach to combine Bradley-Terry and Regression reward modeling. A Llama-3.1-70B-Instruct model tuned with this approach scores 94.1 on RewardBench, emerging top of more than 140 reward models as of 1 Oct 2024. We also demonstrate the effectiveness of this reward model at aligning models to follow instructions in RLHF. \nWe open-source this dataset (CC-BY-4.0 license) and openly release the trained Reward Model."
    },
    {
        "title": "Multi-Modal Foundation Models Induce Interpretable Molecular Graph Languages",
        "link_suffix": "/forum?id=2kfpkTD5ZE",
        "link": "https://openreview.net/forum?id=2kfpkTD5ZE",
        "pdf_link": "https://openreview.net/pdf?id=2kfpkTD5ZE",
        "keywords": "multimodal foundation models, molecular design, interpretability",
        "abstract": "Recently, domain-specific languages (DSLs) for molecular generation have shown advantages in data-efficiency and interpretability. However, constructing such a DSL requires human expertise or significant computational costs. Multi-modal foundation models (MMFMs) have shown remarkable in-context abilities for tasks across vision and text domains, but not graphs. We explore an unconventional solution: we render the molecule as an image, describe it using text, and cast the DSL construction into an equivalent problem of constructing a tree decomposition for the molecular graph. The MMFM performs a chain of discrete decisions to replace traditional heuristics used within the execution of the decomposition, enabling the smooth integration of its prior knowledge without overstepping the limits of the soundness of the algorithm. Furthermore, we collect MMFMâ€™s reasoning for each decision into a design story, have non-expert agents evaluate stories for correctness and persuasiveness, and close the feedback loop to improve the DSL. Our method, Foundation Molecular Grammar (FMG), demonstrates significant advantages in synthesizability, diversity, and data-efficiency on molecule generation benchmarks. Moreover, its compelling chemical interpretability offers built-in transparency over the molecular discovery workflow, paving the way for additional feedback and oversight."
    },
    {
        "title": "Quantum Architecture Search With Unsupervised Representation Learning",
        "link_suffix": "/forum?id=waf6HreC53",
        "link": "https://openreview.net/forum?id=waf6HreC53",
        "pdf_link": "https://openreview.net/pdf?id=waf6HreC53",
        "keywords": "Quantum Circuit Architecture Search, QAS, unsupervised representation learning",
        "abstract": "Unsupervised representation learning presents new opportunities for advancing Quantum Architecture Search (QAS) on Noisy Intermediate-Scale Quantum (NISQ) devices. QAS is designed to optimize quantum circuits for Variational Quantum Algorithms (VQAs). Most QAS algorithms tightly couple the search space and search algorithm, typically requiring the evaluation of numerous quantum circuits, resulting in high computational costs and limiting scalability to larger quantum circuits. Predictor-based QAS algorithms mitigate this issue by estimating circuit performance based on structure or embedding. However, these methods often demand time-intensive labeling to optimize gate parameters across many circuits, which is crucial for training accurate predictors. Inspired by the classical neural architecture search algorithm \\textit{Arch2vec}, we investigate the potential of unsupervised representation learning for QAS without relying on predictors. Our framework decouples unsupervised architecture representation learning from the search process, enabling the learned representations to be applied across various downstream tasks. Additionally, it integrates an improved quantum circuit graph encoding scheme, addressing the limitations of existing representations and enhancing search efficiency. This predictor-free approach removes the need for large labeled datasets. During the search, we employ REINFORCE and Bayesian Optimization to explore the latent representation space and compare their performance against baseline methods. Our results demonstrate that the framework efficiently identifies high-performing quantum circuits with fewer search iterations."
    },
    {
        "title": "Policy optimization emerges from noisy representation learning",
        "link_suffix": "/forum?id=cH4VTcCVYs",
        "link": "https://openreview.net/forum?id=cH4VTcCVYs",
        "pdf_link": "https://openreview.net/pdf?id=cH4VTcCVYs",
        "keywords": "natural intelligence, reinforcement learning, representation learning, noise",
        "abstract": "Nervous systems learn representations of the world and policies to act within it. We present a framework that uses reward-dependent noise to facilitate policy optimization in representation learning networks. These networks balance extracting normative features and task-relevant information to solve tasks. Moreover, their representation changes reproduce several experimentally observed shifts in the neural code during task learning. Our framework presents a biologically plausible mechanism for emergent policy optimization amid evidence that representation learning plays a vital role in governing neural dynamics."
    },
    {
        "title": "Aya in Action: An Investigation of its Abilities in Aspect-Based Sentiment Analysis, Hate Speech Detection, Irony Detection, and Question-Answering",
        "link_suffix": "/forum?id=3GMuudWmMV",
        "link": "https://openreview.net/forum?id=3GMuudWmMV",
        "pdf_link": "https://openreview.net/pdf?id=3GMuudWmMV",
        "keywords": "Sentiment Analysis, Hate Speech Detection, Irony Detection, Question-Answering, Large Language Models, Few-shot Learning, Portuguese Language.",
        "abstract": "While resource-rich languages such as English and Mandarin drive considerable advancements, low-resource languages face challenges due to the scarcity of substantial digital and annotated linguistic resources. Within this context, \nin 2024, Aya was introduced, a multilingual generative language model supporting 101 languages, over half of which are lower-resourced. This study aims to assess Aya's performance in tasks such as Aspect-Based Sentiment Analysis, Hate Speech Detection, Irony Detection, and Question-Answering, using a few-shot methodology in Brazilian Portuguese. The objective is to evaluate Aya's effectiveness in these tasks without fine-tuning the pre-trained model, thereby exploring its potential to improve the quality and accuracy of outputs in various natural language understanding tasks.\nResults indicate that while Aya performs well in certain tasks like Question-Answering, where it surpassed Portuguese-specific models with an Exact Match score of 58.79%, it struggles in others. For the Hate Speech Detection task, Aya's F1-score of 0.64 was significantly lower than the 0.94 achieved by the SabiÃ¡-7B model. Additionally, the model's performance on the Aspect-Based Sentiment Analysis task improved considerably when neutral examples were excluded, but its handling of complex slang and context-dependent features in other tasks remained challenging. These results suggest that multilingual models like Aya can perform competitively in some contexts but may require further tuning to match the effectiveness of models specifically trained for Portuguese."
    },
    {
        "title": "NaN Pooling and Convolution Accelerate U-Nets",
        "link_suffix": "/forum?id=9GJ6JKoCVp",
        "link": "https://openreview.net/forum?id=9GJ6JKoCVp",
        "pdf_link": "https://openreview.net/pdf?id=9GJ6JKoCVp",
        "keywords": "Pooling, Convolutions, Deep learning, Optimization, Neuroimaging, Convolutional Neural Networks, Numerical Analysis",
        "abstract": "Recent advancements in deep learning for neuroimaging have resulted in the development of increasingly complex models designed for a wide range of tasks. Despite significant improvements in hardware, enhancing inference and training times for these models remains crucial. Through a numerical analysis of convolutional neural networks (CNNs) inference, we found that a substantial amount of operations in these models are applied to pure numerical noise, with little to no impact on the final output. As a result, some CNNs consume up to two-thirds of their floating-point operations unnecessarily.To address this inefficiency, we introduce NaN Pooling & Convolution---novel variations of PyTorch's max pooling and 2D convolution operations. These techniques identify numerically unstable voxels and replace them with NaNs, allowing  models to bypass operations on irrelevant data. We evaluate NaN Pooling and Convolution on the FastSurfer CNN, a widely used neuroimaging tool. Our approach significantly enhances computational efficiency, skipping at least 33.24% and up to 69.30% of convolutions for some layers, all while maintaining the original models' accuracy."
    },
    {
        "title": "Parsing the Language of Expressions: Enhancing Symbolic Regression with Domain-Aware Symbolic Priors",
        "link_suffix": "/forum?id=FwjEZZ3j91",
        "link": "https://openreview.net/forum?id=FwjEZZ3j91",
        "pdf_link": "https://openreview.net/pdf?id=FwjEZZ3j91",
        "keywords": "Symbolic regression, Reinforcement learning, Recurrent neural network, Domain knowledge prior",
        "abstract": "Symbolic regression is pivotal for discovering interpretable expressions that unravel complex phenomena by revealing underlying mathematical and physical relationships within data. In this paper, we introduce an enhanced symbolic regression method that integrates symbol priors derived from diverse scientific domainsâ€”including physics, biology, chemistry, and engineeringâ€”into the regression process. By organizing and analyzing domain-specific expressions, we examine the probability distributions of symbols across different topics. We introduce a novel tree-structured recurrent neural networks (RNNs) infused with these symbol priors to guide the learning process using domain knowledge. In our approach, we introduce a new tree structure to represent expressions, where unary operators connected by the same binary operator are positioned at the same hierarchical level. By analyzing the combinations of symbols at different heights and levels within the tree, we are able to examine symbol priors across the entire hierarchical structure. This effectively incorporates the structural information of expressions into the regression process. Additionally, we compile characteristic expression blocks from each domain and incorporate them into the operator dictionary during training, expediting learning by providing relevant building blocks. Experimental results demonstrate that incorporating symbol priors significantly boosts the performance of symbolic regression methods. Specifically, it accelerates the efficiency of reinforcement learning algorithms in obtaining optimal policies. Our findings confirm that leveraging domain-specific symbol priors not only hastens convergence but also yields more accurate and interpretable models, effectively bridging the gap between data-driven learning and expert expertise in symbolic regression."
    },
    {
        "title": "Does Instruction Tuning Reduce Diversity? A Case Study Using Code Generation",
        "link_suffix": "/forum?id=hMEHnLJyrU",
        "link": "https://openreview.net/forum?id=hMEHnLJyrU",
        "pdf_link": "https://openreview.net/pdf?id=hMEHnLJyrU",
        "keywords": "LLM, Diversity, RLHF, DPO, SFT, Program Synthesis, Evaluation",
        "abstract": "Large Language Models (LLMs) should ideally generate diverse content for open-ended prompts (e.g., variety in cooking recipes). Anecdotal evidence has suggested that preference-tuned language models struggle to generate diverse content, which would have important implications for how we align models. However, research on this question has been limited by the difficulty of measuring diversity, which naively would require costly human evaluation. We propose to leverage code as a means to study semantic diversity, since code has executable semantics. To this end, we create an open-ended program synthesis task, enabling us to cheaply evaluate the diversity of hundreds of thousands of generations. Using our methodology, we find that while instruction-tuning reduces syntactic and lexical diversity, it can actually increase semantic diversity. We also study the effect of model size and prompting technique on diversity. Finally, we find that neural diversity metrics correlate poorly with our semantic diversity metrics, highlighting the need for more rigorous methodologies for evaluating diversity."
    },
    {
        "title": "FairDropout: Using Example-Tied Dropout to Enhance Generalization for Minority Groups",
        "link_suffix": "/forum?id=sec09tLQUl",
        "link": "https://openreview.net/forum?id=sec09tLQUl",
        "pdf_link": "https://openreview.net/pdf?id=sec09tLQUl",
        "keywords": "spurious correlation, fairness, worst-group performance",
        "abstract": "Deep learning models frequently exploit spurious features in training data to achieve low training error, often resulting in poor generalization when faced with shifted testing distributions. To address this issue, various methods from imbalanced learning, representation learning, and classifier recalibration have been proposed to enhance the robustness of deep neural networks against spurious correlations. In this paper, we observe that models trained with empirical risk minimization tend to generalize well for examples from the majority groups while memorizing instances from minority groups.\nBuilding on recent findings that show memorization can be localized to a limited number of neurons, we apply example-tied dropout as a method we term \\textit{FairDropout}, aimed at redirecting this memorization to specific neurons that we subsequently drop out during inference. We empirically evaluate FairDropout using the subpopulation benchmark suite encompassing vision, language, and healthcare tasks, demonstrating that it significantly reduces reliance on spurious correlations."
    },
    {
        "title": "Collu-Bench: A Benchmark for Predicting LLM Hallucinations in Code",
        "link_suffix": "/forum?id=5I39Zvlb3Y",
        "link": "https://openreview.net/forum?id=5I39Zvlb3Y",
        "pdf_link": "https://openreview.net/pdf?id=5I39Zvlb3Y",
        "keywords": "large language model, hallucination, code generation, automated program repair, benchmark",
        "abstract": "Despite their success, large language models (LLMs) face the critical challenge of hallucinations, generating plausible but incorrect content. While much research has focused on hallucinations in multiple modalities including images and natural language text, less attention has been given to hallucinations in source code, which leads to incorrect and vulnerable code that causes significant financial loss. To pave the way for research in LLMs' hallucinations in code, we introduce Collu-Bench, a benchmark for predicting code hallucinations of LLMs across code generation (CG) and automated program repair (APR) tasks. Collu-Bench includes 13,234 code hallucination instances collected from five datasets and 11 diverse LLMs, ranging from open-source models to commercial ones. To better understand and predict code hallucinations, Collu-Bench provides detailed features such as the per-step log probabilities of LLMs' output, token types, and the execution feedback of LLMs' generated code for in-depth analysis. In addition, we conduct experiments to predict hallucination on Collu-Bench, using both traditional machine learning techniques and neural networks, which achieves 22.03 - 33.15% accuracy. Our experiments draw insightful findings of code hallucination patterns, reveal the challenge of accurately localizing LLMs' hallucinations, and highlight the need for more sophisticated techniques."
    },
    {
        "title": "The Promises and Pitfalls of Language Models for Structured Numerical Data",
        "link_suffix": "/forum?id=SZpygmv3G1",
        "link": "https://openreview.net/forum?id=SZpygmv3G1",
        "pdf_link": "https://openreview.net/pdf?id=SZpygmv3G1",
        "keywords": "language models, tokenization, transformers, inductive biases, quantum chemistry",
        "abstract": "Autoregressive language models are increasingly capable of processing non-text data, such as images or audio. Are language models also a natural choice for numerical data, such as the 3D structure of molecules? In this work, we use quantum chemistry simulations as a case study in the challenges of applying language models to numerical data, building up a set of simple subproblems that can shed light on key design decisions. We show that language models lag behind domain-specific models on prediction tasks and provide evidence for and against different hypotheses that explain their failure. Many commonly identified pitfalls such as difficulty performing arithmetic operations and choice of discrete vocabulary fall short of explaining the behavior. In contrast, we show that capturing invariance properties exhibits a strong correlation with predictive performance. Finally, we provide a comparison of language models trained from scratch on numerical data with models pretrained on text. We show that text pretraining often provides a surprisingly limited advantage on prediction tasks, and can even hurt performance, despite prior work showing that text-pretraining can offer advantages."
    },
    {
        "title": "Data Augmentation via Genomic Foundation Models for Pseudoknot-Inclusive RNA Secondary Structure Prediction",
        "link_suffix": "/forum?id=ZDaI3aSDTF",
        "link": "https://openreview.net/forum?id=ZDaI3aSDTF",
        "pdf_link": "https://openreview.net/pdf?id=ZDaI3aSDTF",
        "keywords": "Genomic Foundation Models, Data Augmentation, RNA, Secondary Structure Prediction, Pseudoknot",
        "abstract": "Rapid advancements in genomic foundation models (GFMs) have delivered a series of breakthroughs across a diverse set of tasks for RNA, however RNA Secondary Structure Prediction (SSP) remains a pivotal task in computational biology. Despite achieving breakthroughs in pseudoknot-free SSP, where state-of-the-art models can achieve above 80% macro-F1, performance on the pseudoknot-inclusive problem remains stagnate, with previous methods achieving below 50% macro-F1 on all three of our test-sets. This is due to a variety of challenges: a ginormous search space that limits heuristic performance, the major class imbalance problem that limits the usual classification methods, and the inherent lack of data that limits deep learning methods. Further data acquisition is implausible due to requiring extensive biological resources and being associated with a high cost. \nIn this work, we propose a novel approach to enhance RNA secondary structure prediction by implementing a novel data augmentation technique, specifically designed for the pseudoknot-inclusive SSP problem. Our method leverages masked language modelling (MLM) with a surrogate model to produce accurate and useful data augmentations, and we further utilise uncertainty quantification strategies to identify areas within the dataset where augmentation is most effective - thereby helping to mitigate the class imbalance problem, and further improving on the generalisability of the models. We further extend three GFMs, and fine-tune them using the augmented datasets to demonstrate the efficacy and high performance of the models.\nNotably, the newly extended and augmented models achieve state-of-the-art performance, achieving over 89% F1 on RNAStrAlign, and over 66% F1 on bpRNA test sets respectively. We therefore highlight the effectiveness of data augmentation for genomic data, and release our code and datasets to assist future researchers."
    },
    {
        "title": "WaveFormer: Leveraging Wavelet Transformation for Multi-Scale Token Interactions in Hierarchical Transformers",
        "link_suffix": "/forum?id=Yc4zTbR8no",
        "link": "https://openreview.net/forum?id=Yc4zTbR8no",
        "pdf_link": "https://openreview.net/pdf?id=Yc4zTbR8no",
        "keywords": "Transformer, Attention Mechanism, Receptive Field, Discrete Wavelet Transformation, Parseval's Theorem",
        "abstract": "Recent transformer models have achieved state-of-the-art performance for visual tasks involving high-dimensional data like 3D volumetric medical image segmentation. Hierarchical transformers (e.g., Swin Transformers) circumvent the computational challenge of the self-attention mechanism through a shifted window approach to learn token relations within progressively overlapping local regions, thus expanding the receptive field across layers while limiting token attention span in each layer within predefined windows. In this work, we introduce a novel learning paradigm that captures token relations through progressive summarization of features. We leverage the compaction capability of discrete wavelet transform (DWT) on high-dimensional features and learn token relation in multi-scale approximation coefficients obtained from DWT. This approach efficiently represents fine-grained local to coarse global contexts within each network layer. Furthermore, computing self-attention on the DWT-transformed features significantly reduces the computational complexity, effectively addressing the challenges posed by high-dimensional data in vision transformers. Our proposed network, termed WaveFormer, competes favorably with current SOTA transformers (e.g., SwinUNETR) using three challenging public datasets on volumetric medical imaging: (1) MICCAI Challenge 2021 FLARE, (2) MICCAI Challenge 2019 KiTS, and (3) MICCAI Challenge 2022 AMOS. WaveFormer consistently outperforms Swin-UNETR, improving from 0.929 to 0.938 Dice (FLARE2021) and 0.880 to 0.900 Dice (AMOS2022). In addition, we explore the WaveFormerâ€™s effectiveness in segmenting organs of varying sizes, demonstrating its robustness across different anatomical structures. The source code will be available with supplementary materials in the complete paper submission."
    },
    {
        "title": "Shared-AE: Unsupervised Identification of Shared Subspaces in High-dimensional Neural and Behavioral Activity",
        "link_suffix": "/forum?id=zXCnIyX9MG",
        "link": "https://openreview.net/forum?id=zXCnIyX9MG",
        "pdf_link": "https://openreview.net/pdf?id=zXCnIyX9MG",
        "keywords": "Computational neuroscience, Multimodal, Social behavior",
        "abstract": "Understanding the relationship between behavior and neural activity is crucial for understanding brain function. One effective method is to learn embeddings for interconnected modalities. For simple behavioral tasks, neural features can be learned based on labels. However, complex behavioral tasks and social behaviors require joint extraction of both behavioral and neural features. In this paper, we present an unsupervised autoencoder (AE) framework, called Shared-AE, which includes a novel regularization term that automatically identifies features shared between neural activity and behavior, while simultaneously capturing the unique private features specific to each modality. We apply Shared-AE, to large-scale neural activity recorded across the entire dorsal cortex of the mouse, during two very different behaviors: (i) head-fixed mice performing a self-initiated decision-making task, and (ii) freely-moving social behavior amongst two mice. Our model successfully captures both 'shared features', shared across the neural and behavioral activity, and 'private features', unique to each modality, significantly enhancing our understanding of the alignment between neural activity and complex behaviors."
    },
    {
        "title": "Enhancing Audio--Language Models through Self--Supervised Post--Training with Text--Audio Pairs",
        "link_suffix": "/forum?id=nplYdpc1Pm",
        "link": "https://openreview.net/forum?id=nplYdpc1Pm",
        "pdf_link": "https://openreview.net/pdf?id=nplYdpc1Pm",
        "keywords": "Self-supervised learning, Contrastive learning, Post-training, Audio, Zero-shot evaluation, Audio-retrieval.",
        "abstract": "Research on multi-modal contrastive learning strategies for audio and text has rapidly gained interest. Contrastively trained Audio-Language Models (ALMs), such as CLAP, which establish a unified representation across audio and language modalities, have enhanced the efficacy in various subsequent tasks by providing good text aligned audio encoders and vice versa. These improvements are evident in areas like zero-shot audio classification and audio retrieval, among others. However, the ability of these models to understand natural language and temporal relations is still a largely unexplored and open field for research. In this paper, we propose to equip the multi-modal ALMs with temporal understanding without loosing their inherent prior capabilities of audio-language tasks with a temporal instillation method $\\textbf{TeminAL}$. We implement a two-stage training scheme TeminAL A & B, where the model first learns to differentiate between multiple sounds in TeminAL A, followed by a phase that instills a sense of time, thereby enhancing its temporal understanding in TeminAL B. This approach results in an average performance gain of $5.28$% in temporal understanding on the benchmark ESC-50 dataset, while the model remains competitive in zero-shot retrieval and classification tasks on the AudioCap/Clotho datasets. We also note the lack of proper evaluation techniques for contrastive ALMs and propose a strategy for evaluating ALMs in zero-shot settings. The general-purpose Zero-Shot Temporal Evaluation $\\textbf{(ZSTE)}$ strategy , is used to evaluate various prior models. ZSTE demonstrates a general strategy to evaluate all ZS contrastive models. The model trained with TeminAL successfully outperforms current models on most downstream tasks."
    },
    {
        "title": "Bounds onLpErrors in Density Ratio Estimation viaf-Divergence Loss Functions",
        "link_suffix": "/forum?id=ttq44QjKda",
        "link": "https://openreview.net/forum?id=ttq44QjKda",
        "pdf_link": "https://openreview.net/pdf?id=ttq44QjKda",
        "keywords": "density ratio estimation, variational divergence optimization, Kullbackâ€“Leibler divergence, $f$-divergence, $L_p$ error, the curse of dimensionality, and GAN.",
        "abstract": "Density ratio estimation (DRE) is a fundamental machine learning technique for identifying relationships between two probability distributions. $f$-divergence loss functions, derived from variational representations of $f$-divergence, are commonly employed in DRE to achieve state-of-the-art results. This study presents a novel perspective on DRE using $f$-divergence loss functions by deriving the upper and lower bounds on $L_p$ errors. These bounds apply to any estimator within a class of Lipschitz continuous estimators, irrespective of the specific $f$-divergence loss functions utilized.\nThe bounds are formulated as a product of terms that include the data dimension and the expected value of the density ratio raised to the power of $p$.\nNotably, the lower bound incorporates an exponential term dependent on the Kullback--Leibler divergence, indicating that the $L_p$ error significantly increases with the Kullback--Leibler divergence for $p > 1$, and this increase becomes more pronounced as $p$ increases.\nFurthermore, these theoretical findings are substantiated through numerical experiments."
    },
    {
        "title": "Transformer-Based CT Anomaly Detection and Auto-Segmentation of Sparse Lung Nodules",
        "link_suffix": "/forum?id=UKZqSYB2ya",
        "link": "https://openreview.net/forum?id=UKZqSYB2ya",
        "pdf_link": "https://openreview.net/pdf?id=UKZqSYB2ya",
        "keywords": "Transformer, CT scans, lung nodules, anomaly detection, auto-segmentation, Deformable-DETR, sparse data, medical imaging, self-attention, multi-scale learning, object detection, Focal Loss, segmentation",
        "abstract": "Accurate segmentation of lung nodules in computed tomography (CT) scans is challenging due to extreme class imbalance, where nodules appear sparsely among healthy tissue. Lung tumor boards often review these scans manually, a time-consuming process. This paper introduces a novel two-stage approach for lung tumor segmentation by framing the problem as anomaly detection. The method is divided into two stages, allowing each model to leverage its strengths. Stage 1 focuses on region proposal, employing a custom Deformable Detection Transformer with Focal Loss to overcome class imbalance and localize sparse tumors. In Stage 2, the predicted bounding boxes are refined into pixel-wise segmentation masks using a fine-tuned variant of Meta's Segment Anything Model (SAM) for semantic segmentation. To address the challenge of nodule sparsity and improve spatial context, a 7.5 mm Maximum Intensity Projection (MIP) is applied, aiding in the differentiation between nodules, bronchioles, and vascular structures. The model achieves a Dice coefficient of 92.4%, with 95.2% sensitivity and 93.2% precision on the LUNA16 dataset, demonstrating robust performance in real-world clinical conditions where nodule sparsity is 5%."
    },
    {
        "title": "A Variational Approach for Generative Speech Language Modeling",
        "link_suffix": "/forum?id=IQN4XnIEhL",
        "link": "https://openreview.net/forum?id=IQN4XnIEhL",
        "pdf_link": "https://openreview.net/pdf?id=IQN4XnIEhL",
        "keywords": "Generative Spoken Language Modeling;Speech Language Model",
        "abstract": "The success of large language models in text processing has inspired their adaptation to speech modeling. However, because speech is continuous and complex, it is often discretized into tokens derived from self-supervised speech models. These speech tokens typically focus on the linguistic aspects of speech and neglect its paralinguistic content. As a result, autoregressive models trained on these tokens may generate speech with suboptimal naturalness. Previous methods attempted to address this limitation by adding pitch features to speech tokens prior to autoregressive modeling. However, pitch alone cannot fully represent the range of paralinguistic attributes, and selecting the right features requires careful hand-engineering. To tackle this issue, we propose a variational approach that automatically learns to encode these continuous speech attributes to enhance the speech tokens. Our proposed approach eliminates the need for manual paralinguistic feature selection and extraction. Moreover, we demonstrate that our proposed approach maintains or improves speech language modeling performance and enhances the naturalness of generated speech compared to baseline approaches."
    },
    {
        "title": "Palu: KV-Cache Compression with Low-Rank Projection",
        "link_suffix": "/forum?id=LWMS4pk2vK",
        "link": "https://openreview.net/forum?id=LWMS4pk2vK",
        "pdf_link": "https://openreview.net/pdf?id=LWMS4pk2vK",
        "keywords": "KV-Cache, Low-Rank Compression, Large Language Model",
        "abstract": "Post-training KV-Cache compression methods typically either sample a subset of effectual tokens or quantize the data into lower numerical bit width. However, these methods cannot exploit redundancy in the hidden dimension of the KV tenors. This paper presents a hidden dimension compression approach called Palu, a KV-Cache compression framework that utilizes low-rank projection to reduce inference-time LLM memory usage. Palu decomposes the linear layers into low-rank matrices, caches compressed intermediate states, and reconstructs the full keys and values on the fly. To improve accuracy, compression rate, and efficiency, Palu further encompasses (1) a medium-grained low-rank decomposition scheme, (2) an efficient rank search algorithm, (3) low-rank-aware quantization compatibility enhancements, and (4) an optimized GPU kernel with matrix fusion. Extensive experiments with popular LLMs show that Palu compresses KV-Cache by 50% and delivers up to 1.87Ã— speedup for the attention module. When combined with quantization, Paluâ€™s inherent quantization-friendly design yields an additional 30% reduction in KV-Cache size. Moreover, it maintains comparable or even better accuracy (up to 1.19 lower perplexity) compared to quantization- only methods. These results demonstrate Paluâ€™s superior capability to effectively address the efficiency and memory challenges of LLM inference posed by KV-Cache."
    },
    {
        "title": "fPLSA: Learning Semantic Structures in Document Collections Using Foundation Models",
        "link_suffix": "/forum?id=4NsYCAxubi",
        "link": "https://openreview.net/forum?id=4NsYCAxubi",
        "pdf_link": "https://openreview.net/pdf?id=4NsYCAxubi",
        "keywords": "Natural Language Processing, Large Language Models, Document Analysis, Latent Semantic Analysis",
        "abstract": "Humans have the ability to learn new tasks by inferring high-level concepts from existing solution, then manipulating these concepts in lieu of the raw data. Can we automate this process by deriving latent semantic structures in a document collection using foundation models? We introduce fPLSA, a foundation-model-based Probabilistic Latent Semantic Analysis (PLSA) method that iteratively clusters and tags document segments based on document-level contexts. These tags can be used to model the structure of given documents and for hierarchical sampling of new texts. Our experiments on story writing, math, and multi-step reasoning datasets demonstrate that fPLSA tags help reconstruct the original texts better than existing tagging methods. Moreover, when used for hierarchical sampling, fPLSA produces more diverse outputs with a higher likelihood of hitting the correct answer than direct sampling and hierarchical sampling with existing tagging methods."
    },
    {
        "title": "If Optimizing for general parameters in chemistry is useful, why is it hardly done?",
        "link_suffix": "/forum?id=lWN2aGg8qJ",
        "link": "https://openreview.net/forum?id=lWN2aGg8qJ",
        "pdf_link": "https://openreview.net/pdf?id=lWN2aGg8qJ",
        "keywords": "Bayesian Optimization, Generality, Transferable Optima, Reaction Conditions, Condition Optimization",
        "abstract": "General parameters are highly desirable in the natural sciences â€” e.g., reaction\nconditions that enable high yields across a range of related transformations. \nThis has a significant practical impact since those general parameters can be transfered to related tasks without the need for laborious and time-intensive re-optimization.\nWhile Bayesian optimization (BO) is widely applied to find optimal parameter\nsets for specific tasks, it has remained underused in experiment planning towards\nsuch general optima. \nIn this work, we consider the the real-world problem of condition optimization for chemical reactions to study whether performing generality-oriented BO can accelerate the identification of general optima, and whether these optima also translate to unseen examples. This is achieved through a careful formulation of the problem as an optimization over curried functions, as well as systematic benchmarking of generality-oriented strategies for optimization tasks on real-world experimental data. \nWe find that the optimization for general reaction conditions are determined by the sampling of substrates, with random selection\noutperforming more data-driven strategies."
    },
    {
        "title": "Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling",
        "link_suffix": "/forum?id=bIlnpVM4bc",
        "link": "https://openreview.net/forum?id=bIlnpVM4bc",
        "pdf_link": "https://openreview.net/pdf?id=bIlnpVM4bc",
        "keywords": "Large Language Models;Length Extrapolation;Efficiency;Hybrid State Space Models",
        "abstract": "Efficiently modeling sequences with infinite context length has long been a challenging problem. Previous approaches have either suffered from quadratic computational complexity or limited extrapolation ability in length generalization. In this\nwork, we present Samba, a simple hybrid architecture that layer-wise combines\nMamba, a selective State Space Model (SSM), with Sliding Window Attention\n(SWA). Samba selectively compresses a given sequence into recurrent hidden\nstates while still maintaining the ability to precisely recall recent memories with the\nattention mechanism. We scale Samba up to 3.8B parameters with 3.2T training\ntokens and demonstrate that it significantly outperforms state-of-the-art models\nacross a variety of benchmarks. Pretrained on sequences of 4K length, Samba\nshows improved perplexity in context lengths of up to 1M in zero-shot. When\nfinetuned on 4K-length sequences, Samba efficiently extrapolates to a 256K context length with perfect memory recall on the Passkey Retrieval task, and exhibits\nsuperior retrieval extrapolation on the challenging Phonebook task compared to\nfull-attention models. As a linear-time sequence model, Samba achieves a 3.73Ã—\nhigher throughput compared to Transformers with grouped-query attention for user\nprompts of 128K length, and a 3.64Ã— speedup when generating 64K tokens with\nunlimited streaming."
    },
    {
        "title": "GLoRA: Geometric Adaptive Ranks for Efficient LoRA Fine-Tuning",
        "link_suffix": "/forum?id=NXnNiT0fdp",
        "link": "https://openreview.net/forum?id=NXnNiT0fdp",
        "pdf_link": "https://openreview.net/pdf?id=NXnNiT0fdp",
        "keywords": "Low-Rank Adaptation, Large Language Models, Intrinsic Dimensionality",
        "abstract": "Fine-tuning large language models is computationally intensive because it requires updating all parameters. Low-Rank Adaptation (LoRA) improves efficiency by modifying only a subset of weights but introduces a trade-off between expressivity and computational cost: lower ranks reduce resources but limit expressiveness, while higher ranks enhance expressivity at increased cost. Despite recent advances in adaptive LoRA techniques, existing methods fail to provide a theoretical basis for optimizing the trade-off between model performance and efficiency. We propose Geometric Low-Rank Adaptation (GLoRA), a novel framework that computes the intrinsic dimensionality of hidden state representations to adaptively select LoRA ranks. We demonstrate that the intrinsic dimension provides a lower bound for the optimal rank of LoRA matrices, allowing for a principled selection that balances efficiency and expressivity. GLoRA dynamically adjusts the rank for each layer based on the intrinsic dimensionality of its input and output representations, recognizing that not all model parameters equally impact fine-tuning. Empirical validation on multiple tasks shows that GLoRA consistently outperforms recent baselines within the same parameter budget."
    }
]