[
    {
        "title": "Zero-shot Novel View Synthesis via Adaptive Modulating Video Diffusion Process",
        "link_suffix": "/forum?id=zDJf7fvdid",
        "link": "https://openreview.net/forum?id=zDJf7fvdid",
        "pdf_link": "https://openreview.net/pdf?id=zDJf7fvdid",
        "keywords": "Diffusion sampling, Novel view synthesis",
        "abstract": "By harnessing the potent generative capabilities of pre-trained large video diffusion models, we propose a new novel view synthesis paradigm that operates \\textit{without} the need for training. The proposed method adaptively modulates the diffusion sampling process with the given views to enable the creation of visually pleasing results from single or multiple views of static scenes or monocular videos of dynamic scenes. Specifically, built upon our theoretical modeling, we iteratively modulate the score function with the given scene priors represented with warped input views to control the video diffusion process. Moreover, by theoretically exploring the boundary of the estimation error, we achieve the modulation in an adaptive fashion according to the view pose and the number of diffusion steps. Extensive evaluations on both static and dynamic scenes substantiate the significant superiority of our method over state-of-the-art methods both quantitatively and qualitatively. The source code can be found on the anonymous webpage:https://github.com/PAPERID5494/VD_NVS. We also refer reviewers to the Supplementary Material for the video demo."
    },
    {
        "title": "Let's Be Self-generated via Step by Step: A Curriculum Learning Approach to Automated Reasoning with Large Language Models",
        "link_suffix": "/forum?id=ixoIAOcTSx",
        "link": "https://openreview.net/forum?id=ixoIAOcTSx",
        "pdf_link": "https://openreview.net/pdf?id=ixoIAOcTSx",
        "keywords": "Large Language Models, Chain of Thought, Automated Reasoning, Curriculum Learning",
        "abstract": "While Chain of Thought (CoT) prompting approaches have significantly consolidated the reasoning capabilities of large language models (LLMs), they still face limitations that require extensive human effort or have performance needs to be improved. Existing endeavors have focused on bridging these gaps; however, these approaches either hinge on external data and cannot completely eliminate manual effort, or they fall short in effectively directing LLMs to generate high-quality exemplary prompts. To address the said pitfalls, we propose a novel prompt approach for automatic reasoning named LBS3, inspired by curriculum learning which better reflects human learning habits. Specifically, LBS3 initially steers LLMs to recall easy-to-hard proxy queries that are pertinent to the target query. Following this, it invokes a progressive strategy that utilizes exemplary prompts stemmed from easy-proxy queries to direct LLMs in solving hard-proxy queries, enabling the high-quality of the proxy solutions. Finally, our extensive experiments in various reasoning-intensive tasks with varying open- and closed-source LLMs show that LBS3 achieves strongly competitive performance compared to the SOTA baselines."
    },
    {
        "title": "Beyond Scaling Laws: Understanding Transformer Performance with Associative Memory",
        "link_suffix": "/forum?id=tJE9WeqHEI",
        "link": "https://openreview.net/forum?id=tJE9WeqHEI",
        "pdf_link": "https://openreview.net/pdf?id=tJE9WeqHEI",
        "keywords": "Transformer; Associative Memory; Energy-Based Model",
        "abstract": "Increasing the size of a Transformer does not always lead to enhanced performance. This phenomenon cannot be explained by the empirical scaling laws. Furthermore, improved generalization ability occurs as the model memorizes the training samples. We present a theoretical framework that sheds light on the memorization process and pre-training of transformer-based language models. We model the behavior of Transformers with associative memories using Hopfield networks, such that each transformer block effectively conducts an approximate nearest-neighbor search. Based on this, we design an energy function analogous to that in the modern continuous Hopfield network which provides an insightful explanation for the attention mechanism. Using the majorization-minimization technique, we construct a global energy function that captures the layered architecture of the Transformer. We show a dependency between the model size and the dataset for the model to attain optimal performance, and the achievable cross-entropy loss is bounded by a constant. We substantiate our theoretical findings through a series of experiments, which include tests conducted with GPT-2, vanilla Transformers, and OpenELM."
    },
    {
        "title": "GESR: A Geometric Evolution Model for Symbolic Regression",
        "link_suffix": "/forum?id=h5NqrrSjlP",
        "link": "https://openreview.net/forum?id=h5NqrrSjlP",
        "pdf_link": "https://openreview.net/pdf?id=h5NqrrSjlP",
        "keywords": "symbolic regression; semantic approximation",
        "abstract": "Symbolic regression is a challenging task in machine learning that aims to automatically discover highly interpretable mathematical equations from limited data. Keen efforts have been devoted to addressing this issue, yielding promising results. However, there are still bottlenecks that current methods struggle with, especially when dealing with complex problems containing various noises or with intricate underlying mathematical formulas.\nIn this work, we propose a novel Geometric Evolution Symbolic Regression(GESR) algorithm. Leveraging geometric semantics, the process of symbolic regression in GESR is transformed into an approximation to an unimodal target in n-dimensional topological space. Then, three key modules are proposed to enhance the approximation: (1) a new semantic gradient concept, proposed to assist the exploration, which aims to improve the accuracy of approximation; (2) a new geometric search operator, tailored for approximating the target formula directly in topological space; (3) the Levenberg-Marquardt algorithm with L2 regularization, used for the adjustment of local expression structures and the optimization of constants. With the proposal of these modules, GESR achieves state-of-the-art accuracy performance on multiple authoritative benchmark datasets and demonstrates its robustness to noise interference. The implementation is available athttps://anonymous.4open.science/r/12331211321-014D."
    },
    {
        "title": "MinMax Bayesian Neural Networks and Uncorrelated Representation",
        "link_suffix": "/forum?id=WoJzHQIIUk",
        "link": "https://openreview.net/forum?id=WoJzHQIIUk",
        "pdf_link": "https://openreview.net/pdf?id=WoJzHQIIUk",
        "keywords": "Minimax game, Bayesian Neural Networks, Brownian Motion, Minimax coding reduction, Uncorrelated representation",
        "abstract": "In deep learning, Bayesian neural networks (BNN) and dropout techniques provide the role of robustness analysis, and the minimax method used to be a conservative choice in the traditional Bayesian field. In this paper, we apply the minimax game to the BNN on the representation level  and formulate as a two-player game between a deterministic neural network $f$ and a sampling stochastic neural network $f + r*\\xi$, which can be seen as a Brownian Motion of $f$. Our simple experiments show that $r$ will be stable with enough dimension space, suitable activation function, and without bias with the minimax coding rate loss, which verify the statement \\cite{yu2020learning} in some sense. And we test the convolutional neural network without bias, with bias and with batch normalization on simple data set like MNIST, Fashion MNIST and others, and visualize the sampling radius as a bias-variance tradeoff study. At last, we also test how noise perturbation will affect radius in stable case."
    },
    {
        "title": "BOOST: Enhanced Jailbreak of Large Language Model via Slient eos Tokens",
        "link_suffix": "/forum?id=JqKh7FLUw1",
        "link": "https://openreview.net/forum?id=JqKh7FLUw1",
        "pdf_link": "https://openreview.net/pdf?id=JqKh7FLUw1",
        "keywords": "large language model, Jailbreak",
        "abstract": "Along with the remarkable successes of Language language models, recent research also started to explore the security threats of LLMs, including jailbreaking attacks. Attackers carefully craft jailbreaking prompts such that a target LLM will respond to the harmful question. Existing jailbreaking attacks require either human experts or leveraging complicated algorithms to craft jailbreaking prompts. In this paper, we introduce BOOST, a simple attack that leverages only the eos tokens. We demonstrate that rather than constructing complicated jailbreaking prompts, the attacker can simply append a few eos tokens to the end of a harmful question. It will bypass the safety alignment of LLMs and lead to successful jailbreaking attacks. We further apply BOOST to four representative jailbreak methods and show that the attack success rates of these methods can be significantly enhanced by simply adding eos tokens to the prompt. To understand this simple but novel phenomenon, we conduct both theoretical and empirical analyses. Our analysis reveals that (1) adding eos tokens makes the target LLM believe the input is much less harmful, and (2) eos tokens have low attention values and do not affect LLM's understanding of the harmful questions, leading the model to actually respond to the questions. Our findings uncover how fragile an LLM is against jailbreak attacks, motivating the development of strong safety alignment approaches.large language model, Jailbreak"
    },
    {
        "title": "q-exponential family for policy optimization",
        "link_suffix": "/forum?id=OyyE1FDdrQ",
        "link": "https://openreview.net/forum?id=OyyE1FDdrQ",
        "pdf_link": "https://openreview.net/pdf?id=OyyE1FDdrQ",
        "keywords": "reinforcement learning, policy optimization, $q$-exponential family, heavy-tailed policies, sparse policies",
        "abstract": "Policy optimization methods benefit from a simple and tractable policy parametrization, usually the Gaussian for continuous action spaces. In this paper, we consider a broader policy family  that remains tractable: the $q$-exponential family. \nThis family of policies is flexible, allowing the specification of both heavy-tailed policies ($q>1$) and light-tailed policies ($q<1$). This paper examines the interplay between $q$-exponential policies for several actor-critic algorithms conducted on both online and offline problems. We find that heavy-tailed policies are more effective in general and can consistently improve on Gaussian. \nIn particular, we find the Student's t-distribution to be more stable than the Gaussian across settings and that a heavy-tailed $q$-Gaussian for Tsallis Advantage Weighted Actor-Critic consistently performs well in offline benchmark problems."
    },
    {
        "title": "SparsitySolver: Efficient Reinforcement Learning-based Pruning for LLMs",
        "link_suffix": "/forum?id=zZU69H8tcr",
        "link": "https://openreview.net/forum?id=zZU69H8tcr",
        "pdf_link": "https://openreview.net/pdf?id=zZU69H8tcr",
        "keywords": "Large Language Models, Model Compression",
        "abstract": "Large Language Models (LLMs) have achieved significant success in the field of Natural Language Processing (NLP). However, due to their large model size and high inference costs, the application of LLMs is restricted. Pruning is regarded as an effective method to reduce the size of LLMs. Mainstream pruning methods for LLMs typically apply a uniform ratio to prune all the layers or determine layerwise sparsity based on simple criteria. Such manually or semi-manually designed pruning strategies often lead to suboptimal results, which makes reinforcement learning a feasible solution. However, current reinforcement learning-based pruning methods usually have redundant environment designs or multiple agents, rendering them ill-suited to massive LLMs. Hence, we propose SparsitySolver, which first incorporates reinforcement learning into the pruning of LLMs, supporting various pruning granularity. SparsitySolver employs an improved reinforcement learning environment, allowing for a rapid pruning strategy search with a small-scale agent. Moreover, to lessen the performance decline caused by structured pruning, we propose a compensation method capable of restoring performance without introducing additional parameters to the model. We evaluate our approach on LLaMA-V1/V2, Mistral, and the OPT families across multiple pruning granularities, achieving performances surpassing the state-of-the-art methods."
    },
    {
        "title": "Dark Miner: Defend against unsafe generation for text-to-image diffusion models",
        "link_suffix": "/forum?id=3rUAS7HCKE",
        "link": "https://openreview.net/forum?id=3rUAS7HCKE",
        "pdf_link": "https://openreview.net/pdf?id=3rUAS7HCKE",
        "keywords": "Text-to-Image Diffusion Models, Unsafe Generation, Concept Erasure",
        "abstract": "Text-to-image diffusion models have been demonstrated with unsafe generation due to unfiltered large-scale training data, such as violent, sexual, and shocking images, necessitating the erasure of unsafe concepts. Most existing methods focus on modifying the generation probabilities conditioned on the texts containing unsafe descriptions. However, they fail to guarantee safe generation for unseen texts in the training phase, especially for the prompts from adversarial attacks. In this paper, we re-analyze the erasure task and point out that existing methods cannot guarantee the minimization of the total probabilities of unsafe generation. To tackle this problem, we propose Dark Miner. It entails a recurring three-stage process that comprises mining, verifying, and circumventing. It greedily mines embeddings with maximum generation probabilities of unsafe concepts and reduces unsafe generation more effectively. In the experiments, we evaluate its performance on two inappropriate concepts, two objects, and two styles. Compared with 6 previous state-of-the-art methods, our method achieves better erasure and defense results in most cases, especially under 4 state-of-the-art attacks, while preserving the model's native generation capability. Our code can be found in Supplementary Material and will be available on GitHub."
    },
    {
        "title": "ASPIRER: Bypassing System Prompts with Permutation-based Backdoors in LLMs",
        "link_suffix": "/forum?id=qkzPr74cMx",
        "link": "https://openreview.net/forum?id=qkzPr74cMx",
        "pdf_link": "https://openreview.net/pdf?id=qkzPr74cMx",
        "keywords": "LLM, Safety, backdoor attack",
        "abstract": "Large Language Models (LLMs) have become integral to many applications, with system prompts serving as a key mechanism to regulate model behavior and ensure ethical outputs. In this paper, we introduce a novel backdoor attack that systematically bypasses these system prompts, posing significant risks to the AI supply chain. Under normal conditions, the model adheres strictly to its system prompts. However, our backdoor allows malicious actors to circumvent these safeguards when triggered. Specifically, we explore a scenario where an LLM provider embeds a covert trigger within the base model. A downstream deployer, unaware of the hidden trigger, fine-tunes the model and offers it as a service to users. Malicious actors can purchase the trigger from the provider and use it to exploit the deployed model, disabling system prompts and achieving restricted outcomes. Our attack utilizes a permutation trigger, which activates only when its components are arranged in a precise order, making it computationally challenging to detect or reverse-engineer. We evaluate our approach on five state-of-the-art models, demonstrating that our method achieves an attack success rate (ASR) of up to 99.50% while maintaining a clean accuracy (CACC) of 98.58%, even after defensive fine-tuning. These findings highlight critical vulnerabilities in LLM deployment pipelines and underscore the need for stronger defenses."
    },
    {
        "title": "Exploring Selective Layer Fine-Tuning in Federated Learning",
        "link_suffix": "/forum?id=eu1PIDPYwC",
        "link": "https://openreview.net/forum?id=eu1PIDPYwC",
        "pdf_link": "https://openreview.net/pdf?id=eu1PIDPYwC",
        "keywords": "Federated Learning, Selective Layer Fine-tuning",
        "abstract": "Federated learning (FL) has emerged as a promising paradigm for fine-tuning foundation models using distributed data in a privacy-preserving manner. Under limited computational resources, clients often find it more practical to fine-tune a selected subset of layers, rather than the entire model, based on their task-specific data. In this study, we provide a thorough theoretical exploration of selective layer fine-tuning in FL, emphasizing a flexible approach that allows the clients to adjust their selected layers according to their local data and resources. We theoretically demonstrate that the layer selection strategy has a significant impact on model convergence in two critical aspects: the importance of selected layers and the heterogeneous choices across clients. Drawing from these insights, we further propose a strategic layer selection method that utilizes local gradients and regulates layer selections across clients. Extensive experiments on both image and text datasets demonstrate the effectiveness of the proposed strategy compared with several baselines, highlighting its advances in identifying critical layers that adapt to the client heterogeneity and training dynamics in FL."
    },
    {
        "title": "Harnessing Spatial Dependency for Domain Generalization in Multivariate Time-series Sensor Data",
        "link_suffix": "/forum?id=KxjKZvyFgx",
        "link": "https://openreview.net/forum?id=KxjKZvyFgx",
        "pdf_link": "https://openreview.net/pdf?id=KxjKZvyFgx",
        "keywords": "Domain Generalization. Multivariate Time-series. sensor and healthcare.",
        "abstract": "Analyzing human-related behavior with multi-variate time-series (MTS) data through multi-sensor structure and temporal features often results in various distributions across different domains. Furthermore, distribution shifts caused by spatial dependencies\u2014such as sensor misalignments\u2014pose significant challenges in domain generalization (DG) settings, where the model is expected to be robust across various domains. While existing spatiotemporal encoders for MTS and DG methods in other domains address related issues, DG approaches specifically tailored for MTS data remain underexplored, particularly in learning and aligning spatial dependencies across domains. To bridge this gap, we propose ASAM (Adaptive Spatial Dependency Alignment in MTS Data for Domain Generalization), a novel framework designed to enhance robustness to spatial dependencies in MTS data. ASAM proposes a DG layer with domain generalization loss function and two- view regularization loss functions to adaptively align spatial dependencies between domains. We adopt a two-phase approach to effectively align different set of domains: the first phase captures spatiotemporal features, and the second phase applies the DG layer and domain generalization loss function to align spatial dependencies across domains. An input-aware graph generation process and a GNN- based DG layer, coupled with the domain generalization loss function, adaptively align the spatial dependencies learned in the second phase with those from the first phase, ensuring a more precise alignment. We additionally incorporate a two- view regularization method to capture underlying spatial dependency effectively in both phases and is comprised of spatial decorrelation loss and Gaussian kernel loss. Our theoretical analysis demonstrates that the DG layer effectively assimilates invariant information, ensuring robustness across diverse distributions. Ex- tensive evaluations of the four real-world datasets show ASAM outperforms ten recent baselines. To the best of our knowledge, this work is among the first to explore DG approaches for MTS data by focusing on spatial dependency alignment. Our code is available athttps://anonymous.4open.science/r/ASAM/."
    },
    {
        "title": "BAME: Block-Aware Mask Evolution for Efficient N:M Sparse Training",
        "link_suffix": "/forum?id=aW7XcFocYr",
        "link": "https://openreview.net/forum?id=aW7XcFocYr",
        "pdf_link": "https://openreview.net/pdf?id=aW7XcFocYr",
        "keywords": "Network Sparsity; Sparse Training; N:M Sparsity",
        "abstract": "N:M sparsity stands as a progressively important tool for DNN compression, achieving practical speedups by stipulating at most N non-zero components within M sequential weights. Unfortunately, most existing works identify the N:M sparse mask through dense backward propagation to update all weights, which incurs exorbitant training costs. In this paper, we introduce BAME, a method that maintains consistent sparsity throughout the N:M sparse training process. BAME perpetually keeps both sparse forward and backward propagation, while iteratively performing weight pruning-and-regrowing within designated weight blocks to tailor the N:M mask. These blocks are selected through a joint assessment based on accumulated mask oscillation frequency and expected loss reduction of mask adaptation, thereby ensuring stable and efficient identification of the optimal N:M mask. Our empirical results substantiate the effectiveness of BAME, illustrating it performs comparably to or better than previous works that fully maintaining dense backward propagation during training. For instance, BAME attains a 72.0% top-1 accuracy while training a 1:16 sparse ResNet-50 on ImageNet, eclipsing SR-STE by 0.5%, despite achieving 2.37$\\times$ training FLOPs reduction. Code will be released."
    },
    {
        "title": "Chain-of-Action: Faithful and Multimodal Question Answering through Large Language Models",
        "link_suffix": "/forum?id=1BdPHbuimc",
        "link": "https://openreview.net/forum?id=1BdPHbuimc",
        "pdf_link": "https://openreview.net/pdf?id=1BdPHbuimc",
        "keywords": "large language model, question answering, chain-of-thought",
        "abstract": "We present a Chain-of-Action (CoA) framework for multimodal and retrieval-augmented Question-Answering (QA). Compared to the literature, CoA overcomes two major challenges of current QA applications: (i) unfaithful hallucination that is inconsistent with real-time or domain facts and (ii) weak reasoning performance over compositional information. Our key contribution is a novel reasoning-retrieval mechanism that decomposes a complex question into a reasoning chain via systematic prompting and pre-designed actions.  Methodologically, we propose three types of domain-adaptable `Plug-and-Play'  actions for retrieving real-time information from heterogeneous sources. We also propose a multi-reference faith score to verify conflicts in the answers.\nIn addition, our system demonstrates that detecting the knowledge boundaries of LLMs can significantly reduce both LLM interaction frequency and tokens usage in QA tasks. Empirically, we exploit both public benchmarks and a Web3 case study to demonstrate the capability of CoA over other methods."
    },
    {
        "title": "EAPCR: A Universal Feature Extractor for Scientific Data Without Explicit Feature Relation Patterns",
        "link_suffix": "/forum?id=YVubckGzED",
        "link": "https://openreview.net/forum?id=YVubckGzED",
        "pdf_link": "https://openreview.net/pdf?id=YVubckGzED",
        "keywords": "AI for Science, Representation Learning",
        "abstract": "Conventional methods, including Decision Tree (DT)-based methods, have been highly effective in scientific tasks, such as non-image medical diagnostics, system anomaly detection, and inorganic catalysis efficiency prediction. However, most deep-learning techniques have struggled to surpass or even match this level of success as traditional machine learning methods.  The primary reason is that these applications involve multi-source, heterogeneous data, where features lack explicit relationships. This contrasts with image data, where pixels exhibit spatial relationships; textual data, where words have sequential dependencies; and graph data, where nodes are connected through established associations. The absence of explicitFeatureRelationPatterns (FRPs) presents a significant challenge for deep learning techniques in scientific applications that are not image, text, and graph-based. In this paper, we introduceEAPCR, a universal feature extractor designed for data without explicit FRPs. Tested across various scientific tasks, EAPCR consistently outperforms traditional methods and bridges the gap where deep learning models fall short. To further demonstrate its robustness, we synthesize a dataset without explicit FRPs. While Kolmogorov\u2013Arnold Network (KAN) and feature extractors like Convolutional Neural Networks (CNNs), Graph Convolutional Networks (GCNs), and Transformers struggle, EAPCR excels, demonstrating its robustness and superior performance in scientific tasks without FRPs."
    },
    {
        "title": "Efficient Alternating Minimization with Applications to Weighted Low Rank Approximation",
        "link_suffix": "/forum?id=rvhu4V7yrX",
        "link": "https://openreview.net/forum?id=rvhu4V7yrX",
        "pdf_link": "https://openreview.net/pdf?id=rvhu4V7yrX",
        "keywords": "weighted low rank approximation, sketching, alternating minimization",
        "abstract": "Weighted low rank approximation is a fundamental problem in numerical linear algebra, and it has many applications in machine learning. Given a matrix $M \\in \\mathbb{R}^{n \\times n}$, a non-negative weight matrix $W \\in \\mathbb{R}_{\\geq 0}^{n \\times n}$, a parameter $k$, the goal is to output two matrices $X,Y\\in \\mathbb{R}^{n \\times k}$ such that $\\| W \\circ (M - X Y^\\top) \\|_F$ is minimized, where $\\circ$ denotes the Hadamard product. It naturally generalizes the well-studied low rank matrix completion problem. Such a problem is known to be NP-hard and even hard to approximate assuming the Exponential Time Hypothesis. Meanwhile, alternating minimization is a good heuristic solution for weighted low rank approximation. In particular, [Li, Liang and Risteski, ICML'16] shows that, under mild assumptions, alternating minimization does provide provable guarantees. In this work, we develop an efficient and robust framework for alternating minimization that allows the alternating updates to be computed approximately. For weighted low rank approximation, this improves the runtime of [Li, Liang and Risteski, ICML'16] from $\\|W\\|_0k^2$ to $\\|W\\|_0 k$ where $\\|W\\|_0$ denotes the number of nonzero entries of the weight matrix. At the heart of our framework is a high-accuracy multiple response regression solver together with a robust analysis of alternating minimization."
    },
    {
        "title": "Multi-modal Prompt Learning Empowers Graph Neural Networks with Semantic Knowledge",
        "link_suffix": "/forum?id=ax4ZOytBV2",
        "link": "https://openreview.net/forum?id=ax4ZOytBV2",
        "pdf_link": "https://openreview.net/pdf?id=ax4ZOytBV2",
        "keywords": "Graph Foundation Model, Multi-modal Prompt Learning, Graph Neural Network, Language Models, Contrastive Learning",
        "abstract": "While great success has been achieved in building generalizable language models, three fundamental issues hinder GNN-based graph foundation models: the scarcity of labeled data, different levels of downstream tasks, and the conceptual gaps between domains. In depth, though the labels of real graphs are associated with semantic information, most graph learning frameworks ignore it by turning semantic labels into numerical labels. In this work, to address these issues, we present a new paradigm that leverages the text modality to align downstream tasks and data with any pre-trained GNN given only a few semantically labeled samples. Our paradigm embeds the graphs directly in the same space as the LLM by learning both graph prompts and text prompts simultaneously. To accomplish this, we improve state-of-the-art graph prompt method based on our theoretical findings. Then, we propose the first multi-modal prompt learning approach for exploiting the knowledge in pre-trained models. Notably, in our paradigm, the pre-trained GNN and the LLM are kept frozen, so the number of learnable parameters is much smaller than fine-tuning any pre-trained model. Through extensive experiments on real-world datasets, we demonstrate the superior performance of our paradigm in few-shot, multi-task-level, and cross-domain settings. Moreover, we build the first zero-shot classification prototype that can generalize GNNs to unseen classes. The code is provided in the supplementary materials."
    },
    {
        "title": "Boosting Concept Bottleneck Models with Supervised, Hierarchical Concept Learning",
        "link_suffix": "/forum?id=Q9Z0c1Rb5i",
        "link": "https://openreview.net/forum?id=Q9Z0c1Rb5i",
        "pdf_link": "https://openreview.net/pdf?id=Q9Z0c1Rb5i",
        "keywords": "Interpretability, explainability, CBM",
        "abstract": "Concept Bottleneck Models (CBMs) aim to deliver interpretable and interventionable predictions by bridging features and labels with human-understandable concepts. While recent CBMs show promising potential, they suffer from information leakage, where unintended information beyond the concepts (either in probabilistic or binary-state form) is leaked to the subsequent label prediction. Consequently, distinct classes are falsely classified via indistinguishable concepts, undermining the interpretation and intervention of CBMs.This paper alleviates the information leakage issue by introducing label supervision in concept prediction and constructing a hierarchical concept set. Accordingly, we propose a new paradigm of CBMs, namely SupCBM, which stands for Structured Understanding of leakage Prevention Concept Bottleneck Model, achieving label prediction via predicted concepts and a deliberately structural-designed intervention matrix. SupCBM focuses on concepts that are mostly relevant to the predicted label and only distinguishes classes when different concepts are presented. Our evaluations show that SupCBM\u2019s label prediction outperforms SOTA CBMs over diverse datasets. Its predicted concepts also exhibit better interpretability. With proper quantification of information leakage in different CBMs, we demonstrate that SupCBM significantly reduces the information leakage."
    },
    {
        "title": "Upcycling Instruction Tuning from Dense to Mixture-of-Experts via Parameter Merging",
        "link_suffix": "/forum?id=RHISYSlLHf",
        "link": "https://openreview.net/forum?id=RHISYSlLHf",
        "pdf_link": "https://openreview.net/pdf?id=RHISYSlLHf",
        "keywords": "Mixture-of-Experts, Model upcyling, Upcycling instruction tuning",
        "abstract": "Mixture-of-Experts (MoE) shines brightly in large language models (LLMs) and demonstrates outstanding performance in plentiful natural language processing tasks. However, existing methods that transform LLMs from dense to MoE face significant data requirements and typically rely on large-scale post-training.\nIn this paper, we propose Upcycling Instruction Tuning (UpIT), a data-efficient approach for tuning a dense pre-trained model into an MoE instruct model.\nSpecifically, we first point out that intermediate checkpoints during instruction tuning of the dense model are naturally suitable for specialized experts, and then propose an expert expansion stage to flexibly achieve models with different numbers of experts, where genetic algorithm and parameter merging are introduced to ensure sufficient diversity of new extended experts.\nTo ensure that each differentiated expert in the MoE model works as expected, we select a small amount of seed data that each expert excels to pre-optimize the router.\nExtensive experiments with various data scales and upcycling settings demonstrate the outstanding performance and data efficiency of UpIT, as well as stable improvement in expert or data scaling. Further analysis reveals the importance of ensuring expert diversity in upcycling."
    },
    {
        "title": "Efficient Ensembles Improve Training Data Attribution",
        "link_suffix": "/forum?id=PKqHT0xZhI",
        "link": "https://openreview.net/forum?id=PKqHT0xZhI",
        "pdf_link": "https://openreview.net/pdf?id=PKqHT0xZhI",
        "keywords": "training data attribution, data-centric AI",
        "abstract": "Training data attribution (TDA) methods aim to quantify the influence of individual training data points on the model predictions, with broad applications in data-centric AI, such as mislabel detection, data selection, and copyright compensation. However, existing methods in this field, which can be categorized as retraining-based and gradient-based, have struggled with the trade-off between computational efficiency and attribution efficacy. Retraining-based methods can accurately attribute complex non-convex models but are computationally prohibitive, while gradient-based methods are efficient but often fail for non-convex models. Recent research has shown that augmenting gradient-based methods with ensembles of multiple independently trained models can achieve significantly better attribution efficacy. However, this approach remains impractical for very large-scale applications.In this work, we discover that expensive, fully independent training is unnecessary for ensembling the gradient-based methods, and we propose two efficient ensemble strategies,  DROPOUT ENSEMBLE and LORA ENSEMBLE, alternative to naive independent ensemble. These strategies significantly reduce training time (up to 80%), serving time (up to 60%), and space cost (up to 80%) while maintaining similar attribution efficacy to the naive independent ensemble. Our extensive experimental results demonstrate that the proposed strategies are effective across multiple TDA methods on diverse datasets and models, including generative settings, significantly advancing the Pareto frontier of TDA methods with better computational efficiency and attribution efficacy. We conduct a theoretical analysis that provides insights into the success of our empirical findings."
    },
    {
        "title": "BrainCodec: Neural fMRI codec for the decoding of cognitive brain states",
        "link_suffix": "/forum?id=o6ddWvoyjK",
        "link": "https://openreview.net/forum?id=o6ddWvoyjK",
        "pdf_link": "https://openreview.net/pdf?id=o6ddWvoyjK",
        "keywords": "fMRI, Neural Audio Codec, NLP, SSL",
        "abstract": "Recently, leveraging big data in deep learning has led to significant performance improvements, as confirmed in applications like mental state decoding using fMRI data. \nHowever, fMRI datasets remain relatively small in scale, and the inherent issue of low signal-to-noise ratios (SNR) in fMRI data further exacerbates these challenges. \nTo address this, we apply compression techniques as a preprocessing step for fMRI data. \nWe propose BrainCodec, a novel fMRI codec inspired by the neural audio codec. \nWe evaluated BrainCodec's compression capability in mental state decoding, demonstrating further improvements over previous methods.\nFurthermore, we analyzed the latent representations obtained through BrainCodec, elucidating the similarities and differences between task and resting state fMRI, highlighting the interpretability of BrainCodec. \nAdditionally, we demonstrated that fMRI reconstructions using BrainCodec can enhance the visibility of brain activity by achieving higher SNR, suggesting its potential as a novel denoising method. \nOur study shows that BrainCodec not only enhances performance over previous methods but also offers new analytical possibilities for neuroscience.\nOur codes, dataset, and model weights are available athttps://anonymous.4open.science/r/BrainCodec."
    },
    {
        "title": "AutoAL: Automated Active Learning with Differentiable Query Strategy Search",
        "link_suffix": "/forum?id=4RRmy9iw3c",
        "link": "https://openreview.net/forum?id=4RRmy9iw3c",
        "pdf_link": "https://openreview.net/pdf?id=4RRmy9iw3c",
        "keywords": "Active Learning, Differentiable Bi-level Optimization",
        "abstract": "As deep learning continues to evolve, the need for data efficiency becomes increasingly important. Considering labeling large datasets is both time-consuming and expensive, active learning (AL) provides a promising solution to this challenge by iteratively selecting the most informative subsets of examples to train deep neural networks, thereby reducing the labeling cost. However, the effectiveness of different AL algorithms can vary significantly across data scenarios, and determining which AL algorithm best fits a given task remains a challenging problem. This work presents the first differentiable AL strategy search method, named AutoAL, which is designed on top of existing AL sampling strategies. AutoAL consists of two neural nets, named SearchNet and FitNet, which are optimized concurrently under a differentiable bi-level optimization framework. For any given task, SearchNet and FitNet are iteratively co-optimized using the labeled data, learning how well a set of candidate AL algorithms perform on that task. With the optimal AL strategies identified, SearchNet selects a small subset from the unlabeled pool for querying their annotations, enabling efficient training of the task model. Experimental results demonstrate that AutoAL consistently achieves superior accuracy compared to all candidate AL algorithms and other selective AL approaches, showcasing its potential for adapting and integrating multiple existing AL methods across diverse tasks and domains."
    },
    {
        "title": "Geometry-aware Score Distillation via 3D Consistent Noising and Gradients",
        "link_suffix": "/forum?id=uyzkKPvVyS",
        "link": "https://openreview.net/forum?id=uyzkKPvVyS",
        "pdf_link": "https://openreview.net/pdf?id=uyzkKPvVyS",
        "keywords": "Diffusion Models, Score Distillation Sampling, Text-to-3D Generation",
        "abstract": "Score distillation sampling (SDS), the methodology in which the score from pretrained 2D diffusion models is distilled into 3D representation, has recently brought significant advancements in text-to-3D generation. However, this approach is still confronted with critical geometric inconsistency problems such as the ``Janus problem''. We provide a novel insight into this problem, hypothesizing that the incorporation of 3D awareness into the 3D noising process and gradient distillation process may bring about enhanced consistency between gradients, leading to improved fidelity and geometric consistency. To achieve this, we propose a simple yet effective approach to achieve a 3D consistent, geometry-aware noising process, leveraging the advantages that 3D Gaussian Splatting possesses as an explicit 3D representation. Combined with our geometry-based gradient warping and our novel gradient dissimilarity loss, we demonstrate that our method significantly improves performance by addressing geometric inconsistency problems in text-to-3D generation with minimal computation cost and being compatible with existing score distillation-based models."
    },
    {
        "title": "M3-Impute: Mask-guided Representation Learning for Missing Value Imputation",
        "link_suffix": "/forum?id=f114duq0Ov",
        "link": "https://openreview.net/forum?id=f114duq0Ov",
        "pdf_link": "https://openreview.net/pdf?id=f114duq0Ov",
        "keywords": "Missing Value Imputation, Graph Representation Learning, Data Correlations",
        "abstract": "Missing values are a common problem that poses significant challenges to data analysis and machine learning. This problem necessitates the development of an effective imputation method to fill in the missing values accurately, thereby enhancing the overall quality and utility of the datasets. Existing imputation methods, however, fall short of explicitly considering the `missingness' information in the data during the embedding initialization stage and modeling the entangled feature and sample correlations during the learning process, thus leading to inferior performance. We propose M$^3$-Impute, which aims to explicitly leverage the missingness information and such correlations with novel masking schemes. M$^3$-Impute first models the data as a bipartite graph and uses a graph neural network to learn node embeddings, where the refined embedding initialization process directly incorporates the missingness information. They are then optimized through M$^3$-Impute's novel feature correlation unit (FRU) and sample correlation unit (SRU) that effectively captures feature and sample correlations for imputation. Experiment results on 25 benchmark datasets under three different missingness settings show the effectiveness of M$^3$-Impute by achieving 20 best and 4 second-best MAE scores on average."
    },
    {
        "title": "Shape Assembly via Equivariant Diffusion",
        "link_suffix": "/forum?id=2v405jBQ5X",
        "link": "https://openreview.net/forum?id=2v405jBQ5X",
        "pdf_link": "https://openreview.net/pdf?id=2v405jBQ5X",
        "keywords": "Diffusion, Equivariant diffusion, Shape assembly",
        "abstract": "We tackle the problem of solving shape puzzles, that is, reassembling randomly-partitioned and scattered pieces of 2D or 3D shapes into an original shape. This task is challenging since it only relies on geometric features without rich visual information. Specifically, we are supposed that target shapes and their randomly-partitioned pieces are pattern-free and irregular. Existing methods tend to rely on specific constraints regarding piece shapes and neglect the consideration of invariance and equivariance. We propose learning a robust puzzle solver through a generative diffusion process in which the roto-translational equivariance holds. Experiments on 2D and 3D puzzle benchmarks including the Breaking Bad dataset demonstrate that our method successfully assembles given geometric pieces into a target shape. We also provide in-depth ablation studies showing the effects of our equivariant design and the components in our proposed framework."
    }
]