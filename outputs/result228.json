[{"title": "Anomaly Detection Exposed: Imagining Anomalies Were Normal", "link_suffix": "/forum?id=gcTKtwWyQm", "link": "https://openreview.net/forum?id=gcTKtwWyQm", "pdf_link": "https://openreview.net/pdf?id=gcTKtwWyQm", "keywords": "anomaly-detection, deep-anomaly-detection, anomaly, deep-learning, one-class-classification, outlier-exposure", "abstract": "Deep learning-based methods have achieved a breakthrough in image anomaly detection, but their complexity introduces a considerable challenge to understanding why an instance is predicted to be anomalous. We introduce a novel explanation method that generates multiple alternative modifications for each anomaly, capturing diverse concepts of anomalousness. Each modification is trained to be perceived as normal by the anomaly detector. The method provides a semantic explanation of the mechanism that triggered the anomaly detector, allowing users to explore ``what-if scenarios.'' Qualitative and quantitative analyses across various image datasets demonstrate that applying this method to state-of-the-art anomaly detectors provides high-quality semantic explanations.", "title_embedding_index": 11350, "title_abs_embedding_index": 11375}, {"title": "Transformers Provably Learn Two-Mixture of Linear Classification via Gradient Flow", "link_suffix": "/forum?id=AuAj4vRPkv", "link": "https://openreview.net/forum?id=AuAj4vRPkv", "pdf_link": "https://openreview.net/pdf?id=AuAj4vRPkv", "keywords": "transformer, training dynamics, gradient flow, mixture of linear classification", "abstract": "Understanding how transformers learn and utilize hidden connections between tokens is crucial to understand the behavior of large language models.\nTo understand this mechanism, we consider the task of two-mixture of linear classification which possesses a hidden correspondence structure among tokens, and study the training dynamics of a symmetric two-headed transformer with ReLU neurons.\nMotivated by the stage-wise learning phenomenon in our experiments, we design and theoretically analyze a three-stage training algorithm, which can effectively characterize the actual gradient descent dynamics when we simultaneously train the neuron weights and the softmax attention.\nThe first stage is a neuron learning stage, where the neurons align with the underlying signals. \nThe second stage is a attention feature learning stage, where we analyze the feature learning process of how the attention learns to utilize the relationship between the tokens to solve certain hard samples.\nIn the meantime, the attention features evolve from a nearly non-separable state (at the initialization) to a well-separated state.\nThe third stage is a convergence stage, where the population loss is driven towards zero.\nThe key technique in our analysis of softmax attention is to identify a critical sub-system inside a large dynamical system and bound the growth of the non-linear sub-system by a linear system. \nFinally, we discuss the setting with more than two mixtures. \nWe empirically show the difficulty of generalizing our analysis of the gradient flow dynamics to the case even when the number of mixtures equals three, although the transformer can still successfully learn such distribution. \nOn the other hand, we show by construction that there exists a transformer that can solve mixture of linear classification given any arbitrary number of mixtures.", "title_embedding_index": 11351, "title_abs_embedding_index": 11376}, {"title": "Alchemy: Amplifying Theorem-Proving Capability Through Symbolic Mutation", "link_suffix": "/forum?id=7NL74jUiMg", "link": "https://openreview.net/forum?id=7NL74jUiMg", "pdf_link": "https://openreview.net/pdf?id=7NL74jUiMg", "keywords": "Synthetic Data, Neural Theorem Proving, Formal Reasoning, Lean Theorem Prover", "abstract": "Formal proofs are challenging to write even for experienced experts. Recent progress in Neural Theorem Proving (NTP) shows promise in expediting this process. However, the formal corpora available on the Internet are limited compared to the general text, posing a significant data scarcity challenge for NTP. To address this issue, this work proposes Alchemy, a general framework for data synthesis that constructs formal theorems through symbolic mutation. Specifically, for each candidate theorem in Mathlib, we identify all invocable theorems that can be used to rewrite or apply to it. Subsequently, we mutate the candidate theorem by replacing the corresponding term in the statement with its equivalent form or antecedent. As a result, our method increases the number of theorems in Mathlib by an order of magnitude, from 110k to 6M. Furthermore, we perform continual pretraining and supervised finetuning on this augmented corpus for large language models. Experimental results demonstrate the effectiveness of our approach, achieving a 5% absolute performance improvement on Leandojo benchmark. Additionally, our synthetic data achieve a 2.5% absolute performance gain on the out-of-distribution miniF2F benchmark. To provide further insights, we conduct a comprehensive analysis of synthetic data composition and the training paradigm, offering valuable guidance for developing a strong theorem prover.", "title_embedding_index": 11352, "title_abs_embedding_index": 11377}, {"title": "Arti-PG: A Procedural Toolbox to Synthesize Large-Scale and Diverse Articulated Objects with Rich Annotations", "link_suffix": "/forum?id=E1HLZcRZI1", "link": "https://openreview.net/forum?id=E1HLZcRZI1", "pdf_link": "https://openreview.net/pdf?id=E1HLZcRZI1", "keywords": "Articulated Object, Articulated Object Manipulation, Robotics", "abstract": "The acquisition of substantial volumes of 3D articulated object data is expensive and time-consuming, and consequently the scarcity of 3D articulated object data becomes an obstacle for deep learning methods to achieve remarkable performance in various articulated object understanding tasks. Meanwhile, pairing these object data with detailed annotations to enable training for various tasks is also difficult and labor-intensive to achieve. In order to expeditiously gather a significant number of 3D articulated objects with comprehensive and detailed annotations for training, we propose Articulated Object Procedural Generation toolbox, a.k.a. Arti-PG toolbox. Arti-PG toolbox consists of i) descriptions of articulated objects by means of a generalized structure program along with their analytic correspondence to the objects\u2019 point cloud, ii) procedural rules about manipulations on the structure program to synthesize large-scale and diverse new articulated objects, and iii) mathematical descriptions of knowledge (e.g. affordance, semantics, etc.) to provide annotations to the synthesized object. Arti-PG has two appealing properties for providing training data for articulated object understanding tasks: i) objects are created with unlimited variations in shape through program-oriented structure manipulation, ii) Arti-PG is widely applicable to diverse tasks by easily providing comprehensive and detailed annotations. Arti-PG now supports the procedural generation of 26 categories of articulate objects and provides annotations across a wide range of both vision and manipulation tasks, and we provide exhaustive experiments which fully demonstrate its advantages. We will make Arti-PG toolbox publicly available for the community to use. More details, analysis and discussions are provided in technical appendices.", "title_embedding_index": 11353, "title_abs_embedding_index": 11378}, {"title": "Revisiting DNN Training for Intermittently-Powered Energy-Harvesting Micro-Computers", "link_suffix": "/forum?id=SFNqrHQTEP", "link": "https://openreview.net/forum?id=SFNqrHQTEP", "pdf_link": "https://openreview.net/pdf?id=SFNqrHQTEP", "keywords": "Intermittent Computing, Energy Harvesting, Intermittency Aware Training, Hardware-Software codesign", "abstract": "The deployment of Deep Neural Networks (DNNs) in energy-constrained environments, such as Energy Harvesting Wireless Sensor Networks (EH-WSNs), introduces significant challenges due to the intermittent nature of power availability. This study introduces NExUME, a novel training methodology designed specifically for DNNs operating under such constraints. We propose a dynamic adjustment of training parameters\u2014dropout rates and quantization levels\u2014that adapt in real-time to the available energy, which varies in energy harvesting scenarios.This approach utilizes a model that integrates the characteristics of the network architecture and the specific energy harvesting profile. It dynamically adjusts training strategies, such as the intensity and timing of dropout and quantization, based on predictions of energy availability. This method not only conserves energy but also enhances the network\u2019s adaptability, ensuring robust learning and inference capabilities even under stringent power constraints. Our results show a 6% to 22% improvement in accuracy over current methods, with an increase of less than 5% in computational overhead. This paper details the development of the adaptive training framework, describes the integration of energy profiles with dropout and quantization adjustments, and presents a comprehensive evaluation using real-world data. Additionally, we introduce a novel dataset aimed at furthering the application of energy harvesting in computational settings.", "title_embedding_index": 11354, "title_abs_embedding_index": 11379}, {"title": "3D Affordance Reconstruction from Egocentric Demonstration Video", "link_suffix": "/forum?id=pxYqG9GSpQ", "link": "https://openreview.net/forum?id=pxYqG9GSpQ", "pdf_link": "https://openreview.net/pdf?id=pxYqG9GSpQ", "keywords": "3d Affordance, Egocentric Vision, Learning from Demonstration", "abstract": "Developing robots capable of generalized skills remains an exceedingly challenging task. Drawing from psychology, the concept of affordance has emerged as a promising intermediate representation to guide robot manipulation. However, prior work has primarily focused on 2D affordances from video, neglecting critical spatial information such as camera positioning, absolute position, depth and geometry. In this paper, we present a novel training-free method that constructs 3D affordances from egocentric demonstration videos. To address the challenge of insufficient static, high-quality frames for 3D reconstruction in egocentric videos, we employ the 3D foundational model DUST3R, which reconstructs scenes from sparse images without requiring COLMAP. We analyze videos using hand detection to identify contact times and 2D contact points, reconstruct these interactions using DUST3R, and project the 2D contact points into 3D space using gaussian heatmaps. Finally, we derive hand trajectories through 3D hand pose estimation and process them using linear regression to integrate the spatiotemporal dynamics of human-object interactions. We demonstrate the effectiveness of our method on the ego4d-exo dataset for seven real-world hand-object manipulation tasks in cooking scenes.", "title_embedding_index": 11355, "title_abs_embedding_index": 11380}, {"title": "Phase-Aware KANGaussian : Phase-Regularized 3D Gaussian Splatting with Kolmogorov-Arnold Network", "link_suffix": "/forum?id=BszvEXQyLM", "link": "https://openreview.net/forum?id=BszvEXQyLM", "pdf_link": "https://openreview.net/pdf?id=BszvEXQyLM", "keywords": "3D Gaussian Splatting, Kolmogorov Arnold Network, Phase Regularization, Specular", "abstract": "Vanilla 3D Gaussian Splatting struggles with modelling high frequency details, especially in unbounded scenes. Recent works such as Scaffold-GS and Spec-Gaussian have made tremendous improvements to the reconstruction quality of these high frequency details, specifically in synthetic and bounded scenes, but still struggle with unbounded real world scenes. Therefore, we propose Phase-Aware KANGaussian, a model building on these earlier contributions to produce state-of-the-art reconstruction quality for unbounded real world scenes with greatly improved high frequency details. Phase-Aware KANGaussian introduces a novel phase regularization method that optimizes models from low-to-high frequency, dramatically improving the quality of high frequency details. Phase-Aware KANGaussian is also one of the first few papers to integrate a Kolmogorov-Arnold Network (KAN) into the Gaussian Splatting rendering pipeline to verify its performance against the Multilayer Perceptron (MLP). All in all, Phase-Aware KANGaussian has three main contributions: (1) Introduce a Gaussian Splatting model with state-of-the-art performance in modelling real-world unbounded scenes with high frequency details, (2) a novel phase regularization technique to encode spatial representation and lastly, (3) first few to introduce a KAN into the Gaussian Splatting rendering pipeline.", "title_embedding_index": 11356, "title_abs_embedding_index": 11381}, {"title": "TopoSD: Topology-Enhanced Lane Segment Perception with SDMap prior", "link_suffix": "/forum?id=9tiQ0aBK7c", "link": "https://openreview.net/forum?id=9tiQ0aBK7c", "pdf_link": "https://openreview.net/pdf?id=9tiQ0aBK7c", "keywords": "autonomous driving; online high-definition map construction; standard-definition map; topology reasoning;", "abstract": "Recent advances in autonomous driving systems have shifted towards reducing reliance on high-definition maps (HDMaps) due to the huge costs of annotation and maintenance. Instead, researchers are focusing on online vectorized HDMap construction using on-board sensors. However, sensor-only approaches still face challenges for long-range perception due to the limited field of view of cameras, as human drivers also need navigation maps for comprehensive geometric and topological road information.\n To address these issues, we propose to train the perception model to \"see\" standard definition maps (SDMaps). We encode SDMap elements into neural spatial map representations and instance tokens, and then incorporate such complementary features as prior information to improve the Bird's Eye View (BEV) feature for lane geometry and topology decoding. Based on the lane segment representation framework, the model simultaneously predicts lanes, centrelines and their topology. To further enhance the ability of geometry prediction and topology reasoning, we also use a topology-guided decoder to refine the predictions\n by exploiting the mutual relationships between topological and geometric features. We perform extensive experiments on OpenLane-V2 datasets to validate the proposed method. The results show that our model outperforms state-of-the-art methods by a large margin, with gains of +6.7 and +9.1 on the mAP and topology metrics. Our analysis also reveals that models trained with SDMap noise augmentation exhibit enhanced robustness.", "title_embedding_index": 11357, "title_abs_embedding_index": 11382}, {"title": "Agent Skill Acquisition for Large Language Models via CycleQD", "link_suffix": "/forum?id=Kvdh12wGC0", "link": "https://openreview.net/forum?id=Kvdh12wGC0", "pdf_link": "https://openreview.net/pdf?id=Kvdh12wGC0", "keywords": "Large Language Models, Skill Acquisition, Quality Diversity", "abstract": "Training large language models to acquire specific skills remains a challenging endeavor. Conventional training approaches often struggle with data distribution imbalances and inadequacies in objective functions that do not align well with task-specific performance. To address these challenges, we introduce CycleQD, a novel approach that leverages the Quality Diversity framework through a cyclic adaptation of the algorithm, along with a model merging based crossover and an SVD-based mutation. In CycleQD, each task\u2019s performance metric is alternated as the quality measure while the others serve as the behavioral characteristics. This cyclic focus on individual tasks allows for concentrated effort on one task at a time, eliminating the need for data ratio tuning and simplifying the design of the objective function. Empirical results from AgentBench indicate that applying CycleQD to LLAMA3-8B-INSTRUCT based models not only enables them to surpass traditional fine-tuning methods in coding, operating systems, and database tasks, but also achieves performance on par with GPT-3.5-TURBO, which potentially contains much more parameters, across these domains. Crucially, this enhanced performance is achieved while retaining robust language capabilities, as evidenced by its performance on widely adopted language benchmark tasks. We highlight the key design choices in CycleQD, detailing how these contribute to its effectiveness. Furthermore, our method is general and can be applied to image segmentation models, highlighting its applicability across different domains.", "title_embedding_index": 11358, "title_abs_embedding_index": 11383}, {"title": "Learning Graph Invariance by Harnessing Spuriosity", "link_suffix": "/forum?id=UsVJlgD1F7", "link": "https://openreview.net/forum?id=UsVJlgD1F7", "pdf_link": "https://openreview.net/pdf?id=UsVJlgD1F7", "keywords": "Out-of-Distribution Generalization, Invariant Learning, Graph Neural Networks", "abstract": "Recently, graph invariant learning has become the \\textit{de facto} approach to tackle the Out-of-Distribution (OOD) generalization failure in graph representation learning.\nThey generically follow the framework of invariant risk minimization to capture the invariance of graph data from different environments.\nDespite some success, it remains unclear to what extent existing approaches have captured invariant features for OOD generalization on graphs.\nIn this work, we find that representative OOD methods such as IRM and VRex, and their variants on graph invariant learning may have captured a limited set of invariant features. \nTo tackle this challenge, we propose LIRS, a novel learning framework designed to Learn graph Invariance by Removing Spurious features.\nDifferent from most existing approaches that \\textit{directly} learn the invariant features, \nLIRS takes an \\textit{indirect} approach by first learning the spurious features and then removing them from the ERM-learned features, which contains both spurious and invariant features. We demonstrate that learning the invariant graph features in an \\textit{indirect} way can learn a more comprehensive set of invariant features. Moreover, our proposed method outperforms the second-best method by as much as 25.50% across all competitive baseline methods, highlighting its effectiveness in learning graph invariant features.", "title_embedding_index": 11359, "title_abs_embedding_index": 11384}, {"title": "Spiking Vision Transformer with Saccadic Attention", "link_suffix": "/forum?id=qzZsz6MuEq", "link": "https://openreview.net/forum?id=qzZsz6MuEq", "pdf_link": "https://openreview.net/pdf?id=qzZsz6MuEq", "keywords": "Spiking Neural Networks, Spiking Transformer, Spike-driven Self-attention", "abstract": "The combination of Spiking Neural Networks (SNNs) and Vision Transformers (ViTs) holds potential for achieving both energy efficiency and high performance, particularly suitable for edge vision applications. However, a significant performance gap still exists between SNN-based ViTs and their ANN counterparts. Here, we first analyze why SNN-based ViTs suffer from limited performance and identify a mismatch between the vanilla self-attention mechanism and spatio-temporal spike trains. This mismatch results in degraded spatial relevance and limited temporal interactions. To address these issues, we draw inspiration from biological saccadic attention mechanisms and introduce an innovative Saccadic Spike Self-Attention (SSSA) method. Specifically, in the spatial domain, SSSA employs a novel spike distribution-based method to effectively assess the relevance between Query and Key pairs in SNN-based ViTs. Temporally, SSSA employs a saccadic interaction module that dynamically focuses on selected visual areas at each timestep and significantly enhances whole scene understanding through temporal interactions.\nBuilding on the SSSA mechanism, we develop a SNN-based Vision Transformer (SNN-ViT). Extensive experiments across various visual tasks demonstrate that SNN-ViT achieves state-of-the-art performance with linear computational complexity. The effectiveness and efficiency of the SNN-ViT highlight its potential for power-critical edge vision applications.", "title_embedding_index": 11360, "title_abs_embedding_index": 11385}, {"title": "Robust Quantum Neural Networks Against Dynamic Noise Landscape in the NISQ Era", "link_suffix": "/forum?id=NuWX55CpIQ", "link": "https://openreview.net/forum?id=NuWX55CpIQ", "pdf_link": "https://openreview.net/pdf?id=NuWX55CpIQ", "keywords": "Quantum Neural Network, Noise-Aware Training, Dynamic Noise", "abstract": "Quantum machine learning, an emerging field in the noisy intermediate-scale quantum (NISQ) era, faces significant challenges in error mitigation during training and inference stages. Current noise-aware training (NAT) methods typically assume static error rates in quantum neural networks (QNNs), often neglecting the inherently dynamic nature of such noise. By addressing this oversight, our work recognizes the dynamics of noise in the NISQ era, evidenced by fluctuating error rates across different times and qubits. Moreover, QNN performance can vary markedly depending on the specific locations of errors, even under similar error rates. This variability underscores the limitations of static NAT strategies in addressing the dynamic nature of noisy environments. We propose a novel NAT strategy that adapts to both standard and fatal error conditions, cooperating with a low-complexity search strategy to efficiently locate fatal errors during optimization. Our approach marks a significant advancement over current NAT methods by maintaining robust performance in fatal error scenarios. Evaluations validate the efficacy of our strategy against fatal errors, while maintaining performance comparable to state-of-the-art NAT approaches under various error rates.", "title_embedding_index": 11361, "title_abs_embedding_index": 11386}, {"title": "What Kind of Pretraining Data Do Large Language Models Rely on When Doing Reasoning?", "link_suffix": "/forum?id=1hQKHHUsMx", "link": "https://openreview.net/forum?id=1hQKHHUsMx", "pdf_link": "https://openreview.net/pdf?id=1hQKHHUsMx", "keywords": "large language model; LLM; reasoning; pretraining data; influence functions; mathematical reasoning", "abstract": "The capabilities and limitations of Large Language Models (LLMs) have been sketched out in great detail in recent years, providing an intriguing yet conflicting picture. On the one hand, LLMs demonstrate a general ability to solve problems. On the other hand, they show surprising reasoning gaps when compared to humans, casting doubt on the robustness of their generalisation strategies. The sheer volume of data used in the design of LLMs has precluded us from applying the method traditionally used to measure generalisation; train-test set separation. In this work, we study what kind of generalisation strategies LLMs employ when performing reasoning tasks by investigating the pretraining data they rely on. For two models of different sizes (7B and 35B) and 2.5B of their pretraining tokens, we identify what documents impact three simple mathematical reasoning tasks and contrast this to the data that are influential for answering factual questions. We find that, while the models rely on mostly distinct sets of data for each factual question, documents often have a similar influence on different reasoning questions with the same task, indicating the presence of procedural knowledge. We further find that the answers to the factual questions often show up in the most influential data. However, for the reasoning questions the answers usually do not show up as highly influential, nor do the answers to the intermediate reasoning steps. When we characterise the top portion of the ranking for the reasoning questions qualitatively, we find that the influential documents often contain procedural knowledge, like demonstrating how to obtain the solution using formulae or code. Our findings indicate that the generalisation strategy the model uses when doing reasoning is unlike retrieval, but more like a strategy using many documents doing a similar form of reasoning.", "title_embedding_index": 11362, "title_abs_embedding_index": 11387}, {"title": "Generative bandit optimization via diffusion posterior sampling", "link_suffix": "/forum?id=ef3tbYHIVn", "link": "https://openreview.net/forum?id=ef3tbYHIVn", "pdf_link": "https://openreview.net/pdf?id=ef3tbYHIVn", "keywords": "bandit optimization, diffusion models, posterior sampling, algorithmic discovery", "abstract": "Many real-world discovery problems, including drug and material design, can be modeled within the bandit optimization framework, where an agent selects a sequence of experiments to efficiently optimize an unknown reward function. However, classic bandit algorithms operate on fixed finite or continuous action sets, making discovering novel designs impossible in the former case, and often leading to the curse of dimensionality in the latter, thus rendering these methods impractical. In this work, we first formalize thegenerative banditsetting, where an agent wishes to maximize an unknown reward function over the support of a data distribution, often calleddata manifold, which implicitly encodes complex constraints (e.g., the geometry of valid molecules), and from which (unlabeled) sample data is available (e.g., a dataset of valid molecules). We then propose Diffusion Posterior Sampling (DiffPS), an algorithm that tackles the exploration-exploitation problem directly on the learned data manifold by leveraging a conditional diffusion model. We formally show that the statistical complexity of DiffPS adapts to theintrinsic dimensionalityof the data, overcoming the curse of dimensionality in high-dimensional settings. Our experimental evaluation supports the theoretical claims and demonstrates promising performance in practice.", "title_embedding_index": 11363, "title_abs_embedding_index": 11388}, {"title": "From Attention to Activation: Unraveling the Enigmas of Large Language Models", "link_suffix": "/forum?id=IjduZQK8gM", "link": "https://openreview.net/forum?id=IjduZQK8gM", "pdf_link": "https://openreview.net/pdf?id=IjduZQK8gM", "keywords": "Transformers, Adam, Optimizer, Outliers, Attention, Quantization", "abstract": "We study two strange phenomena in auto-regressive Transformers: (1) the dominance of the \ufb01rst token in attention heads; (2) the occurrence of large outlier activations in the hidden states. We \ufb01nd that popular large language models, such as Llama attend maximally to the first token in 98% of attention heads, a behaviour we attribute to the softmax function. To mitigate this issue, we propose a reformulation of softmax to softmax-1. Furthermore, we identify adaptive optimisers, e.g. Adam, as the primary contributor to the large outlier activations and introduce OrthoAdam, a novel optimiser that utilises orthogonal matrices to transform gradients, to address this issue. Finally, not only do our methods prevent these phenomena from occurring, but additionally, they enable Transformers to sustain their performance when quantised using basic algorithms, something that standard methods are unable to do. In summary, our methods reduce the attention proportion on the first token from 65% to 3.3%, the activation kurtosis in the hidden states from 1657 to 3.1, and perplexity penalty under 4-bit weight quantisation from 3565 to 0.3. We will publish our code upon acceptance.", "title_embedding_index": 11364, "title_abs_embedding_index": 11389}, {"title": "On the Linear Speedup of Personalized Federated Reinforcement Learning with Shared Representations", "link_suffix": "/forum?id=BfUDZGqCAu", "link": "https://openreview.net/forum?id=BfUDZGqCAu", "pdf_link": "https://openreview.net/pdf?id=BfUDZGqCAu", "keywords": "personalized federated reinforcement learning, shared representations, stochastic approximation", "abstract": "Federated reinforcement learning (FedRL) enables multiple agents to collaboratively learn a policy without sharing their own local trajectories collected during agent-environment interactions. However, in practice, the environments faced by different agents are often heterogeneous, leading to poor performance by the single policy learned by existing FedRL algorithms on individual agents. In this paper, we take a further step and introduce a personalized FedRL framework (PFedRL) by taking advantage of possibly shared common structure among agents in heterogeneous environments. Specifically, we develop a class of PFedRL algorithms named PFedRL-Rep that learns (1) a shared feature representation collaboratively among all agents and (2) an agent-specific weight vector personalized to its local environment. We analyze the convergence of PFedTD-Rep, a particular instance of the framework with temporal difference (TD) learning and linear representations. To the best of our knowledge, we are the first to prove a linear convergence speedup with respect to the number of agents in the PFedRL setting. To achieve this, we show that PFedTD-Rep is an example of the federated two-timescale stochastic approximation with Markovian noise. Experimental results demonstrate that PFedTD-Rep, along with an extension to the control setting based on deep Q-networks (DQN), not only improve learning in heterogeneous settings, but also provide better generalization to new environments.", "title_embedding_index": 11365, "title_abs_embedding_index": 11390}, {"title": "Show or Tell? Effectively prompting Vision-Language Models for semantic segmentation", "link_suffix": "/forum?id=mXh8LbXXpx", "link": "https://openreview.net/forum?id=mXh8LbXXpx", "pdf_link": "https://openreview.net/pdf?id=mXh8LbXXpx", "keywords": "Foundation Models, Prompting, Semantic Segmentation, Vision-Language Models, VLM, Training-free", "abstract": "Large Vision-Language Models (VLMs) are increasingly being regarded as foundation models that can be instructed to solve diverse tasks by prompting, without task-specific training.\nWe examine the seemingly obvious question: \\emph{how to effectively prompt VLMs for semantic segmentation}.\nTo that end, we systematically evaluate the segmentation performance of several recent models guided by either text or visual prompts on the diverse MESS dataset collection.\nWe introduce a scalable prompting scheme, \\emph{few-shot prompted semantic segmentation}, inspired by open-vocabulary segmentation and few-shot learning.\nIt turns out that even the most advanced VLMs lag far behind specialist models trained for a specific segmentation task, by about 30% on average on the Intersection-over-Union metric.\nMoreover, we find that text prompts and visual prompts are complementary: each one of the two modes fails on many examples that the other one can solve.\nOur analysis suggests that being able to anticipate the most effective prompt modality can lead to a 11% improvement in performance.\nMotivated by our findings, we propose PromptMatcher, a remarkably simple baseline that combines both text and visual prompts, achieving state-of-the-art results for training-free semantic segmentation.", "title_embedding_index": 11366, "title_abs_embedding_index": 11391}, {"title": "Global Identifiability of Overcomplete Dictionary Learning via L1 and Volume Minimization", "link_suffix": "/forum?id=4nrcn0YoDG", "link": "https://openreview.net/forum?id=4nrcn0YoDG", "pdf_link": "https://openreview.net/pdf?id=4nrcn0YoDG", "keywords": "Dictionary learning, overcomplete, sparse, identifiability", "abstract": "We propose a novel formulation for dictionary learning with an overcomplete dictionary, i.e., when the number of atoms is larger than the dimension of the dictionary. The proposed formulation consists of a weighted sum of $\\ell_1$ norms of the rows of the sparse coefficient matrix plus the log of the matrix volume of the dictionary matrix. The main contribution of this work is to show that this novel formulation guarantees global identifiability of the overcomplete dictionary, under a mild condition that the sparse coefficient matrix satisfies a strong scattering condition in the hypercube. Furthermore, if every column of the coefficient matrix is sparse and the dictionary guarantees $\\ell_1$ recovery, then the coefficient matrix is identifiable as well. This is a major breakthrough for not only dictionary learning but also general matrix factorization models as identifiability is guaranteed even when the latent dimension is higher than the ambient dimension. We also provide a probabilistic analysis and show that if the sparse coefficient matrix is generated from the widely adopted sparse-Gaussian model, then the $m\\times k$ overcomplete dictionary is globally identifiable if the sample size is bigger than a constant times $(k^2/m)\\log(k^2/m)$, where $k$ is the number of atoms in the dictionary, with overwhelming probability. Finally, we propose an algorithm based on alternating minimization to solve the new proposed formulation.", "title_embedding_index": 11367, "title_abs_embedding_index": 11392}, {"title": "Intrinsic Evaluation of Unlearning Using Parametric Knowledge Traces", "link_suffix": "/forum?id=blNaExRx7Q", "link": "https://openreview.net/forum?id=blNaExRx7Q", "pdf_link": "https://openreview.net/pdf?id=blNaExRx7Q", "keywords": "LLM Safety, LLM Interpretability, LLM Unlearning, benchmark, evaluations", "abstract": "The task of \u201cunlearning\u201d certain concepts in large language models (LLMs) has attracted immense attention recently, due to its importance in mitigating undesirable model behaviours, such as the generation of harmful, private, or incorrect information. Current protocols to evaluate unlearning methods largely rely on behavioral tests, without monitoring the presence of unlearned knowledge within the model\u2019s parameters. This residual knowledge can be adversarially exploited to recover the erased information post-unlearning. We argue that unlearning should also be evaluated internally, by considering changes in the parametric knowledge traces of the unlearned concepts. To this end, we propose a general evaluation methodology that leverages vocabulary projections to inspect concepts encoded in model parameters. We use this approach to localize \u201cconcept vectors\u201d \u2014 parameter vectors that encode concrete concepts \u2014 and construct ConceptVectors, a benchmark dataset containing hundreds of common concepts and their parametric knowledge traces within two open-source LLMs. Evaluation on ConceptVectors shows that existing unlearning methods minimally impact concept vectors and mostly suppress them during inference, while directly ablating these vectors demonstrably removes the associated knowledge and significantly reduces the model\u2019s susceptibility to adversarial manipulation. Our results highlight limitations in behavioral-based unlearning evaluations and call for future work to include parameter-based evaluations. To support this, we release our code and benchmark athttps://anonymous.4open.science/r/ConceptVectors_review-98EF.", "title_embedding_index": 11368, "title_abs_embedding_index": 11393}, {"title": "ShortCircuit: AlphaZero-Driven Generative Circuit Design", "link_suffix": "/forum?id=KjTh5C0z7Y", "link": "https://openreview.net/forum?id=KjTh5C0z7Y", "pdf_link": "https://openreview.net/pdf?id=KjTh5C0z7Y", "keywords": "Graph Generation, MCTS, AlphaZero, AIG, Boolean Logic, EDA", "abstract": "Chip design relies heavily on generating Boolean circuits, such as AND-Inverter Graphs (AIGs), from functional descriptions like truth tables. This generation operation is a key process in logic synthesis, a primary chip design stage. While recent advances in deep learning have aimed to accelerate circuit design, these efforts have mostly focused on tasks other than synthesis, and traditional heuristic methods have plateaued. In this paper, we introduce ShortCircuit, a novel transformer-based architecture that leverages the structural properties of AIGs and performs efficient space exploration. Contrary to prior approaches attempting end-to-end generation of logic circuits using deep networks, ShortCircuit employs a two-phase process combining supervised with reinforcement learning to enhance generalization to unseen truth tables. We also propose an AlphaZero variant to handle the double exponentially large state space and the sparsity of the rewards, enabling the discovery of near-optimal designs. To evaluate the generative performance of our trained model , we extract 500 truth tables from a benchmark set of 20 real-world circuits. ShortCircuit successfully generates AIGs for 98% of the 8-input test truth tables, and outperforms the state-of-the-art logic synthesis tool, ABC, by 18.79% in terms\nof circuits size.", "title_embedding_index": 11369, "title_abs_embedding_index": 11394}, {"title": "Don\u2019t Discard, but Keep It Small: Context-Preserving KV Cache Compression with Importance-Aware Adaptive Precision", "link_suffix": "/forum?id=CRQ8JuQDEd", "link": "https://openreview.net/forum?id=CRQ8JuQDEd", "pdf_link": "https://openreview.net/pdf?id=CRQ8JuQDEd", "keywords": "large language models, safety, hallucination, key-value cache compression, long context", "abstract": "As the length of input sequences in Large Language Models (LLMs) continues to grow, efficient key-value (KV) cache management has become essential for improving inference speed and throughput of autoregressive decoding.\nAlthough several approaches have been proposed to reduce memory usage by selectively retaining only the important KV pairs and discarding the rest, these eviction-based methods can lead to unintended consequences during the generation process.\nIn this paper, we investigate the adverse effects of cache eviction methods and reveal that discarding KV pairs potentially introduces risks such as safety prompt breaches, hallucinations, and loss of critical contextual information.\nInterestingly, we find that preserving even a fraction of the information from evicted KV pairs through reduced precision quantization significantly mitigates these issues.\nOn the other hand, we also observe that important KV pairs need to be maintained at higher precision to preserve generation quality.\nBased on these findings, we propose Mixed-precision KV cache (MiKV), a robust plug-and-play cache compression method that balances performance and memory efficiency.\nMiKV preserves lost contextual information by storing evicted KV pairs in low precision, while maintaining the essential KV pairs in higher precision to ensure generation quality. \nExperimental results across multiple benchmarks and LLM architectures demonstrate that our method achieves a state-of-the-art balance between compression ratio and model performance, outperforming existing baselines.", "title_embedding_index": 11370, "title_abs_embedding_index": 11395}, {"title": "Revisiting Generative Policies: A Simpler Reinforcement Learning Algorithmic Perspective", "link_suffix": "/forum?id=duCs92vmMc", "link": "https://openreview.net/forum?id=duCs92vmMc", "pdf_link": "https://openreview.net/pdf?id=duCs92vmMc", "keywords": "Generative Model, Reinforcement Learning, Diffusion Model, Offline Reinforcement Learning", "abstract": "Generative models, particularly diffusion models, have achieved remarkable success in density estimation for multimodal data, drawing significant interest from the reinforcement learning (RL) community, especially in policy modeling in continuous action spaces. However, existing works exhibit significant variations in training schemes and RL optimization objectives, and some methods are only applicable to diffusion models. In this study, we compare and analyze various generative policy training and deployment techniques, identifying and validating effective designs for generative policy algorithms. Specifically, we revisit existing training objectives and classify them into two categories, each linked to a simpler approach. The first approach, Generative Model Policy Optimization (GMPO), employs a native advantage-weighted regression formulation as the training objective, which is significantly simpler than previous methods. The second approach, Generative Model Policy Gradient (GMPG), offers a numerically stable implementation of the native policy gradient method. We introduce a standardized experimental framework named GenerativeRL. Our experiments demonstrate that the proposed methods achieve state-of-the-art performance on various offline-RL datasets, offering a unified and practical guideline for training and deploying generative policies.", "title_embedding_index": 11371, "title_abs_embedding_index": 11396}, {"title": "Decoupling Dependency Structures: Sklar\u2019s theorem for explainable outlier detection", "link_suffix": "/forum?id=fUZ6rCUZUx", "link": "https://openreview.net/forum?id=fUZ6rCUZUx", "pdf_link": "https://openreview.net/pdf?id=fUZ6rCUZUx", "keywords": "Outlier Detection, Explainability, Copula Modeling", "abstract": "Recent advances in outlier detection have been primarily driven by deep learning models, which, while powerful, have substantial drawbacks in terms of explainability. This is particularly relevant in fields that demand detailed reasoning and understanding of why observations are classified as outliers. To close the gap between state-of-the-art performance and enhanced explainability, we propose Vine Copula-Based Outlier Detection (VC-BOD). We utilize Sklar\u2019s theorem in conjunction with vine copulas and univariate kernel density estimators to decouple marginal distributions and their dependency structure for outlier detection. Our model uses a closed-form equation for the outlier score, which allows for detailed explainability and feature attribution. VC-BOD employs a traceable criterion to determine whether a new observation is an outlier, while also identifying the specific features responsible for this classification. The proposed model further distinguishes whether these features deviate from their own distributions or from interactions with other features. Our empirical assessments reveal that VC-BOD surpasses all classical models in terms of average rank performance and yields competitive results compared with the latest deep learning models.", "title_embedding_index": 11372, "title_abs_embedding_index": 11397}, {"title": "Attributed Graph Clustering via Generalized Quaternion Representation Learning", "link_suffix": "/forum?id=bKCc3USOyv", "link": "https://openreview.net/forum?id=bKCc3USOyv", "pdf_link": "https://openreview.net/pdf?id=bKCc3USOyv", "keywords": "Clustering, graph data, representation learning, quaternion", "abstract": "Clustering complex data in the form of attributed graphs has attracted increasing attention, where appropriate graph representation is a critical prerequisite for accurate cluster analysis. However, the Graph Convolutional Network will homogenize the representation of graph nodes due to the well-known over-smoothing effect. This limits the network architecture to a shallow one, losing the ability to capture the critical global distribution information for clustering. Therefore, we propose a generalized graph auto-encoder network, which introduces quaternion operations to the encoders to achieve efficient structured feature representation learning without incurring deeper network and larger-scale parameters. The generalization of our method lies in the following two aspects: 1) connecting the quaternion operation naturally suitable for four feature components with graph data of arbitrary attribute dimensions, and 2) introducing a generalized graph clustering objective as a loss term to obtain clustering-friendly representations without requiring a pre-specified number of clusters $k$. It turns out that the representations of nodes learned by the proposed Graph Clustering based on Generalized Quaternion representation learning (GCGQ) are more discriminative, containing global distribution information, and are more general, suiting downstream clustering under different $k$s. Extensive experiments including significance tests, ablation studies, and qualitative results, illustrate the superiority of GCGQ. The source code is temporarily opened at \"https://anonymous.4open.science/r/ICLR-25-No7181-codes\".", "title_embedding_index": 11373, "title_abs_embedding_index": 11398}, {"title": "INTRABENCH: Interactive Radiological Benchmark", "link_suffix": "/forum?id=Gvg3nXZvyg", "link": "https://openreview.net/forum?id=Gvg3nXZvyg", "pdf_link": "https://openreview.net/pdf?id=Gvg3nXZvyg", "keywords": "interactive, prompt, segmentation, medical image computing, medical, human in the loop, sam, medical image segmentation", "abstract": "Current interactive segmentation approaches, inspired by the success of META\u2019s Segment Anything model, have achieved notable advancements, however they come with substantial limitations that hinder their practical application in real clinical scenarios. These include unrealistic human interaction requirements, such as slice-by-slice operations for 2D models on 3D data, a lack of iterative refinement, and insufficient evaluation experiments. These shortcomings prevent accurate assessment of model performance and lead to inconsistent outcomes across studies. IntRaBench overcomes these challenges by offering a comprehensive and reproducible framework for evaluating interactive segmentation methods in realistic, clinically relevant scenarios. It includes diverse datasets, target structures, and segmentation models, and provides a flexible codebase that allows seamless integration of new models and prompting strategies. Additionally, we introduce advanced techniques to minimize clinician interaction, ensuring fair comparisons between 2D and 3D models. By open-sourcing IntRaBench, we invite the research community to integrate their models and prompting techniques, ensuring continuous and transparent evaluation of interactive segmentation models in 3D medical\nimaging.", "title_embedding_index": 11374, "title_abs_embedding_index": 11399}]