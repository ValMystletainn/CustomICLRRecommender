[{"title": "I Can Hear You: Selective Robust Training for Deepfake Audio Detection", "link_suffix": "/forum?id=2GcR9bO620", "link": "https://openreview.net/forum?id=2GcR9bO620", "pdf_link": "https://openreview.net/pdf?id=2GcR9bO620", "keywords": "Deepfake audio detection, Audio augmentations, Frequency-Selective Adversarial Training", "abstract": "Recent advances in AI-generated voices have intensified the challenge of detecting deepfake audio, posing risks for scams and the spread of disinformation. To tackle this issue, we establish the largest public voice dataset to date, named DeepFakeVox-HQ, comprising 1.3 million samples, including 270,000 high-quality deepfake samples from 14 diverse sources. Despite previously reported high accuracy, existing deepfake voice detectors struggle with our diversely collected dataset, and their detection success rates drop even further under realistic corruptions and adversarial attacks. We conduct a holistic investigation into factors that enhance model robustness and show that incorporating a diversified set of voice augmentations is beneficial. Moreover, we find that the best detection models often rely on high-frequency features, which are imperceptible to humans and can be easily manipulated by an attacker. To address this, we propose the F-SAT: Frequency-Selective Adversarial Training method focusing on high-frequency components. Empirical results demonstrate that using our training dataset boosts baseline model performance (without robust training) by 33%, and our robust training further improves accuracy by 7.7% on clean samples and by 29.3% on corrupted and attacked samples, over the state-of-the-art RawNet3 model.", "title_embedding_index": 22350, "title_abs_embedding_index": 22375}, {"title": "Learning from Contrastive Prompts: Automated Optimization and Adaptation", "link_suffix": "/forum?id=lGWaAIC9gU", "link": "https://openreview.net/forum?id=lGWaAIC9gU", "pdf_link": "https://openreview.net/pdf?id=lGWaAIC9gU", "keywords": "prompt optimization, large language models", "abstract": "As LLMs evolve, significant effort is spent on manually crafting prompts. While existing prompt optimization methods automate this process, they rely solely on learning from incorrect samples, leading to a sub-optimal performance. Additionally, an unexplored challenge in the literature is prompts effective for prior models may not perform well on newer versions or different languages. We propose the Learning from Contrastive Prompts (LCP) framework to address these gaps, enhancing both prompt optimization and adaptation. LCP employs contrastive learning to generate effective prompts by analyzing patterns in good and bad prompt examples. Our evaluation on the Big-Bench Hard dataset shows that LCP has a win rate of over 76% over existing methods in prompt optimization and demonstrates strong adaptability across different model versions, families, and languages. LCP offers a systematic approach to prompt engineering, reducing manual effort in deploying LLMs across varied contexts.", "title_embedding_index": 22351, "title_abs_embedding_index": 22376}, {"title": "ReferEverything: Towards segmenting everything we can speak of in videos", "link_suffix": "/forum?id=Ir6JxcuP6H", "link": "https://openreview.net/forum?id=Ir6JxcuP6H", "pdf_link": "https://openreview.net/pdf?id=Ir6JxcuP6H", "keywords": "Referring Video Segmentation, Video Diffusion Models", "abstract": "We present REM, a framework for segmenting a wide variety of concepts in video that can be described through natural language. To achieve this level of generalization, our method capitalizes on visual-language representations learned by video diffusion models on Internet-scale datasets. A key insight of our approach is preserving as much of the generative model\u2019s original representation as possible, while fine-tuning it on narrow-domain Referral Object Segmentation datasets. As a result, despite being exclusively trained on object masks from a limited set of categories, our framework is able to accurately segment and track both rare, unseen objects and non-object, dynamic concepts, such as waves crushing in the ocean. To better quantify the generalization capabilities of our model, we introduce a new benchmark for Referral Video Process Segmentation (RVPS), which captures dynamic phenomena that exist at the intersection of video and language. Our experiments show that REM performs comparably to state-of-the-art approaches on in-domain datasets while outperforming them by up to 28% out-of-domain, leveraging the power of Internet-scale pre-training.", "title_embedding_index": 22352, "title_abs_embedding_index": 22377}, {"title": "Distilled Diffusion Language Models", "link_suffix": "/forum?id=xfw92pDy2u", "link": "https://openreview.net/forum?id=xfw92pDy2u", "pdf_link": "https://openreview.net/pdf?id=xfw92pDy2u", "keywords": "diffusion language models, discrete diffusion, distillation", "abstract": "Transformer-based Large Language Models (LLMs) have demonstrated remarkable capa-\nbilities, yet their autoregressive nature forces sequential token-by-token decoding, leading\nto inefficiencies during inference. Furthermore, autoregressive language models lack in-\nherent self-correction abilities, which hinders their capacity to refine and improve gener-\nated content without relying on external prompting or retraining techniques. In contrast,\ndiffusion-based models offer the advantage of fast parallel generation through iterative\nrefinement, while leveraging bi-directional attention to utilize full context at once. How-\never, diffusion models are unable to match their autoregressive counterparts. This moti-\nvates us to explore the possibility of distilling a pre-trained autoregressive (AR) language\nmodel (teacher) into a non-autoregressive diffusion (non-AR) language model (student),\ncombining the best of both worlds. In this work, we present Target Concrete Score (TCS)\ndistillation, a theoretically grounded framework that bridges autoregressive and diffusion\nparadigms. TCS distillation is broadly applicable to both discrete and continuous diffu-\nsion models, with any pre-trained autoregressive teacher model. We propose techniques\nto make TCS distillation scalable and efficient for transformer-based models, and show\nhow it can both improve pre-trained diffusion language models and also train new mod-\nels from scratch. Through comprehensive experiments on language modeling tasks, we\ndemonstrate the effectiveness of our proposed methods.", "title_embedding_index": 22353, "title_abs_embedding_index": 22378}, {"title": "studentSplat: Your Student Model Learns Single-view 3D Gaussian Splatting", "link_suffix": "/forum?id=fRXAQfHlmr", "link": "https://openreview.net/forum?id=fRXAQfHlmr", "pdf_link": "https://openreview.net/pdf?id=fRXAQfHlmr", "keywords": "3d reconstruction; 3d gaussian splatting; self-supervised learning", "abstract": "Recent advance in feed-forward 3D Gaussian splatting has enable remarkable multi-view 3D scene reconstruction or single-view 3D object reconstruction but single-view 3D scene reconstruction remain under-explored due to inherited ambiguity in single-view. We present studentSplat, the first single-view 3D Gaussian splatting method for scene reconstruction. To overcome the scale ambiguity and extrapolation problems inherent in novel-view supervision from a single input, we introduce two techniques: 1) a teacher-student architecture where a multi-view teacher model provides geometric supervision to the single-view student during training, addressing scale ambiguity and encourage geometric validity; and 2) an extrapolation network that completes missing scene context, enabling high-quality extrapolation. Extensive experiments show studentSplat achieves state-of-the-art single-view novel-view reconstruction quality and comparable performance to multi-view methods at the scene level. Furthermore, studentSplat demonstrates competitive performance as a self-supervised single-view depth estimation method, highlighting its potential for general single-view 3D understanding tasks.", "title_embedding_index": 22354, "title_abs_embedding_index": 22379}, {"title": "Learning Conditionally Independent Marginals Enables Logical Compositions in Conditional Diffusion Models", "link_suffix": "/forum?id=cCRlEvjrx4", "link": "https://openreview.net/forum?id=cCRlEvjrx4", "pdf_link": "https://openreview.net/pdf?id=cCRlEvjrx4", "keywords": "logical compositionality, generative models, diffusion models", "abstract": "How can we learn generative models to sample data with arbitrary logical compositions of statistically independent attributes? The prevailing solution is to sample from distributions expressed as a composition of attributes' conditional marginal distributions under the assumption that they are statistically independent. This paper shows that standard conditional diffusion models violate this assumption, even when all attribute compositions are observed during training. And, this violation is significantly more severe when only a subset of the compositions is observed. We propose CoInD to address this problem. It explicitly enforces statistical independence between the conditional marginal distributions by minimizing Fisher\u2019s divergence between the joint and marginal distributions. The theoretical advantages of CoInD are reflected in both qualitative and quantitative experiments, demonstrating a significantly more faithful and controlled generation of samples for arbitrary logical compositions of attributes. The benefit is more pronounced for scenarios that current solutions relying on the assumption of conditionally independent marginals struggle with, namely, logical compositions involving the NOT operation and when only a subset of compositions are observed during training.", "title_embedding_index": 22355, "title_abs_embedding_index": 22380}, {"title": "SPIN: Self-Supervised Prompt INjection", "link_suffix": "/forum?id=PNHGYziAsL", "link": "https://openreview.net/forum?id=PNHGYziAsL", "pdf_link": "https://openreview.net/pdf?id=PNHGYziAsL", "keywords": "Safety, Alignment, Defense, Adversarial Attack, Inference, LLM, NLP", "abstract": "Large Language Models (LLMs) are increasingly used in a variety of important applications, yet their safety and reliability remain major concerns. Various adversarial and jailbreak attacks have been proposed to bypass the safety alignment and cause the model to produce harmful responses. We introduce Self-supervised Prompt INjection (SPIN) which can detect and reverse these various attacks on LLMs. Just by injecting an adaptive defense prompt at inference-time, our method is simple, effective, and compatible with existing safety-aligned models. Our benchmarks demonstrate that our system can reduce the attack success rate by up to 87.9%, while maintaining the performance on benign user requests. In addition, we discuss the situation of an adaptive attacker and show that our method is still resilient against attackers who are aware of our defense.", "title_embedding_index": 22356, "title_abs_embedding_index": 22381}, {"title": "HAMSTER: Hierarchical Action Models for Open-World Robot Manipulation", "link_suffix": "/forum?id=h7aQxzKbq6", "link": "https://openreview.net/forum?id=h7aQxzKbq6", "pdf_link": "https://openreview.net/pdf?id=h7aQxzKbq6", "keywords": "vision language model; cross-domain generalization; sim-to-real transfer; robot manipulation; vision language action model", "abstract": "Large models have shown strong open-world generalization to complex problems in vision and language, but they have been relatively more difficult to deploy in robotics. This challenge stems from several factors, the foremost of which is the lack of scalable robotic training data since this requires expensive on-robot collection. For scalable training, these models must show considerable transfer across domains, to make use of cheaply available \"off-domain\" data such as videos, hand-drawn sketches, or data from simulation. In this work, we posit that hierarchical vision-language-action models can be more effective at transferring behavior across domains than standard monolithic vision-language-action models. In particular, we study a class of hierarchical vision-language-action models, where high-level vision-language models (VLMs) are trained on relatively cheap data to produce semantically meaningful intermediate predictions such as 2D paths indicating desired behavior. These predicted 2D paths can serve as guidance for low-level control policies that are 3D-aware and capable of precise manipulation. In this work, we show that separating prediction into semantic high-level predictions, and 3D-aware low-level predictions allows such hierarchical VLA policies to transfer across significant domain gaps, for instance from simulation to the real world or across scenes with widely varying visual appearance. Doing so allows for the usage of cheap, abundant data sources beyond teleoperated on-robot data thereby enabling broad semantic and visual generalization. We demonstrate how hierarchical architectures trained on this type of cheap off-domain data can enable robotic manipulation with semantic, visual, and geometric generalization through experiments in simulation and the real world.", "title_embedding_index": 22357, "title_abs_embedding_index": 22382}, {"title": "Understanding Adversarially Robust Generalization via Weight-Curvature Index", "link_suffix": "/forum?id=q9T51gF0fr", "link": "https://openreview.net/forum?id=q9T51gF0fr", "pdf_link": "https://openreview.net/pdf?id=q9T51gF0fr", "keywords": "Adversarially Robust Generalization; Weight-Curvature Index; Robust overfitting; PAC-Bayesian framework", "abstract": "Despite extensive research on adversarial examples, the underlying mechanisms of adversarially robust generalization, a critical yet challenging task for deep learning, remain largely unknown. In this work, we propose a novel perspective to decipher adversarially robust generalization through the lens of the Weight-Curvature Index (WCI). The proposed WCI quantifies the vulnerability of models to adversarial perturbations using the Frobenius norm of weight matrices and the trace of Hessian matrices. We prove generalization bounds based on PAC-Bayesian theory and second-order loss function approximations to elucidate the interplay between robust generalization gap, model parameters, and loss landscape curvature. Our theory and experiments show that WCI effectively captures the robust generalization performance of adversarially trained models. By offering a nuanced understanding of adversarial robustness based on the scale of model parameters and the curvature of the loss landscape, our work provides crucial insights for designing more resilient deep learning models, enhancing their reliability and security.", "title_embedding_index": 22358, "title_abs_embedding_index": 22383}, {"title": "What do we learn from inverting CLIP models?", "link_suffix": "/forum?id=Dyo2tS5A8b", "link": "https://openreview.net/forum?id=Dyo2tS5A8b", "pdf_link": "https://openreview.net/pdf?id=Dyo2tS5A8b", "keywords": "CLIP; NSFW; Interpretability; Gender Bias", "abstract": "We employ an inversion-based approach to examine CLIP models. Our examination reveals that inverting CLIP models results in the generation of images that exhibit semantic alignment with the specified target prompts. We leverage these inverted images to gain insights into various aspects of CLIP models, such as their ability to blend concepts and inclusion of gender biases. We notably observe instances of NSFW (Not Safe For Work) images during model inversion. This phenomenon occurs even for semantically innocuous prompts, like `a beautiful landscape,' as well as for prompts involving the names of celebrities.", "title_embedding_index": 22359, "title_abs_embedding_index": 22384}, {"title": "Talking Vehicles: Cooperative Driving via Natural Language", "link_suffix": "/forum?id=VYlfoA8I6A", "link": "https://openreview.net/forum?id=VYlfoA8I6A", "pdf_link": "https://openreview.net/pdf?id=VYlfoA8I6A", "keywords": "multi-agent learning, communication, llm, autonomous driving", "abstract": "Using natural language as a vehicle-to-vehicle (V2V) communication protocol offers the potential for autonomous vehicles to drive cooperatively not only with each other but also with human drivers. Simple and effective messages for sharing critical observations or negotiating plans to achieve coordination could improve traffic safety and efficiency compared to methods without communication. In this work, we propose a suite of traffic tasks in vehicle-to-vehicle autonomous driving where vehicles in a traffic scenario need to communicate in natural language to facilitate coordination in order to avoid an imminent collision and/or support efficient traffic flow, which we model as a general-sum partially observable stochastic game. To this end, this paper introduces a novel method, LLM+Debrief, to learn a message generation and control policy for autonomous vehicles through multi-agent discussion. To evaluate our method, we developed a gym-like simulation environment that contains a range of accident-prone driving scenarios that could be alleviated by communication. Our experimental results demonstrate that our method is more effective at generating meaningful and human-understandable natural language messages to facilitate cooperation and coordination than untrained LLMs. Our anonymous code is available in supplementary materials.", "title_embedding_index": 22360, "title_abs_embedding_index": 22385}, {"title": "Data-Augmented Phrase-Level Alignment for Mitigating Object Hallucination", "link_suffix": "/forum?id=yG1fW8igzP", "link": "https://openreview.net/forum?id=yG1fW8igzP", "pdf_link": "https://openreview.net/pdf?id=yG1fW8igzP", "keywords": "Multimodal LLMs, Object Hallucination, Vision-language Models", "abstract": "Despite their significant advancements, Multimodal Large Language Models\n(MLLMs) often generate factually inaccurate information, referred to as hallucination.\nIn this work, we address object hallucinations in MLLMs, where information\nis generated about an object not present in the input image. We introduce Dataaugmented\nPhrase-level Alignment (DPA), a novel loss which can be applied to\ninstruction-tuned off-the-shelf MLLMs to mitigate hallucinations, while preserving\ntheir general vision-language capabilities. To fine-tune MLLMs with DPA, we first\ngenerate a set of 'hallucinated' and 'correct' response pairs through generative data\naugmentation by selectively altering the ground-truth information of the correct\nresponses at a phrase level. The DPA loss is then used to train MLLMs to reduce\nthe likelihood of hallucinated phrases compared to the correct ones. Our thorough\nevaluation on various benchmarks confirms the effectiveness of DPA in mitigating\nhallucination while retaining the out-of-the-box performance of the MLLMs on\ngeneral tasks. For instance, MLLMs finetuned with DPA, which we refer to as Hallucination\nAttenuated Language and Vision Assistant (HALVA), improve F1 by up\nto 13.4% on hallucination visual question-answering and reduce the hallucination\nrate by up to 4.2% on image description tasks.", "title_embedding_index": 22361, "title_abs_embedding_index": 22386}, {"title": "DTVLT: A Multi-modal Diverse Text Benchmark for Visual Language Tracking Based on LLM", "link_suffix": "/forum?id=ydH8nU5csJ", "link": "https://openreview.net/forum?id=ydH8nU5csJ", "pdf_link": "https://openreview.net/pdf?id=ydH8nU5csJ", "keywords": "Visual Language Tracking, Video Understanding, Large Language Model", "abstract": "Visual language tracking (VLT) has emerged as a cutting-edge research area, harnessing linguistic data to enhance algorithms with multi-modal inputs and broadening the scope of traditional single object tracking (SOT) to encompass video understanding applications. Despite this, most VLT benchmarks still depend on succinct, human-annotated text descriptions for each video. These descriptions often fall short in capturing the nuances of video content dynamics and lack stylistic variety in language, constrained by their uniform level of detail and a fixed annotation frequency. As a result, algorithms tend to default to a \u201cmemorize the answer\u201d strategy, diverging from the core objective of achieving a deeper understanding of video content. Fortunately, the emergence of large language models (LLMs) has enabled the generation of diverse text. This work utilizes LLMs to generate varied semantic annotations (in terms of text lengths and granularities) for representative SOT benchmarks, thereby establishing a novel multi-modal benchmark. Specifically, we (1) propose a new visual language tracking benchmark with diverse texts, named DTVLT, based on five prominent VLT and SOT benchmarks, including three sub-tasks: short-term tracking, long-term tracking, and global instance tracking. (2) We offer four granularity texts in our benchmark, considering the extent and density of semantic information. This is achieved through DTLLM-VLT, a method for generating high-quality, diverse text by leveraging the extensive knowledge base of LLMs to produce descriptions rich in world knowledge. We expect this multi-granular generation strategy to foster a favorable environment for VLT and video understanding research. (3) We conduct comprehensive experimental analyses on DTVLT, evaluating the impact of diverse text on tracking performance and hope the identified performance bottlenecks of existing algorithms can support further research in VLT and video understanding.", "title_embedding_index": 22362, "title_abs_embedding_index": 22387}, {"title": "ImageNet-RIB Benchmark: Large Pre-Training Datasets Don't Guarantee Robustness after Fine-Tuning", "link_suffix": "/forum?id=qHAblIFenP", "link": "https://openreview.net/forum?id=qHAblIFenP", "pdf_link": "https://openreview.net/pdf?id=qHAblIFenP", "keywords": "robust fine-tuning, robustness, transfer learning, representation learning, continual learning", "abstract": "Highly performant large-scale pre-trained models promise to also provide a valuable foundation for learning specialized tasks, by fine-tuning the model to the desired task. \nBy starting from a good general-purpose model, the goal is to achieve both specialization in the target task and maintain robustness.\nTo assess the robustness of models to out-of-distribution samples after fine-tuning on downstream datasets,\nwe introduce a new robust fine-tuning benchmark, ImageNet-RIB (Robustness Inheritance Benchmark).\nThe benchmark consists of a set of related but distinct specialized (downstream) tasks; pre-trained models are fine-tuned on one task in the set and their robustness is assessed on the rest, iterating across all tasks for fine-tuning and assessment. \nWe find that the continual learning methods, EWC and LwF maintain robustness after fine-tuning though fine-tuning generally does reduce performance on generalization to related downstream tasks across models.\nNot surprisingly, models pre-trained on large and rich datasets exhibit higher initial robustness across datasets and suffer more pronounced degradation during fine-tuning. \nThe distance between the pre-training and downstream datasets, measured by optimal transport, predicts this performance degradation on the pre-training dataset. \nHowever, counterintuitively, model robustness after fine-tuning on related downstream tasks is the worst when the pre-training dataset is the richest and the most diverse. \nThis suggests that starting with the strongest foundation model is not necessarily the best approach for performance on specialist tasks.\nThe benchmark thus offers key insights for developing more resilient fine-tuning strategies and building robust machine learning models.", "title_embedding_index": 22363, "title_abs_embedding_index": 22388}, {"title": "Breaking Neural Network Scaling Laws with Modularity", "link_suffix": "/forum?id=5Qxx5KpFms", "link": "https://openreview.net/forum?id=5Qxx5KpFms", "pdf_link": "https://openreview.net/pdf?id=5Qxx5KpFms", "keywords": "scaling laws, modularity, neural network, generalization, compositionality, combinatorial generalization", "abstract": "Modular neural networks outperform nonmodular neural networks on tasks ranging from visual question answering to robotics. These performance improvements are thought to be due to modular networks' superior ability to model the compositional and combinatorial structure of real-world problems. However, a theoretical explanation of how modularity improves generalizability, and how to leverage task modularity while training networks remains elusive. Using recent theoretical progress in explaining neural network generalization, we investigate how the amount of training data required to generalize on a task varies with the intrinsic dimensionality of a task's input. We show theoretically that when applied to modularly structured tasks, while nonmodular networks require an exponential number of samples with task dimensionality, modular networks' sample complexity is independent of task dimensionality: modular networks can generalize in high dimensions. We then develop a novel learning rule for modular networks to exploit this advantage and empirically show the improved generalization of the rule, both in- and out-of-distribution, on high-dimensional, modular tasks.", "title_embedding_index": 22364, "title_abs_embedding_index": 22389}, {"title": "Unified Neural Network Scaling Laws and Scale-time Equivalence", "link_suffix": "/forum?id=ewZSzO6bts", "link": "https://openreview.net/forum?id=ewZSzO6bts", "pdf_link": "https://openreview.net/pdf?id=ewZSzO6bts", "keywords": "generalization, neural network, scaling law, double descent", "abstract": "As neural networks continue to grow in size but datasets might not, it is vital to understand how much performance improvement can be expected: is it more important to scale network size or data volume? Thus, neural network scaling laws, which characterize how test error varies with network size and data volume, have become increasingly important. However, existing scaling laws are often applicable only in limited regimes and often do not incorporate or predict well-known phenomena such as double descent. Here, we present a novel theoretical characterization of how three factors --- model size, training time, and data volume --- interact to determine the performance of deep neural networks. We first establish a theoretical and empirical equivalence between scaling the size of a neural network and increasing its training time proportionally. Scale-time equivalence challenges the current practice, wherein large models are trained for small durations, and suggests that smaller models trained over extended periods could match their efficacy. It also leads to a novel method for predicting the performance of large-scale networks from small-scale networks trained for extended epochs, and vice versa. We next combine scale-time equivalence with a linear model analysis of double descent to obtain a unified theoretical scaling law, which we confirm with experiments across vision benchmarks and network architectures. These laws explain several previously unexplained phenomena: reduced data requirements for generalization in larger models, heightened sensitivity to label noise in overparameterized models, and instances where increasing model scale does not necessarily enhance performance. Our findings hold significant implications for the practical deployment of neural networks, offering a more accessible and efficient path to training and fine-tuning large models.", "title_embedding_index": 22365, "title_abs_embedding_index": 22390}, {"title": "Automated Design of Agentic Systems", "link_suffix": "/forum?id=t9U3LW7JVX", "link": "https://openreview.net/forum?id=t9U3LW7JVX", "pdf_link": "https://openreview.net/pdf?id=t9U3LW7JVX", "keywords": "LLMs, Language Model Agents, Agents, Agentic Systems, Reasoning, Meta Learning, Open-endedness", "abstract": "Researchers are investing substantial effort in developing powerful general-purpose agents, wherein Foundation Models are used as modules within agentic systems (e.g. Chain-of-Thought, Self-Reflection, Toolformer). However, the history of machine learning teaches us that hand-designed solutions are eventually replaced by learned solutions. We describe a newly forming research area, Automated Design of Agentic Systems (ADAS), which aims to automatically create powerful agentic system designs, including inventing novel building blocks and/or combining them in new ways. We further demonstrate that there is an unexplored yet promising approach within ADAS where agents can be defined in code and new agents can be automatically discovered by a meta agent programming ever better ones in code. Given that programming languages are Turing Complete, this approach theoretically enables the learning of any possible agentic system: including novel prompts, tool use, workflows, and combinations thereof. We present a simple yet effective algorithm named Meta Agent Search to demonstrate this idea, where a meta agent iteratively programs interesting new agents based on an ever-growing archive of previous discoveries. Through extensive experiments across multiple domains including coding, science, and math, we show that our algorithm can progressively invent agents with novel designs that greatly outperform state-of-the-art hand-designed agents. Importantly, we consistently observe the surprising result that agents invented by Meta Agent Search maintain superior performance even when transferred across domains and models, demonstrating their robustness and generality. Provided we develop it safely, our work illustrates the potential of an exciting new research direction toward automatically designing ever-more powerful agentic systems to benefit humanity.", "title_embedding_index": 22366, "title_abs_embedding_index": 22391}, {"title": "Curvature Diversity-Driven Deformation and Domain Alignment for Point Cloud", "link_suffix": "/forum?id=bSzygH9bYg", "link": "https://openreview.net/forum?id=bSzygH9bYg", "pdf_link": "https://openreview.net/pdf?id=bSzygH9bYg", "keywords": "Unsupervised Domain Adaptation, Point Cloud", "abstract": "Unsupervised Domain Adaptation (UDA) is crucial for reducing the need for extensive manual data annotation when training deep networks on point cloud data. A significant challenge of UDA lies in effectively bridging the domain gap. To tackle this challenge, we propose Curvature Diversity-Driven Nuclear-Norm Wasserstein Domain Alignment (CDND). Our approach first introduces a Curvature Diversity-driven Deformation Reconstruction (CurvRec) task, which effectively mitigates the gap between the source and target domains by enabling the model to extract salient features from semantically rich regions of a given point cloud. We then propose Deformation-based Nuclear-norm Wasserstein Discrepancy (D-NWD), which applies the Nuclear-norm Wasserstein Discrepancy to both deformed and original data samples to align the source and target domains. Furthermore, we contribute a theoretical justification for the effectiveness of D-NWD in distribution alignment and demonstrate that it is generic enough to be applied to any deformations. To validate our method, we conduct extensive experiments on two public domain adaptation datasets for point cloud classification and segmentation tasks. Empirical experiment results show that our CDND achieves state-of-the-art performance by a noticeable margin over existing approaches.", "title_embedding_index": 22367, "title_abs_embedding_index": 22392}, {"title": "STORM: Spatio-TempOral Reconstruction Model For Large-Scale Outdoor Scenes", "link_suffix": "/forum?id=M2NFWRPMUd", "link": "https://openreview.net/forum?id=M2NFWRPMUd", "pdf_link": "https://openreview.net/pdf?id=M2NFWRPMUd", "keywords": "autonomous driving; reconstruction model; spatiotemporal", "abstract": "We present STORM, a spatio-temporal reconstruction model designed to reconstruct in-the-wild dynamic outdoor scenes from sparse observations. Existing dynamic reconstruction methods rely heavily on dense observations across space and time and strong motion supervision, therefore suffering from lengthy optimization time, limited generalizability to novel views or scenes, and degenerated quality caused by noisy pseudo-labels. To bridge the gap, STORM introduces a data-driven Transformer architecture that jointly infers 3D scenes and their dynamics in a single forward pass. A key design of our scene representation is to aggregate 3D Gaussians and their motion predicted from all frames, which are later transformed to the target timestep for a more complete (i.e. \u201camodal\u201d) reconstruction at any given time from any viewpoint. As an emergent property, STORM can automatically capture dynamic instances and their high-quality masks using just the reconstruction loss. Extensive experiments show that STORM accurately reconstructs dynamic scenes and outperforms other per-scene optimization (+3.7 PSNR) or feed-forward approaches (+1.5 PSNR), it can reconstruct large-scale outdoor scenes within just 200ms and render in real-time. Beyond reconstruction, we qualitatively demonstrate four additional applications of our model, demonstrating the potential of self-supervised learning for advancing dynamic scene understanding. Our code and model will be released.", "title_embedding_index": 22368, "title_abs_embedding_index": 22393}, {"title": "Training Language Models on Synthetic Edit Sequences Improves Code Synthesis", "link_suffix": "/forum?id=AqfUa08PCH", "link": "https://openreview.net/forum?id=AqfUa08PCH", "pdf_link": "https://openreview.net/pdf?id=AqfUa08PCH", "keywords": "language model, code synthesis, reasoning, synthetic data", "abstract": "Software engineers mainly write code by editing existing programs. In contrast, large language models (LLMs)  autoregressively synthesize programs in a single pass. One explanation for this is the scarcity of open-sourced edit data. While high-quality instruction data for code synthesis is already scarce, high-quality edit data is even scarcer. To fill this gap, we develop a synthetic data generation algorithm called LintSeq. This algorithm refactors existing code into a sequence of code edits by using a linter to procedurally sample across the error-free insertions that can be used to sequentially write programs. It outputs edit sequences as text strings consisting of consecutive program diffs. To test LintSeq, we use it to refactor a dataset of instruction + program pairs into instruction + program-diff-sequence tuples. Then, we instruction finetune a series of smaller LLMs ranging from 2.6B to 14B parameters on both the re-factored and original versions of this dataset, comparing zero-shot performance on code synthesis benchmarks. We show that during repeated sampling, edit sequence finetuned models produce more diverse programs than baselines. This results in better inference-time scaling for benchmark coverage as a function of samples, i.e. the fraction of problems \"pass@k\" solved by any attempt given \"k\" tries. For example, on HumanEval pass@50, small LLMs finetuned on synthetic edit sequences are competitive with GPT-4 and outperform models finetuned on the baseline dataset by +20% (+/-3%) in absolute score. Finally, we also pretrain our own tiny LMs for code understanding. We show that finetuning tiny models on synthetic code edits results in state-of-the-art code synthesis for the on-device model class. Our 150M parameter edit sequence LM matches or outperforms code models with twice as many parameters, both with and without repeated sampling, including Codex and AlphaCode.", "title_embedding_index": 22369, "title_abs_embedding_index": 22394}, {"title": "Trust but Verify: Programmatic VLM Evaluation in the Wild", "link_suffix": "/forum?id=zeBhcfP8tN", "link": "https://openreview.net/forum?id=zeBhcfP8tN", "pdf_link": "https://openreview.net/pdf?id=zeBhcfP8tN", "keywords": "vision-language models, evaluation, hallucinations", "abstract": "Vision-Language Models (VLMs) often generate plausible but incorrect responses to visual queries. However, reliably quantifying the effect of such hallucinations in free-form responses to open-ended queries is challenging as it requires visually verifying each claim within the response. We propose Programmatic VLM Evaluation (PROVE), a new benchmarking paradigm for evaluating VLM responses to open-ended queries. To construct PROVE, we provide a large language model with a high-fidelity scene-graph representation constructed from a hyper-detailed image caption, and prompt it to generate diverse question-answer (QA) pairs, as well as programs that can be executed over the scene graph object toverifyeach QA pair. We thus construct a benchmark of 10.5k challenging but grounded visual QA pairs. Next, to evaluate free-form model responses to queries in PROVE, we propose aprogrammaticevaluation strategy that measures both the helpfulness and truthfulness of a response within a unified scene graph-based framework. We benchmark the helpfulness-truthfulness trade-offs of a range of VLMs on PROVE, finding that very few are in-fact able to achieve a good balance between the two.", "title_embedding_index": 22370, "title_abs_embedding_index": 22395}, {"title": "Provable In-context Learning for Mixture of Linear Regressions using Transformers", "link_suffix": "/forum?id=nxQ0Bjp8zD", "link": "https://openreview.net/forum?id=nxQ0Bjp8zD", "pdf_link": "https://openreview.net/pdf?id=nxQ0Bjp8zD", "keywords": "Transformer, Mixture of Linear Regression, EM algorithm", "abstract": "We theoretically investigate the in-context learning capabilities of transformers in the context of learning mixtures of linear regression models. For the case of two mixtures, we demonstrate the existence of transformers that can achieve an accuracy, relative to the oracle predictor, of order $\\mathcal{\\tilde{O}}((d/n)^{1/4})$ in the low signal-to-noise ratio (SNR) regime and $\\mathcal{\\tilde{O}}(\\sqrt{d/n})$ in the high SNR regime, where $n$ is the length of the prompt, and $d$ is the dimension of the problem. Additionally, we derive in-context excess risk bounds of order $\\mathcal{O}(L/\\sqrt{B})$, where $B$ denotes the number of (training) prompts, and $L$ represents the number of attention layers. The order of $L$ depends on whether the SNR is low or high. In the high SNR regime, we extend the results to $K$-component mixture models for finite $K$. Extensive simulations also highlight the advantages of transformers for this task, outperforming other baselines such as the EM algorithm.", "title_embedding_index": 22371, "title_abs_embedding_index": 22396}, {"title": "Pre-Training Graph Contrastive Masked Autoencoders are Strong Distillers for EEG", "link_suffix": "/forum?id=YKfJFTiRz8", "link": "https://openreview.net/forum?id=YKfJFTiRz8", "pdf_link": "https://openreview.net/pdf?id=YKfJFTiRz8", "keywords": "EEG Signals Recognition, Graph Learning, Self-supervised Graph Pre-training, Graph Knowledge Distillation", "abstract": "Effectively utilizing extensive unlabeled high-density EEG data to improve performance in scenarios with limited labeled low-density EEG data presents a significant challenge. In this paper, we address this by framing it as a graph transfer learning and knowledge distillation problem. We propose a Unified Pre-trained Graph Contrastive Masked Autoencoder Distiller, named EEG-DisGCMAE, to bridge the gap between unlabeled/labeled and high/low-density EEG data. To fully leverage the abundant unlabeled EEG data, we introduce a novel unified graph self-supervised pre-training paradigm, which seamlessly integrates Graph Contrastive Pre-training and Graph Masked Autoencoder Pre-training. This approach synergistically combines contrastive and generative pre-training techniques by reconstructing contrastive samples and contrasting the reconstructions. For knowledge distillation from high-density to low-density EEG data, we propose a Graph Topology Distillation loss function, allowing a lightweight student model trained on low-density data to learn from a teacher model trained on high-density data, effectively handling missing electrodes through contrastive distillation. To integrate transfer learning and distillation, we jointly pre-train the teacher and student models by contrasting their queries and keys during pre-training, enabling robust distillers for downstream tasks. We demonstrate the effectiveness of our method on four classification tasks across two clinical EEG datasets with abundant unlabeled data and limited labeled data. The experimental results show that our approach significantly outperforms contemporary methods in both efficiency and accuracy.", "title_embedding_index": 22372, "title_abs_embedding_index": 22397}, {"title": "Sparse identification of nonlinear dynamics with Shallow Recurrent Decoder Networks", "link_suffix": "/forum?id=iyGkoWP6nA", "link": "https://openreview.net/forum?id=iyGkoWP6nA", "pdf_link": "https://openreview.net/pdf?id=iyGkoWP6nA", "keywords": "Sata-driven modeling, scientific machine learning, sparse identification of nonlinear dynamics, AI in dynamic systems, spatiotemporal modeling, PDEs", "abstract": "Spatio-temporal modeling of real-world data is a challenging problem as a result of inherent high-dimensionality, noisy observations, and expensive data collection procedures. In this paper, we present Sparse Identification of Nonlinear Dynamics with SHallow Recurrent Decoder networks (SINDy-SHRED) to jointly solve the sensing and model identification problems with simple implementation, efficient computation, and robust performance. SINDy-SHRED utilizes Gated Recurrent Units (GRUs) to model the temporal sequence of sensor measurements along with a shallow decoder network to reconstruct the full spatio-temporal field from the latent state space using only a few available sensors. Our proposed algorithm in\u0002troduces a SINDy-based regularization. Beginning with an arbitrary latent state space, the dynamics of the latent space progressively converges to a SINDy-class functional, provided the projection remains within the set. We conduct a system\u0002atic experimental study including synthetic PDE data, real-world sensor measure\u0002ments for sea surface temperature, and direct video data. With no explicit encoder, SINDy-SHRED allows for efficient training with minimal hyperparameter tuning and laptop-level computing. SINDy-SHRED demonstrates robust generalization in a variety of applications with minimal to no hyperparameter adjustments. Additionally, the interpretable SINDy model of latent state dynamics enables accurate long-term video predictions, achieving state-of-the-art performance and outperforming all baseline methods considered, including Convolutional LSTM, PredRNN, ResNet, and SimVP.", "title_embedding_index": 22373, "title_abs_embedding_index": 22398}, {"title": "Empirical Guidelines for Deploying LLMs onto Resource-constrained Edge Devices", "link_suffix": "/forum?id=3xjc9PhEPd", "link": "https://openreview.net/forum?id=3xjc9PhEPd", "pdf_link": "https://openreview.net/pdf?id=3xjc9PhEPd", "keywords": "On-device Learning, Edge Computing, Efficient ML, Large Language Models", "abstract": "The scaling laws have become the de facto guidelines for designing large language models (LLMs), but they were studied under the assumption of unlimited computing resources for both training and inference. As LLMs are increasingly used as personalized intelligent assistants, their customization (i.e., learning through fine-tuning) and deployment onto resource-constrained edge devices will become more and more prevalent. An urgent but open question is how a resource-constrained computing environment would affect the design choices for a personalized LLM. We study this problem empirically in this work. In particular, we consider the tradeoffs among a number of key design factors and their intertwined impacts on learning efficiency and accuracy. The factors include the learning methods for LLM customization, the amount of personalized data used for learning customization, the types and sizes of LLMs, the compression methods of LLMs, the amount of time afforded to learn, and the difficulty levels of the target use cases. Through extensive experimentation and benchmarking, we draw a number of surprisingly insightful guidelines for deploying LLMs onto resource-constrained devices. For example, an optimal choice between parameter learning and RAG may vary depending on the difficulty of the downstream task, the longer fine-tuning time does not necessarily help the model, and a compressed LLM may be a better choice than an uncompressed LLM to learn from limited personalized data.", "title_embedding_index": 22374, "title_abs_embedding_index": 22399}]