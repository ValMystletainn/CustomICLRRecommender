[{"title": "MaxInfoRL: Boosting exploration in reinforcement learning through information gain maximization", "link_suffix": "/forum?id=R4q3cY3kQf", "link": "https://openreview.net/forum?id=R4q3cY3kQf", "pdf_link": "https://openreview.net/pdf?id=R4q3cY3kQf", "keywords": "Reinforcement learning, Exploration in off-policy methods, Continuous control", "abstract": "Reinforcement learning (RL) algorithms aim to balance exploiting the current best strategy with exploring new options that could lead to higher rewards. Most common RL algorithms use undirected exploration, i.e., select random sequences of actions.\nExploration can also be directed using intrinsic rewards, such as curiosity or model epistemic uncertainty. However, effectively balancing task and intrinsic rewards is challenging and often task-dependent. In this work, we introduce a framework, MaxInfoRL, for balancing intrinsic and extrinsic exploration. MaxInfoRL steers exploration towards informative transitions, by maximizing intrinsic rewards such as the information gain about the underlying task. \nWhen combined with Boltzmann exploration, this approach naturally trades off maximization of the value function with that of the entropy over states, rewards, and actions. We show that our approach achieves sublinear regret in the simplified setting of multi-armed bandits. We then apply this general formulation to a variety of off-policy model-free RL methods for continuous state-action spaces, yielding novel algorithms that achieve superior performance across hard exploration problems and complex scenarios such as visual control tasks.", "title_embedding_index": 1550, "title_abs_embedding_index": 1575}, {"title": "Value Explicit Pretraining for Learning Transferable Representations", "link_suffix": "/forum?id=APCjgjFy5M", "link": "https://openreview.net/forum?id=APCjgjFy5M", "pdf_link": "https://openreview.net/pdf?id=APCjgjFy5M", "keywords": "Pretraining for RL, Contrastive Learning, Transfer learning for Robotics", "abstract": "Understanding visual inputs for a given task amidst varied changes is a key challenge posed by visual reinforcement learning agents. We propose \\textit{Value Explicit Pretraining} (VEP), a method that learns generalizable representations for transfer reinforcement learning. VEP enables efficient learning of new tasks that share similar objectives as previously learned tasks, by learning an objective-conditioned encoder that is invariant to changes in environment dynamics and appearance. To pre-train the encoder from a sequence of observations, we use a self-supervised contrastive loss that enables the model to relate states across different tasks based on the Bellman return estimate that is reflective of task progress, resulting in temporally smooth representations that capture the objective of the task. Experiments on a realistic navigation simulator and Atari benchmark show VEP outperforms current SoTA pretraining methods on the ability to generalize to unseen tasks. VEP achieves up to a $2\\times$ improvement in rewards, and up to a $3\\times$ improvement in sample efficiency. For videos of policy performance visit our \\href{https://sites.google.com/view/value-explicit-pretraining/}{website}.", "title_embedding_index": 1551, "title_abs_embedding_index": 1576}, {"title": "LancBiO: Dynamic Lanczos-aided Bilevel Optimization via Krylov Subspace", "link_suffix": "/forum?id=wLmJIs1uqG", "link": "https://openreview.net/forum?id=wLmJIs1uqG", "pdf_link": "https://openreview.net/pdf?id=wLmJIs1uqG", "keywords": "Bilevel Optimization, Lanczos Process, Krylov Subspace", "abstract": "Bilevel optimization, with broad applications in machine learning, has an intricate hierarchical structure. Gradient-based methods have emerged as a common approach to large-scale bilevel problems. However, the computation of the hyper-gradient, which involves a Hessian inverse vector product, confines the efficiency and is regarded as a bottleneck. To circumvent the inverse, we construct a sequence of low-dimensional approximate Krylov subspaces with the aid of the Lanczos process. As a result, the constructed subspace is able to dynamically and incrementally approximate the Hessian inverse vector product with less effort and thus leads to a favorable estimate of the hyper-gradient. Moreover, we propose a provable subspace-based framework for bilevel problems where one central step is to solve a small-size tridiagonal linear system. To the best of our knowledge, this is the first time that subspace techniques are incorporated into bilevel optimization. This successful trial not only enjoys $\\mathcal{O}(\\epsilon^{-1})$ convergence rate but also demonstrates efficiency in a synthetic problem and two deep learning tasks.", "title_embedding_index": 1552, "title_abs_embedding_index": 1577}, {"title": "Toward Understanding In-context vs. In-weight Learning", "link_suffix": "/forum?id=aKJr5NnN8U", "link": "https://openreview.net/forum?id=aKJr5NnN8U", "pdf_link": "https://openreview.net/pdf?id=aKJr5NnN8U", "keywords": "In-context learning, generalization error, transformers", "abstract": "It has recently been demonstrated empirically that in-context learning\nemerges in transformers when certain distributional properties are present in the training data, but this ability can also diminish upon further training.\nWe provide a new theoretical understanding of these phenomena by identifying simplified distributional properties that give rise to the emergence and eventual disappearance of in-context learning.\nWe do so by first analyzing a simplified model that uses a gating mechanism to choose between an in-weight and an in-context predictor.\nThrough a combination of a generalization error and regret analysis we identify conditions where in-context and in-weight learning emerge.\nThese theoretical findings are then corroborated experimentally by comparing the behaviour of a full transformer on the simplified distributions to that of the stylized model, demonstrating aligned results.\nWe then extend the study to a full large language model,\nshowing how fine-tuning on various collections of natural language prompts can elicit similar in-context and in-weight learning behaviour.", "title_embedding_index": 1553, "title_abs_embedding_index": 1578}, {"title": "Can Neural Networks Achieve Optimal Computational-statistical Tradeoff? An Analysis on Single-Index Model", "link_suffix": "/forum?id=is4nCVkSFA", "link": "https://openreview.net/forum?id=is4nCVkSFA", "pdf_link": "https://openreview.net/pdf?id=is4nCVkSFA", "keywords": "single-index model, feature learning, gradient-based method, computational-statistical tradeoff", "abstract": "In this work, we tackle the following question: Can neural networks trained with gradient-based methods achieve the optimal statistical-computational tradeoff in learning Gaussian single-index models? \nPrior research has shown that any polynomial-time algorithm under the statistical query (SQ) framework requires $\\Omega(d^{s^\\star/2}\\lor d)$ samples, where $s^\\star$ is the generative exponent representing the intrinsic difficulty of learning the underlying model.\nHowever, it remains unknown whether neural networks can achieve this sample complexity. \nInspired by prior techniques such as label transformation and landscape smoothing for learning single-index models, we propose a unified gradient-based algorithm for training a two-layer neural network in polynomial time.\nOur method is adaptable to a variety of loss and activation functions, covering a broad class of existing approaches.\nWe show that our algorithm learns a feature representation that strongly aligns with the unknown signal $\\theta^\\star$, with sample complexity $\\tilde O (d^{s^\\star/2} \\lor d)$, matching the SQ lower bound up to a polylogarithmic factor for all generative exponents $s^\\star\\geq 1$.\nFurthermore, we extend our approach to the setting where $\\theta^\\star$ is $k$-sparse for $k = o(\\sqrt{d})$ by introducing a novel weight perturbation technique that leverages the sparsity structure. \nWe derive a corresponding SQ lower bound \nof order $\\tilde\\Omega(k^{s^\\star})$, matched by our method up to a polylogarithmic factor.\nOur framework, especially the weight perturbation technique, is of independent interest, and suggests potential gradient-based solutions to other problems such as sparse tensor PCA.", "title_embedding_index": 1554, "title_abs_embedding_index": 1579}, {"title": "RouteLLM: Learning to Route LLMs from Preference Data", "link_suffix": "/forum?id=8sSqNntaMr", "link": "https://openreview.net/forum?id=8sSqNntaMr", "pdf_link": "https://openreview.net/pdf?id=8sSqNntaMr", "keywords": "Large language models, query routing", "abstract": "Large language models (LLMs) excel at a wide range of tasks, but choosing the right model often involves balancing performance and cost. Powerful models offer better results but are expensive, while smaller models are more cost-effective but less capable. To address this trade-off, we introduce a training framework for learning efficient router models that dynamically select between a stronger and weaker LLM during inference. Our framework leverages human preference data and employs data augmentation techniques to enhance performance. Evaluations on public benchmarks show that our approach can reduce costs by over 2 times without sacrificing response quality. Moreover, our routers exhibit strong generalization capabilities, maintaining performance even when routing between LLMs not included in training. This highlights the potential of our framework to deliver cost-effective, high-performance LLM solutions.", "title_embedding_index": 1555, "title_abs_embedding_index": 1580}, {"title": "Mitigating Unobserved Confounding via Diffusion Probabilistic Models", "link_suffix": "/forum?id=oos6KyAUsW", "link": "https://openreview.net/forum?id=oos6KyAUsW", "pdf_link": "https://openreview.net/pdf?id=oos6KyAUsW", "keywords": "causal inference", "abstract": "Learning Conditional average treatment effect estimation from observational data is a challenging task due to the existence of unobserved confounders. Previous methods mostly focus on assuming the Ignorability assumption ignoring the unobserved confounders or overlooking the impact of an a priori knowledge on the generation process of the latent variable, which can be quite impractical in real-world scenarios. Motivated by the recent advances in the latent variable modeling, we propose to capture the unobserved latent space using diffusion model, and accordingly to estimate the causal effect. More concretely, we build on the reverse diffusion process for the unobserved confounders as a Markov chain conditioned on an apriori knowledge. In order to implement our model in a feasible way, we derive the variational bound in closed form. In the experiments, we compare our model with the state-of-the-art methods based on both synthetic and real-world datasets, demonstrating consistent improvements of our model.", "title_embedding_index": 1556, "title_abs_embedding_index": 1581}, {"title": "PLAY2PROMPT: Zero-shot Tool Instruction Optimization for LLM Agents via Tool Play", "link_suffix": "/forum?id=EBaMTeWi2K", "link": "https://openreview.net/forum?id=EBaMTeWi2K", "pdf_link": "https://openreview.net/pdf?id=EBaMTeWi2K", "keywords": "large language models, zero-shot tool use, prompt optimization", "abstract": "Large language models (LLMs) are increasingly integrated with external tools to complete user requests. Many real-world applications require LLMs to use specialized tools in a zero-shot setting. To achieve this, current methods primarily rely on prompting LLMs with tool-specific information, yet tool documentation is often underspecified or noisy, limiting effectiveness. Manual improvements are inefficient and impractical, as they require domain expertise to rewrite documentation and test on carefully curated held-out datasets to evaluate performance gains. Automatic prompt engineering techniques are not applicable either, because they require labeled examples, which is unavailable in the zero-shot setting. In this work, we introduce PLAY2PROMPT, an automated framework that iteratively refines tool documentation and generates usage examples. PLAY2PROMPT enables LLMs to explore tool input-output behaviors, allowing us to effectively search the space of possible tool descriptions and examples. The generated examples not only guide LLM inference but also serve as validation data to ensure more effective tool use. Extensive experiments on real-world tasks demonstrate significant improvements in zero-shot tool performance across both open- and closed-source models.", "title_embedding_index": 1557, "title_abs_embedding_index": 1582}, {"title": "ShadowPunch: fast actions spotting benchmark", "link_suffix": "/forum?id=Jq8HYNZG9s", "link": "https://openreview.net/forum?id=Jq8HYNZG9s", "pdf_link": "https://openreview.net/pdf?id=Jq8HYNZG9s", "keywords": "Action Spotting, Pose Estimation, Dataset, Boxing, Sports, Video Understanding", "abstract": "We introduce an open dataset for video event spotting focused on fast-paced events in shadowboxing videos captured at high frame rates. The dataset features accurate frame-level annotations for diverse punch types alongside pose keypoint annotations, enabling the development of robust event recognition models. This work presents a novel benchmark exploring two distinct approaches to event spotting: direct prediction from image data and a staged approach involving intermediate pose estimation followed by event detection based on the detected keypoints. We provide baseline neural network solutions incorporating temporal information for both tracks, facilitating comparative analysis of these methodologies. This shadowboxing dataset advances the field of automatic sports analysis and contributes to the broader understanding of video events recognition.", "title_embedding_index": 1558, "title_abs_embedding_index": 1583}, {"title": "Score-based Self-supervised MRI Denoising", "link_suffix": "/forum?id=uNd289HjLi", "link": "https://openreview.net/forum?id=uNd289HjLi", "pdf_link": "https://openreview.net/pdf?id=uNd289HjLi", "keywords": "Self-supervised Learning; Score-based denoising; Medical Image Denoising", "abstract": "Magnetic resonance imaging (MRI) is a powerful noninvasive diagnostic imaging tool that provides unparalleled soft tissue contrast and anatomical detail. Noise contamination, especially in accelerated and/or low-field acquisitions, can significantly degrade image quality and diagnostic accuracy. Supervised learning based denoising approaches have achieved impressive performance but require high signal-to-noise ratio (SNR) labels, which are often unavailable. Self-supervised learning holds promise to address the label scarcity issue, but existing self-supervised denoising methods tend to oversmooth fine spatial features and often yield inferior performance than supervised methods. We introduce Corruption2Self (C2S), a novel score-based self-supervised framework for MRI denoising. At the core of C2S is a generalized ambient denoising score matching (GADSM) loss, which extends denoising score matching to the ambient noise setting by modeling the conditional expectation of higher-SNR images given further corrupted observations. This allows the model to effectively learn denoising across multiple noise levels directly from noisy data. Additionally, we incorporate a reparameterization of noise levels to stabilize training and enhance convergence, and introduce a detail refinement extension to balance noise reduction with the preservation of fine spatial features. Moreover, C2S can be extended to multi-contrast denoising by leveraging complementary information across different MRI contrasts. We demonstrate that our method achieves state-of-the-art performance among self-supervised methods and competitive results compared to supervised counterparts across varying noise conditions and MRI contrasts on the M4Raw and fastMRI dataset.", "title_embedding_index": 1559, "title_abs_embedding_index": 1584}, {"title": "Output Alignment: A Top-down Approach to Length Generalization", "link_suffix": "/forum?id=qxobgbamw9", "link": "https://openreview.net/forum?id=qxobgbamw9", "pdf_link": "https://openreview.net/pdf?id=qxobgbamw9", "keywords": "Length Generalization, Output Alignment", "abstract": "Recently, large language models have exhibited impressive performance and surprising emergent properties. However, their abilities remain constrained by the preset context window of the Transformer architecture, and they continue to struggle with length generalization. In this work, we propose a new perspective on length generalization by focusing on the output distribution rather than the input, as most prior studies have done (e.g., through positional encodings or data structure). First, through case studies on simple synthetic tasks, we highlight the importance ofoutput alignment---the consistency of output distributions across sequences of varying lengths. We then extend this observation to natural language tasks and introduce a metric named Long-Short Misalignment to quantify output alignment, finding a strong correlation between this metric and length generalization performance. Based on these insights, we propose a regularization loss during training that improves output alignment. Extensive experiments confirm the effectiveness of this approach. Overall, our work provides a novel perspective for understanding and enhancing length generalization in large language models.", "title_embedding_index": 1560, "title_abs_embedding_index": 1585}, {"title": "Teaching Transformers Causal Reasoning through Axiomatic Training", "link_suffix": "/forum?id=te30nmLaFf", "link": "https://openreview.net/forum?id=te30nmLaFf", "pdf_link": "https://openreview.net/pdf?id=te30nmLaFf", "keywords": "Causal Axioms, Transformers, Generalization, LLMs", "abstract": "For text-based AI systems to interact in the real world, causal reasoning is an essential skill. Since active interventions are costly to execute, we study to what extent an agent can learn  causal reasoning from symbolic demonstrations of causal axioms. Specifically, we consider an axiomatic training setup where an agent learns from multiple demonstrations of a causal axiom (or rule), rather than incorporating the axiom as an inductive bias or inferring it from data values. A key question is whether the agent would learn to generalize from the axiom demonstrations to new scenarios. For example, if a transformer model is trained on demonstrations of the causal transitivity axiom over small graphs, would it generalize to applying the transitivity axiom over large graphs? \nOur results, based on a novel axiomatic training scheme, indicate that such generalization is possible. For the transitivity axiom, we find that a 67 million parameter transformer model, when trained on linear causal chains (along with some noisy variations) can generalize well to new kinds of graphs, including longer causal chains, causal chains with reversed order, and graphs with branching; even when it is not explicitly trained for such settings. We extend axiomatic training to a harder task of inferring causation from correlation statements and find similar generalization. On both tasks, our model performs at par (or even better) than many larger language models such as GPT-4, Gemini Pro, and Phi-3. Overall, our axiomatic training framework provides a new paradigm of learning causal reasoning in language models that can be extended to arbitrary axioms, as long as sufficient demonstrations can be generated.", "title_embedding_index": 1561, "title_abs_embedding_index": 1586}, {"title": "PokeChamp: an Expert-level Minimax Language Agent for Competitive Pokemon", "link_suffix": "/forum?id=zi8YBcmXqA", "link": "https://openreview.net/forum?id=zi8YBcmXqA", "pdf_link": "https://openreview.net/pdf?id=zi8YBcmXqA", "keywords": "multiagent, LLM agents, competitive games, game theory, reinforcement learning", "abstract": "We introduce \\texttt{Pok'eChamp}, a Large Language Model (LLM) powered game-theoretic aware agent for two-player competitive Pok'emon battles, that uses an LLM prior and collected high-Elo human data to model minimax search without any additional training. \\texttt{Pok'eChamp} uses a depth-limited minimax search online where the LLM replaces three key components: 1) action sampling from the LLM guided by prompts (including from a damage calculation tool), 2) opponent-modeling via the historical likelihood of actions from our dataset to model the effect of LLM-predicted opponent actions, and 3) state value calculation for the LLM to reflect on each intrinsic state. \\texttt{Pok'eChamp} outperforms all existing AIs (76%) and heuristic bots (84%) by an enormous margin, including winning consistently (>50%) against prior human-parity work run with a frontier model, GPT 4-o, while using an open-source 8 billion parameter Llama 3.1 model. \\texttt{Pok'eChamp} achieves expert performance in the top 10% of players on the online ladder against competitive human players at an Elo of 1500. Finally, we collect the largest Pok'emon battling dataset, including 1 million+ games with 150k+ high Elo games, prepare a series of battling benchmarks based on real player data and puzzles to analyze specific battling abilities, and provide crucial updates to the local game engine. Our code is available \\href{https://sites.google.com/view/pokechamp-llm}{online}.", "title_embedding_index": 1562, "title_abs_embedding_index": 1587}, {"title": "Effectively Steer LLM To Follow Preference via Building Confident Directions", "link_suffix": "/forum?id=ZPkNrs6aNO", "link": "https://openreview.net/forum?id=ZPkNrs6aNO", "pdf_link": "https://openreview.net/pdf?id=ZPkNrs6aNO", "keywords": "Language model, Model steer, Explainable LLM", "abstract": "Having an LLM that aligns with human preference is essential for accommodating individual needs, such as maintaining writing style or generating specific topics of interest.The majority of current alignment methods rely on fine-tuning or prompting, which can be either costly or difficult to control. Model steering algorithms, which construct certain steering directions used to modify the model output}, are typically easy to implement and optimization-free. {However, their capabilities are typically limited to steering the model into one of the two directions (i.e., bidreictional steering), and that there has been no theoretical understanding to guarantee their performance. In this work, we propose a theoretical framework to understand and quantify the model steering methods. Inspired by the framework, we propose a confident direction steering method (CONFST) that steers LLMs via modifying their activations in inference time. More specifically, CONFST builds a {\\it confident direction} that is closely aligned with users' preferences, and then this direction is added to the activations of the LLMs to effectively steer the model output. Our approach offers three key advantages over popular bidirectional model steering methods: 1) {It is more powerful, since multiple (i.e. more than two) users' preferences can be aligned simultaneously; 2) It is very simple to implement, since there is no need to determine which layer the steering vector should be added to; 3) No explicit user instruction is required. We validate our method on GPT-2 XL (1.5B), Mistral (7B) and Gemma-it (9B) models for tasks that require shifting the output of LLMs across a number of different topics and styles.", "title_embedding_index": 1563, "title_abs_embedding_index": 1588}, {"title": "Unlocking Video-LLM via Agent-of-Thoughts Distillation", "link_suffix": "/forum?id=mMfDfJ8JFJ", "link": "https://openreview.net/forum?id=mMfDfJ8JFJ", "pdf_link": "https://openreview.net/pdf?id=mMfDfJ8JFJ", "keywords": "Video QA, Visual Understanding, MLLM, Agent-based Video Analysis", "abstract": "This paper tackles the problem of video question answering (VideoQA), \na task that often requires multi-step reasoning and a profound understanding of spatial-temporal dynamics. While large generative video-language models perform well on benchmarks, they often lack explainability and spatial-temporal grounding. \nIn this paper, we proposeAgent-of-ThoughtsDistillation (AoTD), a method that enhances generative models by incorporating automatically generated Chain-of-Thoughts (CoTs) into the instruction-tuning process. Specifically, we leverage an agent-based system to decompose complex questions into sub-tasks, and address them with specialized vision models, the intermediate results are then treated as reasoning chains. \nWe also introduce a verification mechanism using a large language model (LLM) to ensure the reliability of generated CoTs. Extensive experiments demonstrate that AoTD improves the performance on multiple-choice and open-ended benchmarks.", "title_embedding_index": 1564, "title_abs_embedding_index": 1589}, {"title": "Molecular Active Learning: How can LLMs Help?", "link_suffix": "/forum?id=kYg04pmX7i", "link": "https://openreview.net/forum?id=kYg04pmX7i", "pdf_link": "https://openreview.net/pdf?id=kYg04pmX7i", "keywords": "Active learning, Bayesian optimization, large language model", "abstract": "Drug discovery, and molecular discovery more broadly, can be framed as a sequential active learning problem ---facing a candidate pool, strategies are designed to sequentially acquire molecules to assay, aiming to find the best molecule within the fewest rounds of trial and error.\nTo automate this process, Bayesian optimization (BO) methods can mimic the approach of human medicinal chemists by constructing \\textit{representations} from existing knowledge, quantifying \\textit{uncertainty} for the predictions, and designing \\textit{acquisition} experiments that balance exploitation and exploration.\nTraditionally, these three stages are implemented using building blocks such as graph neural networks (GNN) as representations, variational inference (VI) or Gaussian process (GP) for uncertainty quantification, and analytical expressions as acquisition functions.\nTo facilitate the integration of both domain-specific and general knowledge into various stages of this process, in this paper, we investigate which parts of this workflow can be augmented or replaced by large language models (LLM).\nTo this end, we present \\textbf{COLT}, a software library for \\textbf{C}hemical \\textbf{O}ptimization with \\textbf{L}anguage- and \\textbf{T}opology-based modules, and thoroughly benchmark the combination thereof.\nWe found that \\textit{none} of the LLMs, no matter incorporated at what stage, can outperform the simple and fast Bayesian baseline with GNN and GP.\nAs a remedy, we offer a new tuning recipe with direct preference optimization (DPO), where the optimization of synthetic properties can be used to increase the efficiency of the acquisition in real-world tasks.", "title_embedding_index": 1565, "title_abs_embedding_index": 1590}, {"title": "Learning to Optimize for Mixed-Integer Nonlinear Programming", "link_suffix": "/forum?id=1oIXRWK2WO", "link": "https://openreview.net/forum?id=1oIXRWK2WO", "pdf_link": "https://openreview.net/pdf?id=1oIXRWK2WO", "keywords": "Mixed-Integer Nonlinear Programming, Learning to Optimize, Differentiable Optimization, Constrained Neural Networks, Deep Learning, Operations Research", "abstract": "Mixed-integer nonlinear programs (MINLPs) arise in various domains, such as energy systems and transportation, but are notoriously difficult to solve. Recent advances in machine learning have achieved remarkable success in optimization tasks, an area known as learning to optimize. This approach includes using predictive models to generate solutions for optimization problems with continuous decision variables, thereby avoiding the need for computationally expensive optimization algorithms. However, applying learning to MINLPs remains challenging primarily due to integer decision variables, which complicate gradient-based learning. To address this limitation, we propose two differentiable correction layers that generate integer outputs while preserving gradient information. The experiments demonstrate that the proposed learning-based approach consistently produces high-quality solutions for parametric MINLPs extremely quickly. As problem size increases, traditional exact solvers and heuristic methods struggle to find feasible solutions, whereas our approach continues to deliver reliable results. Our work extends the scope of learning-to-optimize to MINLP, paving the way for integrating integer constraints into deep learning models.", "title_embedding_index": 1566, "title_abs_embedding_index": 1591}, {"title": "Variable Forward Regularization to Replace Ridge in Online Linear Regression", "link_suffix": "/forum?id=lFzUHGebeb", "link": "https://openreview.net/forum?id=lFzUHGebeb", "pdf_link": "https://openreview.net/pdf?id=lFzUHGebeb", "keywords": "Forward regularization, linear regression, regret bound, continual learning", "abstract": "Forward regularization (-F) with unsupervised knowledge was proposed to replace canonical Ridge regularization (-R) in online linear learners, which achieves lower relative regret bounds. However, we observe that -F cannot perform as expected in practice, even possibly losing to -R for online learning tasks. We identify two main causes for this: (1) inappropriate intervention penalty; (2) potential non-i.i.d nature in online learning, both of which result in unstable posterior distribution and optima offset of the learner. To improve these, we propose Variable Forward regularization (-$k$F), a more general style with -F intensity modulated by a variable $k$. We further derive -$k$F algorithm to online learning tasks, which shows holistic recursive closed-form updates and superior performance compared to both -R and -F. Moreover, we theoretically establish the relative regrets of -$k$F in online learning, showing that it has a tighter upper bound than -F in adversarial settings. We also introduce an adaptive -$k$F, termed -$k$F-Bayes, to curb unstable penalties caused by non-i.i.d and mitigate intractable tuning of hard $k$ based on Bayesian learning for online learning. In experiments, we adapted -$k$F and -$k$F-Bayes into class incremental scenario, where it realized less forgetting and non-replay. Results distinctly demonstrate the efficacy of using -$k$F and -$k$F-Bayes.", "title_embedding_index": 1567, "title_abs_embedding_index": 1592}, {"title": "Consistency Checks for Language Model Forecasters", "link_suffix": "/forum?id=r5IXBlTCGc", "link": "https://openreview.net/forum?id=r5IXBlTCGc", "pdf_link": "https://openreview.net/pdf?id=r5IXBlTCGc", "keywords": "forecasting, markets, trading, LLM, evaluation, eval, consistency, robustness", "abstract": "Forecasting is a task that is difficult to evaluate: the ground truth can only be known in the future. Recent work showing LLM forecasters rapidly approaching human-level performance begs the question: how can we benchmark and evaluate these forecastersinstantaneously? Following the consistency check framework, we measure the performance of forecasters in terms of the consistency of their predictions on different logically-related questions. We propose a new, general consistency metric based onarbitrage: for example, if a forecasting AI illogically predicts that both the Democratic and Republican parties have 60% probability of winning the 2024 US presidential election, an arbitrageur could trade against the forecaster's predictions and make a profit. We build an automated evaluation system that generates a set of base questions, instantiates consistency checks from these questions, elicits the predictions of the forecaster, and measures the consistency of the predictions. We then build a standard, proper-scoring-rule forecasting benchmark, and show that our (instantaneous) consistency metrics correlate strongly with LLM forecasters' ground truth Brier scores (which are only known in the future). We also release a consistency benchmark that resolves in 2028, providing a long-term evaluation tool for forecasting.", "title_embedding_index": 1568, "title_abs_embedding_index": 1593}, {"title": "RetFormer: Enhancing Multimodal Retrieval for Image Recognition", "link_suffix": "/forum?id=rwdeKOdAwY", "link": "https://openreview.net/forum?id=rwdeKOdAwY", "pdf_link": "https://openreview.net/pdf?id=rwdeKOdAwY", "keywords": "retrieval-augmented, long-tailed learning", "abstract": "The expansion of Transformers and the collection of high-quality multimodal datasets have propelled deep neural networks to achieve unprecedented performance in vision and language tasks. However, applying these advances is non-trivial in real-world applications. The extensive number of parameters complicates model updates, and real-world data often features a long-tailed distribution along with noisy labels. To address the above issues, we propose to explore the internal structure of the neural network for learning with sample relationships, rather than just increasing the number of model parameters. Specifically, we introduce RetFormer, a model enhanced with a multimodal knowledge base for storing world knowledge, and a retrieval cross-fusion module designed to establish robust multimodal sample relationships by leveraging content from the knowledge base. RetFormer establishes a robust relationship between image and text modalities by integrating information from external knowledge bases into the model's decision-making process, thus overcoming the limitations of traditional approaches on model size and datasets. Our experiments demonstrate the benefits of integrating large-scale image-text datasets into vision tasks and exemplify the importance of modeling the relationship between image and text modalities.  We have evaluated our approach on the task of long-tailed recognition and learning with noisy labels and have shown that it achieves state-of-the-art accuracies.", "title_embedding_index": 1569, "title_abs_embedding_index": 1594}, {"title": "Diverse Graph-based Nearest Neighbor Search", "link_suffix": "/forum?id=oRNus243R6", "link": "https://openreview.net/forum?id=oRNus243R6", "pdf_link": "https://openreview.net/pdf?id=oRNus243R6", "keywords": "nearest neighbor search; diversity; algorithms", "abstract": "Nearest neighbor search is a fundamental data structure problem with many applications in machine learning, computer vision, recommendation systems and other fields. Although the main objective of the data structure is to quickly report data points that are closest to a given query, it has long been noted (Carbonell et al, 1998) that without additional constraints the reported answers can be redundant and/or duplicative. This issue is typically addressed in two stages: in the first stage, the algorithm retrieves a (large) number of points closest to the query, while in the second stage, the points are post-processed and a small subset is selected to maximize the desired diversity objective. Although popular, this method suffers from a fundamental efficiency bottleneck, as the set of points retrieved in the first stage often needs to be much larger than the final output.In this paper we present provably efficient algorithms for approximate nearest neighbor search with diversity constraints that bypass this two stage process. Our algorithms are based on popular graph-based methods, which allows us to ``piggy-back'' on the existing efficient implementations.  These are the first graph-based algorithms for nearest neighbor search with diversity constraints.   For data sets with low intrinsic dimension, our data structures report a diverse set of $k$ points approximately closest to the query, in time that only depends on $k$ and $\\log \\Delta$, where $\\Delta$ is the ratio of the diameter to the closest pair distance in the data set. This bound is qualitatively similar to the best known bounds for standard (non-diverse) graph-based algorithms. Our experiments show that the search time of our algorithms is substantially lower than that using the standard two-stage approach.", "title_embedding_index": 1570, "title_abs_embedding_index": 1595}, {"title": "Approximating Multiple Robust Optimization Solutions in One Pass via Proximal Point Methods", "link_suffix": "/forum?id=A7LTIuhH4k", "link": "https://openreview.net/forum?id=A7LTIuhH4k", "pdf_link": "https://openreview.net/pdf?id=A7LTIuhH4k", "keywords": "Robust optimization, Robustness-accuracy tradeoff", "abstract": "Robust optimization provides a principled and unified framework to model many problems in modern operations research and computer science applications, such as risk measures minimization and adversarially robust machine learning. To use a robust solution (e.g., to implement an investment portfolio or perform robust machine learning inference), the user has to a priori decide the trade-off between efficiency (nominal performance) and robustness (worst-case performance) of the solution by choosing the uncertainty level hyperparameters.  In many applications, this amounts to solving the problem many times and comparing them, each from a different hyperparameter setting.  This makes robust optimization practically cumbersome or even intractable. We present a novel procedure based on the proximal point method (PPM) to efficiently approximate many Pareto efficient robust solutions at once. This effectively reduces the total compute requirement from $N \\times T$ to $2 \\times T$, where $N$ is the number of robust solutions to be generated, and $T$ is the time to obtain one robust solution. We prove this procedure can produce exact Pareto efficient robust solutions for a class of robust linear optimization problems. For more general problems, we prove that with high probability, our procedure gives a good approximation of the efficiency-robustness trade-off in random robust linear optimization instances. We conduct numerical experiments to demonstrate.", "title_embedding_index": 1571, "title_abs_embedding_index": 1596}, {"title": "ReMoE: Fully Differentiable Mixture-of-Experts with ReLU Routing", "link_suffix": "/forum?id=4D0f16Vwc3", "link": "https://openreview.net/forum?id=4D0f16Vwc3", "pdf_link": "https://openreview.net/pdf?id=4D0f16Vwc3", "keywords": "Mixture-of-Experts, Differentiable Routing, Sparsity", "abstract": "Sparsely activated Mixture-of-Experts (MoE) models are widely adopted to scale up model capacity without increasing the computation budget. However, vanilla TopK routers are trained in a discontinuous, non-differentiable way, limiting their performance and scalability. \nTo address this issue, we propose ReMoE, a fully differentiable MoE architecture that offers a simple yet effective drop-in replacement for the conventional TopK+Softmax routing, utilizing ReLU as the router instead.  We further propose methods to regulate the router's sparsity while balancing the load among experts. ReMoE\u2019s continuous nature enables efficient dynamic allocation of computation across tokens and layers, while also exhibiting domain specialization. Our experiments demonstrate that ReMoE consistently outperforms vanilla TopK-routed MoE across various model sizes, expert counts, and levels of granularity. Furthermore, ReMoE exhibits superior scalability with respect to the number of experts, surpassing traditional MoE architectures.", "title_embedding_index": 1572, "title_abs_embedding_index": 1597}, {"title": "Human-like Communication Strategies for Improved Multi-Agent Reinforcement Learning", "link_suffix": "/forum?id=UsMTuRraOR", "link": "https://openreview.net/forum?id=UsMTuRraOR", "pdf_link": "https://openreview.net/pdf?id=UsMTuRraOR", "keywords": "RL, MARL, communication, RLHF", "abstract": "Multi-Agent Reinforcement Learning (MARL) has seen significant progress in recent years, enabling multiple agents to coordinate and optimize their actions in complex environments. However, integrating effective communication protocols into MARL frameworks remains a challenge, as it introduces issues such as increased state space dimensionality, lack of stationarity, and the need for interpretability. Inspired by human communication, which relies on prior knowledge, contextual awareness, and efficient information exchange, we propose a novel framework for incorporating human-like communication strategies to enhance the learning process. Motivated by recent advancements in natural language processing (NLP), multi-modal AI and object detection, we use text-to-mask models and human feedback to learn compact and informative communication strategies that facilitate coordination among agents to improve the overall performance. We demonstrate the efficiency of our approach on various multi-agent tasks and provide insights into emergent communication behaviors observed during training.", "title_embedding_index": 1573, "title_abs_embedding_index": 1598}, {"title": "LogicVista: Multimodal LLM Logical Reasoning Benchmark in Visual Contexts", "link_suffix": "/forum?id=6ozaf7VRIP", "link": "https://openreview.net/forum?id=6ozaf7VRIP", "pdf_link": "https://openreview.net/pdf?id=6ozaf7VRIP", "keywords": "Multimodal LLM, Reasoning, Visual Context, Benchmark", "abstract": "We propose LogicVista , an evaluation benchmark that examines multimodal large language models\u2019 (MLLMs) integrated Logical reasoning capacities in Visual contexts. Recent advancements in MLLMs have demonstrated various fascinating abilities such as crafting poetry based on an image to engaging in mathematical reasoning. Despite these feats, there remains a gap in the systematic examination of MLLMs\u2019 proficiency in logical reasoning tasks. These skills are routinely invoked in navigation, puzzle-solving, etc. Thus we present LogicVista, which evaluates general logical cognition abilities across a spectrum of 5 logical reasoning tasks with 3 broad capabilities and 11 specific capabilities through a sample of 448 multiple-choice questions. Each is annotated with not only the correct answer but also the human written reasoning behind the selection, allowing for rich open- ended evaluation as well as MCQ evaluation. A total of 11 MLLMs undergo comprehensive evaluation using LogicVista. We are also introducing a crowdsourced annotation tool to further scale LogicVista with support from the community. Code and Data Available athttps://anonymous.4open.science/r/LogicVista.", "title_embedding_index": 1574, "title_abs_embedding_index": 1599}]