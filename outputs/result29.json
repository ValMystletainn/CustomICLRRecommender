[{"title": "Reasoning of Large Language Models over Knowledge Graphs with Super-Relations", "link_suffix": "/forum?id=rTCJ29pkuA", "link": "https://openreview.net/forum?id=rTCJ29pkuA", "pdf_link": "https://openreview.net/pdf?id=rTCJ29pkuA", "keywords": "Knowledge Graphs, Large Language Models, Question Answering", "abstract": "While large language models (LLMs) have made significant progress in processing and reasoning over knowledge graphs, current methods suffer from a high non-retrieval rate. This limitation reduces the accuracy of answering questions based on these graphs. Our analysis reveals that the combination of greedy search and forward reasoning is a major contributor to this issue. To overcome these challenges, we introduce the concept of super-relations, which enables both forward and backward reasoning by summarizing and connecting various relational paths within the graph. This holistic approach not only expands the search space, but also significantly improves retrieval efficiency. In this paper, we propose the ReKnoS framework, which aims to Reason over Knowledge Graphs with Super-Relations. Our framework\u2019s key advantages include the inclusion of multiple relation paths through super-relations, enhanced forward and backward reasoning capabilities, and increased efficiency in querying LLMs. These enhancements collectively lead to a substantial improvement in the successful retrieval rate and overall reasoning performance. We conduct extensive experiments on a variety of datasets to evaluate ReKnoS, and the results demonstrate the superior performance of ReKnoS over existing state-of-the-art baselines, with an average accuracy gain of 2.92% across nine real-world datasets.", "title_embedding_index": 1400, "title_abs_embedding_index": 1425}, {"title": "A Guide to Misinformation Detection Datasets", "link_suffix": "/forum?id=Jztt1nrjAM", "link": "https://openreview.net/forum?id=Jztt1nrjAM", "pdf_link": "https://openreview.net/pdf?id=Jztt1nrjAM", "keywords": "Misinformation, Fact-Checking, Survey, Datasets, LLM Judge, Evaluation", "abstract": "Misinformation is a complex societal issue, and mitigating solutions are difficult to create due to data deficiencies. To address this problem, we have curated the largest collection of (mis)information datasets in the literature, totaling 75. From these, we evaluated the quality of all of the 35 datasets that consist of statements or claims. We assess these datasets to identify those with solid foundations for empirical work and those with flaws that could result in misleading and non-generalizable results, such as insufficient label quality, spurious correlations, or political bias. We further provide state-of-the-art baselines on all these datasets, but show that regardless of label quality, categorical labels may no longer give an accurate evaluation of detection model performance. We discuss alternatives to mitigate this problem. Overall, this guide aims to provide a roadmap for obtaining higher quality data and conducting more effective evaluations, ultimately improving research in misinformation detection.", "title_embedding_index": 1401, "title_abs_embedding_index": 1426}, {"title": "Adversarial Training of Two-Layer Polynomial and ReLU Activation Networks via Convex Optimization", "link_suffix": "/forum?id=hrLKzCETcf", "link": "https://openreview.net/forum?id=hrLKzCETcf", "pdf_link": "https://openreview.net/pdf?id=hrLKzCETcf", "keywords": "convex optimization, adversarial training", "abstract": "Training neural networks which are robust to adversarial attacks remains an important problem in deep learning, especially as heavily overparameterized models are adopted in safety-critical settings. Drawing from recent work which reformulates the training problems for two-layer ReLU and polynomial activation networks as convex programs, we devise a convex semidefinite program (SDP) for adversarial training of two-layer polynomial activation networks and prove that the convex SDP achieves the same globally optimal solution as its nonconvex counterpart. The convex adversarial SDP is observed to improve robust test accuracy against $\\ell_\\infty$\n attacks relative to the original convex training formulation on multiple datasets. Additionally, we present scalable implementations of adversarial training for two-layer polynomial and ReLU networks which are compatible with standard machine learning libraries and GPU acceleration. Leveraging these implementations, we retrain the final two fully connected layers of a Pre-Activation ResNet-18 model on the CIFAR-10 dataset with both polynomial and ReLU activations. The two `robustified' models achieve significantly higher robust test accuracies against $\\ell_\\infty$ attacks than a Pre-Activation ResNet-18 model trained with sharpness-aware minimization, demonstrating the practical utility of convex adversarial training on large-scale problems.", "title_embedding_index": 1402, "title_abs_embedding_index": 1427}, {"title": "Language Models, Grade-School Math, and the Hidden Reasoning Process", "link_suffix": "/forum?id=Tn5B6Udq3E", "link": "https://openreview.net/forum?id=Tn5B6Udq3E", "pdf_link": "https://openreview.net/pdf?id=Tn5B6Udq3E", "keywords": "linear probing, language model, grade math problems, logic following, reasoning", "abstract": "Recent advances in language models have demonstrated their capability to solve mathematical reasoning problems, achieving near-perfect accuracy on grade-school level math benchmarks like GSM8K. In this paper, we formally study how language models solve these problems. We design a series of controlled experiments to address several fundamental questions: (1) Can language models truly develop reasoning skills, or do they simply memorize templates? (2) What is the model's hidden (mental) reasoning process? (3) Do models solve math questions using skills similar to or different from humans? (4) Do models trained on GSM8K-like datasets develop reasoning skills beyond those necessary for solving GSM8K problems? (5) What mental process causes models to make reasoning mistakes? (6) How large or deep must a model be to effectively solve GSM8K-level math questions?Our study uncovers many hidden mechanisms by which language models solve mathematical questions, providing insights that extend beyond current understandings of LLMs.", "title_embedding_index": 1403, "title_abs_embedding_index": 1428}, {"title": "Agent-Oriented Planning in Multi-Agent Systems", "link_suffix": "/forum?id=EqcLAU6gyU", "link": "https://openreview.net/forum?id=EqcLAU6gyU", "pdf_link": "https://openreview.net/pdf?id=EqcLAU6gyU", "keywords": "Multi-Agent System; Planning", "abstract": "Through the collaboration of multiple agents possessing diverse expertise and tools, multi-agent systems achieve impressive progress in solving real-world problems. Given the user queries, the meta-agents, serving as the brain within these systems, are required to decompose the queries into multiple sub-tasks that can be allocated to suitable agents capable of solving them, so-called agent-oriented planning. In this study, we identify three critical design principles of agent-oriented planning, including solvability, completeness, and non-redundancy, to ensure that each sub-task is effectively resolved, leading to satisfactory responses to the original queries. These principles further inspire us to propose a novel framework for agent-oriented planning in multi-agent systems, leveraging a fast task decomposition and allocation process followed by an effective and efficient evaluation via a reward model. During the planning process, the meta-agent is also responsible for evaluating the performance of the expert agents, making timely adjustments to the sub-tasks and scheduling as necessary. Besides, we integrate a feedback loop into the proposed framework to further enhance the effectiveness and robustness of such a problem-solving process. Extensive experiments demonstrate the advancement of the proposed framework in solving real-world problems compared to both single-agent systems and existing planning strategies for multi-agent systems.", "title_embedding_index": 1404, "title_abs_embedding_index": 1429}, {"title": "Budget-constrained Active Learning to De-censor Survival Data", "link_suffix": "/forum?id=y2ch7iQSJu", "link": "https://openreview.net/forum?id=y2ch7iQSJu", "pdf_link": "https://openreview.net/pdf?id=y2ch7iQSJu", "keywords": "Active Learning, Survival Analysis, Budgeted Constraints, Bayesian Model, Mutual Information, De-censoring Data", "abstract": "Standard supervised learners attempt to learn a model from a labeled dataset.  Given a small set of labeled instances, and a pool of unlabeled instances, a budgeted learner can use its given budget to pay to acquire the labels of some unlabeled instances, which it can then use to produce a model. Here, we explore budgeted learning in the context of  survival datasets, which include (right) censored instances, where we know only a lower bound c_i on that instance\u2019s time-to-event t_i.  Here, that learner can pay to (partially) label a censored instance \u2013 eg, to acquire the actual time t_i for an instance [eg, go from (3yr, censor) to (7.2yr, uncensored)], or other variants [eg, learn about 1 more year, so go from (3yr, censor) to either (3.2yr, uncensored) or (4yr, censor)].  This serves as a model of real world data collection, where followup with censored patients does not always lead to complete uncensoring, and how much information is given to the learner model during data collection is a function of the budget and the nature of the data itself. Many fields, such as medicine, finance, and engineering contain survival datasets with a large number of censored instance, and also operate under budget constraints with respect to the learning process, thus making it important to be able to apply this budgeted learning approach. Despite this importance; to our knowledge no other work has looked into doing this. We provide both experimental and theoretical results for how to apply state-of-the-art budgeted learning algorithms to survival data and the respective limitations that exist in doing so. Our approach provides bounds and time complexity  theoretically equivalent to standard active learning methods. Moreover, empirical analysis on several survival tasks show that our model performs better than other potential approaches that might be considered on several benchmarks.", "title_embedding_index": 1405, "title_abs_embedding_index": 1430}, {"title": "Retrieval or Global Context Understanding? On Many-Shot In-Context Learning for Long-Context Evaluation", "link_suffix": "/forum?id=j8HU5aNpd3", "link": "https://openreview.net/forum?id=j8HU5aNpd3", "pdf_link": "https://openreview.net/pdf?id=j8HU5aNpd3", "keywords": "long context evaluation, many-shot ICL", "abstract": "Language models (LMs) have demonstrated an improved capacity to handle long-context information, yet existing long-context benchmarks primarily measure LMs' retrieval abilities with extended inputs, e.g., pinpointing a short phrase from long-form text. \nTherefore, they may fall short when evaluating models' global context understanding capacity, such as synthesizing and reasoning over content across input to generate the response. \nIn this paper, we study long-context language model (LCLM) evaluation through many-shot in-context learning (ICL). Concretely, we identify the skills each ICL task requires, and examine models' long-context capabilities on them. We ask the first question: What types of ICL tasks benefit from additional demonstrations, and are these tasks effective at evaluating LCLMs? We find that classification and summarization tasks show notable performance improvements with additional demonstrations, while translation and reasoning tasks do not exhibit clear trends. This suggests the classification tasks predominantly test models' retrieval skills. Next, we ask: To what extent does each task require retrieval skills versus global context understanding from LCLMs? We develop metrics to categorize ICL tasks into two groups: (i) retrieval tasks that require strong retrieval ability to pinpoint relevant examples, and (ii) global context understanding tasks that necessitate a deeper comprehension of the full input. We find that not all datasets can effectively evaluate these long-context capabilities. \nTo address this gap, we introduce a new many-shot ICL benchmark, MANYICLBENCH, designed to characterize LCLMs' retrieval and global context understanding capabilities separately. We benchmark 11 open-weight LCLMs using MANYICLBENCH. We find that while state-of-the-art models demonstrate satisfactory performance up to 64k tokens in retrieval tasks, many models experience significant performance drops at only 16k tokens in global context understanding tasks.", "title_embedding_index": 1406, "title_abs_embedding_index": 1431}, {"title": "STEER-ME: Assessing the Microeconomic Reasoning of Large Language Models", "link_suffix": "/forum?id=g3nxy8N3bQ", "link": "https://openreview.net/forum?id=g3nxy8N3bQ", "pdf_link": "https://openreview.net/pdf?id=g3nxy8N3bQ", "keywords": "large language models, microeconomics, benchmarking, decision-making, economic agents, llm agents", "abstract": "Large language models (LLMs) are increasingly being applied to economic tasks like stock picking and financial analysis. Existing LLM benchmarks tend to focus on specific applications and often fail to describe a rich variety of economic tasks. Raman et al. (2024) offer a blueprint for comprehensively benchmarking strategic decision-making. However, their work failed to address the non-strategic settings prevalent in micro-economics. We address this gap by taxonomizing micro-economic reasoning into $57$ distinct elements, each grounded in up to $10$ distinct domains, $5$ perspectives, and $2$ types. The generation of benchmark data across this combinatorial space is powered by a novel LLM-assisted data generation protocol that we dub auto-STEER which generates a set of questions by adapting handwritten templates to target new domains and perspectives. By generating fresh questions for each element, auto-STEER  helps reduce the risk of data contamination, ensuring that \\model evaluations remain valuable over time. We leveraged our benchmark to evaluate $15$ LLMs over each of the instantiated elements, examined their ability to reason through and solve microeconomic problems and compared LLM performance across a suite of adaptations and metrics. Our work provides insights into the current capabilities and limitations of LLMs in non-strategic economic decision-making and a tool for fine-tuning these models to improve performance.", "title_embedding_index": 1407, "title_abs_embedding_index": 1432}, {"title": "Large Language Models Can Self-Improve At Web Agent Tasks", "link_suffix": "/forum?id=jwME4SY0an", "link": "https://openreview.net/forum?id=jwME4SY0an", "pdf_link": "https://openreview.net/pdf?id=jwME4SY0an", "keywords": "llm, llms, synthetic data, web agents, agents, self-improvement, unsupervised learning", "abstract": "Training models to act as agents that can effectively navigate and perform actions in a complex environment, such as a web browser, has typically been challenging due to lack of training data. Large language models (LLMs) have recently demonstrated some capability to navigate novel environments as agents in a zero-shot or few-shot fashion, purely guided by natural language instructions as prompts. Recent research has also demonstrated LLMs have the capability to exceed their base performance through self-improvement, i.e. fine-tuning on data generated by the model itself. In this work, we explore the extent to which LLMs can self-improve their performance as agents in long-horizon tasks in a complex environment using the WebArena benchmark. In WebArena, an agent must autonomously navigate and perform actions on web pages to achieve a specified objective. We explore fine-tuning on three distinct synthetic training data mixtures and achieve a 31% improvement in task completion rate over the base model on the WebArena benchmark through a self-improvement procedure. We additionally contribute novel evaluation metrics for assessing the performance, robustness, capabilities, and quality of trajectories of our fine-tuned agent models to a greater degree than simple, aggregate-level benchmark scores currently used to measure self-improvement.", "title_embedding_index": 1408, "title_abs_embedding_index": 1433}, {"title": "Multimodal Depression Detection with Contextual Position Encoding and Latent Space Regularization", "link_suffix": "/forum?id=miOYgWl60q", "link": "https://openreview.net/forum?id=miOYgWl60q", "pdf_link": "https://openreview.net/pdf?id=miOYgWl60q", "keywords": "depression detection, speech and language processing, contextual position encoding, latent space regularization", "abstract": "Clinical interviews are the gold standard for detecting depression, and previous work using multimodal features from subjects' audio, text, and video have shown promising results. Recent methods further improve performance by incorporating an additional modality\u2014the interviewer\u2019s prompts (text)\u2014during training. However, these approaches risk introducing biases, as models may over-rely on specific prompt-response pairs, which may not always be present in real-world settings. This leads to models exploiting these cues as shortcuts for detecting depression rather than learning the language and behaviors that genuinely indicate the subject's mental health, ultimately undermining consistency and objectivity. To address this, we propose a novel approach that combines Contextual Position Encoding (\\textbf{CoPE}) and Latent Space Regularization (\\textbf{LSR}), leveraging both subjects' responses (audio) and the interviewer's prompts (text). CoPE captures the evolving context of the interview, ensuring that the model utilizes insights from the entire conversation, preventing over-reliance on isolated or late-stage cues. This helps the model understand interactions holistically and more accurately reflect mental health indicators. LSR introduces constraints to enforce consistency in the model\u2019s learned representations, reducing overfitting to superficial cues and guiding the model toward more generalizable patterns. By smoothing the latent space, LSR helps the model focus on meaningful, high-level representations of both audio and text.\nOur approach yields competitive results on the DAIC-WOZ benchmark and surpasses the state-of-the-art on the EATD benchmark. The code is released.", "title_embedding_index": 1409, "title_abs_embedding_index": 1434}, {"title": "Cross Attention for Oddly Shaped Data and Applications in Ionospheric Modeling", "link_suffix": "/forum?id=ReccFdn4zE", "link": "https://openreview.net/forum?id=ReccFdn4zE", "pdf_link": "https://openreview.net/pdf?id=ReccFdn4zE", "keywords": "Ionosphere, Cross Attention, Attention, Transformers, Electron Density, Total Electron Content, TEC, Misshapen Data, Variable Sequence Length", "abstract": "It is desirable to have models of many physical phenomena, yet often data for these phenomena are oddly structured. These structures, such as ungridded and arbitrary length data prevents the use of many types of machine learning techniques, such as feed-forward neural networks. It is thus quite desirable to be able to move this data into a fixed size and shape for easier data ingest. We propose a method of using cross attention to do this.\nAn example of oddly shaped data is Total Electron Content (TEC), or the vertical integral of electron density in the atmosphere. TEC data is calculated using both the position of a satellite and a position on the surface of Earth, giving a non-fixed location per sample. This leads to a splattering of points on the globe where measurements exist that change in shape and amount each time step.\nWe apply our technique to TEC in an autoregressive approach. This allows us to both obtain an embedding describing the global TEC and create completed TEC maps, filling in where measurements are not taken. The global embedding can then be further used in other models.", "title_embedding_index": 1410, "title_abs_embedding_index": 1435}, {"title": "Context Clues: Evaluating Long Context Models for Clinical Prediction Tasks on EHR Data", "link_suffix": "/forum?id=zg3ec1TdAP", "link": "https://openreview.net/forum?id=zg3ec1TdAP", "pdf_link": "https://openreview.net/pdf?id=zg3ec1TdAP", "keywords": "ehr, foundation model, long context, clinical prediction making, healthcare", "abstract": "Foundation Models (FMs) trained on Electronic Health Records (EHRs) have achieved state-of-the-art results on numerous clinical prediction tasks. However, these EHR FMs typically have limited context windows of $<$1k tokens due to computational constraints, which prevents them from modeling full patient EHRs which can easily span 10k's of events. For making clinical predictions, both model performance and robustness to the unique properties of EHR data are crucial. Recent advancements in subquadratic long-context architectures offer a promising solution. However, the application of this long-context paradigm to EHR data has not been well-studied. We address this gap by presenting the first systematic evaluation of the effect of context length on modeling EHR data across four state-of-the-art transformer and non-transformer architectures. We find that longer context models indeed improve predictive performance -- our Mamba-based model surpasses the prior state-of-the-art on 9/14 tasks on the EHRSHOT prediction benchmark. Additionally, we measure robustness to three unique, previously underexplored properties of EHR data: (1) the prevalence of \"copy-forwarded\" diagnoses which create artificial token repetition in EHR sequences; (2) the irregular time intervals between EHR events which can lead to a wide range of timespans within a context window; and (3) the natural increase in disease complexity over time which makes later tokens in the EHR harder to predict than earlier ones. Stratifying our EHRSHOT results, we find that while higher levels of each property correlate negatively with model performance (e.g., a 50% higher Brier loss between the least and most irregular patients), longer context models are more robust to patients exhibiting extreme degrees of each property. Our work highlights the potential for using long-context architectures to model EHR data, and offers a case study on identifying and quantifying new challenges in modeling sequential data that are motivated by domains outside of natural language. We release our model checkpoints, data preprocessing pipelines, and evaluation code.", "title_embedding_index": 1411, "title_abs_embedding_index": 1436}, {"title": "PrivateChat: A Secure Encrypted Communication Framework with Black-box LLMs", "link_suffix": "/forum?id=SX2Z5tgiUu", "link": "https://openreview.net/forum?id=SX2Z5tgiUu", "pdf_link": "https://openreview.net/pdf?id=SX2Z5tgiUu", "keywords": "cloud LLM privacy protection, encrypted communication, black-box optimization", "abstract": "With the growing applications of large language models (LLMs), privacy leakage has emerged as a significant concern. However, widely used LLMs are often deployed on cloud platforms and accessible only through relatively expensive API calls, complicating the realization of secure communication between users and cloud LLMs. In this paper, we introduce PrivateChat, a novel private communication framework that enables users to safely interact with cloud LLMs using user-customized encryption methods (e.g., AES). Our core idea is to learn a private system prompt, which instructs the cloud LLM to process and respond in encrypted text while concealing encryption details from potential attackers. Additionally, to optimize such prompts with few API calls, we propose a Sample-Efficient Simultaneous Perturbation Stochastic Approximation (SE-SPSA) black-box optimization algorithm, which incorporates a baseline-based variance reduction strategy with SPSA for effective and economical training. Extensive experiments on several benchmark datasets with various encryption methods show the effectiveness of our approach in achieving secure and reliable communication with cloud LLMs.", "title_embedding_index": 1412, "title_abs_embedding_index": 1437}, {"title": "MSR-ViR: Modularized Self-reflected Video Reasoner for Video Question Answering", "link_suffix": "/forum?id=4Sv5MQ931E", "link": "https://openreview.net/forum?id=4Sv5MQ931E", "pdf_link": "https://openreview.net/pdf?id=4Sv5MQ931E", "keywords": "Video Question Answering, Multimodal LLM, Modular Network, Self-reflected Training", "abstract": "Recently, multimodal large language models (multimodal LLMs) have been applied to a wide range of video understanding tasks, particularly for Video Question Answering (VideoQA). However, existing multimodal LLMs suffer from the following challenge: the classic end-to-end training strategies of multimodal LLMs for VideoQA tasks are black-box, thus lacking interpretability as they can neither present a reasoning path nor indicate where the answer is derived from the video. To tackle this challenge, we propose MSR-ViR (Modularized Self-Reflected Video Reasoner), a self-reflected framework that introduces a Modularized Spatial-Temporal Grounding (MoST-Grounding) module to multimodal LLMs for VideoQA tasks. MoST-Grounding utilizes a question parser LLM to generate execution policies, which serve as a reasoning path from questions to answers providing interpretability for our VideoQA framework. Based on the execution policies, MoST-Grounding invokes various small modules to localize temporal segments and spatial regions in videos which provide multimodal LLMs with most relevant visual information, while presenting visual evidence of our final answers. To avoid the question parser LLM generating unreasonable policies, we further propose a reinforcement learning-based Alternate Self-reflection training strategy to optimize the Multimodal LLM and the question parser LLM. Experiments on VideoQA datasets (NExT-QA and STAR) and grounded VideoQA dataset (NExT-GQA) demonstrate that our method significantly improves video understanding capabilities of multimodal LLMs, while providing interpretable reasoning paths together with temporal and spatial localization evidence within the video.", "title_embedding_index": 1413, "title_abs_embedding_index": 1438}, {"title": "Adversarial Robustness of Count-Min Sketch", "link_suffix": "/forum?id=IuEBdNsWKb", "link": "https://openreview.net/forum?id=IuEBdNsWKb", "pdf_link": "https://openreview.net/pdf?id=IuEBdNsWKb", "keywords": "randomized methods, frequency estimation, adversarial attacks, universal hashing", "abstract": "Small\u2013space frequency estimators play a crucial role in a multitude of settings related to both machine learning and data processing for evolving data. Many frequency estimators use internal randomness to compress the information about the frequencies of items to a small sketch that can be used to provide estimates. Historically, these types of estimators were designed without considering the scenario in which the user with access to the estimator can accidentally or maliciously manipulate estimates. This can be achieved by the user who makes adaptive updates and uses queries to gain information about the estimator's internal randomness.In this work, we consider one of the simplest such estimators: Count-Min Sketch. On the one hand, we show how to make it resistant to adversarial attacks in both the random oracle model, which corresponds to cryptographically hard hash functions, and using universal hash functions if the domain size is in polynomial relationship with with the size of hash tables.On the other hand, we also explore adaptive attacks on Count-Min Sketch. In particular, we show how to speed up multirow hashing attacks for a popular family of universal hash functions and demonstrate the efficiency of our attack for a popular implementation of Count-Min Sketch.", "title_embedding_index": 1414, "title_abs_embedding_index": 1439}, {"title": "The Latent Road to Atoms: Backmapping Coarse-grained Protein Structures with Latent Diffusion", "link_suffix": "/forum?id=zIqLQVBxdd", "link": "https://openreview.net/forum?id=zIqLQVBxdd", "pdf_link": "https://openreview.net/pdf?id=zIqLQVBxdd", "keywords": "Protein Structure Reconstruction, Latent Diffusion, Discrete Protein Representations", "abstract": "Coarse-grained molecular dynamics simulations offer computational efficiency for exploring protein conformational ensembles and thermodynamic properties.\nThough coarse representations enable large-scale simulations across extended temporal and spatial ranges, the sacrifice of atomic-level details limits their utility in tasks such as ligand docking and protein-protein interaction prediction.\nBackmapping, the process of reconstructing all-atom structures from coarse-grained representations, is crucial for recovering these fine details.\nWhile recent machine learning methods have made strides in protein structure generation, challenges persist in reconstructing diverse atomistic conformations that maintain geometric accuracy and chemical validity.\nIn this paper, we present Latent Diffusion Backmapping (LDB), a novel approach leveraging denoising diffusion within latent space to address these challenges. \nBy combining discrete latent encoding with diffusion, LDB bypasses the need for equivariant and internal coordinate manipulation, significantly simplifying the training and sampling processes as well as facilitating better and wider exploration in configuration space. \nWe evaluate LDB\u2019s state-of-the-art performance on three distinct protein datasets, demonstrating its ability to efficiently reconstruct structures with high structural accuracy and chemical validity.\nMoreover, LDB shows exceptional versatility in capturing diverse protein ensembles, highlighting its capability to explore intricate conformational spaces. \nOur results position LDB as a powerful and scalable approach for backmapping, effectively bridging the gap between CG simulations and atomic-level analyses in computational biology.", "title_embedding_index": 1415, "title_abs_embedding_index": 1440}, {"title": "Not Only Vision: Evolve Visual Speech Recognition via Peripheral Information", "link_suffix": "/forum?id=TykT5YB89r", "link": "https://openreview.net/forum?id=TykT5YB89r", "pdf_link": "https://openreview.net/pdf?id=TykT5YB89r", "keywords": "Visual Speech Recognition, Multimodal Learning, Large Language Model", "abstract": "Visual Speech Recognition (VSR) aims to infer what was said by analyzing the speaker's facial dynamics. However, is reliance solely on visual information sufficient in challenging real-world scenarios? In human visual perception, peripheral vision refers to non-central areas of the visual field, crucial for providing overall awareness and detailed perception of central objects. Similarly, human lip-readers do not rely exclusively on lip movements but integrate contextual cues and prior knowledge to achieve more accurate transcribing. For the first time in machine lip-reading, we frame these non-lip-movement factors into a new concept of semantic-level peripheral information, Specifically, we select three representative types varying in relevance to the spoken content: (1) Contextual peripheral information, such as the general topic or some basic knowledge of the speech, can significantly narrow the range of potential recognition hypotheses. (2) Experiential peripheral information emerges from the recognition process itself. The very act of recognizing speech in a specific language provides implicit knowledge of grammar, word collocations, and related linguistic aspects, thereby guiding the recognition effectively. (3) Perturbative peripheral information introduces disturbance factors into the recognition process, analogous to noise injection in visual tasks. Semantic-level peripheral information is indirectly linked to transcripts; thus fusing it into VSR necessitates strong contextual understanding and inference capabilities. Here, we propose a multimodal learning framework built on a large language model (LLM), leveraging its powerful contextual modeling capabilities to take advantage of peripheral information. Our method's efficacy is demonstrated on two popular datasets. On the widely-used LRS3 dataset, we achieved a Word Error Rate (WER) of 24.5% with readily available peripheral information, leading to an impressive 14.3% relative improvement over the model without such information. To the best of our knowledge, our work sets a new state-of-the-art when utilizing similar hours of lip-reading videos. We further reported the evaluation on the more challenging AVSpeech dataset. Results across both datasets and various experimental settings demonstrate the promising potential of the proposed semantic-level peripheral information for VSR.", "title_embedding_index": 1416, "title_abs_embedding_index": 1441}, {"title": "Plastic Learning with Deep Fourier Features", "link_suffix": "/forum?id=NIkfix2eDQ", "link": "https://openreview.net/forum?id=NIkfix2eDQ", "pdf_link": "https://openreview.net/pdf?id=NIkfix2eDQ", "keywords": "Fourier, plasticity, neural networks, continual learning", "abstract": "Deep neural networks can struggle to learn continually in the face of non-stationarity. This phenomenon is known as loss of plasticity. In this paper, we identify underlying principles that lead to plastic algorithms. In particular, we provide theoretical results showing that shallow linear networks, as well as a special case of deep linear networks, do not suffer from loss of plasticity. We then propose \\emph{deep Fourier features}, which are the concatenation of a sine and cosine in every layer, and we show that this combination provides a dynamic balance between the trainability obtained through linearity and the effectiveness obtained through the nonlinearity of neural networks. Deep networks composed entirely of deep Fourier features are highly trainable and sustain their trainability over the course of learning. Our empirical results show that continual learning performance can be drastically improved by replacing \\texttt{ReLU} activations with deep Fourier features. These results hold for different continual learning scenarios (e.g., label noise, class incremental learning, pixel permutations) on all major supervised learning datasets used for continual learning research, such as CIFAR10, CIFAR100, and tiny-ImageNet.", "title_embedding_index": 1417, "title_abs_embedding_index": 1442}, {"title": "MCCE: Missingness-aware Causal Concept Explainer", "link_suffix": "/forum?id=UoGv8d3MMy", "link": "https://openreview.net/forum?id=UoGv8d3MMy", "pdf_link": "https://openreview.net/pdf?id=UoGv8d3MMy", "keywords": "interpretable machine learning, concept-based explanation, causal concept effect", "abstract": "Causal concept effect estimation is gaining increasing interest in the field of interpretable machine learning. This general approach explains the behaviors of machine learning models by estimating the causal effect of human-understandable concepts, which represent high-level knowledge more comprehensibly than raw inputs like tokens. However, existing causal concept effect explanation methods assume complete observation of all concepts involved within the dataset, which can fail in practice due to incomplete annotations or missing concept data. We theoretically demonstrate that unobserved concepts can bias the estimation of the causal effects of observed concepts. To address this limitation, we introduce the Missingness-aware Causal Concept Explainer (MCCE), a novel framework specifically designed to estimate causal concept effects when not all concepts are observable. Our framework learns to account for residual bias resulting from missing concepts and utilizes a linear predictor to model the relationships between these concepts and the outputs of black-box machine learning models. It can offer explanations on both local and global levels. We conduct validations using a real-world dataset, demonstrating that MCCE outperforms existing state-of-the-art explanation methods in causal concept effect estimation.", "title_embedding_index": 1418, "title_abs_embedding_index": 1443}, {"title": "STAFF: Speculative Coreset Selection for Task-Specific Fine-tuning", "link_suffix": "/forum?id=FAfxvdv1Dy", "link": "https://openreview.net/forum?id=FAfxvdv1Dy", "pdf_link": "https://openreview.net/pdf?id=FAfxvdv1Dy", "keywords": "Task-specific fine-tuning, coreset selection, speculative execution", "abstract": "Task-specific fine-tuning is essential for the deployment of large language models (LLMs), but it requires significant computational resources and time. Existing solutions have proposed coreset selection methods to improve data efficiency and reduce model training overhead, but they still have limitations: \u2776 Overlooking valuable samples at high pruning rates, which degrades the coreset\u2019s performance.\n\u2777 Requiring high time overhead during coreset selection to fine-tune and evaluate the target LLM. In this paper, we introduce STAFF, a speculative coreset selection method. STAFF leverages a small model from the same family as the target LLM to efficiently estimate data scores and then verifies the scores on the target LLM to accurately identify and allocate more selection budget to important regions while maintaining coverage of easy regions. We evaluate STAFF on three LLMs and three downstream tasks and show that STAFF improves the performance of SOTA methods by up to 54.3% and reduces selection overhead by up to 70.5% at different pruning rates. Furthermore, we observe that the coreset selected by STAFF at low pruning rates (i.e., 20%) can even obtain better fine-tuning performance than the full dataset.", "title_embedding_index": 1419, "title_abs_embedding_index": 1444}, {"title": "ELEMENTAL: Interactive Learning from Demonstrations and Vision-Language Models for Interpretable Reward Design in Robotics", "link_suffix": "/forum?id=QnkhVwSu7u", "link": "https://openreview.net/forum?id=QnkhVwSu7u", "pdf_link": "https://openreview.net/pdf?id=QnkhVwSu7u", "keywords": "Interactive Robot Learning, Inverse Reinforcement Learning, Feature Abstraction, Vision-Language Models", "abstract": "Reinforcement learning (RL) has demonstrated compelling performance in robotic tasks, but its success often hinges on the design of complex, ad hoc reward functions. Researchers have explored how Large Language Models (LLMs) could enable non-expert users to specify reward functions more easily. However, LLMs struggle to balance the importance of different features, generalize poorly to out-of-distribution robotic tasks, and cannot represent the problem properly with only text-based descriptions. To address these challenges, we propose ELEMENTAL (intEractive LEarning froM dEmoNstraTion And Language), a novel framework that combines natural language guidance with visual user demonstrations to align robot behavior with user intentions better. By incorporating visual inputs, ELEMENTAL overcomes the limitations of text-only task specifications, while leveraging inverse reinforcement learning (IRL) to balance feature weights and match the demonstrated behaviors optimally. ELEMENTAL also introduces an iterative feedback-loop through self-reflection to improve feature, reward, and policy learning. Further, ELEMENTAL reward functions are interpretable. Our experiment results demonstrate that ELEMENTAL outperforms prior work by 22.7% on task success, and achieves 45.0% better generalization in out-of-distribution tasks, highlighting its robustness in LfD.", "title_embedding_index": 1420, "title_abs_embedding_index": 1445}, {"title": "A diffusion model on toric varieties with application to protein loop modeling", "link_suffix": "/forum?id=FuXtwQs7pj", "link": "https://openreview.net/forum?id=FuXtwQs7pj", "pdf_link": "https://openreview.net/pdf?id=FuXtwQs7pj", "keywords": "Protein loop, Toric varieties, Jacobian, Tangent space, MHC, Nanobody", "abstract": "The conformation spaces of loop regions in proteins as well as closed kinematic linkages in robotics can be described by systems of polynomial equations, forming Toric varieties. These are real algebraic varieties, formulated as the zero sets of polynomial equations constraining the rotor angles in a linkage or macromolecular chain. These spaces are essentially stitched manifolds and contain singularities. Diffusion models have achieved spectacular success in applications in Cartesian space and smooth manifolds but have not been extended to varieties. Here we develop a diffusion model that performs a random walk on the underlying variety by utilizing an appropriate Jacobian, whose vanishing indicates singularities. This allows our method to explore the variety, without encountering singular or infeasible states. We demonstrated the approach on two important protein structure prediction problems: one is prediction of Major Histocompatibility Complex (MHC) peptide interactions, a critical part in the design of  neoantigen vaccines, and the other is loop prediction for nanobodies, an important class of drugs. In both, we improve upon the state of the art open source AlphaFold.", "title_embedding_index": 1421, "title_abs_embedding_index": 1446}, {"title": "DebUnc: Improving Large Language Model Agent Communication Via Uncertainty Metrics", "link_suffix": "/forum?id=ByLO7p0oCF", "link": "https://openreview.net/forum?id=ByLO7p0oCF", "pdf_link": "https://openreview.net/pdf?id=ByLO7p0oCF", "keywords": "multiagent debate, model uncertainty, agent communication, large language models", "abstract": "To enhance Large Language Model (LLM) capabilities, multi-agent debates have been introduced, where multiple LLMs discuss solutions to a problem over several rounds of debate. However, LLMs often produce incorrect responses that appear confident, which can mislead other agents. This is partly because agents do not express their confidence levels during standard debates. To address this, we introduce DebUnc, a multi-agent debate framework that uses uncertainty metrics to assess agent confidence levels. We adapted the LLM attention mechanism to adjust token weights based on confidence levels and also explored using textual prompts to convey confidence. Our evaluations across various benchmarks show that attention-based methods are particularly effective, and that as uncertainty metrics improve, performance will continue to increase.", "title_embedding_index": 1422, "title_abs_embedding_index": 1447}, {"title": "Future-Guided Pretraining via Time-to-Event Supervision for 3D Medical Imaging", "link_suffix": "/forum?id=zcTLpIfj9u", "link": "https://openreview.net/forum?id=zcTLpIfj9u", "pdf_link": "https://openreview.net/pdf?id=zcTLpIfj9u", "keywords": "Multimodal learning, medical imaging, Electronic Health Records", "abstract": "In medicine, making effective treatment decisions requires detecting early warning signs of disease. With the rise of 3D medical foundation models, there is promise in large-scale pretraining to capture new and more informative imaging biomarkers associated with future disease risk. Current self-supervised learning (SSL) techniques for 3D medical imaging largely capture structural properties via reconstruction and contrastive losses \u2013 local features that provide only indirect signal on disease progression. Electronic health records (EHRs) present an underutilized resource for future information, offering an easily paired and scalable amount of weak supervision representative of patient outcomes. To this end, we propose future-guided pretraining to explore the benefits of training 3D image encoders on future medical events. By combining classic techniques from timeto- event modeling and recent pretraining techniques using longitudinal event data from EHRs, we show that future-guided pretraining enhances the ability to predict future patient outcomes (average AUROC increase of 25.3% and time-dependent c-statistics increase of 23% compared to baseline models) without degrading the ability to perform standard binary classification tasks (e.g. image labeling for diagnostic tasks). This study lays the groundwork for innovative ways to combine EHR and imaging modalities for clinical risk prediction.", "title_embedding_index": 1423, "title_abs_embedding_index": 1448}, {"title": "From Static to Dynamic: Leveraging Implicit Behavioral Models to Facilitate Transition in Offline-to-Online Reinforcement Learning", "link_suffix": "/forum?id=d159zNCmOq", "link": "https://openreview.net/forum?id=d159zNCmOq", "pdf_link": "https://openreview.net/pdf?id=d159zNCmOq", "keywords": "Offline-to-Online Reinforcement Learning, Behavioral Adaptation, Q-value Estimation, Priority Sampling Strategy", "abstract": "Transitioning reinforcement learning (RL) models from offline training environments to dynamic online settings faces critical challenges because of the distributional shift and the model inability in effectively adapting to new, unseen scenarios. This work proposes the \\textbf{B}ehavior \\textbf{A}daption \\textbf{Q}-Learning (BAQ), a novel framework facilitating smoother transitions in offline-to-online RL. BAQ strategically leverages the implicit behavioral model to imitate and adapt behaviors of offline datasets, enabling the model to handle out-of-distribution state-action pairs more effectively during its online deployment. The key to our approach is the integration of a composite loss function that not only mimics the offline data-driven policy but also dynamically adjusts to new experiences encountered online. This dual-focus mechanism enhances the model's adaptability and robustness, reducing Q-value estimation errors and improving the overall learning efficiency. Extensive empirical evaluations demonstrate that BAQ significantly outperforms existing methods, achieving enhanced adaptability and reduced performance degradation in diverse RL settings. Our framework sets a new standard for offline-to-online RL, offering a robust solution for applications requiring reliable transitions from theoretical training to practical, real-world execution.", "title_embedding_index": 1424, "title_abs_embedding_index": 1449}]