[{"title": "Isk\u00d7kMatrix Eigendecomposition Sufficient for Spectral Clustering?", "link_suffix": "/forum?id=Feg9xrbFcn", "link": "https://openreview.net/forum?id=Feg9xrbFcn", "pdf_link": "https://openreview.net/pdf?id=Feg9xrbFcn", "keywords": "Spectral clustering, Kernel mean embedding, Matrix eigendecomposition", "abstract": "Spectral clustering has been widely used in clustering tasks due to its effectiveness. However, its key step, eigendecomposition of an $n\\times n$ matrix, is computationally expensive for large-scale datasets. Recent works have proposed methods to reduce this complexity, such as Nystr\"om method approximation and landmark-based approaches. While these methods aim to maintain good clustering quality while performing eigendecomposition on smaller matrix. The minimum matrix size required for spectral decomposition in spectral clustering is $k\\times k$ (where $k$ is the number of clusters), as it needs to obtain $n\\times k$ k-dimensional spectral embedding features. However, no algorithm can achieve good clustering performance with only a $k\\times k$ matrix eigendecomposition currently. In this paper, we propose a novel distribution-based spectral clustering. Our method constructs an $n\\times k$ bipartite graph between n data points and k distributions, enabling the eigendecomposition of only a $k\\times k$ matrix while preserving clustering quality. We demonstrate that our approach can achieve efficient and effective spectral clustering through $k\\times k $matrix eigendecomposition.", "title_embedding_index": 11650, "title_abs_embedding_index": 11675}, {"title": "Structure-Guided Large Language Models for Text-to-SQL Generation", "link_suffix": "/forum?id=ReKWjKvkJE", "link": "https://openreview.net/forum?id=ReKWjKvkJE", "pdf_link": "https://openreview.net/pdf?id=ReKWjKvkJE", "keywords": "Text-to-SQL, large language model, structure learning", "abstract": "Recent advancements in large language models (LLMs) have shown promise in bridging the gap between natural language queries and database management systems, enabling users to interact with databases without the background of SQL. However, LLMs often struggle to fully exploit and comprehend the user intention and complex structures of databases. Decomposition-based methods have been proposed to enhance the performance of LLMs on complex tasks, but decomposing SQL generation into subtasks is non-trivial due to the declarative structure of SQL syntax and the intricate connections between query concepts and database elements. In this paper, we propose a novel $\\textbf{S}$tructure $\\textbf{GU}$ided text-to-$\\textbf{SQL}$ framework ($\\textbf{SGU-SQL}$) that incorporates syntax-based prompting to enhance the SQL generation capabilities of LLMs. Specifically, SGU-SQL establishes structure-aware links between user queries and database schema and recursively decomposes the complex generation task using syntax-based prompting to guide LLMs in incrementally constructing target SQLs. Extensive experiments on two benchmark datasets demonstrate that SGU-SQL consistently outperforms state-of-the-art text-to-SQL baselines. These results highlight the importance of incorporating structural syntax information for effective text-to-SQL generation and pave the way for more robust and reliable interfaces to databases in the era of artificial intelligence.", "title_embedding_index": 11651, "title_abs_embedding_index": 11676}, {"title": "ViSAGe: Video-to-Spatial Audio Generation", "link_suffix": "/forum?id=8bF1Vaj9tm", "link": "https://openreview.net/forum?id=8bF1Vaj9tm", "pdf_link": "https://openreview.net/pdf?id=8bF1Vaj9tm", "keywords": "Audio Generation, Audio-Visual Learning, Spatial Audio", "abstract": "Spatial audio is essential for enhancing the immersiveness of audio-visual experiences, yet its production typically demands complex recording systems and specialized expertise. In this work, we address a novel problem of generating first-order ambisonics, a widely used spatial audio format, directly from silent videos. To support this task, we introduce YT-Ambigen, a dataset comprising 102K 5-second YouTube video clips paired with corresponding first-order ambisonics. We also propose new evaluation metrics to assess the spatial aspect of generated audio based on audio energy maps and saliency metrics. Furthermore, we present Video-to-Spatial Audio Generation (ViSAGe), an end-to-end framework that generates first-order ambisonics from silent video frames by leveraging CLIP visual features, autoregressive neural audio codec modeling with both directional and visual guidance. Experimental results demonstrate that ViSAGe produces plausible and coherent first-order ambisonics, outperforming two-stage approaches consisting of video-to-audio generation and audio spatialization. Qualitative examples further illustrate that ViSAGe generates temporally aligned high-quality spatial audio that adapts to viewpoint changes. We will make public our codes and dataset.", "title_embedding_index": 11652, "title_abs_embedding_index": 11677}, {"title": "AIDBench: A benchmark for evaluating the authorship identification capability of large language models", "link_suffix": "/forum?id=zEm5nXxiXU", "link": "https://openreview.net/forum?id=zEm5nXxiXU", "pdf_link": "https://openreview.net/pdf?id=zEm5nXxiXU", "keywords": "large language models, privacy, authorship identification", "abstract": "As large language models (LLMs) rapidly advance and integrate into daily life, the privacy risks they pose are attracting increasing attention. We focus on a specific privacy risk where LLMs may help identify the authorship of anonymous texts, which challenges the effectiveness of anonymity in real-world systems such as anonymous peer review systems. To investigate these risks, we present AIDBench, a new benchmark that incorporates several author identification datasets, including emails, blogs, reviews, articles, and research papers. AIDBench utilizes two evaluation methods: one-to-one authorship identification, which determines whether two texts are from the same author; and one-to-many authorship identification, which, given a query text and a list of candidate texts, identifies the candidate most likely written by the same author as the query text. We also introduce a Retrieval-Augmented Generation (RAG)-based method to enhance the large-scale authorship identification capabilities of LLMs, particularly when input lengths exceed the models' context windows, thereby establishing a new baseline for authorship identification using LLMs. Our experiments with AIDBench demonstrate that LLMs can correctly guess authorship at rates well above random chance, revealing new privacy risks posed by these powerful models.", "title_embedding_index": 11653, "title_abs_embedding_index": 11678}, {"title": "Learning Long Range Dependencies on Graphs via Random Walks", "link_suffix": "/forum?id=kJ5H7oGT2M", "link": "https://openreview.net/forum?id=kJ5H7oGT2M", "pdf_link": "https://openreview.net/pdf?id=kJ5H7oGT2M", "keywords": "Graph neural networks, Graph transformers, Random walks, Long-range dependencies", "abstract": "Message-passing graph neural networks (GNNs) excel at capturing local relationships but struggle with long-range dependencies in graphs. In contrast, graph transformers (GTs) enable global information exchange but often oversimplify the graph structure by representing graphs as sets of fixed-length vectors. This work introduces a novel architecture that overcomes the shortcomings of both approaches by combining the long-range information of random walks with local message passing. By treating random walks as sequences, our architecture leverages recent advances in sequence models to effectively capture long-range dependencies within these walks. Based on this concept, we propose a framework that offers (1) more expressive graph representations through random walk sequences, (2) the ability to utilize any sequence model for capturing long-range dependencies, and (3) the flexibility by integrating various GNN and GT architectures. Our experimental evaluations demonstrate that our approach achieves significant performance improvements on 19 graph and node benchmark datasets, notably outperforming existing methods by up to 13% on the PascalVoc-SP and COCO-SP datasets.", "title_embedding_index": 11654, "title_abs_embedding_index": 11679}, {"title": "Benchmarking End-To-End Performance of AI-Based Chip Placement Algorithms", "link_suffix": "/forum?id=cx46JSD2qn", "link": "https://openreview.net/forum?id=cx46JSD2qn", "pdf_link": "https://openreview.net/pdf?id=cx46JSD2qn", "keywords": "Electronic Design Automation, Chip Placement Algorithms, End to End Performance Evaluation Benchmark, Physical Design, Reinforcement Learning and Evolutionary Algorithm", "abstract": "The increasing complexity of modern very-large-scale integration (VLSI) design highlights the significance of Electronic Design Automation (EDA) technologies. Chip placement is a critical step in the EDA workflow, which positions chip modules on the canvas with the goal of optimizing performance, power, and area (PPA) metrics of final chip designs. Recent advances have demonstrated the great potential of AI-based algorithms in enhancing chip placement. However, due to the lengthy workflow of chip design, the evaluations of these algorithms often focus on $\\textit{intermediate surrogate metrics}$, which are easy to compute but frequently reveal a substantial misalignment with the $\\textit{end-to-end performance}$ (i.e., the final design PPA). To address this challenge, we introduce ChiPBench, which can effectively facilitate research in chip placement within the AI community. ChiPBench is a comprehensive benchmark specifically designed to evaluate the effectiveness of existing AI-based chip placement algorithms in improving final design PPA metrics. Specifically, we have gathered $20$ circuits from various domains (e.g., CPU, GPU, and microcontrollers). These designs are compiled by executing the workflow from the verilog source code, which preserves necessary physical implementation kits, enabling evaluations for the placement algorithms on their impacts on the final design PPA. We executed six state-of-the-art AI-based chip placement algorithms on these designs and plugged the results of each single-point algorithm into the physical implementation workflow to obtain the final PPA results. Experimental results show that even if intermediate metric of a single-point algorithm is dominant, while the final PPA results are unsatisfactory. This suggests that the AI community should concentrate more on enhancing end-to-end performance rather than those intermediate surrogates. We believe that our benchmark will serve as an effective evaluation framework to bridge the gap between academia and industry.", "title_embedding_index": 11655, "title_abs_embedding_index": 11680}, {"title": "Vision State Space Duality for Medical Image Segmentation: Enhancing Precision through Non-Causal Modeling", "link_suffix": "/forum?id=GLuzjuG0lo", "link": "https://openreview.net/forum?id=GLuzjuG0lo", "pdf_link": "https://openreview.net/pdf?id=GLuzjuG0lo", "keywords": "State Space Models, Vision State Space Duality, Medical Image Segmentation, self-attention, skip connections", "abstract": "In medical image analysis, Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) have set significant benchmarks. However, CNNs exhibit limitations in long-range modeling capabilities, whereas Transformers are hampered by their quadratic computational complexity. Recently, State Space Models (SSMs) have gained prominence in vision tasks as they offer linear computational complexity. State Space Duality (SSD), an improved variant of SSMs, was introduced in Mamba2 to enhance model performance and efficiency. Inspired by this, we have tailored the Vision State Space Duality (VSSD) model for medical image segmentation tasks by integrating it within a UNet-like architecture, which is renowned for its effectiveness in the field. Our modified model, named VSSD-UNet, employs skip connections to preserve spatial information and utilizes a series of VSSD blocks for feature extraction. In addition, VSSD-UNet employs a hybrid structure of VSSD and self-attention in the decoder part, ensuring that both local details and global contexts are captured. Finally, we conducted comparative and ablation experiments on two public lesion segmentation datasets: ISIC2017 and ISIC2018. The results show that VSSD-UNet outperforms several types of UNet in medical image segmentation under the same hyper-parameter setting. Our code will be released soon.", "title_embedding_index": 11656, "title_abs_embedding_index": 11681}, {"title": "ReSi: A Comprehensive Benchmark for Representational Similarity Measures", "link_suffix": "/forum?id=PRvdO3nfFi", "link": "https://openreview.net/forum?id=PRvdO3nfFi", "pdf_link": "https://openreview.net/pdf?id=PRvdO3nfFi", "keywords": "representational similarity, benchmark, grounding, representations", "abstract": "Measuring the similarity of different representations of neural architectures is a fundamental task and an open research challenge for the machine learning community. This paper presents the first comprehensive benchmark for evaluating representational similarity measures based on well-defined groundings of similarity. The representational similarity (ReSi) benchmark consists of (i) six carefully designed tests for similarity measures, (ii) 23 similarity measures, (iii) twelve neural network architectures, and (iv) six datasets, spanning over the graph, language, and vision domains. The benchmark opens up several important avenues of research on representational similarity that enable novel explorations and applications of neural architectures. We demonstrate the utility of the ReSi benchmark by conducting experiments on various neural network architectures, real world datasets and similarity measures. All components of the benchmark are publicly available and thereby facilitate systematic reproduction and production of research results. The benchmark is extensible, future research can build on and further expand it. We believe that the ReSi benchmark can serve as a sound platform catalyzing future research that aims to systematically evaluate existing and explore novel ways of comparing representations of neural architectures.", "title_embedding_index": 11657, "title_abs_embedding_index": 11682}, {"title": "EvA: Erasing Spurious Correlations with Activations", "link_suffix": "/forum?id=zKvrOOBouT", "link": "https://openreview.net/forum?id=zKvrOOBouT", "pdf_link": "https://openreview.net/pdf?id=zKvrOOBouT", "keywords": "spurious correlation, compute efficiency, data efficiency", "abstract": "Spurious correlations often arise when models associate features strongly correlated with, but not causally related to, the label e.g. an image classifier associates bodies of water with ducks. To mitigate spurious correlations, existing methods focus on learning unbiased representation or incorporating additional information about the correlations during training. This work removes spurious correlations by ``ErasingwithActivations'' (EvA).  EvA learns class-specific spurious indicator on each channel for the fully connected layer of pretrained networks. By erasing spurious connections during re-weighting, EvA achieves state-of-the-art performance across diverse datasets (6.2% relative gain on BAR and achieves 4.1% on Waterbirds). For biased datasets without any information about the spurious correlations, EvA can outperform previous methods (4.8% relative gain on Waterbirds) with 6 orders of magnitude less compute, highlighting its data and computational efficiency.", "title_embedding_index": 11658, "title_abs_embedding_index": 11683}, {"title": "Eliminating Oversaturation and Artifacts of High Guidance Scales in Diffusion Models", "link_suffix": "/forum?id=e2ONKX6qzJ", "link": "https://openreview.net/forum?id=e2ONKX6qzJ", "pdf_link": "https://openreview.net/pdf?id=e2ONKX6qzJ", "keywords": "classifier-free guidance, diffusion models", "abstract": "Classifier-free guidance (CFG) is crucial for improving both generation quality and alignment between the input condition and final output in diffusion models. While a high guidance scale is generally required to enhance these aspects, it also causes oversaturation and unrealistic artifacts. In this paper, we revisit the CFG update rule and introduce modifications to address this issue. We first decompose the update term in CFG into parallel and orthogonal components with respect to the conditional model prediction and observe that the parallel component primarily causes oversaturation, while the orthogonal component enhances image quality. Accordingly, we propose down-weighting the parallel component to achieve high-quality generations without oversaturation. Additionally, we draw a connection between CFG and gradient ascent and introduce a new rescaling and momentum method for the CFG update rule based on this insight. Our approach, termed adaptive projected guidance (APG), retains the quality-boosting advantages of CFG while enabling the use of higher guidance scales without oversaturation. APG is easy to implement and introduces practically no additional computational overhead to the sampling process. Through extensive experiments, we demonstrate that APG is compatible with various conditional diffusion models and samplers, leading to improved FID, recall, and saturation scores while maintaining precision comparable to CFG, making our method a superior plug-and-play alternative to standard classifier-free guidance.", "title_embedding_index": 11659, "title_abs_embedding_index": 11684}, {"title": "Efficient Neural Common Neighbor for Temporal Graph Link Prediction", "link_suffix": "/forum?id=XLt0eudh8t", "link": "https://openreview.net/forum?id=XLt0eudh8t", "pdf_link": "https://openreview.net/pdf?id=XLt0eudh8t", "keywords": "Temporal graph, Neural common neighbor, Efficient", "abstract": "Temporal graphs are ubiquitous in real-world scenarios, such as social network, trade and transportation. Predicting dynamic links between nodes in a temporal graph is of vital importance. Traditional methods usually leverage the temporal neighborhood of interaction history to generate node embeddings first and then aggregate the source and target node embeddings to predict the link. However, such methods focus on learning individual node representations, but overlook the pairwise representation learning nature of link prediction and fail to capture the important pairwise features of links such as common neighbors (CN). Motivated by the success of Neural Common Neighbor (NCN) for static graph link prediction, we propose \\textbf{TNCN}, a temporal version of NCN for link prediction in temporal graphs. Based on a memory-based backbone instead of traditional static graph neural network, TNCN dynamically updates a temporal neighbor dictionary for each node, and utilizes multi-hop common neighbors between the source and target node to learn a more effective pairwise representation. We validate our model on five large-scale real-world datasets from the Temporal Graph Benchmark (TGB), and find that it achieves new state-of-the-art performance on three of them. Additionally, TNCN demonstrates excellent scalability on large datasets, outperforming popular GNN baselines by up to 6.4 times in speed.", "title_embedding_index": 11660, "title_abs_embedding_index": 11685}, {"title": "An Online Learning Theory of Trading-Volume Maximization", "link_suffix": "/forum?id=OvU9u6wS2J", "link": "https://openreview.net/forum?id=OvU9u6wS2J", "pdf_link": "https://openreview.net/pdf?id=OvU9u6wS2J", "keywords": "regret minimization, bilateral trade, online learning, fairness, theory", "abstract": "We explore brokerage between traders in an online learning framework.\nAt any round $t$, two traders meet to exchange an asset, provided the exchange is mutually beneficial.\nThe broker proposes a trading price, and each trader tries to sell their asset or buy the asset from the other party, depending on whether the price is higher or lower than their private valuations.\nA trade happens if one trader is willing to sell and the other is willing to buy at the proposed price.\nPrevious work provided guidance to a broker aiming at enhancing traders' total earnings by maximizing thegain from trade, defined as the sum of the traders' net utilities after each interaction.\nThis classical notion of reward can be highly unfair to traders with small profit margins, and far from the real-life utility of the broker.\nFor these reasons, we investigate how the broker should behave to maximize the trading volume, i.e., thetotal number of trades.\nWe model the traders' valuations as an i.i.d. process with an unknown distribution.\nIf the traders' valuations are revealed after each interaction (full-feedback), and the traders' valuations cumulative distribution function (cdf) is continuous, we provide an algorithm achieving logarithmic regret and show its optimality up to constants.\nIf only their willingness to sell or buy at the proposed price is revealed after each interaction ($2$-bit feedback), we provide an algorithm achieving poly-logarithmic regret when the traders' valuations cdf is Lipschitz and show its near-optimality.\nWe complement our results by analyzing the implications of dropping the regularity assumptions on the unknown traders' valuations cdf. \nIf we drop the continuous cdf assumption, the regret rate degrades to $\\Theta(\\sqrt{T})$ in the full-feedback case, where $T$ is the time horizon. \nIf we drop the Lipschitz cdf assumption, learning becomes impossible in the $2$-bit feedback case.", "title_embedding_index": 11661, "title_abs_embedding_index": 11686}, {"title": "Planning-Driven Programming: A Large Language Model Programming Workflow", "link_suffix": "/forum?id=Fr6bjeqRec", "link": "https://openreview.net/forum?id=Fr6bjeqRec", "pdf_link": "https://openreview.net/pdf?id=Fr6bjeqRec", "keywords": "program synthesis, large language model, code generation, reasoning", "abstract": "The strong performance of large language models (LLMs) on natural language processing tasks raises extensive discussion on their application to code generation. Recent work suggests multiple sampling approaches to improve initial code generation accuracy or program repair approaches to refine the code. However, these methods suffer from LLMs' inefficiencies and limited reasoning capacity. In this work, we propose an LLM programming workflow (LPW) designed to improve both initial code generation and subsequent refinements within a structured two-phase workflow. Specifically, in the solution generation phase, the LLM first outlines a solution plan that decomposes the problem into manageable sub-problems and then verifies the generated solution plan through visible test cases. Subsequently, in the code implementation phase, the LLM initially drafts a code according to the solution plan and its verification. If the generated code fails the visible tests, the plan verification serves as the intended natural language solution to consistently inform the refinement process for correcting bugs. We further introduce SLPW, a sampling variant of LPW, which initially generates multiple solution plans and plan verifications, produces a program for each plan and its verification, and refines each program as necessary until one successfully passes the visible tests. Compared to the state-of-the-art methods across various existing LLMs, our experimental results show that LPW significantly improves the Pass@1 accuracy by up to 16.4% on well-established text-to-code generation benchmarks, especially with a notable improvement of around 10% on challenging benchmarks. Additionally, SLPW  demonstrates up to a 5.6% improvement over LPW and sets new state-of-the-art Pass@1 accuracy on various benchmarks, e.g., 98.2% on HumanEval, 84.8% on MBPP, 64.0% on APPS, and 35.3% on CodeContest, using the advanced LLM GPT-4o as the backbone.", "title_embedding_index": 11662, "title_abs_embedding_index": 11687}, {"title": "Nexus: Specialization meets Adaptability for Efficiently Training Mixture of Experts", "link_suffix": "/forum?id=WkHkwo8rpL", "link": "https://openreview.net/forum?id=WkHkwo8rpL", "pdf_link": "https://openreview.net/pdf?id=WkHkwo8rpL", "keywords": "Mixture of Experts, MoE, Routing, Efficiency, Adaptability, Upcycling", "abstract": "Efficiency, specialization, and adaptability to new data distributions are qualities that are hard to combine in current Large Language Models. The Mixture of Experts (MoE) architecture has been the focus of significant research because its inherent conditional computation enables such desirable properties. In this work, we focus on \"upcycling\" dense expert models into an MoE, aiming to improve specialization while also adding the ability to adapt to new tasks easily. We introduce Nexus, an enhanced MoE architecture with adaptive routing where the model learns to project expert embeddings from domain representations. This approach allows Nexus to flexibly add new experts after the initial upcycling through separately trained dense models, without requiring large-scale MoE training for unseen data domains. Our experiments show that Nexus achieves a relative gain of up to 2.1% over the baseline for initial upcycling, and a 18.8% relative gain for extending the MoE with a new expert by using limited finetuning data. This flexibility of Nexus is crucial to enable an open-source ecosystem where every user continuously assembles their own MoE-mix according to their needs.", "title_embedding_index": 11663, "title_abs_embedding_index": 11688}, {"title": "SUPERMERGE: An Approach For Gradient-Based Model Merging", "link_suffix": "/forum?id=lIdc5DUplq", "link": "https://openreview.net/forum?id=lIdc5DUplq", "pdf_link": "https://openreview.net/pdf?id=lIdc5DUplq", "keywords": "Model merging, Large language models, Multi-task Learning", "abstract": "Large language models, such as ChatGPT, Claude, or LLaMA, are gigantic, monolithic, and possess the superpower to simultaneously support thousands of tasks. However, high-throughput applications often prefer smaller task-specific models because of their lower latency and cost. One challenge of using task-specific models is the incremental need for solving newer tasks after the model is already deployed for existing tasks. A straightforward solution requires fine-tuning the model again for both existing and new tasks, which is computationally expensive and time-consuming. To address this issue, we propose a model merging based approach called SUPERMERGE. SUPERMERGE is a gradient-based method to systematically merge several fine-tuned models trained on existing and new tasks. SUPERMERGE is designed to be lightweight and fast, and the merged model achieves similar performance to fully fine-tuned models on all tasks. Furthermore, we proposed a hierarchical model merging strategy to reduce the peak space requirement without sacrificing the performance of the merged model. We experimentally demonstrate that SUPERMERGE outperforms existing model merging methods on common natural language processing and computer vision tasks.", "title_embedding_index": 11664, "title_abs_embedding_index": 11689}, {"title": "A Normalizing Flows based Difference-of-Entropies Estimator for Mutual Information", "link_suffix": "/forum?id=vgQmK5HHfz", "link": "https://openreview.net/forum?id=vgQmK5HHfz", "pdf_link": "https://openreview.net/pdf?id=vgQmK5HHfz", "keywords": "Normalizing flows, mutual information, generative models", "abstract": "Estimating Mutual Information (MI), a key measure of dependence of random quantities without specific modelling assumptions, is a challenging problem in high dimensions. We propose a novel mutual information estimator based on parametrizing conditional densities using normalizing flows, a deep generative model that has gained popularity in recent years. This estimator leverages a block autoregressive structure to achieve improved bias-variance trade-offs on standard benchmark tasks.", "title_embedding_index": 11665, "title_abs_embedding_index": 11690}, {"title": "LAMDA: Two-Phase Multi-Fidelity HPO via Learning Promising Regions from Data", "link_suffix": "/forum?id=wPStvOAtjR", "link": "https://openreview.net/forum?id=wPStvOAtjR", "pdf_link": "https://openreview.net/pdf?id=wPStvOAtjR", "keywords": "HPO, multi-fidelity, overlapping, promising regions", "abstract": "Hyperparameter Optimization (HPO) plays a critical role in machine learning, aiming to automatically find the best hyperparameter configurations to maximize model performance. Existing multi-fidelity HPO methods combine data from both high-fidelity (HF) and low-fidelity (LF) problems during the optimization process, aiding in effective sampling or preliminary screening. Despite this, they require exhaustive searches across the whole search space. Additionally, while approaches that incorporate prior knowledge can limit the search space, such knowledge is not always accessible. We have observed that high-quality solutions in HPO exhibit some overlapping between high- and low-fidelity problems. Bearing the above consideration in mind, this paper proposes a simple yet effective two-phase search framework named $\\texttt{Lamda}$ to streamline multi-fidelity HPO. Specifically, in the first phase, it searches in the LF landscape to identify the promising region therein. Thereafter, in the second phase, we transfer such promising regions to navigate the HPO in the HF landscape. Further, the $\\texttt{Lamda}$ framework is integrated with various HPO techniques to boost their performance. We demonstrate the rational of the framework by showcasing theoretical analysis towards the prior-based Bayesian optimization and bandit-based Hyperband. Empirically, we demonstrate the efficiency of this method across a range of HPO tasks.", "title_embedding_index": 11666, "title_abs_embedding_index": 11691}, {"title": "MOUCHI: Mitigating Over-forgetting in Unlearning Copyrighted Information", "link_suffix": "/forum?id=N2wPtFVK6o", "link": "https://openreview.net/forum?id=N2wPtFVK6o", "pdf_link": "https://openreview.net/pdf?id=N2wPtFVK6o", "keywords": "machine unlearning, LLM unlearning, derivative knowledge", "abstract": "Large language models are trained on massive internet datasets, which may inadvertently memorize illegal copyrighted content, making its inclusion unavoidable. Unlearning is a potential solution to remove such content. However, existing unlearning methods often suffer fromover-forgetting, where the process unintentionally erases knowledge similar to the copyrighted content that falls under fair use and should be preserved. To address this issue, we proposeMOUCHI, a novel unlearning framework that introduces the concept ofderivative knowledge, a subset of information derived from copyrighted content that must be retained during unlearning. MOUCHI first generates derivative knowledge and then incorporates a derivative loss function into the unlearning process to mitigate over-forgetting in unlearning copyrighted content. Due to its plug-and-play nature, MOUCHI can be effortlessly integrated into existing unlearning methods. Experimental results show that MOUCHI reduces unintended knowledge loss, improving performance byup to 145%compared to baseline methods when evaluated on the derivative set.", "title_embedding_index": 11667, "title_abs_embedding_index": 11692}, {"title": "Dimension-Independent Rates for Structured Neural Density Estimation", "link_suffix": "/forum?id=99YEbiBbdy", "link": "https://openreview.net/forum?id=99YEbiBbdy", "pdf_link": "https://openreview.net/pdf?id=99YEbiBbdy", "keywords": "density estimation, nonparametric density estimation, graphical model, nonparametric, neural network, deep learning, learning theory, Markov random field, generative model, convergence rate, image processing, curse of dimensionality", "abstract": "We show that deep neural networks achieve dimension-independent rates of convergence for learning structured densities such as those arising in image, audio, video, and text applications. More precisely, we show that neural networks with a simple $L^2$-minimizing loss achieve a rate of $n^{-1/(4+r)}$ in nonparametric density estimation when the underlying density is Markov to a graph whose maximum clique size is at most $r$, and we show that in the aforementioned applications, this size is typically constant, i.e., $r=O(1)$. We then show that the optimal rate in $L^1$ is $n^{-1/(2+r)}$ which, compared to the standard nonparametric rate of $n^{-1/(2+d)}$, shows that the effective dimension of such problems is the size of the largest clique in the Markov random field. These rates are independent of the data's ambient dimension, making them applicable to realistic models of image, sound, video, and text data. Our results provide a novel justification for deep learning's ability to circumvent the curse of dimensionality, demonstrating dimension-independent convergence rates in these contexts.", "title_embedding_index": 11668, "title_abs_embedding_index": 11693}, {"title": "Wide Neural Networks Trained with Weight Decay Provably Exhibit Neural Collapse", "link_suffix": "/forum?id=1HCN4pjTb4", "link": "https://openreview.net/forum?id=1HCN4pjTb4", "pdf_link": "https://openreview.net/pdf?id=1HCN4pjTb4", "keywords": "neural collapse, gradient descent training, weight decay, balancedness", "abstract": "Deep neural networks (DNNs) at convergence consistently represent the training data in the last layer via a highly symmetric geometric structure referred to as neural collapse. This empirical evidence has spurred a line of theoretical research aimed at proving the emergence of neural collapse, mostly focusing on the unconstrained features model. Here, the features of the penultimate layer are free variables, which makes the model data-agnostic and, hence, puts into question its ability to capture DNN training. Our work addresses the issue, moving away from unconstrained features and studying DNNs that end with at least two linear layers. We first prove generic guarantees on neural collapse that assume (i) low training error and balancedness of the linear layers (for within-class variability collapse), and (ii) bounded conditioning of the features before the linear part (for orthogonality of class-means, as well as their alignment with weight matrices). We then show that such assumptions hold for gradient descent training with weight decay: (i) for networks with a wide first layer, we prove low training error and balancedness, and (ii) for solutions that are either nearly optimal or stable under large learning rates, we additionally prove the bounded conditioning. Taken together, our results are the first to show neural collapse in the end-to-end training of DNNs.", "title_embedding_index": 11669, "title_abs_embedding_index": 11694}, {"title": "Conic Linear Units: Orthogonal Equivariance Improves General-Purpose Nonlinearities", "link_suffix": "/forum?id=UCttY1NZra", "link": "https://openreview.net/forum?id=UCttY1NZra", "pdf_link": "https://openreview.net/pdf?id=UCttY1NZra", "keywords": "Neural Network Architectures, Activation Functions", "abstract": "Most activation functions operate component-wise, which restricts the equivariance of neural networks to permutations. We introduce Conic Linear Units (CoLU) and generalize the symmetry of neural networks to continuous orthogonal groups. By interpreting ReLU as a projection onto its invariant set\u2014the positive orthant\u2014we propose a conic activation function that uses a Lorentz cone instead. Its performance can be further improved by considering multi-head structures, soft scaling, and axis sharing. CoLU associated with low-dimensional cones outperforms the component-wise ReLU in a wide range of models\u2014including MLP, ResNet, and UNet, etc, achieving better loss values and faster convergence. It significantly improves diffusion models's training and performance. CoLU originates from a first-principles approach to various forms of neural networks and fundamentally changes their algebraic structure.", "title_embedding_index": 11670, "title_abs_embedding_index": 11695}, {"title": "Fast and Accurate Blind Flexible Docking", "link_suffix": "/forum?id=iezDdA9oeB", "link": "https://openreview.net/forum?id=iezDdA9oeB", "pdf_link": "https://openreview.net/pdf?id=iezDdA9oeB", "keywords": "Blind Flexible Molecular Docking, Structure Prediction, AI4Science, Ligand-Protein Graph", "abstract": "Molecular docking that predicts the bound structures of small molecules (ligands) to their protein targets, plays a vital role in drug discovery. However, existing docking methods often face limitations: they either overlook crucial structural changes by assuming protein rigidity or suffer from low computational efficiency due to their reliance on generative models for structure sampling. To address these challenges, we propose FABFlex, a fast and accurate regression-based multi-task learning model designed for realistic blind flexible docking scenarios, where proteins exhibit flexibility and binding pocket sites are unknown (blind). Specifically, FABFlex's architecture comprises three specialized modules working in concert: (1) A pocket prediction module that identifies potential binding sites, addressing the challenges inherent in blind docking scenarios. (2) A ligand docking module that predicts the bound (holo) structures of ligands from their unbound (apo) states. (3) A pocket docking module that forecasts the holo structures of protein pockets from their apo conformations. Notably, FABFlex incorporates an iterative update mechanism that serves as a conduit between the ligand and pocket docking modules, enabling continuous structural refinements. This approach effectively integrates the three subtasks of blind flexible docking\u2014pocket identification, ligand conformation prediction, and protein flexibility modeling\u2014into a unified, coherent framework. Extensive experiments on public benchmark datasets demonstrate that FABFlex not only achieves superior effectiveness in predicting accurate binding modes but also exhibits a significant speed advantage (208$\\times$) compared to existing state-of-the-art methods. Our code is released at~\\url{https://anonymous.4open.science/r/FABFlex-51FC}.", "title_embedding_index": 11671, "title_abs_embedding_index": 11696}, {"title": "LADI v2: Multi-label Dataset and Classifiers for Low-Altitude Disaster Imagery", "link_suffix": "/forum?id=wD2sfTDy1W", "link": "https://openreview.net/forum?id=wD2sfTDy1W", "pdf_link": "https://openreview.net/pdf?id=wD2sfTDy1W", "keywords": "computer vision, dataset, disaster, aerial imagery, multilabel classification, damage assessment", "abstract": "ML-based computer vision models are promising tools for supporting emergency management operations following natural disasters. Imagery taken from small manned and unmanned aircraft can be available soon after a disaster and provide valuable information from multiple perspectives for situational awareness and damage assessment applications. However, emergency managers often face challenges in effectively utilizing this data due to the difficulties in finding the most relevant imagery among the tens of thousands of images that may be taken after an event. Despite this promise, there is still a lack of training data for imagery of this type from multiple perspectives and for multiple hazard types. To address this, we present the LADI v2 (Low Altitude Disaster Imagery version 2) dataset, a curated set of about 10,000 disaster images captured by the Civil Air Patrol (CAP) in response to over 100 federal emergency declarations (2015-2023) from over 30 US states and territories and annotated for multi-label classification by trained CAP volunteers. We also provide two pretrained baseline classifiers and compare their performance to state-of-the-art vision-language models in multi-label classification. The data and code are released publicly to support the development of computer vision models for emergency management research and applications.", "title_embedding_index": 11672, "title_abs_embedding_index": 11697}, {"title": "Formulating AutoML as a Variable-Length Optimization Problem: A Tree of Thought Approach with LLM-Driven Code Generation", "link_suffix": "/forum?id=ytn0rbIfOx", "link": "https://openreview.net/forum?id=ytn0rbIfOx", "pdf_link": "https://openreview.net/pdf?id=ytn0rbIfOx", "keywords": "AutoML, Tree of Thought, LLM", "abstract": "Recent advancements in machine learning have created a demand for automated systems that enable efficient development and deployment of machine learning applications. Traditional Automated Machine Learning (AutoML) approaches often rely on fixed pipeline structures, which limit adaptability to diverse task complexities. In this paper, we introduce a novel formulation of AutoML as a variable-length optimization problem, allowing for dynamic adjustment of model architectures based on task requirements. To effectively navigate the expanded search space of variable-length models, we employ the Tree of Thoughts (ToT) method combined with Large Language Models (LLMs). This framework utilizes a sequential decision-making process, allowing models to be incrementally constructed by evaluating prior outcomes. Additionally, LLMs automatically generate the code corresponding to each decision, transforming model configurations into executable pipelines and reducing manual intervention. Our approach enhances efficiency by focusing on promising pathways and improves transparency by explicitly showcasing how each decision contributes to the overall optimization. Experiments conducted on diverse datasets, including OpenML and clinical tasks, demonstrate that our method outperforms traditional AutoML systems, delivering superior model performance and better adaptability across different task complexities.", "title_embedding_index": 11673, "title_abs_embedding_index": 11698}, {"title": "Learning Disease Progression Models That Capture Health Disparities", "link_suffix": "/forum?id=yfkvUJEY6i", "link": "https://openreview.net/forum?id=yfkvUJEY6i", "pdf_link": "https://openreview.net/pdf?id=yfkvUJEY6i", "keywords": "fairness, equity, bias, health disparities, disease progression, bayesian model", "abstract": "Disease progression models are widely used to inform the diagnosis and treatment of many progressive diseases. However, a significant limitation of existing models is that they do not account for health disparities that can bias the observed data. To address this, we develop an interpretable Bayesian disease progression model that captures three key health disparities: certain patient populations may (1) start receiving care only when their disease is more severe, (2) experience faster disease progression even while receiving care, or (3) receive follow-up care less frequently conditional on disease severity. We show theoretically and empirically that failing to account for disparities produces biased estimates of severity (underestimating severity for disadvantaged groups, for example). On a dataset of heart failure patients, we show that our model can identify groups that face each type of health disparity, and that accounting for these disparities meaningfully shifts which patients are considered high-risk.", "title_embedding_index": 11674, "title_abs_embedding_index": 11699}]