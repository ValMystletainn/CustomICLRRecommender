[{"title": "GraphProp: Training the Graph Foundation Models using Graph Properties", "link_suffix": "/forum?id=7WgOB2nUaS", "link": "https://openreview.net/forum?id=7WgOB2nUaS", "pdf_link": "https://openreview.net/pdf?id=7WgOB2nUaS", "keywords": "Graph Foundation Models (GFM), graph transformer;graph property", "abstract": "In this work, we focus on training Graph Foundation Models (GFMs) for graph-level tasks like protein classification. Effective GFM training requires capturing information consistent across different domains. We have discovered that graph structures provide more consistent cross-domain information compared to node features and graph labels.\nHowever, traditional in-context learning methods primarily focus on transferring node features from various domains into a unified representation space but often lack structural cross-domain generalization.\nTo address this, we introduce a method called GraphProp, which emphasizes structural generalization. The GraphProp training process consists of two main phases: initially, it trains a structural GFM through the supervised prediction of graph structural properties. It then uses the structural representation from this GFM as positional encoding to train a comprehensive GFM. This phase of training utilizes in-context learning with domain-specific node features and graph labels to improve cross-domain node feature generalization.\nAdditionally, employing data augmentation in training the structural GFM helps address the scarcity of labeled graph data and facilitates explicit cross-domain structural generalization.\nOur experimental results demonstrate that GraphProp significantly outperforms traditional in-context learning methods, especially in handling graphs without node features.", "title_embedding_index": 16000, "title_abs_embedding_index": 16025}, {"title": "CIDA3D: Conformal Inference aided unsupervised Domain Adaptation for 3D-Aware Classification", "link_suffix": "/forum?id=XYFBmp08sP", "link": "https://openreview.net/forum?id=XYFBmp08sP", "pdf_link": "https://openreview.net/pdf?id=XYFBmp08sP", "keywords": "UDA, 3D pose estimation, 3D-Aware classification, occlusion, robustness", "abstract": "Cognitive Science studies show that human perception becomes robust to occlusions and other nuisances due to internal 3D representations of objects. This idea has been incorporated into computer vision models to improve their ability to understand and reason about the 3D world. However, collecting 3D annotations in vision datasets is expensive. This makes the robustness of the perception model to distribution shifts challenging. We introduce Conformal Inference aided unsupervised Domain Adaptation (CIDA)-3D for the complex setting of multiclass pose estimation. Our method adapts category level pose estimation (3D) models in nuisance ridden target domains directly from images without class label information, by harnessing uncertainty in model predictions (using conformal sets). This allows for significantly better and computationally efficient adaptation to target domains with synthetic and real-world noise. We also show a robust adaptation from fully synthetic data to complex real-world domains. To the best of our knowledge, this method is the first to attempt unsupervised domain adaptation for robust 3D-aware classification and multiclass pose estimation in real-world scenarios by adapting models trained on procedurally generated synthetic data.", "title_embedding_index": 16001, "title_abs_embedding_index": 16026}, {"title": "Understanding the Role of LLMs in Multimodal Evaluation Benchmarks", "link_suffix": "/forum?id=skHPtDnYGa", "link": "https://openreview.net/forum?id=skHPtDnYGa", "pdf_link": "https://openreview.net/pdf?id=skHPtDnYGa", "keywords": "Multi-modal Language Models, benchmark purification", "abstract": "The rapid advancement of Multimodal Large Language Models (MLLMs) has been accompanied by the development of various benchmarks to evaluate their capabilities.  However, the true nature of these evaluations and the extent to which they assess multimodal reasoning versus merely leveraging the underlying Large Language Model (LLM) backbone remain unclear. This paper presents a comprehensive investigation into the role of LLM backbones in MLLM evaluation, focusing on two critical aspects: the degree to which current benchmarks truly assess multimodal reasoning and the influence of LLM prior knowledge on performance. Specifically, we introduce a modified evaluation protocol to disentangle the contributions of the LLM backbone from multimodal integration, and an automatic knowledge identification technique for diagnosing whether LLMs equip the necessary knowledge for corresponding multimodal questions. Our study encompasses four diverse MLLM benchmarks and eight state-of-the-art MLLMs. Key findings reveal that some benchmarks allow high performance even without visual inputs and up to 50% of error rates can be attributed to insufficient world knowledge in the LLM backbone, indicating a heavy reliance on language capabilities. To address knowledge deficiencies, we propose a knowledge augmentation pipeline that achieves significant performance gains, with improvements of up to 60% on certain datasets, resulting in a approximately 4x increase in performance. Our work provides crucial insights into the role of the LLM backbone in MLLMs, and highlights the need for more nuanced benchmarking approaches.", "title_embedding_index": 16002, "title_abs_embedding_index": 16027}, {"title": "REBIND: Enhancing Ground-state Molecular Conformation Prediction via Force-Based Graph Rewiring", "link_suffix": "/forum?id=WNIEr5kydF", "link": "https://openreview.net/forum?id=WNIEr5kydF", "pdf_link": "https://openreview.net/pdf?id=WNIEr5kydF", "keywords": "molecular conformation prediction, molecule modeling, graph neural network", "abstract": "Predicting the ground-state 3D molecular conformations from 2D molecular graphs is critical in computational chemistry due to its profound impact on molecular properties. Deep learning (DL) approaches have recently emerged as promising alternatives to computationally-heavy classical methods such as density functional theory (DFT). However, we discover that existing DL methods inadequately model inter-atomic forces, particularly for non-bonded atomic pairs, due to their naive usage of bonds and pairwise distances. Consequently, significant prediction errors occur for atoms with low degree (i.e., low coordination numbers) whose conformations are primarily influenced by non-bonded interactions. To address this, we proposeReBind, a novel framework that rewires molecular graphs by adding edges based on the Lennard-Jones potential to capture non-bonded interactions for low-degree atoms. Experimental results demonstrate thatReBindsignificantly outperforms state-of-the-art methods across various molecular sizes, achieving up to a 20% reduction in prediction error.", "title_embedding_index": 16003, "title_abs_embedding_index": 16028}, {"title": "Does Diffusion Beat GAN in Image Super Resolution?", "link_suffix": "/forum?id=46mbA3vu25", "link": "https://openreview.net/forum?id=46mbA3vu25", "pdf_link": "https://openreview.net/pdf?id=46mbA3vu25", "keywords": "Image Super-Resolution, GANs, Diffusion Models, Generative Models, Deep Learning", "abstract": "There is a prevalent opinion that diffusion-based models outperform GAN-based counterparts in the Image Super Resolution (ISR) problem. However, in most studies, diffusion-based ISR models employ larger networks and are trained longer than the GAN baselines. This raises the question of whether the high performance stems from the superiority of the diffusion paradigm or if it is a consequence of the increased scale and the greater computational resources of the contemporary studies. In our work, we thoroughly compare diffusion-based and GAN-based super resolution models under controlled settings, with both approaches having matched architecture, model and dataset sizes, and computational budget. We show that a GAN-based model can achieve results comparable or superior to a diffusion-based model. Additionally, we explore the impact of popular design choices, such as text conditioning and augmentation on the performance of ISR models, showcasing their effect in several downstream tasks.", "title_embedding_index": 16004, "title_abs_embedding_index": 16029}, {"title": "FV-NeRV: Neural Compression for Free Viewpoint Videos", "link_suffix": "/forum?id=hrXt6Fdl2P", "link": "https://openreview.net/forum?id=hrXt6Fdl2P", "pdf_link": "https://openreview.net/pdf?id=hrXt6Fdl2P", "keywords": "Implicit Neural Representation, Free Viewpoint Video", "abstract": "The delivery of free viewpoint videos (FVVs) is gaining popularity because of their ability to provide freely switchable perspectives to remote users as immersive experiences. While smooth view switching is crucial for enhancing user's experiences, FVV delivery faces a significant challenge in balancing traffic and decoding latency. The typical approach sends limited viewpoints and synthesizes the remainings on the user, reducing traffic, but increasing decoding delays. Alternatively, sending more viewpoints reduces the delay, but requires more bandwidth for transmission.  In this paper, we propose a novel FVV representation format, Free Viewpoint-Neural Representation for Videos~(FV-NeRV), to address this dilemma in FVV delivery. FV-NeRV reduces both traffic and decoding delay even for content with a large number of virtual viewpoints by overfitting compact neural networks to all viewpoints and pruning and quantizing the trained model.   Experiments show that FV-NeRV achieves a comparable or even superior traffic reduction with a faster decoding speed compared to existing FVV codecs.", "title_embedding_index": 16005, "title_abs_embedding_index": 16030}, {"title": "Truncation Is All You Need: Improved Sampling Of Diffusion Models For Physics-Based Simulations", "link_suffix": "/forum?id=0FbzC7B9xI", "link": "https://openreview.net/forum?id=0FbzC7B9xI", "pdf_link": "https://openreview.net/pdf?id=0FbzC7B9xI", "keywords": "physics-based simulations, diffusion models, improved sampling", "abstract": "State-of-the-art Denoising Diffusion Probabilistic Models (DDPMs) rely on an expensive sampling process with a large Number of Function Evaluations (NFEs) to provide high-fidelity predictions. This computational bottleneck renders diffusion models less appealing as surrogates for the spatio-temporal prediction of physics-based problems with long rollout horizons. We propose Truncated Sampling Models, enabling single-step and few-step sampling with elevated fidelity by simple truncation of the diffusion process, reducing the gap between DDPMs and deterministic single-step approaches. We also introduce a novel approach, Iterative Refinement, to sample pre-trained DDPMs by reformulating the generative process as a refinement process with few sampling steps. Both proposed methods enable significant improvements in accuracy compared to DDPMs with NFEs $\\leq$ 10 on a diverse set of experiments, including incompressible and compressible turbulent flow and airfoil flow uncertainty simulations. Our proposed methods provide stable predictions for long rollout horizons in time-dependent problems and are able to learn all modes of the data distribution in steady-state problems with high uncertainty.", "title_embedding_index": 16006, "title_abs_embedding_index": 16031}, {"title": "QT-DoG: Quantization-Aware Training for Domain Generalization", "link_suffix": "/forum?id=EXnDAXyVxw", "link": "https://openreview.net/forum?id=EXnDAXyVxw", "pdf_link": "https://openreview.net/pdf?id=EXnDAXyVxw", "keywords": "Domain Generalization, Quantization, Ensemble, Network Compression, Flat Minima, Regularization", "abstract": "Domain Generalization (DG) aims to train models that perform well not only on the training (source) domains but also on novel, unseen target data distributions. A key challenge in DG is preventing overfitting to source domains, which can be mitigated by finding flatter minima in the loss landscape. In this work, we propose Quantization-aware Training for Domain Generalization (QT-DoG) and demonstrate that weight quantization effectively leads to flatter minima in the loss landscape, thereby enhancing domain generalization. Unlike traditional quantization methods focused on model compression, QT-DoG exploits quantization as an implicit regularizer by inducing noise in model weights, guiding the optimization process toward flatter minima that are less sensitive to perturbations and overfitting. We provide both theoretical insights and empirical evidence demonstrating that quantization inherently encourages flatter minima, leading to better generalization across domains. Moreover, with the benefit of reducing the model size through quantization, we demonstrate that an ensemble of multiple quantized models further yields superior accuracy than the state-of-the-art DG approaches with no computational or memory overheads. Our extensive experiments demonstrate that QT-DoG generalizes across various datasets, architectures, and quantization algorithms, and can be combined with other DG methods, establishing its versatility and robustness.", "title_embedding_index": 16007, "title_abs_embedding_index": 16032}, {"title": "SpikSSD: Better Extraction and Fusion for Object Detection with Spiking Neuron Networks", "link_suffix": "/forum?id=hpZ5zpudH8", "link": "https://openreview.net/forum?id=hpZ5zpudH8", "pdf_link": "https://openreview.net/pdf?id=hpZ5zpudH8", "keywords": "Spiking Neural Networks, Object Detection, Membrane-based Deformed Shortcut, Spiking Bi-direction Fusion Module", "abstract": "As the third generation of neural networks, Spiking Neural Networks (SNNs) have gained widespread attention due to their low energy consumption and biological interpretability. Recently, SNNs have made considerable advancements in computer vision. However, efficiently conducting feature extraction and fusion under the spiking characteristics of SNNs for object detection remains a pressing challenge. To address this problem, we propose the SpikSSD, a novel Spiking Single Shot Multibox Detector. Specifically, we design a full-spiking backbone network, MDS-ResNet, which effectively adjusts the membrane synaptic input distribution at each layer, achieving better spiking feature extraction. Additionally, for spiking feature fusion, we introduce the Spiking Bi-direction Fusion Module (SBFM), which for the first time realizes bi-direction fusion of spiking features, enhancing the multi-scale detection capability of the model. Experimental results show that SpikSSD achieves 40.8% mAP on the GEN1 dataset and 76.0%mAP@0.5on the VOC 2007 dataset with only around 10% firing rate, outperforming existing SNN-based approaches at ultralow energy consumption. This work sets a new benchmark for future research in SNN-based object detection. Our code is publicly available in supplementary materials.", "title_embedding_index": 16008, "title_abs_embedding_index": 16033}, {"title": "Large language models as windows on the mental structure of psychopathology", "link_suffix": "/forum?id=UXCfRU2Qs4", "link": "https://openreview.net/forum?id=UXCfRU2Qs4", "pdf_link": "https://openreview.net/pdf?id=UXCfRU2Qs4", "keywords": "LLMs, psychopathology, mental structure, computational psychiatry", "abstract": "How people represent the world determines how they act on it, as these internal representations bias what information is retrieved from memory, the inferences that are made and which actions are preferred. The structure of these representations are built through experience by extracting relevant information from the environment. Recent research has demonstrated that representational structure can also respond to the internal motives of agents, such as their aversion to uncertainty, which impacts their behavior. This opens the possibility to directly target internal structures to cause behavioral change in psychopathologies, one of the tenets of cognitive-behavioral therapy. For this purpose, it is crucial to understand how internal structures differ across psychopatologies. In this work, we show that Large Language Models (LLMs) could be viable tool to infer structural differences linked to distinct psychopathologies. We first demonstrate that we can reliably prompt LLMs to generate (verbal) behavior that can be detected as psychopathological by standard clinical assessment questionnaires. Next, we show that such prompting can capture correlational structure between the scores of diagnostic questionnaires observed in human data. We then analyze the lexical output patterns of LLMs  (a proxy of their internal representations) induced with distinct psychopathologies. This analysis allows us to generate several empirical hypotheses on the link between mental representation and psychopathologies. Finally, we illustrate the usefulness of our approach in a case study involving data from Schizophrenic patients. Specifically, we show that these patients and LLMs prompted to exhibit behavior related to schizophrenia generate qualitatively similar semantic structures. We suggest that our novel computational framework could expand our understanding of psychopathologies by creating novel research hypotheses, which might eventually lead to novel diagnostic tools.", "title_embedding_index": 16009, "title_abs_embedding_index": 16034}, {"title": "Amortized Posterior Sampling with Diffusion Prior Distillation", "link_suffix": "/forum?id=85VWxAwsaF", "link": "https://openreview.net/forum?id=85VWxAwsaF", "pdf_link": "https://openreview.net/pdf?id=85VWxAwsaF", "keywords": "Inverse Problems, Diffusion Models, Variational Inference", "abstract": "We propose Amortized Posterior Sampling (APS), a novel variational inference approach for efficient posterior sampling in inverse problems. Our method trains a conditional flow model to minimize the divergence between the variational distribution and the posterior distribution implicitly defined by the diffusion model. This results in a powerful, amortized sampler capable of generating diverse posterior samples with a single neural function evaluation, generalizing across various measurements. Unlike existing methods, our approach is unsupervised, requires no paired training data, and is applicable to both Euclidean and non-Euclidean domains. We demonstrate its effectiveness on a range of tasks, including image restoration, manifold signal reconstruction, and climate data imputation. APS significantly outperforms existing approaches in computational efficiency while maintaining competitive reconstruction quality, enabling real-time, high-quality solutions to inverse problems across diverse domains.", "title_embedding_index": 16010, "title_abs_embedding_index": 16035}, {"title": "Blind Inversion using Latent Diffusion Priors", "link_suffix": "/forum?id=jMffFIWHic", "link": "https://openreview.net/forum?id=jMffFIWHic", "pdf_link": "https://openreview.net/pdf?id=jMffFIWHic", "keywords": "Computational Imaging, Diffusion Model, Inverse Problem", "abstract": "Diffusion models have emerged as powerful tools for solving inverse problems due to their exceptional ability to model complex prior distributions. However, existing methods predominantly assume known forward operators (i.e., non-blind), limiting their applicability in practical settings where acquiring such operators is costly. Additionally, many current approaches rely on pixel-space diffusion models, leaving the potential of more powerful latent diffusion models (LDMs) underexplored. In this paper, we introduce LatentDEM, an innovative technique that addresses more challenging blind inverse problems using latent diffusion priors. At the core of our method is solving blind inverse problems within an iterative Expectation-Maximization (EM) framework: (1) the E-step recovers clean images from corrupted observations using LDM priors and a known forward model, and (2) the M-step estimates the forward operator based on the recovered images. Additionally, we propose two novel optimization techniques tailored for LDM priors and EM frameworks, yielding more accurate and efficient blind inversion results. As a general framework, LatentDEM supports both linear and non-linear inverse problems. Beyond common 2D image restoration tasks, it enables new capabilities in non-linear 3D inverse rendering problems. We validate LatentDEM's performance on representative 2D blind deblurring and 3D sparse-view reconstruction tasks, demonstrating its superior efficacy over prior arts.", "title_embedding_index": 16011, "title_abs_embedding_index": 16036}, {"title": "Regret measure in continuous time limit for a stochastic Multi-armed bandit problem", "link_suffix": "/forum?id=4jzjexvjI7", "link": "https://openreview.net/forum?id=4jzjexvjI7", "pdf_link": "https://openreview.net/pdf?id=4jzjexvjI7", "keywords": "Stochastic multi-armed bandit, Risk-sensitive regret, Hamilton-Jacobi-Bellman equation, Continuous time-limit", "abstract": "We study a class of stochastic multi-armed bandit problems with a risk-sensitive regret measure within a continuous limit setting. This problem is interesting when optimizing the expected reward is not the foremost objective, and the problem horizon is long. Through scaling the state parameters, including the number of pulls and cumulative reward for each arm we study the  bandit problem with infinite horizon, we delineate such risk using a Hamilton-Jacobi-Bellman equation with quadratic growth. Using this approach, we establish an explicit form of the optimal policy associated with the considered risk. As an application, we present examples where the results obtained in continuous time offer insights into the optimal policy for each case. Finally, numerical experiments confirm the theoretical results are presented.", "title_embedding_index": 16012, "title_abs_embedding_index": 16037}, {"title": "Enhance the Transferability of Adversarial Attacks through Channel Pruning", "link_suffix": "/forum?id=4NtrMSkvOy", "link": "https://openreview.net/forum?id=4NtrMSkvOy", "pdf_link": "https://openreview.net/pdf?id=4NtrMSkvOy", "keywords": "adversarial attacks transferability, channel pruning, model augmentation", "abstract": "Recent studies have shown that neural networks are vulnerable to adversarial attacks, where attackers generate adversarial samples by imposing tiny noise. The tiny noise can not misguide human perception, though leading the neural networks to generate wrong predictions. Transfer-based black-box attacks play a more significant role in recent studies due to their more realistic setting and considerable progress in performance. Previous studies have shown that some different channels of the same layer in convolution neural networks (CNN) contain lots of repetitive information, and we find that existing transferable attacks tend to exploit those redundant features more, which limits their transferability. Hence, we advocate using channel pruning and knowledge distillation to conduct model augmentation. In addition, we introduce a method of regularization on the gradients of intermediate feature maps of augmented models, which further enhances the transferability of our method. Comprehensive experiments demonstrate that imposing our method of model augmentation on existing methods can significantly improve the transferability of adversarial attacks in untargeted or targeted scenarios. Furthermore, our method outperforms state-of-the-art model augmentation techniques without the usage of additional training datasets.", "title_embedding_index": 16013, "title_abs_embedding_index": 16038}, {"title": "CATCH: Channel-Aware Multivariate Time Series Anomaly Detection via Frequency Patching", "link_suffix": "/forum?id=m08aK3xxdJ", "link": "https://openreview.net/forum?id=m08aK3xxdJ", "pdf_link": "https://openreview.net/pdf?id=m08aK3xxdJ", "keywords": "Multivariate Time Series, Anomaly Detection", "abstract": "Anomaly detection in multivariate time series is challenging as heterogeneous subsequence anomalies may occur. Reconstruction-based methods, which focus on learning nomral patterns in the frequency domain to detect diverse abnormal subsequences, achieve promising resutls, while still falling short on capturing fine-grained frequency characteristics and channel correlations. To contend with the limitations, we introduce CATCH, a framework based on frequency patching. We propose to patchify the frequency domain into frequency bands, which enhances its ability to capture fine-grained frequency characteristics. To perceive appropriate channel correlations, we propose a Channel Fusion Module (CFM), which features a patch-wise mask generator and a masked-attention mechanism. Driven by a bi-level multi-objective optimization algorithm, the CFM is encouraged to iteratively discover appropriate patch-wise channel correlations, and to cluster relevant channels while isolating adverse effects from irrelevant channels. Extensive experiments on 9 real-world datasets and 12 synthetic datasets demonstrate that CATCH achieves state-of-the-art performance. We make our code and datasets available athttps://anonymous.4open.science/r/CATCH-E535", "title_embedding_index": 16014, "title_abs_embedding_index": 16039}, {"title": "Swift Hydra:  Self-Reinforcing Generative Framework for Anomaly Detection with Multiple Mamba Models", "link_suffix": "/forum?id=P7t2niLbvw", "link": "https://openreview.net/forum?id=P7t2niLbvw", "pdf_link": "https://openreview.net/pdf?id=P7t2niLbvw", "keywords": "Anomaly detection, Reinforcement learning, Generative AI", "abstract": "Despite a plethora of anomaly detection models developed over the years, their ability to generalize to unseen anomalies remains an issue, particularly in critical systems. This paper aims to address this challenge by introducing Swift Hydra, a new framework for training an anomaly detection method based on generative AI and reinforcement learning (RL). Through featuring an RL policy that operates on the latent variables of a generative model, the framework synthesizes novel and diverse anomaly samples that are capable of bypassing a detection model. These generated synthetic samples are, in turn, used to augment the detection model, further improving its ability to handle challenging anomalies. Swift Hydra also incorporates Mamba models structured as a Mixture of Experts (MoE) to enable scalable adaptation of the number of Mamba experts based on data complexity, effectively capturing diverse feature distributions without increasing the model\u2019s inference time. Empirical evaluations on ADBench benchmark demonstrate that Swift Hydra  outperforms other state-of-the-art anomaly detection models while maintaining a relatively short inference time. From these results, our research highlights a new and auspicious paradigm of integrating RL and generative AI for advancing anomaly detection.", "title_embedding_index": 16015, "title_abs_embedding_index": 16040}, {"title": "GrEASE: Generalizable Spectral Embedding with an Application to UMAP", "link_suffix": "/forum?id=W7rProQocE", "link": "https://openreview.net/forum?id=W7rProQocE", "pdf_link": "https://openreview.net/pdf?id=W7rProQocE", "keywords": "Spectral Embedding, Eigenvectors separation, Visualization, Laplacian, UMAP", "abstract": "Spectral Embedding (SE) is a popular method for dimensionality reduction, applicable across diverse domains. Nevertheless, its current implementations face three prominent drawbacks which curtail its broader applicability: generalizability (i.e., out-of-sample extension), scalability, and eigenvectors separation. In this paper, we introduce $\\textit{GrEASE}$: Generalizable and Efficient Approximate Spectral Embedding, a novel deep-learning approach designed to address these limitations. GrEASE incorporates an efficient post-processing step to achieve eigenvectors separation, while ensuring both generalizability and scalability, allowing for the computation of the Laplacian\u2019s eigenvectors on unseen data. This method expands the applicability of SE to a wider range of tasks and can enhance its performance in existing applications. We empirically demonstrate GrEASE's ability to consistently approximate and generalize SE, while ensuring scalability. Additionally, we show how GrEASE can be leveraged to enhance existing methods. Specifically, we focus on UMAP, a leading visualization technique, and introduce $\\textit{NUMAP}$, a generalizable version of UMAP powered by GrEASE. Our code will be publicly available upon acceptance.", "title_embedding_index": 16016, "title_abs_embedding_index": 16041}, {"title": "PaI is getting competitive by training longer", "link_suffix": "/forum?id=8s1GMWsLlj", "link": "https://openreview.net/forum?id=8s1GMWsLlj", "pdf_link": "https://openreview.net/pdf?id=8s1GMWsLlj", "keywords": "sparse training, lottery ticket hypothesis, iterative pruning", "abstract": "The success of iterative pruning methods in achieving state-of-the-art sparse networks has largely been attributed to improved mask identification and an implicit regularization induced by pruning. We challenge this hypothesis and instead posit that their increased training epochs enable improved optimization. To verify this, we show that pruning at initialization (PaI) is significantly boosted by increased training epochs with repeating (cyclic) learning rate schedules akin to iterative pruning, even outperforming standard iterative pruning methods. The dominant mechanism how this is achieved, as we conjecture, can be attributed to a better exploration of the loss landscape leading to a lower training loss. However, at high sparsity, increased training alone is not enough for competitive performance. A strong coupling between learnt parameter initialization and mask seems to be required. Standard methods obtain this coupling via expensive pruning-training iterations, starting from a dense network. To achieve this with sparse training instead, we propose SCULPT-ing, i.e., cyclic training of any sparse mask followed by a single pruning step to couple the parameters and the mask, which is able to match the performance of state-of-the-art iterative pruning methods in the high sparsity regime at reduced computational cost.", "title_embedding_index": 16017, "title_abs_embedding_index": 16042}, {"title": "A2-DP: Annotation-aware Data Pruning for Object Detection", "link_suffix": "/forum?id=Pc94ncbkoo", "link": "https://openreview.net/forum?id=Pc94ncbkoo", "pdf_link": "https://openreview.net/pdf?id=Pc94ncbkoo", "keywords": "Object Detection, Data Pruning, Active Learning", "abstract": "As the size of datasets for training deep neural networks expands, data pruning has become an intriguing area of research due to its ability to achieve lossless performance with a reduced overall data volume. However, traditional data pruning usually demands complete dataset annotations, incurring high costs. To tackle this, we propose an innovative Annotation-Aware Data Pruning paradigm tailored for object detection, dubbed\nas $A^2$-DP, which aims to reduce the burdens of both annotation and storage. Our approach, consisting of two phases, integrates a hard sample mining module to extract crucial hidden objects, a class balance module to identify important objects in rare or challenging classes and a global similarity removal module that enhances the elimination of redundant information through object-level similarity assessments.\nExtensive experiments on 2D and 3D detection tasks validate the effectiveness of the $A^2$-DP, consistently achieving a minimum pruning rate of 20% across various datasets, showcasing the practical value and efficiency of our methods.", "title_embedding_index": 16018, "title_abs_embedding_index": 16043}, {"title": "Moment Constrained Optimal Transport for Control Applications", "link_suffix": "/forum?id=2kje23LSOE", "link": "https://openreview.net/forum?id=2kje23LSOE", "pdf_link": "https://openreview.net/pdf?id=2kje23LSOE", "keywords": "Optimal Transport, Mean Field Control, Signal Tracking", "abstract": "This paper concerns the application of techniques from optimal transport (OT) to mean field control,  in which the probability measures of interest in OT correspond to empirical distributions associated with a large collection of controlled agents. The control objective of interest motivates a one-sided relaxation of OT,  in which the first marginal is fixed and the second marginal is constrained to a  \u201cmoment class\u201d: a set of probability measures defined by generalized moment constraints. This relaxation is particularly interesting for control problems as it enables the coordination of agents without the need to know the desired distribution beforehand. The inclusion of an entropic regularizer is motivated by both computational considerations, and also to impose hard constraints on agent behavior. A computational approach inspired by the Sinkhorn algorithm is proposed to solve this problem. This new approach to distributed control is illustrated with an application of charging a fleet of electric vehicles while satisfying grid constraints. An online version is proposed and applied in a case study on the ElaadNL dataset containing 10,000 EV charging transactions in the Netherlands. This empirical validation demonstrates the effectiveness of the proposed approach to optimizing flexibility while respecting grid constraints.", "title_embedding_index": 16019, "title_abs_embedding_index": 16044}, {"title": "Select before Act: Spatially Decoupled Action Repetition for Continuous Control", "link_suffix": "/forum?id=PDgZ3rvqHn", "link": "https://openreview.net/forum?id=PDgZ3rvqHn", "pdf_link": "https://openreview.net/pdf?id=PDgZ3rvqHn", "keywords": "Reinforcement Learning, Action Repetition", "abstract": "Reinforcement Learning (RL) has achieved remarkable success in various continuous control tasks, such as robot manipulation and locomotion.\nDifferent to mainstream RL which makes decisions at individual steps, recent studies have incorporated action repetition into RL, achieving enhanced action persistence with improved sample efficiency and superior performance.\nHowever, existing methods treat all action dimensions as a whole during repetition, ignoring variations among them.\nThis constraint leads to inflexibility in decisions, which reduces policy agility with inferior effectiveness. \nIn this work, we propose a novel repetition framework called SDAR, which implements Spatially Decoupled Action Repetition through performing closed-loop act-or-repeat selection for each action dimension individually.\nSDAR achieves more flexible repetition strategies, leading to an improved balance between action persistence and diversity.\nCompared to existing repetition frameworks, SDAR is more sample efficient with higher policy performance and reduced action fluctuation.\nExperiments are conducted on various continuous control scenarios, \ndemonstrating the effectiveness and necessity of spatially decoupled repetition design proposed in this work.", "title_embedding_index": 16020, "title_abs_embedding_index": 16045}, {"title": "Optimizing Detection Techniques for High-Precision Icon Recognition in Sparse Feature Spaces", "link_suffix": "/forum?id=MuXF0UZsoW", "link": "https://openreview.net/forum?id=MuXF0UZsoW", "pdf_link": "https://openreview.net/pdf?id=MuXF0UZsoW", "keywords": "CNN, Feature Sparse Images, Contrastive Learning, Adversarial Training, Dynamic Margins, Attention Mechanisms", "abstract": "CNNs usually work well when they can extract progressively higher-level features through the layers. In small, low-resolution images, the depth of feature extraction is limited, leading to a sparsity in the feature space. Icon detection presents a unique challenge due to the small, feature-sparse nature of the target images, which often results in limited discriminative features. To address this, we propose an icon detection model based on a Siamese network architecture. This approach draws inspiration from face recognition frameworks. The modified architectures are aimed at being well-suited for distinguishing subtle differences between icon pairs. Given the relatively sparse feature space of these icons compared to larger images, we explore several enhancements to improve performance. Key innovations include the integration of attention mechanisms to focus on informative features, multi-scale feature extraction for better detail capture and contrastive learning. We additionally employ adversarial training to enhance performance. Additionally, we investigate dynamic margins in metric learning to model icon similarities. Self-supervised pretraining and Neural Architecture Search are employed to further refine and optimize the network. Our comprehensive evaluation demonstrates significant improvements in icon detection, highlighting the effectiveness of these advanced techniques in handling small, feature-sparse image data. This solution\noffers a valuable advancement in high-precision icon recognition, with potential applications in user interface design, software development, and digital asset management.", "title_embedding_index": 16021, "title_abs_embedding_index": 16046}, {"title": "Topological Blindspots: Understanding and Extending Topological Deep Learning Through the Lens of Expressivity", "link_suffix": "/forum?id=EzjsoomYEb", "link": "https://openreview.net/forum?id=EzjsoomYEb", "pdf_link": "https://openreview.net/pdf?id=EzjsoomYEb", "keywords": "Topological Deep Learning, Message Passing, Higher Order Message Passing, Expressivity, Graph Neural Networks, GNNs, Topology, Homology, Symmetry", "abstract": "Topological deep learning (TDL) is a rapidly growing field that seeks to leverage topological structure in data and facilitate learning from data supported on topological objects, ranging from molecules to 3D shapes. Most TDL architectures can be unified under the framework of higher-order message-passing (HOMP), which generalizes graph message-passing to higher-order domains. In the first part of the paper, we explore HOMP's expressive power from a topological perspective, demonstrating the framework's inability to capture fundamental topological and metric invariants such as diameter, orientability, planarity, and homology. In addition, we demonstrate HOMP's limitations in fully leveraging lifting and pooling methods on graphs. To the best of our knowledge, this is the first work to study the expressivity of TDL from a topological perspective. In the second part of the paper, we develop two new classes of architectures -- multi-cellular networks (MCN) and scalable MCN (SMCN) -- which draw inspiration from expressive GNNs. MCN can reach full expressivity, but scaling it to large data objects can be computationally expansive. Designed as a more scalable alternative, SMCN still mitigates many of HOMP's expressivity limitations. Finally, we design new benchmarks for evaluating models based on their ability to learn topological properties of complexes. We then evaluate SMCN on these benchmarks as well as on real-world graph datasets, demonstrating improvements over both HOMP baselines and expressive graph methods, highlighting the value of expressively leveraging topological information.", "title_embedding_index": 16022, "title_abs_embedding_index": 16047}, {"title": "Adversaries With Incentives:  A Strategic Alternative to Adversarial Robustness", "link_suffix": "/forum?id=U9j40EohfY", "link": "https://openreview.net/forum?id=U9j40EohfY", "pdf_link": "https://openreview.net/pdf?id=U9j40EohfY", "keywords": "adversarial training, strategic classification, adversarial robustness, strategic robustness", "abstract": "Adversarial training aims to defend againstadversaries: malicious opponents whose sole aim is to harm predictive performance in any way possible. This presents a rather harsh perspective, which we assert results in unnecessarily conservative training. As an alternative, we propose to model opponents as simply pursuing their own goals\u2014rather than working directly against the classifier. Employing tools from strategic modeling, our approach enables knowledge or beliefs regarding the opponent's possible incentives to be used as inductive bias for learning. Accordingly, our method ofstrategic trainingis designed to defend against all opponents within an `incentive uncertainty set'. This resorts to adversarial training when the set is maximal, but offers potential gains when the set can be appropriately reduced. We conduct a series of experiments that show how even mild knowledge regarding the opponent's incentives can be useful, and that the degree of potential gains depends on how these incentives relate to the structure of the learning task.", "title_embedding_index": 16023, "title_abs_embedding_index": 16048}, {"title": "BOND: Aligning LLMs with Best-of-N Distillation", "link_suffix": "/forum?id=0tAXMiSufG", "link": "https://openreview.net/forum?id=0tAXMiSufG", "pdf_link": "https://openreview.net/pdf?id=0tAXMiSufG", "keywords": "LLM, Alignment, RLHF, Best-of-N", "abstract": "Reinforcement learning from human feedback (RLHF) is a key driver of quality and safety in state-of-the-art large language models.\nYet, a surprisingly simple and strong inference-time strategy is Best-of-N sampling that selects the best generation among N candidates.\nIn this paper, we propose Best-of-N Distillation (BOND), a novel RLHF algorithm that seeks to emulate Best-of-N but without its significant computational overhead at inference time. Specifically, BOND is a distribution matching algorithm that forces the distribution of generations from the policy to get closer to the Best-of-N distribution. We use the Jeffreys divergence (a linear combination of forward and backward KL) to balance between mode-covering and mode-seeking behavior, and derive an iterative formulation that utilizes a moving anchor for efficiency. We demonstrate the effectiveness of our approach and several design choices through experiments on abstractive summarization and Gemma models.", "title_embedding_index": 16024, "title_abs_embedding_index": 16049}]