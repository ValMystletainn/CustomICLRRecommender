[{"title": "HuMouS: Human Motion Synthesis with Fine-Grained Control using Latent Space Manipulation of Cycle-Consistent Diffusion Models", "link_suffix": "/forum?id=QRERAL4r2k", "link": "https://openreview.net/forum?id=QRERAL4r2k", "pdf_link": "https://openreview.net/pdf?id=QRERAL4r2k", "keywords": "3d humans, human motion synthesis", "abstract": "We address the problem of spatially guided text-to-motion synthesis. While there has been work to incorporate spatial constraints in text-to-motion diffusion models, existing methods still face significant challenges in generating motions that align with the conditional controls. To this end, we propose Cycle Consistent Diffusion, a novel approach that improves controllable generation by explicitly optimizing frame-level cycle consistency between generated motions and conditional controls. Specifically, for an input conditional control, we ensure that the output motion and the input spatial constraint are forced to be consistent. A straightforward implementation though consistent with the input often does not match fine-grained control signals. To this end, we introduce a novel test-time optimization framework that directs our pre-trained cycle consistent diffusion model towards user-defined sparse constraints. We demonstrate approximately 5 to 10 percent improvement in controllability of motion synthesis on the HumanML3D dataset, while significantly reducing foot skating artifacts.", "title_embedding_index": 21850, "title_abs_embedding_index": 21875}, {"title": "ME-Switch: A Memory-Efficient Expert Switching Framework for Large Language Models", "link_suffix": "/forum?id=CKdlPUWDEE", "link": "https://openreview.net/forum?id=CKdlPUWDEE", "pdf_link": "https://openreview.net/pdf?id=CKdlPUWDEE", "keywords": "Large Language Model, Memory Efficient Compression", "abstract": "The typical process for LLM\u2019s development involves pre-training a general foundation model on massive data, followed by fine-tuning on task-specific data to obtain a series of specialized experts. Serving these experts can pose significant memory challenges, as loading all experts onto devices is impractical, and frequent switching between experts in response to user requests can incur substantial I/O costs. Previous approaches decompose the expert weights as the pre-trained weights plus delta weights, followed by quantizing the delta weights using output channel-wise step sizes to reduce the model size. However, these methods overlook the fact that certain input channels of delta weights can cause significant quantization errors at extremely low bitwidths. Additionally, existing methods assume that the appropriate model for a user request is known in advance, which is not the case in practice. To this end, we introduce ME-Switch, a memory-efficient expert switching framework tailored for serving multiple LLMs. To condense the number of bits required for describing the delta weights, we propose a salient-aware delta compression method that first identifies which input channels of delta weights are salient based on reconstruction error and then employs mixed-precision quantization that selectively quantizes non-salient input channels of delta weights to extremely low bits while keeping the salient ones intact, significantly reducing storage demand while maintaining performance. Moreover, we develop a model-level routing method that efficiently directs user queries to the most suitable expert by performing domain classification. Extensive experiments show the promising memory efficiency and routing performance of ME-Switch. For example, when serving three models from the Mistral-7B family, ME-Switch reduces the model size by 1.74$\\times$ and maintains nearly lossless performance on instruction, mathematical reasoning, and code generation tasks. Furthermore, our method can efficiently serve 16 Mistral-7B models on an NVIDIA A100 GPU.", "title_embedding_index": 21851, "title_abs_embedding_index": 21876}, {"title": "MutualNeRF: Improve the Performance of NeRF under Limited Samples with Mutual Information Theory", "link_suffix": "/forum?id=5RPpwW82vs", "link": "https://openreview.net/forum?id=5RPpwW82vs", "pdf_link": "https://openreview.net/pdf?id=5RPpwW82vs", "keywords": "nerf, mutual information, sparse view sampling, few-shot view synthesis", "abstract": "This paper introduces MutualNeRF, a framework enhancing Neural Radiance Field (NeRF) performance under limited samples using Mutual Information Theory. While NeRF excels in 3D scene synthesis, challenges arise with limited data and existing methods that aim to introduce prior knowledge lack theoretical support in a unified framework. We introduce a simple but theoretically robust concept, Mutual Information, as a metric to uniformly measure the correlation between images, considering both macro (semantic) and micro (pixel) levels.\n  For sparse view sampling, we strategically select additional viewpoints containing more non-overlapping scene information by minimizing mutual information without knowing the ground truth images beforehand. Our framework employs a greedy algorithm, offering a near-optimal solution for this task.\n  For few-shot view synthesis, we maximize the mutual information between inferred images and ground truth, expecting inferred images to gain more relevant information from known images. This is achieved by incorporating efficient, plug-and-play regularization terms.\n  Experiments under limited samples show consistent improvement over state-of-the-art baselines in different settings, affirming the efficacy of our framework.", "title_embedding_index": 21852, "title_abs_embedding_index": 21877}, {"title": "SceneFunctioner: Tailoring Large Language Model for Function-Oriented Interactive Scene Synthesis", "link_suffix": "/forum?id=IXFCPqFHMQ", "link": "https://openreview.net/forum?id=IXFCPqFHMQ", "pdf_link": "https://openreview.net/pdf?id=IXFCPqFHMQ", "keywords": "Scene Synthesis, Multi-Function Design, Large Language Model, User Interaction", "abstract": "With the Large Language Model (LLM) skyrocketing in recent years, an increasing body of research has focused on leveraging these models for 3D scene synthesis. However, most existing works do not emphasize homeowner's functional preferences, often resulting in scenes that are logically arranged but fall short of serving practical functions. To address this gap, we introduce SceneFunctioner, an interactive scene synthesis framework that tailors the LLM to prioritize functional requirements. The framework is interactive, enabling users to select functions and room shapes. SceneFunctioner first distributes these selected functions into separate areas called zones and determines the furniture for each zone. It then organizes the furniture into groups before arranging them within their respective zones to complete the scene design. Quantitative analyses and user studies showcase our framework\u2019s state-of-the-art performance in terms of both design quality and functional consistency with the user input.", "title_embedding_index": 21853, "title_abs_embedding_index": 21878}, {"title": "Efficient Time Series Processing for Transformers and State-Space Models through Token Merging", "link_suffix": "/forum?id=Qvo0RBDEwD", "link": "https://openreview.net/forum?id=Qvo0RBDEwD", "pdf_link": "https://openreview.net/pdf?id=Qvo0RBDEwD", "keywords": "Token Merging, Efficient Time Series Processing, Transformers, State-Space Models", "abstract": "Transformer architectures have shown promising results in time series processing. However, despite recent advances in subquadratic attention mechanisms or state-space models, processing very long sequences still imposes significant computational requirements. Token merging, which involves replacing multiple tokens with a single one calculated as their linear combination, has shown to considerably improve the throughput of vision transformer architectures while maintaining accuracy. In this work, we go beyond computer vision and perform the first investigations of token merging in time series analysis on both time series transformers and state-space models. We further introduce local merging, a domain-specific token merging algorithm that selectively combines tokens within a local neighborhood, achieving two major benefits:  a) Local merging can adjust its the computational complexity from quadratic to linear based on the neighborhood size to effectively scale token merging to long sequences; b) Local merging is the first causal merging scheme enabling token merging in transformer decoders. Our comprehensive empirical evaluation demonstrates that token merging offers substantial computational benefits with minimal impact on accuracy across various models and datasets. On the recently proposed Chronos foundation model, we achieve accelerations up to 5400% with only minor accuracy degradations.", "title_embedding_index": 21854, "title_abs_embedding_index": 21879}, {"title": "Active Audio Cancellation with Multi-Band Mamba Network", "link_suffix": "/forum?id=LwLaFpJpfM", "link": "https://openreview.net/forum?id=LwLaFpJpfM", "pdf_link": "https://openreview.net/pdf?id=LwLaFpJpfM", "keywords": "Active Noise Cancellation, Audio, Speech, Mamba", "abstract": "A novel deep learning approach for Active Audio Cancellation (AAC) is presented, which surpasses traditional Active Noise Cancellation (ANC) by effectively canceling any audio signal, regardless of its spectral content. We propose, for the first time, a deep learning approach to AAC using a novel multi-band Mamba architecture. This architecture partitions input audio into multiple frequency bands, allowing for precise anti-signal generation and enhanced phase alignment across frequencies, thereby improving overall cancellation performance. Additionally, we introduce an optimization-driven loss function that provides near-optimal supervisory signals for anti-signal generation. Our experimental results demonstrate substantial improvements over existing methods, achieving up to 7.2dB gain in ANC scenarios and up to 6.2dB improvement in AAC for voice audio signals, outperforming existing methods.", "title_embedding_index": 21855, "title_abs_embedding_index": 21880}, {"title": "HE-Drive: Human-Like End-to-End Driving with Vision Language Models", "link_suffix": "/forum?id=DWISGL63PC", "link": "https://openreview.net/forum?id=DWISGL63PC", "pdf_link": "https://openreview.net/pdf?id=DWISGL63PC", "keywords": "autonomous driving, motion planning, trajectory generation, diffusion model", "abstract": "In this paper, we propose HE-Drive: the first human-like-centric end-to-end autonomous driving system to generate trajectories that are both temporally consistent and comfortable. Recent studies have shown that imitation learning-based planners and learning-based trajectory scorers can effectively generate and select accuracy trajectories that closely mimic expert demonstrations. However, such trajectory planners and scorers face the dilemma of generating temporally inconsistent and uncomfortable trajectories. To solve the above problems, Our HE-Drive first extracts key 3D spatial representations through sparse perception, which then serves as conditional inputs for a Conditional Denoising Diffusion Probabilistic Models (DDPMs)-based motion planner to generate temporal consistency multi-modal trajectories. A Vision-Language Models (VLMs)-guided trajectory scorer subsequently selects the most comfortable trajectory from these candidates to control the vehicle, ensuring human-like end-to-end driving. Experiments show that HE-Drive not only achieves state-of-the-art performance (i.e., reduces the average collision rate by 71% than VAD) and efficiency (i.e., 1.9X faster than SparseDrive) on the challenging nuScenes and OpenScene datasets but also provides the most comfortable driving experience on real-world data.", "title_embedding_index": 21856, "title_abs_embedding_index": 21881}, {"title": "T3-S2S: Training-free Triplet Tuning for Sketch to Scene Generation", "link_suffix": "/forum?id=3m6VqesEMw", "link": "https://openreview.net/forum?id=3m6VqesEMw", "pdf_link": "https://openreview.net/pdf?id=3m6VqesEMw", "keywords": "Sketch-to-scene generation, training-free diffusion model, cross-attention mechnism", "abstract": "Scene generation is crucial to many computer graphics applications. Recent advances in generative AI have streamlined sketch-to-image workflows, easing the workload for artists and designers in creating scene concept art. However, these methods often struggle with complex scenes with multiple detailed objects, sometimes missing small or uncommon instances.\nIn this paper, we propose a Training-free Triplet Tuning for Sketch-to-Scene (T$^3$-S2S) generation after reviewing the entire cross-attention mechanism. This scheme revitalizes the existing ControlNet model, enabling effective handling of multi-instance generations, involving prompt balance, characteristics prominence, and dense tuning. \nSpecifically, this approach enhances keyword representation via the prompt balance module, reducing the risk of missing critical instances. It also includes a characteristics prominence module that highlights TopK indices in each channel, ensuring essential features are better represented based on token sketches. Additionally, it employs dense tuning to refine contour details in the attention map, compensating for instance-related regions.\nExperiments validate that our triplet tuning approach substantially improves the performance of existing sketch-to-image models. It consistently generates detailed, multi-instance 2D images, closely adhering to the input prompts and enhancing visual quality in complex multi-instance scenes.", "title_embedding_index": 21857, "title_abs_embedding_index": 21882}, {"title": "One-Prompt-One-Story: Free-Lunch Consistent Text-to-Image Generation Using a Single Prompt", "link_suffix": "/forum?id=cD1kl2QKv1", "link": "https://openreview.net/forum?id=cD1kl2QKv1", "pdf_link": "https://openreview.net/pdf?id=cD1kl2QKv1", "keywords": "diffusion model; consistent T2I image generation; storytelling", "abstract": "Text-to-image generation models can create high-quality images from input prompts. However, they struggle to support the consistent generation of identity-preserving requirements for storytelling. Existing approaches to this problem typically require extensive training in large datasets or additional modifications to the original model architectures. This limits their applicability across different domains and diverse diffusion model configurations. In this paper, we first observe the inherent capability of language models, coined $\\textit{context consistency}$, to comprehend identity through context with a single prompt. Drawing inspiration from the inherent $\\textit{context consistency}$, we propose a novel $\\textit{training-free}$ method for consistent text-to-image (T2I) generation, termed \"One-Prompt-One-Story\" ($\\textit{1Prompt1Story}$). Our approach $\\textit{1Prompt1Story}$ concatenates all prompts into a single input for T2I diffusion models, initially preserving character identities. We then refine the generation process using two novel techniques: $\\textit{Singular-Value\nReweighting}$ and $\\textit{Identity-Preserving Cross-Attention}$, ensuring better alignment with the input description for each frame. In our experiments, we compare our method against various existing consistent T2I generation approaches to demonstrate its effectiveness, through quantitative metrics and qualitative assessments.", "title_embedding_index": 21858, "title_abs_embedding_index": 21883}, {"title": "Robust Domain Generalisation with Causal Invariant Bayesian Neural Networks", "link_suffix": "/forum?id=7f5hNhzVAe", "link": "https://openreview.net/forum?id=7f5hNhzVAe", "pdf_link": "https://openreview.net/pdf?id=7f5hNhzVAe", "keywords": "Causality, Domain Generalisation, Variational Inference", "abstract": "Deep neural networks can obtain impressive performance on various tasks under the assumption that their training domain is identical to their target domain. Performance can drop dramatically when this assumption does not hold. One explanation for this discrepancy is the presence of spurious domain-specific correlations in the training data that the network exploits. Causal mechanisms, in the other hand, can be made invariant under distribution changes as they allow disentangling the factors of distribution underlying the data generation. Yet, learning causal mechanisms to improve out-of-distribution generalisation remains an under-explored area. We propose a Bayesian neural architecture that disentangles the learning of the the data distribution from the inference process mechanisms. We show theoretically and experimentally that our model approximates reasoning under causal interventions. We demonstrate the performance of our method, outperforming point estimate-counterparts, on out-of-distribution image recognition tasks where the data distribution acts as strong adversarial confounders.", "title_embedding_index": 21859, "title_abs_embedding_index": 21884}, {"title": "Beyond Expected Returns: A Policy Gradient Algorithm for Cumulative Prospect Theoretic Reinforcement Learning", "link_suffix": "/forum?id=Axc3ZD1Nds", "link": "https://openreview.net/forum?id=Axc3ZD1Nds", "pdf_link": "https://openreview.net/pdf?id=Axc3ZD1Nds", "keywords": "Cumulative prospect theory, policy gradient, policy optimization, reinforcement learning", "abstract": "The widely used expected utility theory has been shown to be empirically inconsistent with human preferences in the psychology and behavioral economy literatures. Cumulative Prospect Theory (CPT) has been developed to fill in this gap and provide a better model for human-based decision-making supported by empirical evidence. It allows to express a wide range of attitudes and perceptions towards risk, gains and losses. A few years ago, CPT has been combined with Reinforcement Learning (RL) to formulate a CPT policy optimization problem where the goal of the agent is to search for a policy generating long-term returns which are aligned with their preferences. In this work, we revisit this policy optimization problem and provide new insights on optimal policies and their nature depending on the utility function under consideration. We further derive a novel policy gradient theorem for the CPT policy optimization objective generalizing the seminal corresponding result in standard RL. This result enables us to design a model-free policy gradient algorithm to solve the CPT-RL problem. We illustrate the performance of our algorithm in simple examples motivated by traffic control and electricity management applications. We also demonstrate that our policy gradient algorithm scales better to larger state spaces compared to the existing zeroth order algorithm for solving the same problem.", "title_embedding_index": 21860, "title_abs_embedding_index": 21885}, {"title": "I-LLM: Efficient Integer-Only Inference for Fully-Quantized Low-Bit Large Language Models", "link_suffix": "/forum?id=44pbCtAdLx", "link": "https://openreview.net/forum?id=44pbCtAdLx", "pdf_link": "https://openreview.net/pdf?id=44pbCtAdLx", "keywords": "LLM Quantization, Large Language Models, Neural Network Compression", "abstract": "Post-training quantization (PTQ) serves as a potent technique to accelerate the inference of large language models (LLMs). Nonetheless, existing works still necessitate a considerable number of floating-point (FP) operations during inference, including additional quantization and de-quantization, as well as non-linear operators such as RMSNorm and Softmax. This limitation hinders the deployment of LLMs on the edge and cloud devices. In this paper, we identify the primary obstacle to integer-only quantization for LLMs lies in the large fluctuation of activations across channels and tokens in both linear and non-linear operations. To address this issue, we propose I-LLM, a novel integer-only fully-quantized PTQ framework tailored for LLMs. Specifically, (1) we develop Fully-Smooth Block-Reconstruction (FSBR) to aggressively smooth inter-channel variations of all activations and weights. (2) to alleviate degradation caused by inter-token variations, we introduce a novel approach called Dynamic Integer-only MatMul (DI-MatMul). This method enables dynamic quantization in full-integer matrix multiplication by dynamically quantizing the input and outputs with integer-only operations. (3) we design DI-ClippedSoftmax, DI-Exp, and DI-Normalization, which utilize bit shift to execute non-linear operators efficiently while maintaining accuracy. The experiment shows that our I-LLM achieves comparable accuracy to the FP baseline and outperforms non-integer quantization methods. For example, I-LLM can operate at W4A4 with negligible loss of accuracy. To our knowledge, we are the first to bridge the gap between integer-only quantization and LLMs.", "title_embedding_index": 21861, "title_abs_embedding_index": 21886}, {"title": "CFBD: COARSE-TO-FINE DETECTION OF BACKDOOR ATTACKS IN MULTIMODAL CONTRASTIVE LEARNING", "link_suffix": "/forum?id=CdqQKXGKq3", "link": "https://openreview.net/forum?id=CdqQKXGKq3", "pdf_link": "https://openreview.net/pdf?id=CdqQKXGKq3", "keywords": "backdoor", "abstract": "The backdoor attack in Multimodal Contrastive Learning (MCL) task has been receiving increasing attention in recent years, due to numerous downstream tasks that rely on pre-trained MCL models. Backdoor detection has been one of the effective protection solutions to fight against backdoor attacks. However, the majority of existing backdoor detection methods in MCL usually produces nonsatisfying detection results. Two main factors are responsible for this: 1) one-stage detection lacks subsequent dynamic adaptation to the distribution of poisoned and benign pairs when faced with different attacks, and 2) the criteria used in existing methods, specifically the cosine similarity between image and caption, are insufficient to distinguish between poisoned and benign pairs. To address these problems, we extend the conventional one-stage detection architecture to a two-stage architecture and propose a better metric in the second stage with high precision and high fault tolerance. To this end, we design a novel Coarse-to-Fine two-stage Backdoor Detection method, termed CFBD, which primarily focuses on multimodal learning involving image-caption relationships, such as CLIP. The objective of the coarse stage is to roughly partition dataset into poisoned, benign and suspicious subset. In the fine-grained stage, we use the average textual correlation with the poisoned subset to improve the detection quality. Extensive experiments demonstrate that CFBD achieves superior backdoor detection performance, e.g., almost 100% True Positive Rate (TPR) for diverse attacks over the large scale dataset CC-3M, markedly outperforming state-of-the-art methods.", "title_embedding_index": 21862, "title_abs_embedding_index": 21887}, {"title": "Regulatory DNA Sequence Design with Reinforcement Learning", "link_suffix": "/forum?id=F4IMiNhim1", "link": "https://openreview.net/forum?id=F4IMiNhim1", "pdf_link": "https://openreview.net/pdf?id=F4IMiNhim1", "keywords": "dna optimize, sequence optimize, autoregressive generative models, ai4science", "abstract": "Cis-regulatory elements (CREs), such as promoters and enhancers, are relatively short DNA sequences that directly regulate the expression of specific genes. The fitness of CRE, i.e., its functionality to enhance gene expression, highly depend on its nucleotide sequence, especially the composition of some special motifs known as transcription factor binding sites (TFBSs). Designing CREs to optimize their fitness is crucial for therapeutic and bioengineering applications. Existing CRE design methods often rely on simple strategies, such as iteratively introducing random mutations and selecting variants with high fitness from a large number of candidates through an oracle, i.e., a pre-trained gene expression prediction model. Due to the vast search space and lack of prior biological knowledge guidance, these methods are prone to getting trapped in local optima and tend to produce CREs with low diversity. In this paper, we propose the first method that leverages reinforcement learning (RL) to fine-tune a pre-trained autoregressive (AR) generative model for designing high-fitness cell-type-specific CREs while maintaining sequence diversity. We employ prior knowledge of CRE regulatory mechanisms to guide the optimization by incorporating the role of TFBSs into the RL process. In this way, our method encourages the removal of repressor motifs and the addition of activator motifs. We evaluate our method on enhancer design tasks for three distinct human cell types and promoter design tasks in two different yeast media conditions, demonstrating its effectiveness and robustness in generating high-fitness CREs.", "title_embedding_index": 21863, "title_abs_embedding_index": 21888}, {"title": "In-Context Editing: Learning Knowledge from Self-Induced Distributions", "link_suffix": "/forum?id=w6rHCuN3YG", "link": "https://openreview.net/forum?id=w6rHCuN3YG", "pdf_link": "https://openreview.net/pdf?id=w6rHCuN3YG", "keywords": "Knowledge Editing, In-Context Learning, Language Models", "abstract": "In scenarios where language models must incorporate new information efficiently without extensive retraining, traditional fine-tuning methods are prone to overfitting, degraded generalization, and unnatural language generation. To address these limitations, we introduce Consistent In-Context Editing (ICE), a novel approach leveraging the model's in-context learning capability to optimize towards a contextual distribution rather than a one-hot target. ICE introduces a simple yet effective optimization framework for the model to internalize new knowledge by aligning its output distributions with and without additional context. This method enhances the robustness and effectiveness of gradient-based tuning methods, preventing overfitting and preserving the model's integrity. We analyze ICE across four critical aspects of knowledge editing: accuracy, locality, generalization, and linguistic quality, demonstrating its advantages. Experimental results confirm the effectiveness of ICE and demonstrate its potential for continual editing, ensuring that the integrity of the model is preserved while updating information.", "title_embedding_index": 21864, "title_abs_embedding_index": 21889}, {"title": "FIPER: Generalizable Factorized Fields for Joint Image Compression and Super-Resolution", "link_suffix": "/forum?id=ga3DPo6BML", "link": "https://openreview.net/forum?id=ga3DPo6BML", "pdf_link": "https://openreview.net/pdf?id=ga3DPo6BML", "keywords": "Factor fields, Image compression, Super-resolution", "abstract": "In this work, we propose a unified representation for Super-Resolution (SR) and Image Compression, termedFactorized Fields, motivated by the shared principles between these two tasks. Both SISR and Image Compression require recovering and preserving fine image details\u2014whether by enhancing resolution or reconstructing compressed data. Unlike previous methods that mainly focus network architecture, our proposed approach utilizes a basis-coefficient decomposition to explicitly capture multi-scale visual features and structural components in images, addressing the core challenges of both tasks. We first derive our SR model, which includes Coefficient Backbone and Basis Swin Transformer for generalizable Factorized Fields. Then, to further unify these two tasks, we leverage the strong information-recovery capabilities of the trained SR modules as priors in the compression pipeline, improving both compression efficiency and detail reconstruction. Additionally, we introduce a merged-basis compression branch that consolidates shared structures, further optimizing the compression process. Extensive experiments show that our unified representation delivers state-of-the-art performance, achieving an average improvement of 204.4% over the baseline in Super-Resolution (SR) and 156.1% in Image Compression compared to the previous SOTA.", "title_embedding_index": 21865, "title_abs_embedding_index": 21890}, {"title": "SpectroMotion: Dynamic 3D Reconstruction of Specular Scenes", "link_suffix": "/forum?id=nvCJqxJS2Y", "link": "https://openreview.net/forum?id=nvCJqxJS2Y", "pdf_link": "https://openreview.net/pdf?id=nvCJqxJS2Y", "keywords": "Specular objects, Dynamic scenes, 3D Gaussian Splatting, Novel view synthesis", "abstract": "We present SpectroMotion, a novel approach that combines 3D Gaussian Splatting (3DGS) with physically-based rendering (PBR) and deformation fields to reconstruct dynamic specular scenes. Previous methods extending 3DGS to model dynamic scenes have struggled to accurately represent specular surfaces. Our method addresses this limitation by introducing a residual correction technique for accurate surface normal computation during deformation, complemented by a deformable environment map that adapts to time-varying lighting conditions. We implement a coarse-to-fine training strategy that significantly enhances both scene geometry and specular color prediction. We demonstrate that our model outperforms prior methods for view synthesis of scenes containing dynamic specular objects and that it is the only existing 3DGS method capable of synthesizing photorealistic real-world dynamic specular scenes, outperforming state-of-the-art methods in rendering complex, dynamic, and specular scenes.", "title_embedding_index": 21866, "title_abs_embedding_index": 21891}, {"title": "DiffIR2VR-Zero: Zero-Shot Video Restoration with Diffusion-based Image Restoration Models", "link_suffix": "/forum?id=qpDqO7qa3R", "link": "https://openreview.net/forum?id=qpDqO7qa3R", "pdf_link": "https://openreview.net/pdf?id=qpDqO7qa3R", "keywords": "Video restoration, Zero-shot, Training-free, Diffusion models", "abstract": "This paper introduces a method for zero-shot video restoration using pre-trained image restoration diffusion models. Traditional video restoration methods often need retraining for different settings and struggle with limited generalization across various degradation types and datasets. Our approach uses a hierarchical latent warping strategy for keyframes and local frames, combined with token merging that uses a hybrid correspondence mechanism that integrates spatial information, optical flow, and feature-based matching. We show that our method not only achieves top performance in zero-shot video restoration but also significantly surpasses trained models in generalization across diverse datasets and extreme degradations (8$\\times$ super-resolution and high-standard deviation video denoising). We present evidence through quantitative metrics and visual comparisons on various challenging datasets. Additionally, our technique works with any 2D restoration diffusion model, offering a versatile and powerful tool for video enhancement tasks without extensive retraining.", "title_embedding_index": 21867, "title_abs_embedding_index": 21892}, {"title": "DeNVeR: Deformable Neural Vessel Representations for Unsupervised Video Vessel Segmentation", "link_suffix": "/forum?id=30FCIyWWSU", "link": "https://openreview.net/forum?id=30FCIyWWSU", "pdf_link": "https://openreview.net/pdf?id=30FCIyWWSU", "keywords": "Video vessel segmentation, Unsupervised learning, X-ray angiography videos dataset", "abstract": "This paper presentsDeformableNeuralVesselRepresentations (DeNVeR), an unsupervised approach for vessel segmentation in X-ray angiography videos without annotated ground truth. DeNVeR utilizes optical flow and layer separation techniques, enhancing segmentation accuracy and adaptability through test-time training. Key contributions include a novel layer separation bootstrapping technique, a parallel vessel motion loss, and the integration of Eulerian motion fields for modeling complex vessel dynamics. A significant component of this research is the introduction of the XACV dataset, the first X-ray angiography coronary video dataset with high-quality, manually labeled segmentation ground truth. Extensive evaluations on both XACV and CADICA datasets demonstrate that DeNVeR outperforms current state-of-the-art methods in vessel segmentation accuracy and generalization capability while maintaining temporal coherency. This work advances medical imaging by providing a robust, data-efficient tool for vessel segmentation. It sets a new standard for video-based vessel segmentation research, offering greater flexibility and potential for clinical applications.", "title_embedding_index": 21868, "title_abs_embedding_index": 21893}, {"title": "FrugalNeRF: Fast Convergence for Few-shot Novel View Synthesis without Learned Priors", "link_suffix": "/forum?id=7YAgP1CR8u", "link": "https://openreview.net/forum?id=7YAgP1CR8u", "pdf_link": "https://openreview.net/pdf?id=7YAgP1CR8u", "keywords": "Neural rendering, Novel view synthesis, Few-shot NeRF", "abstract": "Neural Radiance Fields (NeRF) face significant challenges in few-shot scenarios, particularly due to overfitting and long training times for high-fidelity rendering. While current approaches like FreeNeRF and SparseNeRF use frequency regularization or pre-trained priors, they can be limited by complex scheduling or potential biases. We introduce FrugalNeRF, a novel few-shot NeRF framework that leverages weight-sharing voxels across multiple scales to efficiently represent scene details. Our key contribution is a cross-scale geometric adaptation training scheme that selects pseudo ground truth depth based on reprojection error from both training and novel views across scales. This guides training without relying on externally learned priors, allowing FrugalNeRF to fully utilize available data. While not dependent on pre-trained priors, FrugalNeRF can optionally integrate them for enhanced quality without affecting convergence speed. Our method generalizes effectively across diverse scenes and converges more rapidly than state-of-the-art approaches. Our experiments on standard LLFF, DTU, and RealEstate-10K datasets demonstrate that FrugalNeRF outperforms existing few-shot NeRF models, including those using pre-trained priors, while significantly reducing training time, making it a practical solution for efficient and accurate 3D scene reconstruction.", "title_embedding_index": 21869, "title_abs_embedding_index": 21894}, {"title": "Closed-loop Diffusion Control of Complex Physical Systems", "link_suffix": "/forum?id=PiHGrTTnvb", "link": "https://openreview.net/forum?id=PiHGrTTnvb", "pdf_link": "https://openreview.net/pdf?id=PiHGrTTnvb", "keywords": "physical systems control, closed-loop control, PDE, physical simulation, generative models", "abstract": "The control problems of complex physical systems have broad applications in science and engineering.  Previous studies have shown that generative control methods based on diffusion models offer significant advantages for solving these problems. However, existing generative control approaches face challenges in both performance and efficiency when extended to the closed-loop setting, which is essential for effective control. In this paper, we propose an efficient Closed-Loop Diffusion method for Physical systems Control (CL-DiffPhyCon). By employing an asynchronous denoising framework for different physical time steps, CL-DiffPhyCon generates control signals conditioned on real-time feedback from the environment with significantly reduced computational cost during sampling. Additionally, the control process could be further accelerated by incorporating fast sampling techniques, such as DDIM. We evaluate CL-DiffPhyCon on two tasks: 1D Burgers' equation control and 2D incompressible fluid control. The results demonstrate that CL-DiffPhyCon achieves superior control performance with significant improvements in sampling efficiency.", "title_embedding_index": 21870, "title_abs_embedding_index": 21895}, {"title": "Towards Universal Mono-to-Binaural Speech Synthesis", "link_suffix": "/forum?id=S8VFVe6MWL", "link": "https://openreview.net/forum?id=S8VFVe6MWL", "pdf_link": "https://openreview.net/pdf?id=S8VFVe6MWL", "keywords": "Binaural audio, sound spatialization, neural sound synthesis, binaural speech", "abstract": "We consider the problem of synthesis of binaural speech from mono audio in arbitrary environments, which is important for modern telepresence and extended-reality applications. We find that existing neural mono-to-binaural methods are overfit to non-spatial acoustic properties, via analysis using a new benchmark (TUT Mono-to-Binaural), the first introduced since the original dataset of Richard et al. (2021). While these past methods focus on learning neural geometric transforms of monaural audio, we propose BinauralZero, a strong initial baseline for universal mono-to-binaural synthesis, which can subjectively match or outperform existing state-of-the-art neural mono-to-binaural renderers trained in their target environment despite never seeing any binaural data. It leverages the surprising discovery that an off-the-shelf mono audio denoising model can competently enhance the initial binauralization given by simple parameter-free transforms. We perform comprehensive ablations to understand how BinauralZero bridges the representation gap between mono and binaural audio, and analyze how current mono-to-binaural automated metrics are decorrelated from human ratings.", "title_embedding_index": 21871, "title_abs_embedding_index": 21896}, {"title": "OSTQuant: Refining Large Language Model Quantization with Orthogonal and Scaling Transformations for Better Distribution Fitting", "link_suffix": "/forum?id=rAcgDBdKnP", "link": "https://openreview.net/forum?id=rAcgDBdKnP", "pdf_link": "https://openreview.net/pdf?id=rAcgDBdKnP", "keywords": "Large Language Models, Quantization", "abstract": "Post-training quantization (PTQ) has emerged as a widely adopted technique for\ncompressing and accelerating Large Language Models (LLMs). The major chal-\nlenge in LLM quantization is that uneven and heavy-tailed data distributions can\nexpand the quantization range, thereby reducing bit precision for most values.\nRecent methods attempt to eliminate outliers and balance inter-channel differences\nby employing linear transformations; however, they remain heuristic and are often\noverlook optimizing the data distribution across the entire quantization space. In\nthis paper, we introduce Quantization Space Utilization Rate (QSUR), a novel\nmetric that effectively assesses the quantizability of transformed data by measur-\ning the space utilization of the data in the quantization space. We complement\nQSUR with mathematical derivations that examine the effects and limitations\nof various transformations, guiding our development of Orthogonal and Scaling\nTransformation-based Quantization (OSTQuant). OSTQuant employs a learn-\nable equivalent transformation, consisting of an orthogonal transformation and a\nscaling transformation, to optimize the distributions of weights and activations\nacross the entire quantization space. Futhermore, we propose the KL-Top loss\nfunction, designed to mitigate noise during optimization while retaining richer se-\nmantic information within the limited calibration data imposed by PTQ. OSTQuant\noutperforms existing work on various LLMs and benchmarks. In the W4-only\nsetting, it retains 99.5% of the floating-point accuracy. In the more challenging\nW4A4KV4 configuration, OSTQuant reduces the performance gap by 32% on the\nLLaMA-3-8B model compared to state-of-the-art (SOTA) methods.", "title_embedding_index": 21872, "title_abs_embedding_index": 21897}, {"title": "Distribution Backtracking Builds A Faster Convergence Trajectory for Diffusion Distillation", "link_suffix": "/forum?id=2ySt3cdGfJ", "link": "https://openreview.net/forum?id=2ySt3cdGfJ", "pdf_link": "https://openreview.net/pdf?id=2ySt3cdGfJ", "keywords": "Diffusion Model, Diffusion Distillation, One-step Generation", "abstract": "Accelerating the sampling speed of diffusion models remains a significant challenge. Recent score distillation methods distill a heavy teacher model into a student generator to achieve one-step generation, which is optimized by calculating the difference between two score functions on the samples generated by the student model.\nHowever, there is a score mismatch issue in the early stage of the score distillation process, since existing methods mainly focus on using the endpoint of pre-trained diffusion models as teacher models, overlooking the importance of the convergence trajectory between the student generator and the teacher model.\nTo address this issue, we extend the score distillation process by introducing the entire convergence trajectory of the teacher model and propose $\\textbf{Dis}$tribution $\\textbf{Back}$tracking Distillation ($\\textbf{DisBack}$). DisBask is composed of two stages: $\\textit{Degradation Recording}$ and $\\textit{Distribution Backtracking}$. \n$\\textit{Degradation Recording}$ is designed to obtain the convergence trajectory by recording the degradation path from the pre-trained teacher model to the untrained student generator.\nThe degradation path implicitly represents the intermediate distributions between the teacher and the student, and its reverse can be viewed as the convergence trajectory from the student generator to the teacher model.\nThen $\\textit{Distribution Backtracking}$ trains the student generator to backtrack the intermediate distributions along the path to approximate the convergence trajectory of the teacher model.\nExtensive experiments show that DisBack achieves faster and better convergence than the existing distillation method and achieves comparable or better generation performance, with an FID score of 1.38 on the ImageNet 64$\\times$64 dataset.\nDisBack is easy to implement and can be generalized to existing distillation methods to boost performance.", "title_embedding_index": 21873, "title_abs_embedding_index": 21898}, {"title": "Pursuing Better Decision Boundaries for Long-Tailed Object Detection via Category Information Amount", "link_suffix": "/forum?id=LW55JrLYPg", "link": "https://openreview.net/forum?id=LW55JrLYPg", "pdf_link": "https://openreview.net/pdf?id=LW55JrLYPg", "keywords": "Long-tailed recognition, Class Imbalanced, Image processing", "abstract": "In object detection, the number of instances is commonly used to determine whether a dataset follows a long-tailed distribution, implicitly assuming that the model will perform poorly on categories with fewer instances. This assumption has led to extensive research on category bias in datasets with imbalanced instance distributions. However, even in datasets with relatively balanced instance counts, models still exhibit bias toward certain categories, indicating that instance count alone cannot explain this phenomenon. In this work, we first introduce the concept and measurement of category informativeness. We observe a significant negative correlation between a category\u2019s informativeness and its accuracy, suggesting that informativeness more accurately reflects the learning difficulty of a category. Based on this observation, we propose the Informativeness-Guided Angular Margin Loss (IGAM Loss), which dynamically adjusts the decision space of categories according to their informativeness, thereby mitigating category bias in long-tailed datasets. IGAM Loss not only achieves superior performance on long-tailed benchmark datasets such as LVIS v1.0 and COCO-LT but also demonstrates significant improvements for underrepresented categories in non-long-tailed datasets like Pascal VOC. Extensive experiments confirm the potential of category informativeness as a tool and the generalizability of our proposed method.", "title_embedding_index": 21874, "title_abs_embedding_index": 21899}]