[
    {
        "title": "Revisiting Positional Information in Transformers in the era of Fused Attention",
        "link_suffix": "/forum?id=1Iq1qIsc2s",
        "link": "https://openreview.net/forum?id=1Iq1qIsc2s",
        "pdf_link": "https://openreview.net/pdf?id=1Iq1qIsc2s",
        "keywords": "Efficient Vision Transformers, Position Embeddings, CUDA",
        "abstract": "Imparting positional information has been a crucial component in Transformers due to attention's invariance to permutation. Methods that bias attention weights, like Relative Positional Bias (RPB), have been preferred choice in more recent transformer-based architectures for vision. In parallel, fused attention has become the standard implementation for attention, largely thanks to open source solutions such as Flash Attention and FMHA. However, it is not trivial to fuse explicit biasing or masking of attention weights into a fused attention kernel without affecting its performance. In this scenario, position embeddings present themselves as a viable replacement for attention weight biases. Position embeddings are applied to the tokens directly, decoupled from the attention mechanism, thereby sidestepping the problems that arise with attention weight biases in fused kernels. In this work, inspired by the booming LLM landscape, we analyze the applicability of Rotary Position Embeddings (RoPE) as a replacement for RPBs in vision models. Unlike RPB which explicitly biases attention weights, RoPE biases the dot product inputs (query and key) directly and ahead of the attention operation. We empirically show the prowess of RoPE over RPBs in terms of accuracy and speed. We study multiple implementations of RoPE and show that it is sufficient to use only a fraction of hidden dimensions for RoPE to achieve competitive performance. We also develop a fast implementation for Axial RoPE. Together with the most performant fused attention implementations, and our fast RoPE implementation, we observe  inference speedups compared to RPB with improved or similar accuracy.  We foresee RoPE as a replacement for RPBs, paving the way for the widespread adoption of fused attention in transformer-based vision models."
    },
    {
        "title": "Deep Generative Modeling for Identification of Noisy, Non-Stationary Dynamical Systems",
        "link_suffix": "/forum?id=1762Fbr4HK",
        "link": "https://openreview.net/forum?id=1762Fbr4HK",
        "pdf_link": "https://openreview.net/pdf?id=1762Fbr4HK",
        "keywords": "system identification, non-autonomous differential equations, dynamical systems, variational inference, variational autoencoders, SINDy, sparse regression, uncertainty quantification, latent variable discovery, biophysics applications, biology, neuroscience",
        "abstract": "An important challenge in many fields of science and engineering is making sense of time-dependent measurement data by recovering governing equations in the form of differential equations. We focus on finding parsimonious ordinary differential equation (ODE) models for nonlinear, noisy, and non-autonomous dynamical systems and propose a machine learning method for data-driven system identification. While many methods tackle noisy and limited data, non-stationarity – where differential equation parameters change over time – has received less attention. Our method, dynamic SINDy, combines variational inference with SINDy (sparse identification of nonlinear dynamics) to model time-varying coefficients of sparse ODEs. This framework allows for uncertainty quantification of ODE coefficients,\nexpanding on previous methods for autonomous systems. These coefficients are then interpreted as latent variables and added to the system to obtain an autonomous dynamical model. We validate our approach using synthetic data, including nonlinear oscillators and the Lorenz system, and apply it to neuronal activity data from C. elegans. Dynamic SINDy uncovers a global nonlinear model, showing it can\nhandle real, noisy, and chaotic datasets. We aim to apply our method to a wide range of problems, specifically to dynamic systems where complex parametric time dependencies are expected."
    },
    {
        "title": "Efficient Model-Based Reinforcement Learning Through Optimistic Thompson Sampling",
        "link_suffix": "/forum?id=Ian00SaFHg",
        "link": "https://openreview.net/forum?id=Ian00SaFHg",
        "pdf_link": "https://openreview.net/pdf?id=Ian00SaFHg",
        "keywords": "reinforcement learning, model-based reinforcement learning, optimistic exploration",
        "abstract": "Learning complex robot behavior through interactions with the environment necessitates principled exploration. Effective strategies should prioritize exploring regions of the state-action space that maximize rewards, with optimistic exploration emerging as a promising direction aligned with this idea and enabling sample-efficient reinforcement learning. However, existing methods overlook a crucial aspect: the need for optimism to be informed by a belief connecting the reward and state. To address this, we propose a practical, theoretically grounded approach to optimistic exploration based on Thompson sampling. Our model structure is the first that allows for reasoning aboutjointuncertainty over transitions and rewards. We apply our method on a set of MuJoCo and VMAS continuous control tasks. Our experiments demonstrate that optimistic exploration significantly accelerates learning in environments with sparse rewards, action penalties, and difficult-to-explore regions. Furthermore, we provide insights into when optimism is beneficial and emphasize the critical role of model uncertainty in guiding exploration."
    },
    {
        "title": "SPEED: Selective Prediction for Early Exit DNNs",
        "link_suffix": "/forum?id=sceqRsa0oo",
        "link": "https://openreview.net/forum?id=sceqRsa0oo",
        "pdf_link": "https://openreview.net/pdf?id=sceqRsa0oo",
        "keywords": "Selective prediction; Early Exits",
        "abstract": "Inference latency and trustworthiness of Deep Neural Networks (DNNs) are the bottlenecks in deploying them in critical applications like autonomous driving.  Early Exit (EE) DDNs overcome the latency issues by allowing samples to exit from intermediary layers if they attain high confidence scores on the predicted class. However, the DNNs are known to exhibit overconfidence, which can lead to many samples exiting early and render EE strategies untrustworthy. We use Selective Prediction (SP) to overcome this issue by checking the hardness of the samples rather than just relying on the confidence score alone.  We propose SPEED, a novel approach that uses Deferral Classifiers (DCs) at each layer to check the hardness of samples before performing EEs. The DCs at each layer identify if a sample is hard and either differ its inference to the next layer or directly send it to an expert. Early detection of hard samples and using an expert for inference prevents the wastage of computational resources and improves trust. We also investigate the generalization capability of DCs trained on one domain when applied to other domains where target domain data is not readily available. We observe that EE aided with SP improves both accuracy and latency. Our method minimizes the risk by 50% with a speedup of $2.05\\times$ as compared to the final layer. The anonymized source code is available athttps://anonymous.4open.science/r/SPEED-35DC/README.md."
    },
    {
        "title": "Mitigating Memorization in Language Models",
        "link_suffix": "/forum?id=MGKDBuyv4p",
        "link": "https://openreview.net/forum?id=MGKDBuyv4p",
        "pdf_link": "https://openreview.net/pdf?id=MGKDBuyv4p",
        "keywords": "language models, memorization, machine unlearning, regularization, fine-tuning, natural language processing",
        "abstract": "Language models (LMs) can “memorize” information, i.e., encode training data in their weights in such a way that inference-time queries can lead to verbatim regurgitation of that data. This ability to extract training data can be problematic, for example, when data are private or sensitive. In this work, we investigate methods to mitigate memorization: three regularizer-based, three fine-tuning-based, and eleven machine unlearning-based methods, with five of the latter being new methods that we introduce. We also introduce TinyMem, a suite of small, computationally-efficient LMs for the rapid development and evaluation of memorization-mitigation methods. We demonstrate that the mitigation methods that we develop using TinyMem can successfully be applied to production-grade LMs, and we determine via experiment that: regularizer-based mitigation methods are slow and ineffective at curbing memorization; fine-tuning-based methods\nare effective at curbing memorization, but overly expensive, especially for retaining higher accuracies; and unlearning-based methods are faster and more effective, allowing for the precise localization and removal of memorized information from LM weights prior to inference. We show, in particular, that our proposed unlearning method BalancedSubnet outperforms other mitigation methods at removing\nmemorized information while preserving performance on target tasks."
    },
    {
        "title": "Detecting Out-of-Distribution through the Lens of Neural Collapse",
        "link_suffix": "/forum?id=Gr8nHvOivO",
        "link": "https://openreview.net/forum?id=Gr8nHvOivO",
        "pdf_link": "https://openreview.net/pdf?id=Gr8nHvOivO",
        "keywords": "Out-of-Distribution Detection",
        "abstract": "Out-of-Distribution (OOD) detection is essential for safe deployment; however, existing detectors exhibit generalization discrepancies and cost concerns.   To address this, we propose a highly versatile and efficient OOD detector inspired by the trend of Neural Collapse on practical models, without requiring complete collapse.  By analyzing this trend, we discover that features of in-distribution (ID) samples cluster closer to the weight vectors compared to features of OOD samples. Additionally, we reveal that ID features tend to expand in space to structure a simplex Equiangular Tight Framework, which explains the prevalent observation that ID features reside further from the origin than OOD features. Taking both insights from Neural Collapse into consideration, our OOD detector utilizes feature proximity to weight vectors and further complements this perspective by using feature norms to filter OOD samples. Extensive experiments on \\emph{off-the-shelf} models demonstrate the efficiency and effectiveness of our OOD detector across diverse classification tasks and model architectures, mitigating generalization discrepancies and improving \\emph{overall} performance."
    },
    {
        "title": "Directed Structural Adaptation to Overcome Statistical Conflicts and Enable Continual Learning",
        "link_suffix": "/forum?id=ZHTYtXijEn",
        "link": "https://openreview.net/forum?id=ZHTYtXijEn",
        "pdf_link": "https://openreview.net/pdf?id=ZHTYtXijEn",
        "keywords": "structural adaptation, continual learning, growth",
        "abstract": "Adaptive networks today rely on overparameterized fixed topologies that cannot break through the statistical conflicts they encounter in the data they are exposed to, and are prone to \"catastrophic forgetting\" as the network attempts to reuse the existing structures to learn new task. We propose a structural adaptation method, DIRAD, that can complexify as needed and in a directed manner without being limited by statistical conflicts within a dataset. We then extend this method and present the PREVAL framework, designed to prevent \"catastrophic forgetting\" in continual learning by detection of new data and assigning encountered data to suitable models adapted to process them, without needing task labels anywhere in the workflow. We show the reliability of the DIRAD in growing a network with high performance and orders-of-magnitude simpler than fixed topology networks; and demonstrate the proof-of-concept operation of PREVAL, in which continual adaptation to new tasks is observed while being able to detect and discern previously-encountered tasks."
    },
    {
        "title": "Latent Safety-Constrained Policy Approach for Safe Offline Reinforcement Learning",
        "link_suffix": "/forum?id=bDt5qc7TfO",
        "link": "https://openreview.net/forum?id=bDt5qc7TfO",
        "pdf_link": "https://openreview.net/pdf?id=bDt5qc7TfO",
        "keywords": "Safe RL, Offline RL, Variational Autoencoders, Latent Safety Constraints",
        "abstract": "In safe offline reinforcement learning, the objective is to develop a policy that maximizes cumulative rewards while strictly adhering to safety constraints, utilizing only offline data. Traditional methods often face difficulties in balancing these constraints, leading to either diminished performance or increased safety risks. We address these issues with a novel approach that begins by learning a conservatively safe policy through the use of Conditional Variational Autoencoders, which model the latent safety constraints. Subsequently, we frame this as a Constrained Reward-Return Maximization problem, wherein the policy aims to optimize rewards while complying with the inferred latent safety constraints. This is achieved by training an encoder with a reward-Advantage Weighted Regression objective within the latent constraint space. Our methodology is supported by theoretical analysis, including bounds on policy performance and sample complexity. Extensive empirical evaluation on benchmark datasets, including challenging autonomous driving scenarios, demonstrates that our approach not only maintains safety compliance but also excels in cumulative reward optimization, surpassing existing methods. Additional visualizations provide further insights into the effectiveness and underlying mechanisms of our approach."
    },
    {
        "title": "Verbosity≠Veracity: Demystify Verbosity Compensation Behavior of Large Language Models",
        "link_suffix": "/forum?id=l49uZcEIcq",
        "link": "https://openreview.net/forum?id=l49uZcEIcq",
        "pdf_link": "https://openreview.net/pdf?id=l49uZcEIcq",
        "keywords": "Large Language Models, Verbosity of LLM, LLM Uncertainty, LLM Routing",
        "abstract": "When unsure about an answer, humans often respond with more words than necessary, hoping that part of the response will be correct. We observe a similar behavior in large language models (LLMs), which we term ``Verbosity Compensation\" (VC). VC is harmful because it confuses the user understanding, leading to low efficiency, and influences the LLM services by increasing the latency and cost of generating useless tokens. In this paper, we present the first work that defines and analyzes Verbosity Compensation (VC), explores its causes, and proposes a simple mitigating approach. We define Verbosity Compensation (VC) as the behavior of generating responses that can be compressed without information loss when prompted to write concisely. Our experiments, conducted on five datasets of knowledge and reasoning-based QA tasks with 14 newly developed LLMs, reveal three conclusions. 1) We reveal a pervasive presence of verbosity compensation (VC) across all models and all datasets. Notably, GPT-4 exhibits a VC frequency of 50.40%. 2) We reveal the large performance gap between verbose and concise responses, with a notable difference of 27.61% on the Qasper dataset. We also show this difference cannot be naturally mitigated with the capability of LLM increases Both 1) and 2) highlight the urgent need to mitigate the frequency of VC behavior and disentangle verbosity with veracity. We propose a simple yet effective cascade algorithm that replaces the verbose responses with the other model-generated responses. The results show that our approach effectively alleviates the VC of the Mistral model from 63.81% to 16.16% on the Qasper dataset. 3) We also find that verbose responses exhibit higher uncertainty across all five datasets, suggesting a strong connection between verbosity and model uncertainty. Our dataset and code will be released."
    },
    {
        "title": "MOSAIC: Multiple Observers Spotting AI Content, a Robust Approach to Machine-Generated Text Detection",
        "link_suffix": "/forum?id=tWjLGWrtqy",
        "link": "https://openreview.net/forum?id=tWjLGWrtqy",
        "pdf_link": "https://openreview.net/pdf?id=tWjLGWrtqy",
        "keywords": "Machine-generated text detection, Mixture of models, Optimal detector, Robustness",
        "abstract": "The dissemination of Large Language Models (LLMs), trained at scale, and endowed with powerful text-generating abilities has vastly increased the threats posed by generative AI technologies by reducing the cost of producing harmful, toxic, faked or forged content. In response, various proposals have been made to automatically discriminate artificially generated from human-written texts, typically framing the problem as a classification problem. Most approaches evaluate an input document by a well-chosen detector LLM, assuming that low-perplexity scores reliably signal machine-made content. As using one single detector can induce brittleness of performance, we instead consider several and derive a new, theoretically grounded approach to combine their respective strengths. Our experiments, using a variety of generator LLMs, suggest that our method effectively increases the robustness of detection."
    },
    {
        "title": "Bootstrap Sampling Rate Greater than 1.0 May Improve Random Forest Performance",
        "link_suffix": "/forum?id=dD6b5RREws",
        "link": "https://openreview.net/forum?id=dD6b5RREws",
        "pdf_link": "https://openreview.net/pdf?id=dD6b5RREws",
        "keywords": "random forest, bootstrap sampling rate",
        "abstract": "Random forests utilize bootstrap sampling to create an individual training set for each component tree. This involves sampling with replacement, with the number of instances equal to the size of the original training set ($N$). Research literature indicates that drawing fewer than $N$ observations can also yield satisfactory results. The ratio of the number of observations in each bootstrap sample to the total number of training instances is called the bootstrap rate (BR). Sampling more than $N$ observations (BR $>$ 1) has been explored in the literature only to a limited extent and has generally proven ineffective. In this paper, we re-examine this approach using 36 diverse datasets and consider BR values ranging from 1.2 to 5.0. Contrary to previous findings, we show that such parameterization can result in statistically significant improvements in classification accuracy compared to standard settings (BR $\\leq$ 1). Furthermore, we investigate what the optimal BR depends on and conclude that it is more a property of the dataset than a dependence on the random forest hyperparameters. Finally, we develop a binary classifier to predict whether the optimal BR is $\\leq$ 1 or $>$ 1 for a given dataset, achieving between  81.88% and 88.81% accuracy, depending on the experiment configuration. The code is available at: $<$placeholder$>$."
    },
    {
        "title": "Large (Vision) Language Models are Unsupervised In-Context Learners",
        "link_suffix": "/forum?id=ohJxgRLlLt",
        "link": "https://openreview.net/forum?id=ohJxgRLlLt",
        "pdf_link": "https://openreview.net/pdf?id=ohJxgRLlLt",
        "keywords": "llm, unsupervised, in-context learning, few-shot learning",
        "abstract": "Recent advancements in large language and vision-language models have made it possible to solve new tasks via zero-shot inference without task-specific training. Various adaptation techniques, such as In-Context Learning (ICL), supervised fine-tuning, and prompt engineering, can further enhance the model’s performance on a given task. However, these methods require either labeled examples or substantial manual effort to construct effective prompts. In this work, we introduce a joint inference framework extending the standard zero-shot inference. In contrast to independent zero-shot predictions, joint inference makes predictions simultaneously for all inputs for a given task. Since direct joint inference involves a computationally expensive optimization, we develop efficient approximation techniques resulting in two unsupervised adaptation methods that are compatible with language and vision-language models: unsupervised fine-tuning and unsupervised ICL. We demonstrate the effectiveness of both approaches across a broad range of tasks and models, including language-only Llama 3.1, vision-language Open-Flamingo and API-only access GPT-4o models. Our experiments reveal substantial improvements over the standard zero-shot approach. Furthermore, our approach, although unsupervised, often performs on par with supervised approaches that use ground truth labels."
    },
    {
        "title": "Effective post-training embedding compression via temperature control in contrastive training",
        "link_suffix": "/forum?id=szRmEM8Kx5",
        "link": "https://openreview.net/forum?id=szRmEM8Kx5",
        "pdf_link": "https://openreview.net/pdf?id=szRmEM8Kx5",
        "keywords": "representation learning, embeddings, text retrieval, nlp",
        "abstract": "Fixed-size learned representations (dense representations, or embeddings) are widely used in many machine learning applications across language, vision or speech modalities. This paper investigates the role of the temperature parameter in contrastive training for text embeddings. We shed light on the impact this parameter has on the intrinsic dimensionality of the embedding spaces obtained, and show that lower intrinsic dimensionality is further correlated with effective compression of embeddings. We still observe a trade-off between absolute performance and effective compression and we propose temperature aggregation methods which reduce embedding size by an order of magnitude with minimal impact on quality."
    },
    {
        "title": "LLMs Can Plan Only If We Tell Them",
        "link_suffix": "/forum?id=K3KrOsR6y9",
        "link": "https://openreview.net/forum?id=K3KrOsR6y9",
        "pdf_link": "https://openreview.net/pdf?id=K3KrOsR6y9",
        "keywords": "large language models, decision-making, planning",
        "abstract": "Large language models (LLMs) have demonstrated significant capabilities in natural language processing and reasoning, yet their effectiveness in autonomous planning has been under debate. While existing studies have utilized LLMs with external feedback mechanisms or in controlled environments for planning, these approaches often involve substantial computational and development resources due to the requirement for careful design and iterative backprompting. Moreover, even the most advanced LLMs like GPT-4 struggle to match human performance on standard planning benchmarks, such as the Blocksworld, without additional support. This paper investigates whether LLMs can independently generate long-horizon plans that rival human baselines. Our novel enhancements help achieve state-of-the-art results in planning benchmarks out-competing prior methods and human baselines all autonomously."
    },
    {
        "title": "Feature Responsiveness Scores: Model-Agnostic Explanations for Recourse",
        "link_suffix": "/forum?id=wsWCVrH9dv",
        "link": "https://openreview.net/forum?id=wsWCVrH9dv",
        "pdf_link": "https://openreview.net/pdf?id=wsWCVrH9dv",
        "keywords": "explainability, feature attribution, algorithmic recourse, regulation",
        "abstract": "Machine learning models are often used to automate or support decisions in applications such as lending and hiring. In such tasks, consumer protection rules mandate that we provide a list of ``principal reasons” to consumers who receive\nadverse decisions. In practice, lenders and employers identify principal reasons by\nreturning the top-scoring features from a feature attribution method. In this work,\nwe study how this approach aligns with one of the underlying goals of consumer\nprotection: recourse, i.e., educating individuals on how they can attain a desired\noutcome. We show that standard attribution methods can mislead individuals by\nhighlighting features that cannot be changed to achieve recourse – i.e., by providing\nthem with reasons without recourse. We propose to address these issues by scoring\nfeatures on the basis of responsiveness – i.e., the probability that an individual\ncan attain a desired outcome by changing a specific feature. We develop efficient\nmethods to compute feature responsiveness scores for any model and any dataset\nunder complex actionability constraints. We present an extensive empirical study\non the responsiveness of explanations in consumer finance, and demonstrate that\nresponsiveness scores can flag instances with fixed predictions and identify features\nthat lead to recourse."
    },
    {
        "title": "Interpretable point cloud classification using multiple instance learning",
        "link_suffix": "/forum?id=T7ZVzuObcj",
        "link": "https://openreview.net/forum?id=T7ZVzuObcj",
        "pdf_link": "https://openreview.net/pdf?id=T7ZVzuObcj",
        "keywords": "Multiple instance learning, point cloud, interpretable",
        "abstract": "3D image analysis is crucial in fields such as autonomous driving and biomedical research. However, existing 3D point cloud classification models are often black boxes, limiting trust and usability in safety-critical applications. To address this, we propose PointMIL, an inherently locally interpretable point cloud classifier using Multiple Instance Learning (MIL). PointMIL offers local interpretability, providing fine-grained point-specific explanations to point-based models without the need for post-hoc methods, addressing the limitations of global or imprecise interpretability approaches. We applied PointMIL to popular point cloud classifiers, DGCNN and PointNet, and proposed a transformer-based backbone to extract high-quality point-specific features. PointMIL transformed these models to become inherently interpretable while increasing predictive performance on standard benchmarks (ModelNet40, ShapeNetPart) and achieving state-of-the-art mACC ($97.3%$) and F1 ($97.5%$) on the IntrA biomedical data set."
    },
    {
        "title": "InterMask: 3D Human Interaction Generation via Collaborative Masked Modelling",
        "link_suffix": "/forum?id=ZAyuwJYN8N",
        "link": "https://openreview.net/forum?id=ZAyuwJYN8N",
        "pdf_link": "https://openreview.net/pdf?id=ZAyuwJYN8N",
        "keywords": "Motion Synthesis, Human Interaction Generation, Masked Generative Transformer, Text-driven Generation, Vector Quantized VAE",
        "abstract": "Generating realistic 3D human-human interactions from textual descriptions remains a challenging task. Existing approaches, typically based on diffusion models, often generate unnatural and unrealistic results. In this work, we introduceInterMask, a novel framework for generating human interactions using collaborative masked modeling in discrete space. InterMask first employs a VQ-VAE to transform each motion sequence into a 2D discrete motion token map. Unlike traditional 1D VQ token maps, it better preserves fine-grained spatio-temporal details and promotesspatial awarenesswithin each token. Building on this representation, InterMask utilizes a generative masked modeling framework to collaboratively model the tokens of two interacting individuals. This is achieved by employing a transformer architecture specifically designed to capture complex spatio-temporal interdependencies. During training, it randomly masks the motion tokens of both individuals and learns to predict them. In inference, starting from fully masked sequences, it progressively fills in the tokens for both individuals. With its enhanced motion representation, dedicated architecture, and effective learning strategy, InterMask achieves state-of-the-art results, producing high-fidelity and diverse human interactions. It outperforms previous methods, achieving an FID of $5.154$ (vs $5.535$ for in2IN) on the InterHuman dataset and $0.399$ (vs $5.207$ for InterGen) on the InterX dataset. Additionally, InterMask seamlessly supports reaction generation without the need for model redesign or fine-tuning."
    },
    {
        "title": "Generalized Group Data Attribution",
        "link_suffix": "/forum?id=BQgAToASdX",
        "link": "https://openreview.net/forum?id=BQgAToASdX",
        "pdf_link": "https://openreview.net/pdf?id=BQgAToASdX",
        "keywords": "generalized, group, data attribution, efficiency, training data, influence, tracin, trak",
        "abstract": "Data Attribution (DA) methods quantify the influence of individual training data points on model outputs and have broad applications such as explainability, data selection, and noisy label identification. However, existing DA methods are often computationally intensive, limiting their applicability to large-scale machine learning models. To address this challenge, we introduce the Generalized Group Data Attribution (GGDA) framework, which computationally simplifies DA by attributing to groups of training points instead of individual ones. GGDA is a general framework that subsumes existing attribution methods and can be applied to new DA techniques as they emerge. It allows users to optimize the trade-off between efficiency and fidelity based on their needs. Our empirical results demonstrate that GGDA applied to popular DA methods such as Influence Functions, TracIn, and TRAK results in upto 10x-50x speedups over standard DA methods while gracefully trading off attribution fidelity. For downstream applications such as dataset pruning and noisy label identification, \nwe demonstrate that GGDA significantly improves computational efficiency and maintains effectiveness, enabling practical applications in large-scale machine learning scenarios that were previously infeasible."
    },
    {
        "title": "Soft Preference Optimization:  Aligning Language Models to Expert Distributions",
        "link_suffix": "/forum?id=oK1zJCWBqf",
        "link": "https://openreview.net/forum?id=oK1zJCWBqf",
        "pdf_link": "https://openreview.net/pdf?id=oK1zJCWBqf",
        "keywords": "RLHF, DPO, Preference Alignment, Large Language Models",
        "abstract": "We propose Soft Preference Optimization (SPO), a method for aligning generative models, such as Large Language Models (LLMs), with human preferences, without the need for a reward model. SPO optimizes model outputs directly over a preference dataset through a natural loss function that integrates preference loss with a regularization term across the model's entire output distribution rather than limiting it to the preference dataset. Although SPO does not require the assumption of an existing underlying reward model, we demonstrate that, under the Bradley-Terry (BT) model assumption, it converges to a softmax of scaled rewards, with the distribution's ``softness\" adjustable via the softmax exponent, an algorithm parameter. We showcase  SPO's methodology, its theoretical foundation, and its comparative advantages in simplicity and alignment precision."
    },
    {
        "title": "Provable optimal transport with transformers: The essence of depth and prompt engineering",
        "link_suffix": "/forum?id=GFua0WEYGF",
        "link": "https://openreview.net/forum?id=GFua0WEYGF",
        "pdf_link": "https://openreview.net/pdf?id=GFua0WEYGF",
        "keywords": "Theory of Language Models, Deep Learning Theory, Optimal Transport, Optimization",
        "abstract": "Can we establish provable guarantees for transformer performance? Providing such theoretical guarantees is a milestone in developing trustworthy generative AI. In this paper, we take a step toward addressing this question by focusing on optimal transport, a fundamental problem at the intersection of combinatorial and continuous optimization. Leveraging the computational power of attention layers, we prove that a transformer with fixed parameters can effectively solve the optimal transport problem (in Wasserstein-2 with entropic regularization) for an arbitrary number of points. Consequently, the transformer can sort lists of arbitrary size up to an approximation factor. Our results rely on an engineered prompt that enables the transformer to implement gradient descent with adaptive step sizes on the dual optimal transport. Combining the convergence analysis of gradient descent with Sinkhorn dynamics, we establish an explicit approximation bound for optimal transport with transformers, which improves with increasing depth. Our findings provide novel insights into the essence of prompt engineering and depth for transformers."
    },
    {
        "title": "Denoising Autoregressive Transformers for Scalable Text-to-Image Generation",
        "link_suffix": "/forum?id=amDkNPVWcn",
        "link": "https://openreview.net/forum?id=amDkNPVWcn",
        "pdf_link": "https://openreview.net/pdf?id=amDkNPVWcn",
        "keywords": "diffusion models, autoregressive models, Transformer",
        "abstract": "Diffusion models have become the dominant approach for visual generation. They\nare trained by denoising a Markovian process which gradually adds noise to\nthe input. We argue that the Markovian property limits the model’s ability to\nfully utilize the generation trajectory, leading to inefficiencies during training\nand inference. In this paper, we propose DART, a transformer-based model that\nunifies autoregressive (AR) and diffusion within a non-Markovian framework.\nDART iteratively denoises image patches spatially and spectrally using an AR\nmodel that has the same architecture as standard language models. DART does\nnot rely on image quantization, which enables more effective image modeling\nwhile maintaining flexibility. Furthermore, DART seamlessly trains with both\ntext and image data in a unified model. Our approach demonstrates competitive\nperformance on class-conditioned and text-to-image generation tasks, offering a\nscalable, efficient alternative to traditional diffusion models. Through this unified\nframework, DART sets a new benchmark for scalable, high-quality image synthesis."
    },
    {
        "title": "Natural GaLore: Accelerating GaLore for memory-efficient LLM Training and Fine-tuning",
        "link_suffix": "/forum?id=e0bTcdF29g",
        "link": "https://openreview.net/forum?id=e0bTcdF29g",
        "pdf_link": "https://openreview.net/pdf?id=e0bTcdF29g",
        "keywords": "large language models, LLM pre-training, memory-efficient training, optimization",
        "abstract": "Training LLMs presents significant memory challenges due to growing size of data, weights, and optimizer states. Techniques such as data and model parallelism, gradient checkpointing, and offloading strategies address this issue but are often infeasible due to hardware constraints. To mitigate memory usage, alternative methods like Parameter-Efficient-Fine-Tuning (PEFT) and GaLore approximate weights or optimizer states. PEFT methods, such as LoRA, have gained popularity for fine-tuning LLMs, though they require a full-rank warm start. In contrast, GaLore allows full-parameter learning while being more memory-efficient. This work introduces \\textit{Natural GaLore}, a simple drop in replacement for AdamW, which efficiently applies the inverse Empirical Fisher Information Matrix to low-rank gradients using Woodbury's Identity. We demonstrate that incorporating second-order information speeds up optimization significantly, especially when the iteration budget is limited. Empirical pretraining on 60M, 130M, 350M, and 1.1B parameter Llama models on C4 data demonstrate significantly lower perplexity over GaLore without additional memory overhead. By fine-tuning RoBERTa on the GLUE benchmark using \\textit{Natural GaLore}, we demonstrate significant reduction in gap 86.05% vs 86.28% for full-finetuning. Furthermore, fine-tuning the TinyLlama 1.1B model for function calling using the TinyAgent framework shows that \\textit{Natural GaLore} achieving 83.09% accuracy on the TinyAgent dataset, significantly outperforms 16-bit LoRA at 80.06% and even surpasses GPT4-Turbo by 4%, all while using 30% less memory."
    },
    {
        "title": "TopoNets: High performing vision and language models with brain-like topography",
        "link_suffix": "/forum?id=THqWPzL00e",
        "link": "https://openreview.net/forum?id=THqWPzL00e",
        "pdf_link": "https://openreview.net/pdf?id=THqWPzL00e",
        "keywords": "topography, neuro-inspired, convolutional neural networks, Transformers, visual cortex, neuroscience",
        "abstract": "Neurons in the brain are organized such that nearby cells tend to share similar functions. AI models lack this organization, and past efforts to introduce topog- raphy have often led to significant trade-offs between topography and task per- formance. In this work, we present TopoLoss, a new loss function that promotes spatially organized topographic representations in AI models without sacrificing task performance. TopoLoss is highly adaptable and can be seamlessly integrated into the training of leading model architectures. We validate our method on both vision convolutional networks (ResNet-18, ResNet-50) and language transformers (GPT-Neo-125M), collectively TopoNets. TopoNets are the highest-performing topographic models to date, exhibiting brain-like properties such as localized fea- ture processing, lower dimensionality, increased efficiency, and meso-scale inter- pretability. TopoNets also replicate the key topographic signatures observed in the brain’s visual and language cortices, further bridging the gap between biological and artificial systems. This work establishes a robust and generalizable frame- work for integrating topography into AI, advancing the development of models that more closely emulate the computational strategies of the human brain."
    },
    {
        "title": "FlashDP: Memory-Efficient and High-Throughput DP-SGD Training for Large Language Models",
        "link_suffix": "/forum?id=cZZMC8VFZc",
        "link": "https://openreview.net/forum?id=cZZMC8VFZc",
        "pdf_link": "https://openreview.net/pdf?id=cZZMC8VFZc",
        "keywords": "differential privacy, DP-SGD, LLMs, CUDA",
        "abstract": "As large language models (LLMs) increasingly underpin technological advancements, the privacy of their training data emerges as a critical concern. Differential Privacy (DP) serves as a rigorous mechanism to protect this data, yet its integration via Differentially Private Stochastic Gradient Descent (DP-SGD) introduces substantial challenges, primarily due to the complexities of per-sample gradient clipping. Current explicit methods, such as Opacus, necessitate extensive storage for per-sample gradients, significantly inflating memory requirements. Conversely, implicit methods like GhostClip reduce storage needs by recalculating gradients multiple times, which leads to inefficiencies due to redundant computations. This paper introduces FlashDP, an innovative cache-friendly method that consolidates necessary operations into a single task, calculating gradients only once in a fused manner. This approach not only diminishes memory movement by up to $\\textbf{50}$% but also cuts down redundant computations by $\\textbf{20}$%, compared to previous methods. Consequently, FlashDP does not increase memory demands and achieves a $\\textbf{90}$% throughput compared to the Non-DP method on a four-A100 system during the pre-training of the Llama-13B model, while maintaining parity with standard DP-SGD in terms of precision. These advancements establish FlashDP as a pivotal development for efficient and privacy-preserving training of LLMs."
    },
    {
        "title": "Partially Conditioned Patch Parallelism for Accelerated Diffusion Model Inference",
        "link_suffix": "/forum?id=rnTb9dm9zx",
        "link": "https://openreview.net/forum?id=rnTb9dm9zx",
        "pdf_link": "https://openreview.net/pdf?id=rnTb9dm9zx",
        "keywords": "Diffusion model, image generation, parallel algorithm",
        "abstract": "Diffusion models have exhibited exciting capabilities in generating images and are also very promising for video creation. However, the inference speed of diffusion models is limited by the slow sampling process, restricting its use cases. The sequential denoising steps required for generating a single sample could take tens or hundreds of iterations and thus have become a significant bottleneck. This limitation is more salient for applications that are interactive in nature or require small latency. To address this challenge, we propose Partially Conditioned Patch Parallelism (PCPP) to accelerate the inference of high-resolution diffusion models. Using the fact that the difference between the images in adjacent diffusion steps is nearly zero, Patch Parallelism (PP) leverages multiple GPUs communicating asynchronously to compute patches of an image in multiple computing devices based on the entire image (all patches) in the previous diffusion step. PCPP develops PP to reduce computation in inference by conditioning only on parts of the neighboring patches in each diffusion step, which also decreases communication among computing devices. As a result, PCPP decreases the communication cost by around $70$% compared to DistriFusion (the state of the art implementation of PP) and achieves $2.36\\sim 8.02\\times$ inference speed-up using $4\\sim 8$ GPUs compared to $2.32\\sim 6.71\\times$ achieved by DistriFusion depending on the computing device configuration and resolution of generation at the cost of a possible decrease in image quality. PCPP demonstrates the potential to strike a favorable trade-off, enabling high-quality image generation with substantially reduced latency."
    }
]