[{"title": "Mitigating Modality Prior-Induced Hallucinations in Multimodal Large Language Models via Deciphering Attention Causality", "link_suffix": "/forum?id=AV7OXVlAyi", "link": "https://openreview.net/forum?id=AV7OXVlAyi", "pdf_link": "https://openreview.net/pdf?id=AV7OXVlAyi", "keywords": "Modality Priors, Counterfactual Reasoning, Deciphering Attention Causality", "abstract": "Multimodal Large Language Models (MLLMs) have emerged as a central focus in both industry and academia, but often suffer from biases introduced by visual and language priors, which can lead to multimodal hallucination. These biases arise from the visual encoder and the Large Language Model (LLM) backbone, affecting the attention mechanism responsible for aligning multimodal inputs. Existing decoding-based mitigation methods focus on statistical correlations and overlook the causal relationships between attention mechanisms and model output, limiting their effectiveness in addressing these biases. To tackle this issue, we propose a causal inference framework termed CausalMM that applies structural causal modeling to MLLMs, treating modality priors as a confounder between attention mechanisms and output. Specifically, by employing backdoor adjustment and counterfactual reasoning at both the visual and language attention levels, our method mitigates the negative effects of modality priors and enhances the alignment of MLLM's inputs and outputs, with a maximum score improvement of 65.3% on 6 VLind-Bench indicators and 164 points on MME Benchmark compared to conventional methods. Extensive experiments validate the effectiveness of our approach while being a plug-and-play solution.", "title_embedding_index": 19450, "title_abs_embedding_index": 19475}, {"title": "Super(ficial)-alignment: Strong Models May Deceive Weak Models in Weak-to-Strong Generalization", "link_suffix": "/forum?id=HxKSzulSD1", "link": "https://openreview.net/forum?id=HxKSzulSD1", "pdf_link": "https://openreview.net/pdf?id=HxKSzulSD1", "keywords": "Superalignment, Weak-to-Strong, Security", "abstract": "Superalignment, where humans act as weak supervisors for superhuman models, has become a crucial problem with the rapid development of Large Language Models (LLMs). Recent work has preliminarily studied this problem by using weak models to supervise strong models, and discovered that weakly supervised strong students can consistently outperform weak teachers towards the alignment target, leading to a weak-to-strong generalization phenomenon. However, we are concerned that behind such a promising phenomenon, whether there exists an issue of weak-to-strong deception, where strong models deceive weak models by exhibiting well-aligned in areas known to weak models but producing misaligned behaviors in cases weak models do not know. We take an initial step towards exploring this security issue in a specific but realistic multi-objective alignment case, where there may be some alignment targets conflicting with each other (e.g., helpfulness v.s. harmlessness). We aim to explore whether, in such cases, strong models might deliberately make mistakes in areas known to them but unknown to weak models within one alignment dimension, in exchange for a higher reward in another dimension. Through extensive experiments in both the reward modeling and preference optimization scenarios, we find: (1) The weak-to-strong deception phenomenon exists across all settings. (2) The deception intensifies as the capability gap between weak and strong models increases. (3) Bootstrapping with an intermediate model can mitigate the deception to some extent, though its effectiveness remains limited. Our work highlights the urgent need to pay more attention to the true reliability of superalignment.", "title_embedding_index": 19451, "title_abs_embedding_index": 19476}, {"title": "ECHOPulse: ECG Controlled Echocardio-gram Video Generation", "link_suffix": "/forum?id=i2r7LDjba3", "link": "https://openreview.net/forum?id=i2r7LDjba3", "pdf_link": "https://openreview.net/pdf?id=i2r7LDjba3", "keywords": "Medical video generation, ECHO synthesis, Multimodality, Wearable device, Medical foundation model", "abstract": "Echocardiography (ECHO) is essential for cardiac assessments, but its video quality and interpretation heavily relies on manual expertise, leading to inconsistent results from clinical and portable devices. ECHO video generation offers a solution by improving automated monitoring through synthetic data and generating high-quality videos from routine health data. However, existing models often face high computational costs, slow inference, and rely on complex conditional prompts that require experts' annotations. To address these challenges, we propose ECHOPulse, an ECG-conditioned ECHO video generation model. ECHOPulse introduces two key advancements: (1) it accelerates ECHO video generation by leveraging VQ-VAE tokenization and masked visual token modeling for fast decoding, and (2) it conditions on readily accessible ECG signals, which are highly coherent with ECHO videos, bypassing complex conditional prompts. To the best of our knowledge, this is the first work to use time-series prompts like ECG signals for ECHO video generation. ECHOPulse not only enables controllable synthetic ECHO data generation but also provides updated cardiac function information for disease monitoring and prediction beyond ECG alone. Evaluations on three public and private datasets demonstrate state-of-the-art performance in ECHO video generation across both qualitative and quantitative measures. Additionally, ECHOPulse can be easily generalized to other modality generation tasks, such as cardiac MRI, fMRI, and 3D CT generation. We will make the synthetic ECHO dataset, along with the code and model, publicly available upon acceptance.", "title_embedding_index": 19452, "title_abs_embedding_index": 19477}, {"title": "Generalization Error Minimized Deep Learning", "link_suffix": "/forum?id=cKkZEj8n7O", "link": "https://openreview.net/forum?id=cKkZEj8n7O", "pdf_link": "https://openreview.net/pdf?id=cKkZEj8n7O", "keywords": "generalization error, overfitting, bias-variance decomposition, image classification", "abstract": "Despite the vast applications and rapid development of deep learning (DL), understanding and improving the generalization ability of deep neural networks (DNNs) remains a fundamental challenge. To tackle this challenge, in this paper, we first establish a novel bias-variance decomposition framework to analyze the generalization error of DNNs. Based on our new generalization error formula, we then present a new form of DL dubbed generalization error minimized (GEM) DL by jointly minimizing the conventional optimization target and an analytical proxy for the generalization error. Extensive experimental results show that in comparison with DNNs trained within the standard DL, GEM DNNs have smaller generalization errors and better generalization ability, thereby improving DNN prediction accuracy. Notably, GEM DL can increase prediction accuracy by as much as 13.19% on ImageNet in the presence of data distribution shift between training and testing.", "title_embedding_index": 19453, "title_abs_embedding_index": 19478}, {"title": "SeaKR: Self-aware Knowledge Retrieval for Adaptive Retrieval Augmented Generation", "link_suffix": "/forum?id=NhIaRz9Qf5", "link": "https://openreview.net/forum?id=NhIaRz9Qf5", "pdf_link": "https://openreview.net/pdf?id=NhIaRz9Qf5", "keywords": "Retrieval-Augmented Generation, Internal State of LLMs, Open-domain Question Answering", "abstract": "Adaptive Retrieval-Augmented Generation (RAG) is an effective strategy to alleviate hallucination of large language models (LLMs). It dynamically determines whether LLMs need external knowledge for generation and invokes retrieval accordingly. This paper introduces Self-aware Knowledge Retrieval (SeaKR), a novel adaptive RAG model that extracts self-aware uncertainty of LLMs from their internal states. SeaKR activates retrieval when the LLMs present high self-aware uncertainty for generation. To effectively integrate retrieved knowledge snippets, SeaKR re-ranks them based on LLM's self-aware uncertainty to preserve the snippet that reduces their uncertainty to the utmost. To facilitate solving complex tasks that require multiple retrievals, SeaKR utilizes their self-aware uncertainty to choose among different reasoning strategies. Our experiments on both complex and simple Question Answering datasets show that SeaKR outperforms existing adaptive RAG methods.", "title_embedding_index": 19454, "title_abs_embedding_index": 19479}, {"title": "Training-Free Diffusion Model Alignment with Sampling Demons", "link_suffix": "/forum?id=tfemquulED", "link": "https://openreview.net/forum?id=tfemquulED", "pdf_link": "https://openreview.net/pdf?id=tfemquulED", "keywords": "Diffusion Model, Consistency Model, Score-Based Method, EDM, Stochastic Process, It\u00f4's Lemma, RLHF, Value Function, Preference Alignment", "abstract": "Aligning diffusion models with user preferences has been a key challenge.\nExisting methods for aligning diffusion models either require retraining or are limited to differentiable reward functions.\nTo address these limitations, we propose a stochastic optimization approach, dubbed Demon, to guide the denoising process at inference time without backpropagation through reward functions or model retraining.\nOur approach works by controlling noise distribution in denoising steps to concentrate density on regions corresponding to high rewards through stochastic optimization.\nWe provide comprehensive theoretical and empirical evidence to support and validate our approach, including experiments that use non-differentiable sources of rewards such as Visual-Language Model (VLM) APIs and human judgments.\nTo the best of our knowledge, the proposed approach is the first inference-time, backpropagation-free preference alignment method for diffusion models.\nOur method can be easily integrated with existing diffusion models without further training.\nOur experiments show that the proposed approach significantly improves the average aesthetics scores for text-to-image generation.", "title_embedding_index": 19455, "title_abs_embedding_index": 19480}, {"title": "MINER: Mining the Underlying Pattern of Modality-Specific Neurons in Multimodal Large Language Models", "link_suffix": "/forum?id=y3CdSwREZl", "link": "https://openreview.net/forum?id=y3CdSwREZl", "pdf_link": "https://openreview.net/pdf?id=y3CdSwREZl", "keywords": "MLLMs, neuron analysis, interpretability", "abstract": "In recent years, multimodal large language models (MLLMs) have significantly advanced, integrating more modalities into diverse applications. However, the lack of explainability remains a major barrier to their use in scenarios requiring decision transparency. Current neuron-level explanation paradigms mainly focus on knowledge localization or language- and domain-specific analyses, leaving the exploration of multimodality largely unaddressed. To tackle these challenges, we propose MINER, a transferable framework for mining modality-specific neurons (MSNs) in MLLMs, which comprises four stages: (1) modality separation, (2) importance score calculation, (3) importance score aggregation, (4) modality-specific neuron selection. Extensive experiments across six benchmarks and two MLLMs show that (1) deactivating ONLY 2% of MSNs significantly reduce MLLMs performance (0.56 to 0.24 for Qwen2-VL, 0.69 to 0.31 for Qwen2-Audio), (2) different modalities mainly converge in the lower layers, (3) MSNs influence how key information from various modalities converges to the last token, (4) We observed two intriguing phenomena, semantic probing and semantic telomeres.", "title_embedding_index": 19456, "title_abs_embedding_index": 19481}, {"title": "GT-Mean Loss: A Simple Yet Effective Solution for Brightness Mismatch in Low-Light Image Enhancement", "link_suffix": "/forum?id=VtT41Nniu4", "link": "https://openreview.net/forum?id=VtT41Nniu4", "pdf_link": "https://openreview.net/pdf?id=VtT41Nniu4", "keywords": "Low-light image enhancement, loss function, GT-mean", "abstract": "Low-light image enhancement (LLIE) aims to improve the visual quality of images captured under poor lighting conditions. In supervised LLIE tasks, there exists a significant yet often overlooked inconsistency between the overall brightness of an enhanced image and its ground truth counterpart, referred to as brightness mismatch in this study. Brightness mismatch negatively impact supervised LLIE models by misleading model training. However, this issue is largely neglected in current research. In this context, we propose the GT-mean loss, a simple yet effective loss function directly modeling the mean values of images from a probabilistic perspective. The GT-mean loss is flexible, as it extends existing supervised LLIE loss functions into the GT-mean form with minimal additional computational costs. Extensive experiments demonstrate that the incorporation of the GT-mean loss results in consistent performance improvements across various methods and datasets.", "title_embedding_index": 19457, "title_abs_embedding_index": 19482}, {"title": "Efficient Incomplete Multi-view Clustering via Flexible Anchor Learning", "link_suffix": "/forum?id=GFzmAKw3RW", "link": "https://openreview.net/forum?id=GFzmAKw3RW", "pdf_link": "https://openreview.net/pdf?id=GFzmAKw3RW", "keywords": "Multi-view clustering, anchor learning, fast clustering", "abstract": "Multi-view clustering aims to improve the final performance by taking advantages of complementary and consistent information of all views. In real world, data samples with partially available information are common and the issue regarding the clustering for incomplete multi-view data is inevitably raised. To deal with the partial data with large scales, some fast clustering approaches for incomplete multi-view data have been presented. Despite the significant success, few of these methods pay attention to learning anchors with high quality in a unified framework for incomplete multi-view clustering, while ensuring the scalability for large-scale incomplete datasets. In addition, most existing approaches based on incomplete multi-view clustering ignore to build the relation between anchor graph and similarity matrix in symmetric nonnegative matrix factorization and then directly conduct graph partition based on the anchor graph to reduce the space and time consumption. In this paper, we propose a novel fast incomplete multi-view clustering method for the data with large scales, termed Efficient Incomplete Multi-view clustering via flexible anchor Learning (EIML), where graph construction, anchor learning and graph partition are simultaneously integrated into a unified framework for efficient incomplete multi-view clustering. To be specific, we learn a shared anchor graph to guarantee the consistency among multiple views and employ a adaptive weight coefficient to balance the impact for each view. The relation between anchor graph and similarity matrix in symmetric nonnegative matrix factorization can also be built, i.e., each entry in the anchor graph can characterize the similarity between the anchor and original data sample. We then adopt an alternative algorithm for solving the formulated problem. Experiments conducted on different datasets confirm the superiority of EIML compared with other clustering methods for incomplete multi-view data.", "title_embedding_index": 19458, "title_abs_embedding_index": 19483}, {"title": "DilateQuant: Accurate and Efficient Diffusion Quantization via Weight Dilation", "link_suffix": "/forum?id=nAl4bz09Mv", "link": "https://openreview.net/forum?id=nAl4bz09Mv", "pdf_link": "https://openreview.net/pdf?id=nAl4bz09Mv", "keywords": "quantization, diffusion model", "abstract": "Diffusion models have shown excellent performance on various image generation tasks, but the substantial computational costs and huge memory footprint hinder their low-latency applications in real-world scenarios. Quantization is a promising way to compress and accelerate models. Nevertheless, due to the wide range and time-varying activations in diffusion models, existing methods cannot maintain both accuracy and efficiency simultaneously for low-bit quantization. To tackle this issue, we propose DilateQuant, a novel quantization framework for diffusion models that offers comparable accuracy and high efficiency. Specifically, we keenly aware of numerous unsaturated in-channel weights, which can be cleverly exploited to reduce the range of activations without additional computation cost. Based on this insight, we propose Weight Dilation (WD) that maximally dilates the unsaturated in-channel weights to a constrained range through a mathematically equivalent scaling. WD costlessly absorbs the activation quantization errors into weight quantization. The range of activations decreases, which makes activations quantization easy. The range of weights remains constant, which makes model easy to converge in training stage. Considering the temporal network leads to time-varying activations, we design a Temporal Parallel Quantizer (TPQ), which sets time-step quantization parameters and supports parallel quantization for different time steps, significantly improving the performance and reducing time cost. To further enhance performance while preserving efficiency, we introduce a Block-wise Knowledge Distillation (BKD) to align the quantized models with the full-precision models at a block level. The simultaneous training of time-step quantization parameters and weights minimizes the time required, and the shorter backpropagation paths decreases the memory footprint of the quantization process. Extensive experiments demonstrate that DilateQuant significantly outperforms existing methods in terms of accuracy and efficiency.", "title_embedding_index": 19459, "title_abs_embedding_index": 19484}, {"title": "Physics-based Skinned Dance Generation with RL Fine-tuning", "link_suffix": "/forum?id=8Rad5LwSv2", "link": "https://openreview.net/forum?id=8Rad5LwSv2", "pdf_link": "https://openreview.net/pdf?id=8Rad5LwSv2", "keywords": "Dance generation, Reinforcement learning, Physical simulation", "abstract": "Denoising Diffusion Probabilistic Models (DDPMs) have shown great potential in generating high-fidelity, diverse, natural dances consistent with given music. However, due to the scarcity of skinned human motion data and the complexity of mesh data, existing methods mainly focus on generating dance moves in the form of skeletons, overlooking the domain gap between the skeletal structure and the human body geometry. When skeletal motions are visualized with human body mesh, anomalies such as torso interpenetration and imbalanced movements become highly noticeable. This physical implausibility significantly diminishes the aesthetic appeal of the generated dances and hinders their practicality in real-world applications. To address this issue, we propose a physical reward to fine-tune the diffusion model. Specifically, We first train a motion imitation policy in a physical simulator and use it to evaluate the physical plausibility (e.g., penetration, foot sliding) of generated motions. Ideally, generated motions that are more physically plausible will be easier to imitate, which means higher rewards. So we fine-tune the diffusion model to generate more physically plausible motions through Reinforcement Learning Fine-Tuning (RLFT). Furthermore, we find that the physical reward tends to push the model to generate freezing motions for less torso intersections. To mitigate it, we proposed an anti-freezing reward to balance the preference for freezing motions. Experiments on the human dance dataset show that our method can significantly improve the physical plausibility of generated motions, thereby generating dances that are aesthetically pleasing and realistic.", "title_embedding_index": 19460, "title_abs_embedding_index": 19485}, {"title": "Disentangling Regional Primitives for Image Generation", "link_suffix": "/forum?id=FgirWC5TJ6", "link": "https://openreview.net/forum?id=FgirWC5TJ6", "pdf_link": "https://openreview.net/pdf?id=FgirWC5TJ6", "keywords": "Explainable AI, Generative Adversarial Networks, Interactions", "abstract": "This paper presents a method to explain the internal representation structure of a neural network for image generation. Specifically, our method disentangles primitive feature components from the intermediate-layer feature of the neural network, which ensures that each feature component is exclusively used to generate a specific set of image regions. In this way, the generation of the entire image can be considered as the superposition of different pre-encoded primitive regional patterns, each being generated by a feature component. We find that the feature component can be represented as an OR relationship between the demands for generating different image regions, which is encoded by the neural network. Therefore, we extend the Harsanyi interaction to represent such an OR interaction to disentangle the feature component. Experiments show a clear correspondence between each feature component and the generation of specific image regions.", "title_embedding_index": 19461, "title_abs_embedding_index": 19486}, {"title": "Multi-Frame Neural Scene Flow: Learning Bounds and Algorithms", "link_suffix": "/forum?id=syUJqBnuD6", "link": "https://openreview.net/forum?id=syUJqBnuD6", "pdf_link": "https://openreview.net/pdf?id=syUJqBnuD6", "keywords": "Multi-Frame Neural Scene Flow, Spatial and Temporal Feature, Generalization Bound, Large-Scale Point Clouds.", "abstract": "Although Neural Scene Flow Prior (NSFP) and its variants have shown remarkable performance in large out-of-distribution autonomous driving, the underlying explanation for their generalization capabilities remains unclear. To this end, we analyze the generalization capabilities of NSFP via uniform stability and find that it exhibits a generalization bound, which is inversely proportional to the number of point clouds. These findings provide solid theoretical evidence to explain the effectiveness of NSFP in large-scale point cloud scene flow estimation tasks for the first time. To enhance practical scene understanding, we extend NSFP and propose a multi-frame neural scene flow (MNSF) scheme, which extracts temporal information across multiple frames. In this way, MNSF has better temporal consistency than NSFP.\nMoreover, we theoretically analyze its generalization abilities and demonstrate that it achieves a tight generalization bound with a convergence rate similar to NSFP. Extensive experimental results on large-scale autonomous driving Waymo Open and Argoverse datasets demonstrate that MNSF achieves state-of-the-art performance. The code is attached to the submission.", "title_embedding_index": 19462, "title_abs_embedding_index": 19487}, {"title": "MeteoRA: Multiple-tasks Embedded LoRA for Large Language Models", "link_suffix": "/forum?id=yOOJwR15xg", "link": "https://openreview.net/forum?id=yOOJwR15xg", "pdf_link": "https://openreview.net/pdf?id=yOOJwR15xg", "keywords": "LLM, LoRA", "abstract": "The pretrain+fine-tune paradigm is foundational for deploying large language models (LLMs) across various downstream applications. Within this framework, Low-Rank Adaptation (LoRA) stands out for its parameter-efficient fine-tuning (PEFT), producing numerous reusable task-specific LoRA adapters. However, this approach requires explicit task intention selection, posing challenges for autonomous task sensing and switching during inference with multiple existing LoRA adapters embedded in a single LLM. In this work, we introduce MeteoRA (Multiple-Tasks embedded LoRA), a scalable and efficient framework that reuses multiple task-specific LoRA adapters into the base LLM via a full-mode Mixture-of-Experts (MoE) architecture. This framework also includes novel MoE forward acceleration strategies to address the efficiency challenges of traditional MoE implementations. Our evaluation, using the LlaMA2-13B and LlaMA3-8B base models equipped with 28 existing LoRA adapters through MeteoRA, demonstrates equivalent performance with the traditional PEFT method. Moreover, the LLM equipped with MeteoRA achieves superior performance in handling composite tasks, effectively solving ten sequential problems in a single inference pass, thereby demonstrating the framework's enhanced capability for timely adapter switching.", "title_embedding_index": 19463, "title_abs_embedding_index": 19488}, {"title": "Towards Dynamic Graph Neural Networks with Provably High-Order Expressive Power", "link_suffix": "/forum?id=iinqdeuA8x", "link": "https://openreview.net/forum?id=iinqdeuA8x", "pdf_link": "https://openreview.net/pdf?id=iinqdeuA8x", "keywords": "Graph Neural Network, Dynamic Graph, Expressive power", "abstract": "Dynamic Graph Neural Networks (DyGNNs) have garnered increasing research attention for learning representations on evolving graphs. \nDespite their effectiveness, the limited expressive power of existing DyGNNs hinders them from capturing important evolving patterns of dynamic graphs. \nAlthough some works attempt to enhance expressive capability with heuristic features, there remains a lack of DyGNN frameworks with provable and quantifiable high-order expressive power.\nTo address this research gap, we firstly propose the k-dimensional Dynamic WL tests (k-DWL) as the referencing algorithms to quantify the expressive power of DyGNNs. We demonstrate that the expressive power of existing DyGNNs is bounded by the 1-DWL test. \nTo enhance the expressive power, we propose Dynamic Graph Neural Network with High-order expressive power (HopeDGN), which updates the representation of central node pair by aggregating the interaction history with neighbor node pairs. \nOur theoretical results demonstrate that HopeDGN can achieve expressive power equivalent to the 2-DWL test. \nWe then present a Transformer-based implementation for the local variant of \\model.\nExperimental results show that HopeDGN achieved  performance improvement up to 3.12% on seven datasets, demonstrating the effectiveness of HopeDGN.", "title_embedding_index": 19464, "title_abs_embedding_index": 19489}, {"title": "Formation of Representations in Neural Networks", "link_suffix": "/forum?id=Njx1NjHIx4", "link": "https://openreview.net/forum?id=Njx1NjHIx4", "pdf_link": "https://openreview.net/pdf?id=Njx1NjHIx4", "keywords": "representation learning, neural collapse, neural feature ansatz", "abstract": "Understanding neural representations will help open the black box of neural networks and advance our scientific understanding of modern AI systems. However, how complex, structured, and transferable representations emerge in modern neural networks has remained a mystery. Building on previous results, we propose the Canonical Representation Hypothesis (CRH), which posits a set of six alignment relations to universally govern the formation of representations in most hidden layers of a neural network. Under the CRH, the latent representations (R), weights (W), and neuron gradients (G) become mutually aligned during training. This alignment implies that neural networks naturally learn compact representations, where neurons and weights are invariant to task-irrelevant transformations. We then show that the breaking of CRH leads to the emergence of reciprocal power-law relations between R, W, and G, which we refer to as the Polynomial Alignment Hypothesis (PAH). We present a minimal-assumption theory demonstrating that the balance between gradient noise and regularization is crucial for the emergence the canonical representation. The CRH and PAH lead to an exciting possibility of unifying major key deep learning phenomena, including neural collapse and the neural feature ansatz, in a single framework.", "title_embedding_index": 19465, "title_abs_embedding_index": 19490}, {"title": "Benchmarking Visual Cognition of Multimodal LLMs via Matrix Reasoning", "link_suffix": "/forum?id=QrhB9HcgnL", "link": "https://openreview.net/forum?id=QrhB9HcgnL", "pdf_link": "https://openreview.net/pdf?id=QrhB9HcgnL", "keywords": "Visual Cognition, Matrix Reasoning, Psychometrics, Visual Reasoning, Multimodal LLMs", "abstract": "Recently, Multimodal Large Language Models (MLLMs) and Vision Language Models (VLMs) have shown great promise in language-guided perceptual tasks such as recognition, segmentation, and object detection. However, their effectiveness in addressing visual cognition problems that require high-level multi-image reasoning and visual working memory is not well-established. One such challenge is matrix reasoning -- the cognitive ability to discern relationships among patterns in a set of images and extrapolate to predict subsequent patterns. This skill is crucial during the early neurodevelopmental stages of children and is proven to be used to test human intelligence. Inspired by the matrix reasoning tasks in Raven\u2019s Progressive Matrices (RPM) and Wechsler Intelligence Scale for Children (WISC), we propose a new dataset MaRs-VQA and a new benchmark VCog-Bench to evaluate the zero-shot visual cognition capability of MLLMs and compare their performance with existing human intelligent investigation. Our comparative experiments with different open-source and closed-source MLLMs on the VCog-Bench revealed a gap between MLLMs and human intelligence, highlighting the visual cognitive limitations of current MLLMs. We believe that the public release of VCog-Bench, consisting of MaRs-VQA, and the inference pipeline will drive progress toward the next generation of MLLMs with human-like visual cognition abilities.", "title_embedding_index": 19466, "title_abs_embedding_index": 19491}, {"title": "Executing Arithmetic: Fine-Tuning Large Language Models as Turing Machines", "link_suffix": "/forum?id=c0KPBFtGIy", "link": "https://openreview.net/forum?id=c0KPBFtGIy", "pdf_link": "https://openreview.net/pdf?id=c0KPBFtGIy", "keywords": "large language model, arithmetic, learn to execute", "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across a wide range of natural language processing and reasoning tasks. However, their performance in the foundational domain of arithmetic remains unsatisfactory. When dealing with arithmetic tasks, LLMs often memorize specific examples rather than learning the underlying computational logic, limiting their ability to generalize to new problems. In this paper, we propose a Composable Arithmetic Execution Framework (CAEF) that enables LLMs to learn to execute step-by-step computations by emulating Turing Machines, thereby gaining a genuine understanding of computational logic. Moreover, the proposed framework is highly scalable, allowing composing learned operators to significantly reduce the difficulty of learning complex operators. In our evaluation, CAEF achieves nearly $100\\%$ accuracy across seven common mathematical operations on the LLaMA 3.1-8B model, effectively supporting computations involving operands with up to 100 digits, a level where GPT-4o falls short noticeably in some settings.", "title_embedding_index": 19467, "title_abs_embedding_index": 19492}, {"title": "Content-style disentangled representation for controllable artistic image stylization and generation", "link_suffix": "/forum?id=TowvqbPj8m", "link": "https://openreview.net/forum?id=TowvqbPj8m", "pdf_link": "https://openreview.net/pdf?id=TowvqbPj8m", "keywords": "disentangled representation, image stylization, generation", "abstract": "Controllable artistic image stylization and generation aims to render the content provided by text or image with the learned artistic style, where content and style decoupling is the key to achieve satisfactory results. However, current methods for content and style disentanglement primarily rely on image information for supervision, which leads to two problems: 1) models can only support one modality for style or content input;2) incomplete disentanglement resulting in semantic interference from the reference image. To address the above issues, this paper proposes a content-style representation disentangling method for controllable artistic image stylization and generation. We construct a WikiStyle+ dataset consists of artworks with corresponding textual descriptions for style and content. Based on the multimodal dataset, we propose a disentangled content and style representations guided diffusion model. The disentangled representations are first learned by Q-Formers and then injected into a pre-trained diffusion model using learnable multi-step cross-attention layers for better controllable stylization. This approach allows model to accommodate inputs from different modalities. Experimental results show that our method achieves a thorough disentanglement of content and style in reference images under multimodal supervision, thereby enabling a harmonious integration of content and style in the generated outputs, successfully producing style-consistent and expressive stylized images.", "title_embedding_index": 19468, "title_abs_embedding_index": 19493}, {"title": "ImpScore: A Learnable Metric For Quantifying The Implicitness Level of Language", "link_suffix": "/forum?id=gYWqxXE5RJ", "link": "https://openreview.net/forum?id=gYWqxXE5RJ", "pdf_link": "https://openreview.net/pdf?id=gYWqxXE5RJ", "keywords": "implicit language, pragmatics, learnable metric, text evaluation, automatic evaluation, explicit language", "abstract": "Handling implicit language is essential for natural language processing systems to achieve precise text understanding and  facilitate natural interactions with users. Despite its importance, the absence of a robust metric for accurately measuring the implicitness of language significantly constrains the depth of analysis possible in evaluating models' comprehension capabilities. This paper addresses this gap by developing a scalar metric that quantifies the implicitness level of language without relying on external references. Drawing on principles from traditional linguistics, we define \"implicitness\" as the divergence between semantic meaning and pragmatic interpretation. To operationalize this definition, we introduce ImpScore, a novel, reference-free metric formulated through an interpretable regression model. This model is trained using pairwise contrastive learning on a specially curated dataset comprising $112,580$ (implicit sentence, explicit sentence) pairs. We validate ImpScore through a user study that compares its assessments with human evaluations on out-of-distribution data, demonstrating its accuracy and strong correlation with human judgments. Additionally, we apply ImpScore to hate speech detection datasets, illustrating its utility and highlighting significant limitations in current large language models' ability to understand highly implicit content.", "title_embedding_index": 19469, "title_abs_embedding_index": 19494}, {"title": "A Watermark for Low-entropy and Unbiased Generation in Large Language Models", "link_suffix": "/forum?id=hTUrBJqECJ", "link": "https://openreview.net/forum?id=hTUrBJqECJ", "pdf_link": "https://openreview.net/pdf?id=hTUrBJqECJ", "keywords": "Watermark, large language model", "abstract": "Recent advancements in large language models (LLMs) have highlighted the risk of misusing them, raising the need for accurate detection of LLM-generated content. In response, a viable solution is to inject imperceptible identifiers into LLMs, known as watermarks. Previous work demonstrates that unbiased watermarks ensure unforgeability and preserve text quality by maintaining the expectation of the LLM output probability distribution. However, previous unbiased watermarking methods suffer from one or more of the following issues: (1) requiring access to white-box LLMs during detection, (2) incurring long detection time, (3) being not robust against simple watermarking attacks, (4) failing to provide statistical guarantees for the type II error of watermark detection, and (5) being not statistically unbiased for low-entropy scenarios, which hinder their deployment in practice. This study proposes the Sampling One Then Accepting (STA-1) method, a watermark that can address all of these issues. Moreover, we discuss the tradeoff between watermark strength and text quality for unbiased watermarks. We show that in low-entropy scenarios, unbiased watermarks face a tradeoff between watermark strength and the risk of unsatisfactory outputs. Experimental results on both low-entropy and high-entropy datasets demonstrate that STA-1 achieves text quality and watermark strength comparable to existing unbiased watermarks, with a low risk of unsatisfactory outputs. Implementation codes for this study are available online (hidden for peer review).", "title_embedding_index": 19470, "title_abs_embedding_index": 19495}, {"title": "CycleVTON: Improving Diffusion-Based Virtual Try-On with Cycle-Consistent Training", "link_suffix": "/forum?id=zEUDoD9cU9", "link": "https://openreview.net/forum?id=zEUDoD9cU9", "pdf_link": "https://openreview.net/pdf?id=zEUDoD9cU9", "keywords": "virtual try-on, diffusion, cycle-consistency", "abstract": "We present CycleVTON, a cycle-consistent diffusion-based virtual try-on framework. Unlike existing methods that rely on a single try-on network, our model consists of two conjugated networks. In addition to the regular try-on network, we design a clothing extraction network that extracts the clothing worn by the person and standardizes it into a front-facing format. These two networks are symmetrical, enabling alignment between the generated dressed human and real images of dressed human, as well as between the extracted clothing and its front-facing ground truth. This cycle-consistent optimization strategy allows for enhanced retention of clothing textures and structures, ensuring a more realistic and accurate clothing generation in virtual try-on scenarios. Moreover, the conjugated network structure not only supports traditional virtual try-on but also allows flexible clothing extraction and clothing exchange between different individuals. The experiments on VITON-HD demonstrate the effectiveness of our approach.", "title_embedding_index": 19471, "title_abs_embedding_index": 19496}, {"title": "Unlearning Virus Knowledge Toward Safe and Responsible Mutation Effect Predictions", "link_suffix": "/forum?id=ST6i7VMyYn", "link": "https://openreview.net/forum?id=ST6i7VMyYn", "pdf_link": "https://openreview.net/pdf?id=ST6i7VMyYn", "keywords": "protein language model, mutation effect prediction, AI safety", "abstract": "Pre-trained deep protein models have become essential tools in fields such as biomedical research, enzyme engineering, and therapeutics due to their ability to predict and optimize protein properties effectively. However, the diverse and broad training data used to enhance the generalizability of these models may also inadvertently introduce ethical risks and pose biosafety concerns, such as the enhancement of harmful viral properties like transmissibility or drug resistance. To address this issue, we introduce a novel approach using knowledge unlearning to selectively remove virus-related knowledge while retaining other useful capabilities. We propose a learning scheme, PROEDIT, for editing a pre-trained protein language model toward safe and responsible mutation effect prediction. Extensive validation on open benchmarks demonstrates that PROEDIT significantly reduces the model's ability to enhance the properties of virus mutants without compromising its performance on non-virus proteins. As the first thorough exploration of safety issues in deep learning solutions for protein engineering, this study provides a foundational step toward ethical and responsible AI in biology.", "title_embedding_index": 19472, "title_abs_embedding_index": 19497}, {"title": "ToolACE: Enhancing Function Calling with Accuracy, Complexity, and Diversity", "link_suffix": "/forum?id=8EB8k6DdCU", "link": "https://openreview.net/forum?id=8EB8k6DdCU", "pdf_link": "https://openreview.net/pdf?id=8EB8k6DdCU", "keywords": "Tool leaning, Function calling, Large language models", "abstract": "Function calling significantly extends the application boundary of large language models (LLMs), where high-quality and diverse training data is critical for unlocking this capability. However, collecting and annotating real function-calling data is challenging, while synthetic data from existing pipelines often lack coverage and accuracy. In this paper, we present ToolACE, an automatic agentic pipeline designed to generate accurate, complex, and diverse tool-learning data, specifically tailored to the capabilities of LLMs. ToolACE leverages a novel self-evolution synthesis process to curate a comprehensive API pool of 26,507 diverse APIs. Dialogs are further generated through the interplay among multiple agents, under the guidance of a complexity evaluator. To ensure data accuracy, we implement a dual-layer verification system combining rule-based and model-based checks. We demonstrate that models trained on our synthesized data---even with only 8B parameters---achieve state-of-the-art performance, comparable to the latest GPT-4 models. Our model and a subset of the data are publicly available at \\url{https://mega.nz/folder/4ppChYKD#9MnWdtcratmSmnHBwu0CxA}.", "title_embedding_index": 19473, "title_abs_embedding_index": 19498}, {"title": "StretchySnake: Flexible VideoMamba for Short and Long-Form Action Recognition", "link_suffix": "/forum?id=SMlVEeoSyI", "link": "https://openreview.net/forum?id=SMlVEeoSyI", "pdf_link": "https://openreview.net/pdf?id=SMlVEeoSyI", "keywords": "Representation Learning, Video Understanding, State Space Models", "abstract": "State space models (SSMs) have very recently been introduced as an alternative deep architecture to transformers, exhibiting competitive or superior performance across various language and vision tasks. However, both SSMs and transformers share certain limitations in the vision domain, namely spatio-temporal inflexibility. Traditionally, deep video models are trained on a fixed resolution and number of frames, often arbitrarily chosen as a trade-off between performance and computational cost. Changing the resolution and/or number of frames a model can ingest usually requires retraining the model, while avoiding re-training by variably changing the weights of a trained model leads to significantly reduced test accuracy. In this paper, we introduce a spatio-temporal flexible training method that encourages a single set of learned weights to adapt well to any input resolution or video length. We achieve this by simply randomly changing the spatial and temporal resolutions of a video during training, and dynamically interpolating the model's weights accordingly. This single change in training not only allows for one model to be applied to both short and long video understanding tasks alike, but also allows for user-specific tailoring of computational cost. We propose and evaluate $5$ different spatio-temporal flexible training methods to find the optimal type for training a video SSM. We then evaluate our best flexibly-trained SSM, which we call StretchySnake, across a variety of short- and long-form action recognition evaluation protocols, such as video retrieval, fine-tuning, and linear probing, and massively outperform the same vanilla video SSM trained in a standard fashion by up to $28$% in some cases. Therefore, our training method can be used as a simple drop-in training technique for any SSM-based video models to strongly improve performance and instill spatio-temporal and compute flexibility.", "title_embedding_index": 19474, "title_abs_embedding_index": 19499}]