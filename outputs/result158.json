[
    {
        "title": "RootTracker: A Lightweight Framework to Trace Original Models of Fine-tuned LLMs in Black-Box Conditions",
        "link_suffix": "/forum?id=IaHzYWSFYY",
        "link": "https://openreview.net/forum?id=IaHzYWSFYY",
        "pdf_link": "https://openreview.net/pdf?id=IaHzYWSFYY",
        "keywords": "Large language models, Fine-tune, Framework, Black-box, Fairness, Safety",
        "abstract": "Large Language Models (LLMs) demonstrate remarkable performance in various applications, yet their training demands extensive resources and time. Consequently, fine-tuning pre-trained LLMs has become a prevalent strategy for adapting these models to diverse downstream tasks, thereby reducing costs. Despite their benefits, LLMs have vulnerabilities, such as susceptibility to adversarial attacks, potential for jailbreaking, fairness issues, backdoor vulnerabilities, and the risk of generating inappropriate or harmful content. Since fine-tuned models inherit some characteristics from their original models, they may also inherit these issues and vulnerabilities. In this work, we propose a lightweight framework, RootTracker, specifically designed to trace the original models of fine-tuned LLMs. The core idea is to identify a set of prompts that can assess which pre-trained LLM a fine-tuned model most closely resembles. This process is conducted in a ''knockout tournament\" style, where the model is repeatedly tested against pairs of LLMs until the original pre-trained model is identified. To evaluate the effectiveness of our framework, we created 200 distinct fine-tuned models, derived from original models including GPT-Neo, GPT-2, TinyLlama, and Pythia. The results demonstrate that our framework accurately identified the original models for 85.7% of the fine-tuned versions. Therefore, we advocate for timely updates to model versions or deliberate obfuscation of model types when deploying large models."
    },
    {
        "title": "Generative Matching Units for Supervised Learning",
        "link_suffix": "/forum?id=PJojB68YBu",
        "link": "https://openreview.net/forum?id=PJojB68YBu",
        "pdf_link": "https://openreview.net/pdf?id=PJojB68YBu",
        "keywords": "supervised learning, classification, robustness",
        "abstract": "We propose an alternative computational unit for feedforward supervised learning architectures, called Generative Matching Units (GMUs). To understand GMUs, we start with the standard perceptron unit and view it as an undirected symmetric measure of computation between the weights $W=[w_1,w_2,..w_d]$ and each input datapoint $X=[x_1,x_2,..,x_d]$. Perceptrons forward $W^TX+b$, which is usually followed by an activation function. In contrast, GMUs compute a directed asymmetric measure of computation that estimates the degree of functional dependency $f$ of the input elements $x_i$ of each datapoint to the weights $w_i$ in terms of latent generative variables $\\theta$, i.e,  $f(w_i,\\theta) \\rightarrow x_i$.  In order to estimate the functional dependency, GMUs measure the minimum error $\\sum (f(w_i,\\theta)-x_i)^2$ incurred in the generation process by optimizing $\\theta$ for each input datapoint. Subsequently, GMUs map the error into a functional dependency measure via an appropriate scalar function, and forward it to the next layer for further computation. In GMUs, the weights $[w_1,w_2,..,w_d]$ can therefore be interpreted as the $\\textit{generative weights}$. We first compare the generalization ability of GMUs and multi-layered-perceptrons (MLPs) via comprehensive synthetic experiments across a range of diverse settings. The most notable finding is that when the input is a sparse linear combination of latent generating variables, GMUs generalize significantly better than MLPs. Subsequently, we evaluate Resnet MLP networks where the first feedforward layer is replaced by GMUs (GMU-MLP) on 30 tabular datasets and find that in most cases, GMU-MLPs generalize better than the MLP baselines. We also compare GMU-MLP to a set of other benchmarks, including TabNet, XGBoost, etc. Lastly, we evaluate GMU-CNNs on three standard vision datasets and find that in all cases they generalize better than the corresponding CNN baselines. We also find that GMU-CNNs are significantly more robust to test-time corruptions."
    },
    {
        "title": "Nested Gloss Makes Large Language Models Lost",
        "link_suffix": "/forum?id=Q3oAX9HoH2",
        "link": "https://openreview.net/forum?id=Q3oAX9HoH2",
        "pdf_link": "https://openreview.net/pdf?id=Q3oAX9HoH2",
        "keywords": "Trustworthy Machine Learning; Large Language Model",
        "abstract": "Large language models (LLMs) have succeeded significantly in various applications but remain susceptible to adversarial jailbreaks that void their safety guardrails. \nPrevious attempts to exploit these vulnerabilities often rely on high-cost computational extrapolations, which may not be practical or efficient. \nIn this paper, inspired by the authority influence demonstrated in the Milgram experiment, we present a lightweight method to take advantage of the LLMs' personification capabilities to construct $\\textit{a virtual, nested scene}$, allowing it to realize an adaptive way to escape the usage control in a normal scenario.\nEmpirically, the contents induced by our approach can achieve leading harmfulness rates with previous counterparts and realize a continuous jailbreak in subsequent interactions, which reveals the critical weakness of self-losing on both open-source and closed-source LLMs, $\\textit{e.g.}$, Llama-2, Llama-3, GPT-3.5, GPT-4, and GPT-4o."
    },
    {
        "title": "MoDeGPT: Modular Decomposition for Large Language Model Compression",
        "link_suffix": "/forum?id=8EfxjTCg2k",
        "link": "https://openreview.net/forum?id=8EfxjTCg2k",
        "pdf_link": "https://openreview.net/pdf?id=8EfxjTCg2k",
        "keywords": "LLM, model compression, matrix decomposition",
        "abstract": "Large Language Models (LLMs) have significantly advanced AI with their exceptional performance across a wide range of tasks. However, their extensive computational requirements restrict their use on devices with limited resources.\nWhile recent compression methods based on low-rank matrices show potential\nsolutions, they often suffer from significant loss of accuracy or introduce substantial\noverhead in parameters and inference time. In this paper, we introduce Modular De-\ncomposition (MoDeGPT), a new, efficient, and structured compression framework\nthat overcomes these limitations. MoDeGPT jointly decomposes pairs of consecu-\ntive subcomponents within Transformer blocks, reduces hidden dimensions through\noutput reconstruction on a larger structural scale than conventional low-rank meth-\nods, and repurposes three classical matrix decomposition algorithms\u2014Nystr\u00f6m\napproximation, CR decomposition, and SVD\u2014to ensure bounded errors in our\nnovel decomposition approach. Our experiments show that MoDeGPT, without\nrelying on backward propagation, consistently matches or surpasses the performance of prior techniques that depend on gradient information, while achieving a\n98% reduction in compute costs when compressing a 13B-parameter model. On\nLLaMA-2/3 and OPT models, MoDeGPT retains 90-95% of zero-shot performance\nwith compression rates of 25-30%. The compression process can be completed on\na single GPU in a few hours, boosting inference throughput by up to 46%."
    },
    {
        "title": "Graphon Neural Differential Equations and Transferabilty of Graph Neural Differential Equations",
        "link_suffix": "/forum?id=8XgC2RDm4W",
        "link": "https://openreview.net/forum?id=8XgC2RDm4W",
        "pdf_link": "https://openreview.net/pdf?id=8XgC2RDm4W",
        "keywords": "Graphon Neural Networks, Graphon Neural Differential Equations, Transferabilty, Data-driven Modeling, Generalization, Graph Limits",
        "abstract": "Graph Neural Differential Equations (GNDEs) extend Graph Neural Networks (GNNs) to a continuous-depth framework, providing a robust tool for modeling complex network dynamics. In this paper, we investigate the potential of GNDEs for transferring knowledge across different graphs with shared convolutional structures. To bridge the gap between discrete and continuous graph representations, we introduce Graphon Neural Differential Equations (Graphon-NDEs) as the continuous limit of GNDEs. Using tools from nonlinear evolution equations and graph limit theory, we rigorously establish this continuum limit and develop a mathematical framework to quantify the approximation error between a GNDE and its corresponding Graphon-NDE, which decreases as the number of nodes increases, ensuring reliable transferability. We further derive specific rates for various graph families, providing practical insights into the performance of GNDEs. These findings extend recent results on GNNs to the continuous-depth setting and reveal a fundamental trade-off between discriminability and transferability in GNDEs."
    },
    {
        "title": "Fight Fire with Fire: Multi-biased Interactions in Hard-Thresholding",
        "link_suffix": "/forum?id=YvOq7jHT6R",
        "link": "https://openreview.net/forum?id=YvOq7jHT6R",
        "pdf_link": "https://openreview.net/pdf?id=YvOq7jHT6R",
        "keywords": "Optimzation, Biased Gardient, Zeroth-Order, Hard-Thresholding",
        "abstract": "$\\ell_0$ constrained optimization is widely used in machine learning, especially for high-dimensional problems, as it effectively promotes sparse learning. A prominent technique for solving these problems is Hard-Thresholding gradient descent. However, the inherent expansibility of Hard-Thresholding operators can lead to convergence issues, necessitating strategies to accelerate the algorithm. In this article, we believe the random Hard-Thresholding algorithm can be interpreted as an equivalent biased gradient algorithm. By introducing appropriate biases, we can mitigate some of the issues of Hard-Thresholding and enhance convergence. We categorize the biases into memory-biased and recursively-biased, examining their distinct applications within Hard-Thresholding algorithms. Next, we explore the Zeroth-Order versions of these algorithms, which introduce additional biases from Zeroth-Order gradients. Our findings indicate that recursively bias effectively counteracts some of the issues caused by Hard-Thresholding, resulting in improved performance for First-Order algorithms. Conversely, due to the accumulation of errors from Zeroth-Order gradients during recursive bias, the performance of Zeroth-Order algorithms is inferior to that influenced by historical gradients. To address these insights, we propose the SARAHT and BVR-SZHT algorithms for First-Order and Zeroth-Order Hard-Thresholding, respectively, both of which demonstrate faster convergence speeds compared to previous methods. We validate our hypotheses through black-box adversarial experiments and ridge regression evaluations."
    },
    {
        "title": "One-step Noisy Label Mitigation",
        "link_suffix": "/forum?id=20mMK8UlFh",
        "link": "https://openreview.net/forum?id=20mMK8UlFh",
        "pdf_link": "https://openreview.net/pdf?id=20mMK8UlFh",
        "keywords": "noisy labels, image-text matching, cross-modal matching, multimodal learning, image classification, noisy correspondences",
        "abstract": "Mitigating the detrimental effects of noisy labels on the training process has become increasingly critical, as obtaining entirely clean or human-annotated samples for large-scale pre-training tasks is often impractical. Nonetheless, existing noise mitigation methods often encounter limitations in practical applications due to their task-specific design, model dependency, and significant computational overhead. In this work, we exploit the properties of high-dimensional orthogonality to identify a robust and effective boundary in cone space for separating clean and noisy samples. Building on this, we propose One-step Anti-Noise (OSA), a model-agnostic noisy label mitigation paradigm that employs an estimator model and a scoring function to assess the noise level of input pairs through just one-step inference, a cost-efficient process. We empirically demonstrate the superiority of OSA, highlighting its enhanced training robustness, improved task transferability, ease of deployment, and reduced computational costs across various benchmarks, models, and tasks. Our code is released athttps://anonymous.4open.science/r/CLIP_OSN-E86C."
    },
    {
        "title": "Kick Bad Guys Out! Conditionally Activated Anomaly Detection in Federated Learning with Zero-Knowledge Proof Verification",
        "link_suffix": "/forum?id=LAsMFAg4Zf",
        "link": "https://openreview.net/forum?id=LAsMFAg4Zf",
        "pdf_link": "https://openreview.net/pdf?id=LAsMFAg4Zf",
        "keywords": "outlier detection, FL security, defense, anomaly detection, zero-knowledge-proof",
        "abstract": "Federated Learning (FL) systems are susceptible to adversarial attacks, where malicious clients submit poisoned models to disrupt the convergence or plant backdoors that cause the global model to misclassify some samples. Current defense methods are often impractical for real-world FL systems, as they either rely on unrealistic prior knowledge or cause accuracy loss even in the absence of attacks. Furthermore, these methods lack a protocol for verifying execution, leaving participants uncertain about the correct execution of the mechanism. To address these challenges, we propose a novel anomaly detection strategy that is designed for real-world FL systems. Our approach activates the defense only when potential attacks are detected, and enables the removal of malicious models without affecting the benign ones. Additionally, we incorporate zero-knowledge proofs to ensure the integrity of the proposed defense mechanism. Experimental results demonstrate the effectiveness of our approach in enhancing FL system security against a comprehensive set of adversarial attacks in various ML tasks."
    },
    {
        "title": "Less is More: Adaptive Coverage for Synthetic Training Data",
        "link_suffix": "/forum?id=NpsgBKlApa",
        "link": "https://openreview.net/forum?id=NpsgBKlApa",
        "pdf_link": "https://openreview.net/pdf?id=NpsgBKlApa",
        "keywords": "Large Language Models (LLMs), Synthetic Data Generation, Sampling Algorithms, Maximum Coverage Problem, Data Efficiency",
        "abstract": "Synthetic training data generation with Large Language Models (LLMs) like Google's Gemma and OpenAI's GPT offer a promising solution to the challenge of obtaining large, labeled datasets for training classifiers, especially when rapid model deployment is critical, such as classifying emerging social media trends or combating new forms of online abuse tied to current events. While prior research has examined the comparability of synthetic data to human-labeled data, this study introduces a novel sampling algorithm based on the maximum coverage problem to select a representative subset from a synthetically generated dataset. Our results demonstrate that training a classifier on this contextually sampled subset achieves superior performance compared to training on the entire dataset. This ``less is more'' approach not only improves accuracy but also reduces the volume of data required, leading to potentially more efficient training."
    },
    {
        "title": "Perplexed by Perplexity: Perplexity-Based Data Pruning With Small Reference Models",
        "link_suffix": "/forum?id=1GTARJhxtq",
        "link": "https://openreview.net/forum?id=1GTARJhxtq",
        "pdf_link": "https://openreview.net/pdf?id=1GTARJhxtq",
        "keywords": "Data, Data Filtering, Data Pruning, Pretraining, Perplexity, Large Language Model, LLM",
        "abstract": "In this work, we investigate whether small language models can determine high-quality subsets of large-scale text datasets that improve the performance of larger language models. While existing work has shown that pruning based on the perplexity of a larger model can yield high-quality data, we investigate whether smaller models can be used for perplexity-based pruning and how pruning is affected by the domain composition of the data being pruned. We demonstrate that for multiple dataset compositions, perplexity-based pruning of pretraining data can significantly improve downstream task performance: pruning based on perplexities computed with a 125 million parameter model improves the average performance on downstream tasks of a 3 billion parameter model by up to 2.04 and achieves up to a 1.45\u00d7 reduction in pretraining steps to reach commensurate baseline performance. Furthermore, we demonstrate that such perplexity-based data pruning also yields downstream performance gains in the over-trained and data-constrained regimes."
    },
    {
        "title": "Scaling Laws for Pre-training Agents and World Models",
        "link_suffix": "/forum?id=D0XpSucS3l",
        "link": "https://openreview.net/forum?id=D0XpSucS3l",
        "pdf_link": "https://openreview.net/pdf?id=D0XpSucS3l",
        "keywords": "world modeling, imitation learning, scaling laws",
        "abstract": "The performance of embodied agents has been shown to improve by increasing model parameters, dataset size, and compute. This has been demonstrated in domains from robotics to video games, when simple learning objectives on offline datasets (pre-training) are used to model an agent's behavior (imitation learning) or their environment (world modeling). This paper characterizes the role of scale in these tasks more precisely. Going beyond the simple intuition that `bigger is better', we show that the same types of power laws found in language modeling (e.g. between loss and optimal model size), also arise in world modeling and imitation learning. However, the coefficients of these laws are influenced by the tokenizer, task & architecture -- this has important implications on optimal sizing of models and data."
    },
    {
        "title": "Progressive LLM Alignments Using Two-Player Games",
        "link_suffix": "/forum?id=E6kQ51yfAj",
        "link": "https://openreview.net/forum?id=E6kQ51yfAj",
        "pdf_link": "https://openreview.net/pdf?id=E6kQ51yfAj",
        "keywords": "large language models, alignment, safety",
        "abstract": "Alignment of large language models (LLM) is a process that ensures the model\u2019s responses to user prompts align with human intentions and social values. This optimization typically relies on pre-collected prompts. The collection of these prompts often either requires careful human interventions or proves to be difficult to have a good coverage over all scenarios an LLM can improve over . To address this issue, we propose an alignment method based on a two-agent game, consisting of an adversarial agent and a defensive agent. The adversarial agent\u2019s task is to generate prompts that expose the deficiencies of the defensive agent. At the same time, the defensive agent improves its performance on the prompts generated by the adversary based on feedback from the reward model. This iterative process is repeated to enhance the model\u2019s performance. We theoretically demonstrate that, under mild assumptions, this iterative alignment process converges to a Nash equilibrium by both agents. Learning in this competitive environment results in policies with better generalization capabilities. We demonstrate the advantage of our framework using extensive experiments."
    },
    {
        "title": "Energy-Based Discrete Mask Approximation for 3D Molecular Graph Explanation",
        "link_suffix": "/forum?id=yarlMUJePB",
        "link": "https://openreview.net/forum?id=yarlMUJePB",
        "pdf_link": "https://openreview.net/pdf?id=yarlMUJePB",
        "keywords": "3D Graph Explanation, 3D Molecular Graphs, Energy-Based Models, Discrete Masks",
        "abstract": "In recent years, Graph Neural Networks (GNNs) have become a powerful tool for modeling molecular data. To enhance their reliability and interpretability, various explanation methods have been developed to identify key molecular substructures, specifically a set of edges, in the decision-making process. Early work with 2D GNNs represented molecules as graphs with atoms as nodes and bonds as edges, neglecting 3D geometric configurations. While existing explanation methods perform well on 2D GNNs, there is a pressing need for 3D explanation methods tailored for 3D GNNs, which outperform 2D GNNs in many tasks. Current explanation methods struggle with 3D GNNs due to the construction of edges based on cut-off distances in 3D GNNs, resulting in an exponentially large number of edges. We identify the sources of errors in explanations and decompose them into two components based on a derived upper bound between the optimized masks and the actual explanatory subgraph. This gap can be significant, especially for 3D GNNs because of the large number of edges. To achieve optimal explanation fidelity, our method aims to bridge this gap by assigning two energy values to each atom based on its contribution to the prediction: one energy reflects the scenario where this node is important in making the decision, while the other represents the scenario where it is unimportant. In analogy to physics, lower energy values indicate greater stability in the prediction, and thus, we are more confident about the scenario with which it is associated. Our approach strives to push up and down the energies, respectively, to distinguish these two scenarios to simultaneously minimize both components of the derived upper bound of error, enabling us to identify a stable subgraph that maintains high explanation fidelity. Experiments conducted on backbone networks and the QM9 dataset demonstrate the effectiveness of our method in providing accurate and reliable explanations for 3D graphs."
    },
    {
        "title": "Self-Taught Evaluators",
        "link_suffix": "/forum?id=I7uCwGxVnl",
        "link": "https://openreview.net/forum?id=I7uCwGxVnl",
        "pdf_link": "https://openreview.net/pdf?id=I7uCwGxVnl",
        "keywords": "LLM, Evaluation, Reward Model, Evaluator",
        "abstract": "Model-based evaluation is at the heart of successful model development -- as a reward model for training, and as a replacement for human evaluation. To train such evaluators, the standard approach is to collect a large amount of human preference judgments over model responses, which is costly and the data becomes stale as models improve. In this work, we present an approach that aims to im-prove evaluators without human annotations, using synthetic training data only. Starting from unlabeled instructions, our iterative self-improvement scheme generates contrasting model outputs and trains an LLM-as-a-Judge to produce reasoning traces and final judgments, repeating this training at each new iteration using the improved predictions. Without any labeled preference data, our Self-Taught Evaluator can improve a strong LLM (Llama3-70B-Instruct) from 75.4 to 88.3 (88.7 with majority vote) on RewardBench. This outperforms commonly used LLM judges such as GPT-4 and matches the performance of the top-performing reward models trained with labeled examples."
    },
    {
        "title": "Tradeoffs Between Alignment and Helpfulness in Language Models with Representation Engineering",
        "link_suffix": "/forum?id=QFmnhgEnIB",
        "link": "https://openreview.net/forum?id=QFmnhgEnIB",
        "pdf_link": "https://openreview.net/pdf?id=QFmnhgEnIB",
        "keywords": "Language model alignment, representation engineering",
        "abstract": "Language model alignment has become an important component of AI safety, allowing safe interactions between humans and language models, by enhancing desired behaviors and inhibiting undesired ones. It is often done by tuning the model or inserting preset aligning prompts. Recently, representation engineering, a method which alters the model\u2019s behavior via changing its representations post-training, was shown to be effective in aligning LLMs (Zou et al., 2023a). Representation engineering yields gains in alignment oriented tasks such as resistance to adversarial attacks and reduction of social biases, but was also shown to cause a decrease in the ability of the model to perform basic tasks. In this paper we study the tradeoff between the increase in alignment and decrease in helpfulness of the model. We propose a theoretical framework which provides bounds for these two quantities, and demonstrate their relevance empirically. First, we find that under the conditions of our framework, alignment can be guaranteed with representation engineering, and at the same time that helpfulness is harmed in the process. Second, we show that helpfulness is harmed quadratically with the norm of the representation engineering vector, while the alignment increases linearly with it, indicating a regime in which it is efficient to use representation engineering. We validate our findings empirically, and chart the boundaries to the usefulness of representation engineering for alignment."
    },
    {
        "title": "Neural ODE with Differentiable Hidden State for Irregular Time Series",
        "link_suffix": "/forum?id=jKNv7Fvurt",
        "link": "https://openreview.net/forum?id=jKNv7Fvurt",
        "pdf_link": "https://openreview.net/pdf?id=jKNv7Fvurt",
        "keywords": "time series analysis, irregular time series, neural ordinary differential equations",
        "abstract": "Capturing the continuous underlying dynamics of irregular time series is essential for accurately reflecting the ongoing evolution and intricate correlations within the data. The discrete nature of current models, including RNN-based models and transformer variants, poses challenges when it comes to generalizing to the continuous-time data paradigms, which is necessary for capturing ongoing dynamics of irregular time series. \nNeural Ordinary Differential Equations (NODEs) assume a continuous latent dynamic and provide an elegant framework for irregular time series analysis. However, integrating new information while maintaining the continuity of latent dynamics remains challenging. \nTo tackle this problem, we introduce Differentiable Hidden State (DHS) enhanced neural ODE, a data-dependent framework that is capable of effectively capturing temporal dependencies and ensuring the continuity of the hidden process. We leverage the theory of generalized inverses to innovatively compute attention mechanism in reverse and obtain a continuous representation. To capture more accurate temporal relationships, we introduce Hoyer metric and maximize the sparsity of it. Experimental results on both synthetic and real-world datasets demonstrate the effectiveness of our model."
    },
    {
        "title": "RealEra:Semantic-level Concept Erasure via Neighbor-Concept Mining",
        "link_suffix": "/forum?id=caY45V0dYt",
        "link": "https://openreview.net/forum?id=caY45V0dYt",
        "pdf_link": "https://openreview.net/pdf?id=caY45V0dYt",
        "keywords": "concept erasure, diffusion model, text-image generation",
        "abstract": "The remarkable development of text-to-image generation models has raised notable security concerns, such as the infringement of portrait rights and the generation of inappropriate content. Concept erasure has been proposed to remove the model's knowledge about protected and inappropriate concepts. Although many methods have tried to balance the efficacy (erasing target concepts) and specificity (retaining irrelevant concepts), they can still generate abundant erasure concepts under the steering of semantically related inputs. In this work, we propose RealEra to address this \"concept residue\" issue. Specifically, we first introduce the mechanism of neighbor-concept mining, digging out the associated concepts by adding random perturbation into the embedding of erasure concept, thus expanding the erasing range and eliminating the generations even through associated concept inputs. Furthermore, to mitigate the negative impact on the generation of irrelevant concepts caused by the expansion of erasure scope, RealEra preserves the specificity through the beyond-concept regularization. This makes irrelevant concepts maintain their corresponding spatial position, thereby preserving their normal generation performance. We also employ the closed-form solution to optimize weights of U-Net for the cross-attention alignment, as well as the prediction noise alignment with the LoRA module. Extensive experiments on multiple benchmarks demonstrate that RealEra outperforms previous concept erasing methods in terms of superior erasing efficacy, specificity, and generality."
    },
    {
        "title": "Measuring LLM Confidence through Stable Explanations",
        "link_suffix": "/forum?id=apPItJe0wO",
        "link": "https://openreview.net/forum?id=apPItJe0wO",
        "pdf_link": "https://openreview.net/pdf?id=apPItJe0wO",
        "keywords": "LLM, Uncertainty Quantification, Confidence Estimation, Trustworthiness",
        "abstract": "In many high-risk machine learning applications it is essential for a model to indicate when it is uncertain about a prediction. While large language models (LLMs) can reach and even surpass human-level accuracy on a variety of benchmarks, their overconfidence in incorrect responses is still a well-documented failure mode. Traditional methods for ML uncertainty quantification can be difficult to directly adapt to LLMs due to the computational cost of implementation and closed-source nature of many models. A variety of black-box methods have recently been proposed, but these often rely on heuristics such as self-verbalized confidence. We instead propose a framework for measuring an LLM's uncertainty with respect to the distribution of generated explanations for an answer. While utilizing explanations is not a new idea in and of itself, by interpreting each possible model+explanation pair as a test-time classifier we can calculate a posterior answer distribution over the most likely of these classifiers. We demonstrate how a specific instance of this framework using explanation entailment as our classifier likelihood improves confidence score metrics (in particular AURC and AUROC) over baselines across five different datasets. We believe these results indicate that our framework is both a well-principled and effective way of quantifying uncertainty in LLMs."
    },
    {
        "title": "AsymDreamer: Safe Reinforcement Learning From Pixels with Privileged World Models",
        "link_suffix": "/forum?id=3rSeDrPj4B",
        "link": "https://openreview.net/forum?id=3rSeDrPj4B",
        "pdf_link": "https://openreview.net/pdf?id=3rSeDrPj4B",
        "keywords": "Safe Reinforcement Learing; World Model",
        "abstract": "Safe Reinforcement Learning from partial observations frequently struggles with rapid performance degradation and often fails to satisfy safety constraints. Upon deeper analysis, we attribute this problem to the lack of necessary information in partial observations and inadequate sample efficiency. World Models can help mitigate this issue, as they offer high sample efficiency and the capacity to memorize historical information. In this work, we introduce AsymDreamer, an approach based on the Dreamer framework that specializes in exploiting low-dimensional privileged information to build world models, thereby enhancing the prediction capability of critics. To ensure safety, we employ the Lagrangian method to incorporate safety constraints. Additionally, we formulate our approach as an Asymmetric CPOMDPs (ACPOMDPs) framework and analyze its superiority compared to the standard CPOMDP framework. Various experiments conducted on the Safety-Gymnasium benchmark demonstrate that our approach outperforms existing approaches dramatically in terms of performance and safety."
    },
    {
        "title": "CoCoGesture: Towards Coherent Co-speech 3D Gesture Generation in the Wild",
        "link_suffix": "/forum?id=g3kK6YBSZ1",
        "link": "https://openreview.net/forum?id=g3kK6YBSZ1",
        "pdf_link": "https://openreview.net/pdf?id=g3kK6YBSZ1",
        "keywords": "Co-speech Gesture Generation, Human Motion Modeling",
        "abstract": "Deriving co-speech 3D gestures has seen tremendous progress in virtual avatar animation. Yet, the existing methods often produce stiff and unreasonable gestures with unseen human speech inputs due to the limited 3D speech-gesture data. In this paper, we propose $\\textbf{CoCoGesture}$, a novel framework enabling coherent and diverse gesture synthesis from unseen human speech prompts. \nOur key insight is built upon the custom-designed pretrain-fintune training paradigm. At the pretraining stage, we aim to formulate a large generalizable gesture diffusion model by learning the abundant postures manifold. Therefore, to alleviate the scarcity of 3D data, we first construct a large-scale co-speech 3D gesture dataset containing more than $40$M meshed posture instances across $4.3$K speakers, dubbed $\\textbf{GES-X}$. Then, we scale up the large unconditional diffusion model to 1B parameters and pre-train it to be our gesture experts. At the finetune stage, we present the audio ControlNet that incorporates the human voice as condition prompts to guide the gesture generation. Here, we construct the audio ControlNet through a trainable copy of our pre-trained diffusion model. Moreover, we design a novel Mixture-of-Gesture-Experts (MoGE) block to adaptively fuse the audio embedding from the human speech and the gesture features from the pre-trained gesture experts with a routing mechanism. Such an effective manner ensures audio embedding is temporal coordinated with motion features while preserving the vivid and diverse gesture generation. Extensive experiments demonstrate that our proposed CoCoGesture outperforms the state-of-the-art methods on the zero-shot speech-to-gesture generation. The dataset will be publicly available at:https://anonymous.4open.science/w/GES-X/."
    },
    {
        "title": "Beyond Squared Error: Exploring Loss Design for Enhanced Training of Generative Flow Networks",
        "link_suffix": "/forum?id=4NTrco82W0",
        "link": "https://openreview.net/forum?id=4NTrco82W0",
        "pdf_link": "https://openreview.net/pdf?id=4NTrco82W0",
        "keywords": "GFlowNet, Generative Models, f-Divergence, Loss Function",
        "abstract": "Generative Flow Networks (GFlowNets) are a novel class of generative models designed to sample from unnormalized distributions and have found applications in various important tasks, attracting great research interest in their training algorithms. In general, GFlowNets are trained by fitting the forward flow to the backward flow on sampled training objects. Prior work focused on the choice of training objects, parameterizations, sampling and resampling strategies, and backward policies, aiming to enhance credit assignment, exploration, or exploitation of the training process. However, the choice of regression loss, which can highly influence the exploration and exploitation behavior of the under-training policy, has been overlooked. Due to the lack of theoretical understanding for choosing an appropriate regression loss, most existing algorithms train the flow network by minimizing the squared error of the forward and backward flows in log-space, i.e., using the quadratic regression loss. In this work, we rigorously prove that distinct regression losses correspond to specific divergence measures, enabling us to design and analyze regression losses according to the desired properties of the corresponding divergence measures. Specifically, we examine two key properties: zero-forcing and zero-avoiding, where the former promotes exploitation and higher rewards, and the latter encourages exploration and enhances diversity. Based on our theoretical framework, we propose three novel regression losses, namely, Shifted-Cosh, Linex(1/2), and Linex(1). We evaluate them across three benchmarks: hyper-grid, bit-sequence generation, and molecule generation. Our proposed losses are compatible with most existing training algorithms, and significantly improve the performances of the algorithms concerning convergence speed, sample diversity, and robustness."
    },
    {
        "title": "MultiTrust: Enhancing Safety and Trustworthiness of Large Language Models from Multiple Perspectives",
        "link_suffix": "/forum?id=icUCCz8pAu",
        "link": "https://openreview.net/forum?id=icUCCz8pAu",
        "pdf_link": "https://openreview.net/pdf?id=icUCCz8pAu",
        "keywords": "Large Language Models, Safety, Trustworthiness, Robustness, Fairness, Truthfulness",
        "abstract": "Large Language Models (LLMs) have shown impressive performance across various tasks, yet they still face significant safety and trustworthiness challenges, such as robustness, fairness, and truthfulness. Addressing these challenges is critical for the reliable deployment of LLMs. Directly fine-tuning LLMs to enhance safety can degrade their performance and is challenging to balance across multiple safety perspectives due to the forgetting phenomenon. In this paper, we propose MultiTrust, a novel and scalable framework designed to enhance LLM safety from multiple safety perspectives. In particular, MultiTrust first generates challenging training data through adversarial optimizations, focusing on LLMs trustworthiness perspectives, such as robustness, fairness, and safety. MultiTrust then separately train safety auxiliary models for each perspective using supervised fine-tuning and Direct Preference Optimization (DPO). MultiTrust augments a base model with these safety auxiliary models on the fly through dynamic routing and logit ensembling, significantly boosting the performance across different trustworthiness metrics for the base model while preserving its helpfulness. Notably, MultiTrust introduces an effective perplexity-based inference-time router to seamlessly integrate these safety auxiliary models by averaging the logit outputs of the selected safety auxiliary model and the base model, which enhances the stability of the final performance. Moreover, MultiTrust's flexible design allows for the augmentation with new safety auxiliary models for different perspectives without necessitating additional training or adaptation. Extensive experimental results show that MultiTrust, which trains a series of 7B safety auxiliary models, significantly improves the trustworthiness of the base LLM across different sizes (7B and 13B). For instance, MultiTrust increased the average performance of Llama2-13B from 35.54% to 51.14%, and Vicuna-13B from 29.91% to 52.82%, outperforming models with similar and even larger sizes across different perspectives. These results underscore the effectiveness and scalability of MultiTrust in enhancing the safety and reliability of LLMs."
    },
    {
        "title": "Discriminator-Guided Embodied Planning for LLM Agent",
        "link_suffix": "/forum?id=TjP1d8PP8l",
        "link": "https://openreview.net/forum?id=TjP1d8PP8l",
        "pdf_link": "https://openreview.net/pdf?id=TjP1d8PP8l",
        "keywords": "LLM Agent, Embodied Planning, Discriminator, Critic-Regularized Optimization",
        "abstract": "Large Language Models (LLMs) have showcased remarkable reasoning capabilities in various domains, yet face challenges in complex embodied tasks due to the need for a coherent long-term policy and context-sensitive environmental understanding. Previous work performed LLM refinement relying on outcome-supervised feedback, which can be costly and ineffective. In this work, we introduce a novel framework, Discriminator-Guided Action Optimization (DGAP), for facilitating the optimization of LLM action plans via step-wise signals. Specifically, we employ a limited set of demonstrations to enable the discriminator to learn a score function, which assesses the alignment between LLM-generated actions and the underlying optimal ones at every step. Based on the discriminator, LLMs are prompted to generate actions that maximize the score, utilizing historical action-score pair trajectories as guidance. Under mild conditions, DGAP resembles critic-regularized optimization and has been demonstrated to achieve a stronger policy than the LLM planner. In experiments across different LLMs (GPT-4, Llama3-70B) in ScienceWorld and VirtualHome, our method achieves superior performance and better efficiency than previous methods."
    },
    {
        "title": "DRIMA: Differential Reward Interaction for Cooperative Multi-Agent Reinforcement Learning",
        "link_suffix": "/forum?id=MUTTUDKb3M",
        "link": "https://openreview.net/forum?id=MUTTUDKb3M",
        "pdf_link": "https://openreview.net/pdf?id=MUTTUDKb3M",
        "keywords": "Multi-agent reinforcement learning, Distributed training and execution, Multi-player stochastic game",
        "abstract": "Multi-agent reinforcement learning (MARL) owning to its potent capabilities in complex systems has gained remarkable research attention nowadays, in which collaborative decision-making and control for multi-agent systems is one of the key research focuses.\nThe prevalent learning framework is centralized training with decentralized execution (CTDE), in which the decentralized execution realizes strategy flexibility, and the use of centralized training ensures stationarity and goal consistency while becoming incapable when facing scalability and complexity situations.\nTo address this issue, we follow the concept of distributed training with decentralized execution (DTDE).\nDecentralization is naturally accompanied by the game during the learning process, which has not been entirely studied in related work, resulting in the constrained strategy combination of MARL.\nIn this paper, we devise a novel approach of differential reward interaction (DRI) with conflict-triggered for the distributed evaluation that enables overall goal consistency through highly efficient local information exchange.\nWith this collaborative learning method, the DRI-based MARL can eliminate the notorious issue of converging to saddle equilibriums of stochastic games.\nMeanwhile, it possesses provable convergence and is well compatible for general value-based and policy-based algorithms.\nExperiments in several benchmark scenarios demonstrate that DRIMA realizes collaborative strategy learning with enhanced global goal-achieving."
    },
    {
        "title": "Adam Exploits\u2113\u221e-geometry of Loss Landscape via Coordinate-wise Adaptivity",
        "link_suffix": "/forum?id=PUnD86UEK5",
        "link": "https://openreview.net/forum?id=PUnD86UEK5",
        "pdf_link": "https://openreview.net/pdf?id=PUnD86UEK5",
        "keywords": "Adam, coordinate-wise adaptivity, adaptive algorithms, infinity norm",
        "abstract": "Adam outperforms SGD when training language models. Yet such benefits are not well-understood theoretically --  previous convergence analysis for Adam and SGD mainly focuses on the number of steps $T$ and is already minimax-optimal in non-convex cases, which are both $O(T^{-1/4})$. In this work, we argue that the better dependence on the loss smoothness is the key advantage of Adam over SGD. More specifically, we give a new convergence analysis for Adam under novel assumptions that loss is smooth under $\\ell_\\infty$ geometry rather than the more common $\\ell_2$ geometry, which yields a much better empirical smoothness constant for GPT-2 and ResNet models. Moreover, we show that if we rotate the training loss randomly, Adam can be outperformed by some variants of SGD which is invariant to rotations. This implies that any practically relevant explanation of Adam's optimization benefit must involve non-rotational invariant properties of loss, such as $\\ell_\\infty$ smoothness as used in our analysis. We also extend the convergence analysis to blockwise Adam, which is a generalization of standard Adam."
    }
]