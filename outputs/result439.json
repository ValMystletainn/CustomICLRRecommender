[{"title": "MotionCLR: Motion Generation and Training-free Editing via Understanding Attention Mechanisms", "link_suffix": "/forum?id=ijQp6HA4rK", "link": "https://openreview.net/forum?id=ijQp6HA4rK", "pdf_link": "https://openreview.net/pdf?id=ijQp6HA4rK", "keywords": "human motion, animation", "abstract": "This research delves into the problem of interactive editing of human motion generation. Previous motion diffusion models lack explicit modeling of the word-level text-motion correspondence and good explainability, hence restricting their fine-grained editing ability. To address this issue, we propose an attention-based motion diffusion model, namely MotionCLR, with CLeaR modeling of attention mechanisms. Technically, MotionCLR models the in-modality and cross-modality interactions with self-attention and cross-attention, respectively. More specifically, the self-attention mechanism aims to measure the sequential similarity between frames and impacts the order of motion features. By contrast, the cross-attention mechanism works to find the fine-grained word-sequence correspondence and activate the corresponding timesteps in the motion sequence. Based on these key properties, we develop a versatile set of simple yet effective motion editing methods via manipulating attention maps, such as motion (de-)emphasizing, in-place motion replacement, and example-based motion generation, etc. For further verification of the explainability of the attention mechanism, we additionally explore the potential of action-counting and grounded motion generation ability via attention maps. Our experimental results show that our method enjoys good generation and editing ability with good explainability. Codes will be public.", "title_embedding_index": 21900, "title_abs_embedding_index": 21925}, {"title": "GOPS: Learning Generative Object Priors for Unsupervised 3D Instance Segmentation", "link_suffix": "/forum?id=wXSshrxlP4", "link": "https://openreview.net/forum?id=wXSshrxlP4", "pdf_link": "https://openreview.net/pdf?id=wXSshrxlP4", "keywords": "3D scene object segmentation, unsupervised learning", "abstract": "We study the hard problem of 3D object segmentation in complex point clouds without requiring human labels of 3D scenes for supervision. By relying on the similarity of pretrained 2D features or external signals such as motion to group 3D points as objects, existing unsupervised methods are usually limited to identifying simple objects like cars or their segmented objects are often inferior due to the lack of objectness in pretrained features. In this paper, we propose a new two-stage pipeline called GOPS. The core concept of our method is to learn generative and discriminative object-centric priors as a foundation from object datasets in the first stage, and then to learn multiple objects by querying against the pretrained priors in the second stage. We extensively evaluate our method on two real-world datasets and a newly created synthetic dataset, demonstrating remarkable segmentation performance, clearly surpassing all existing unsupervised methods.", "title_embedding_index": 21901, "title_abs_embedding_index": 21926}, {"title": "JAILJUDGE: A Comprehensive Jailbreak Judge Benchmark with Multi-Agent Enhanced Explanation Evaluation Framework", "link_suffix": "/forum?id=cLYvhd0pDY", "link": "https://openreview.net/forum?id=cLYvhd0pDY", "pdf_link": "https://openreview.net/pdf?id=cLYvhd0pDY", "keywords": "jailbreak judge, jialbreak evaluation, jialbreak attack, jailbreak defense, LLM", "abstract": "Although significant research efforts have been dedicated to enhancing the safety of large language models (LLMs) by understanding and defending against jailbreak attacks, evaluating the defense capabilities of LLMs against jailbreak attacks also attracts lots of attention. Current evaluation methods lack explainability and do not generalize well to complex scenarios, resulting in incomplete and inaccurate\nassessments (e.g., direct judgment without reasoning explainability, the F1 score of the GPT-4 judge is only 55% in complex scenarios and bias evaluation on multilingual scenarios, etc.). To address these challenges, we have developed a comprehensive evaluation benchmark, JAILJUDGE, which includes a wide range of risk scenarios with complex malicious prompts (e.g., synthetic, adversarial, in-the-wild, and multi-language scenarios, etc.) along with high-quality human-annotated test datasets. Specifically, the JAILJUDGE dataset comprises training data of JAILJUDGE, with over 35k+ instruction-tune training data with reasoning explainability, and JAILJUDGETEST, a 4.5k+ labeled set of broad risk scenarios and a 6k+ labeled set of multilingual scenarios in ten languages. To provide reasoning explanations (e.g., explaining why an LLM is jailbroken or not) and fine-grained evaluations (jailbroken score from 1 to 10), we propose a multi-agent jailbreak judge framework, JailJudge MultiAgent, making the decision inference process explicit and interpretable to enhance evaluation quality. Using this framework, we construct the instruction-tuning ground truth and then instruction-tune an end-to-end jailbreak judge model, JAILJUDGE Guard, which can also provide reasoning explainability with fine-grained evaluations without API costs. Additionally, we introduce JailBoost, an attacker-agnostic attack enhancer, and GuardShield, a safety moderation defense method, both based on JAILJUDGE Guard. Comprehensive experiments demonstrate the superiority of our JAILJUDGE benchmark and jailbreak judge methods. Our jailbreak judge methods (JailJudge MultiAgent and JAILJUDGE Guard) achieve SOTA performance in closed-source models (e.g.,\nGPT-4) and safety moderation models (e.g., Llama-Guard and ShieldGemma, etc.), across a broad range of complex behaviors (e.g., JAILJUDGE benchmark, etc.) to zero-shot scenarios (e.g., other open data, etc.). Importantly, JailBoost and GuardShield, based on JAILJUDGE Guard, can enhance downstream tasks in jailbreak attacks and defenses under zero-shot settings with significant improvement (e.g., JailBoost can increase the average performance by approximately 29.24%, while GuardShield can reduce the average defense ASR from 40.46% to 0.15%).", "title_embedding_index": 21902, "title_abs_embedding_index": 21927}, {"title": "Scalable and Certifiable Graph Unlearning: Overcoming the Approximation Error Barrier", "link_suffix": "/forum?id=pPyJyeLriR", "link": "https://openreview.net/forum?id=pPyJyeLriR", "pdf_link": "https://openreview.net/pdf?id=pPyJyeLriR", "keywords": "Machine Unlearning, Graph Neural Networks, Scalability", "abstract": "Graph unlearning has emerged as a pivotal research area for ensuring privacy protection, given the widespread adoption of Graph Neural Networks (GNNs) in applications involving sensitive user data. Among existing studies, certified graph unlearning is distinguished by providing robust privacy guarantees. However, current certified graph unlearning methods are impractical for large-scale graphs because they necessitate the costly re-computation of graph propagation for each unlearning request. Although numerous scalable techniques have been developed to accelerate graph propagation for GNNs, their integration into certified graph unlearning remains uncertain as these scalable approaches introduce approximation errors into node embeddings. In contrast, certified graph unlearning demands bounded model error on exact node embeddings to maintain its certified guarantee.To address this challenge, we present ScaleGUN, the first approach to scale certified graph unlearning to billion-edge graphs. ScaleGUN integrates the approximate graph propagation technique into certified graph unlearning, offering certified guarantees for three unlearning scenarios: node feature, edge and node unlearning. \n  Extensive experiments on real-world datasets demonstrate the efficiency and unlearning efficacy of ScaleGUN. Remarkably, ScaleGUN accomplishes $(\\epsilon,\\delta)=(1,10^{-4})$ certified unlearning on the billion-edge graph ogbn-papers100M in 20 seconds for a 5,000 random edge removal request -- of which only 5 seconds are required for updating the node embeddings -- compared to 1.91 hours for retraining and 1.89 hours for re-propagation. Our code is available athttps://anonymous.4open.science/r/ScaleGUN-5921.", "title_embedding_index": 21903, "title_abs_embedding_index": 21928}, {"title": "ShortcutsBench: A Large-Scale Real-world Benchmark for API-based Agents", "link_suffix": "/forum?id=kKILfPkhSz", "link": "https://openreview.net/forum?id=kKILfPkhSz", "pdf_link": "https://openreview.net/pdf?id=kKILfPkhSz", "keywords": "Benchmark, Agent, LLM, Shortcuts", "abstract": "Recent advancements in integrating large language models (LLMs) with application programming interfaces (APIs) have gained significant interest in both academia and industry. Recent work demonstrates that these API-based agents exhibit relatively strong autonomy and planning capabilities. However, their ability to handle multi-dimensional difficulty levels, diverse task types, and real-world demands remains unknown. \nIn this paper, we introduce ShortcutsBench, a large-scale benchmark for the comprehensive evaluation of API-based agents in solving real-world complex tasks. ShortcutsBench includes a wealth of real APIs from Apple Inc., refined user queries, human-annotated \nhigh-quality action sequences, detailed parameter filling values, and parameters requesting necessary input from the system or user. We put in significant effort in collecting and processing the data. We revealed how existing benchmarks / datasets struggle to accommodate the advanced reasoning capabilities of existing more intelligent LLMs. Moreover, our extensive evaluation of agents built with 5 leading open-source (size >= 57B) and 5 closed-source LLMs (e.g. Gemini-1.5-Pro and GPT-4o-mini) reveals significant limitations of existing API-based agents in the whole process of handling complex queries related to API selection, parameter filling, and requesting necessary input from the system and the user. These findings highlight the great challenges that API-based agents face in effectively fulfilling real and complex user queries. All datasets, code, experimental logs, and results are available at \\url{https://anonymous.4open.science/r/ShortcutsBench}.", "title_embedding_index": 21904, "title_abs_embedding_index": 21929}, {"title": "Sparse Alignment Enhanced Latent Diffusion Transformer for Zero-Shot Speech Synthesis", "link_suffix": "/forum?id=o362EkNU2z", "link": "https://openreview.net/forum?id=o362EkNU2z", "pdf_link": "https://openreview.net/pdf?id=o362EkNU2z", "keywords": "Zero-Shot Speech Synthesis, Large-Scale TTS, Accented TTS", "abstract": "While recent zero-shot text-to-speech (TTS) models have significantly improved speech quality and expressiveness, \nmainstream systems still suffer from issues related to speech-text alignment modeling: 1) autoregressive large language models are inefficient and not robust in long-sentence inference; 2) non-autoregressive diffusion models without explicit speech-text alignment require substantial model capacity for alignment learning; 3) predefined alignment-based diffusion models suffer from limited expressiveness and a complicated inference pipeline. This paper introduces \\textit{S-DiT}, a TTS system featuring an innovative sparse alignment algorithm that guides the latent diffusion transformer (DiT). Specifically, 1) we provide sparse alignment boundaries to S-DiT to reduce the difficulty of alignment learning without limiting the model's expressiveness; 2) to simplify the overall pipeline, we propose a unified frontend language model (F-LM) training framework to cover various speech processing tasks required by TTS models. Additionally, we adopt the piecewise rectified flow technique to accelerate the generation process and employ a multi-condition classifier-free guidance strategy for accent intensity adjustment. Experiments demonstrate that S-DiT matches state-of-the-art zero-shot TTS speech quality while maintaining a more efficient pipeline. Moreover, our system can generate high-quality one-minute speech with only 8 sampling steps. Audio samples are available athttps://sditdemo.github.io/sditdemo/.", "title_embedding_index": 21905, "title_abs_embedding_index": 21930}, {"title": "Adversarial Tuning: Defending Against Jailbreak Attacks for LLMs", "link_suffix": "/forum?id=mrjOaRyefn", "link": "https://openreview.net/forum?id=mrjOaRyefn", "pdf_link": "https://openreview.net/pdf?id=mrjOaRyefn", "keywords": "jailbreak defense, jialbreak attack, LLM", "abstract": "Although safely enhanced Large Language Models (LLMs) have achieved remarkable success in tackling various complex tasks in a zero-shot manner, they remain susceptible to jailbreak attacks, particularly the unknown jailbreak attack. To enhance LLMs' generalized defense capabilities, we propose a two-stage adversarial tuning framework, which generates adversarial prompts to explore worst-case scenarios by optimizing datasets containing pairs of adversarial prompts and their safe responses. In the first stage, we introduce the hierarchical meta-universal adversarial prompt learning to efficiently and effectively generate token-level adversarial prompts.  In the second stage,  we propose automatic adversarial prompt learning to iteratively construct out-of-distribution adversarial prompts, further enhancing LLM\u2019s defense capabilities.  We conducted comprehensive experiments on three widely used jailbreak datasets, comparing our framework with six defense baselines under five representative attack scenarios. \\fan{ Specifically, for the computational efficiency of generating token-level adversarial prompts, we demonstrate both empirically and theoretically that our method achieves approximately a 15x speedup. Additionally, our methods exhibit superior defense performance against both known and unknown jailbreak attacks. Importantly, our adversarial tuning framework shows broad generalizability across various attack strategies and target LLMs (including the large 110B model), highlighting its potential as a transferable defense mechanism.", "title_embedding_index": 21906, "title_abs_embedding_index": 21931}, {"title": "High-fidelity and realtime 3D Gaussian Head Avatars with Expressive and Compact blenshape representations", "link_suffix": "/forum?id=QmJoF47DIR", "link": "https://openreview.net/forum?id=QmJoF47DIR", "pdf_link": "https://openreview.net/pdf?id=QmJoF47DIR", "keywords": "head avatar, gaussian splatting", "abstract": "Recent studies have combined 3D Gaussian and 3D Morphable Models (3DMM) to achieve real-time, high-quality rendering of controllable head avatars. Several techniques have attempted to express dynamic textures in facial animation when modeling 3D avatars. However, accurately capturing and displaying expressive appearance dynamics while maintaining temporal and spatial efficiency remains a technical challenge. To this end, we propose a novel method for 3D facial avatar modeling that utilizes an expressive and compact model representation, capturing dynamic facial information accurately while ensuring efficiency. We encode texture-related attributes of the 3D Gaussians in the  tensorial feature representation. Specifically, we store color information of the neutral expression in static tri-planes; and represent dynamic texture details for different expressions using lightweight 1D feature lines, which are then decoded into opacity changes relative to the neutral face. Experiments show that this design introduces nonlinear expressiveness to the model, enhancing its performance, while the compact representation maintains real-time rendering capabilities and significantly reduces storage costs. This approach thus broadens the applicability to more scenarios.", "title_embedding_index": 21907, "title_abs_embedding_index": 21932}, {"title": "Learning to Customize Text-to-Image Diffusion In Diverse Context", "link_suffix": "/forum?id=oGYGjPsVWb", "link": "https://openreview.net/forum?id=oGYGjPsVWb", "pdf_link": "https://openreview.net/pdf?id=oGYGjPsVWb", "keywords": "Text-to-Image Generation; Text-to-Image Customization; Subjet-Driven Text-To-Image Generation; Diffusion;", "abstract": "Most text-to-image customization techniques fine-tune models on a small set of \\emph{personal concept} images captured in minimal contexts. This often results in the model becoming overfitted to these training images and unable to generalize to new contexts in future text prompts. Existing customization methods are built on the success of effectively representing personal concepts as textual embeddings. Thus, in this work, we resort to diversifying the context of these personal concepts \\emph{solely} within the textual space by simply creating a contextually rich set of text prompts, together with a widely used self-supervised learning objective. Surprisingly, this straightforward and cost-effective method significantly improves semantic alignment in the textual space, and this effect further extends to the image space, resulting in higher prompt fidelity for generated images. Additionally, our approach does not require any architectural modifications, making it highly compatible with existing text-to-image customization methods. We demonstrate the broad applicability of our approach by combining it with four different baseline methods, achieving notable CLIP score improvements.", "title_embedding_index": 21908, "title_abs_embedding_index": 21933}, {"title": "Which Experiences Are Influential for RL Agents? Efficiently Estimating The Influence of Experiences", "link_suffix": "/forum?id=EWNH3QTSxd", "link": "https://openreview.net/forum?id=EWNH3QTSxd", "pdf_link": "https://openreview.net/pdf?id=EWNH3QTSxd", "keywords": "reinforcement learning, data influence estimation", "abstract": "In reinforcement learning (RL) with experience replay, experiences stored in a replay buffer influence the RL agent's performance. \nInformation about how these experiences influence the agent's performance is valuable for various purposes, such as identifying experiences that negatively influence underperforming agents. \nOne method for estimating the influence of experiences is the leave-one-out (LOO) method. \nHowever, this method is usually computationally prohibitive. \nIn this paper, we present Policy Iteration with Turn-over Dropout (PIToD), which efficiently estimates the influence of experiences. \nWe evaluate how accurately PIToD estimates the influence of experiences and its efficiency compared to LOO. \nWe then apply PIToD to amend underperforming RL agents, i.e., we use PIToD to estimate negatively influential experiences for the RL agents and to delete the influence of these experiences. \nWe show that RL agents' performance is significantly improved via amendments with PIToD.", "title_embedding_index": 21909, "title_abs_embedding_index": 21934}, {"title": "WorldSimBench: Towards Video Generation  Models as World Simulators", "link_suffix": "/forum?id=ejGAytoWoe", "link": "https://openreview.net/forum?id=ejGAytoWoe", "pdf_link": "https://openreview.net/pdf?id=ejGAytoWoe", "keywords": "World Simulator, Embodied AI, Video Generation, Benchmark", "abstract": "Recent advancements in predictive models have demonstrated exceptional capabilities in predicting the future state of objects and scenes. However, the lack of categorization based on inherent characteristics continues to hinder the progress of predictive model development. Additionally, existing benchmarks are unable to effectively evaluate higher-capability, highly embodied predictive models from an embodied perspective. In this work, we classify the functionalities of predictive models into a hierarchy and take the first step in evaluating World Simula\u0002tors by proposing a dual evaluation framework called WorldSimBench. World\u0002SimBench includes Explicit Perceptual Evaluation and Implicit Manipulative Evaluation, encompassing human preference assessments from the visual perspective and action-level evaluations in embodied tasks, covering three representative embodied scenarios: Open-Ended Embodied Environment, Autonomous, Driving, and Robot Manipulation. In the Explicit Perceptual Evaluation, we introduce the HF-Embodied Dataset, a video assessment dataset based on fine-grained human feedback, which we use to train a Human Preference Evaluator that aligns with human perception and explicitly assesses the visual fidelity of World Simu\u0002later. In the Implicit Manipulative Evaluation, we assess the video-action consistency of World Simulators by evaluating whether the generated situation-aware video can be accurately translated into the correct control signals in dynamic environments. Our comprehensive evaluation offers key insights that can drive further innovation in video generation models, positioning World Simulators as a pivotal advancement toward embodied artificial intelligence.", "title_embedding_index": 21910, "title_abs_embedding_index": 21935}, {"title": "DODA: Diffusion for Object-detection Domain Adaptation in Agriculture", "link_suffix": "/forum?id=KUpUO7aSSg", "link": "https://openreview.net/forum?id=KUpUO7aSSg", "pdf_link": "https://openreview.net/pdf?id=KUpUO7aSSg", "keywords": "diffusion model, object detection, domain adaptation", "abstract": "Object detection has wide applications in agriculture, but the trained models often struggle to generalize across diverse agricultural environments. To address this challenge, we propose DODA ($\\underline{D}$iffusion for $\\underline{O}$bject-detection $\\underline{D}$omain Adaptation in $\\underline{A}$griculture), a unified framework that leverages diffusion models to generate high-quality, domain-specific detection data for multiple agricultural scenarios. DODA incorporates external domain embeddings and an improved layout-to-image (L2I) approach, allowing it to generate high-quality detection data for new domains without additional training. We demonstrate DODA's effectiveness on the Global Wheat Head Detection dataset, where fine-tuning detectors on DODA-generated data yields significant improvements across multiple domains (maximum +15.6 AP). DODA provides a simple yet powerful approach to adapt object detectors to diverse agricultural scenarios, lowering barriers for more growers to use detection in their personalized environments.", "title_embedding_index": 21911, "title_abs_embedding_index": 21936}, {"title": "Individualized Private Graph Neural Network via Node Influence-based Noise Adaptation", "link_suffix": "/forum?id=nqGqIzDCRY", "link": "https://openreview.net/forum?id=nqGqIzDCRY", "pdf_link": "https://openreview.net/pdf?id=nqGqIzDCRY", "keywords": "Graph Neural Network, Differential Privacy", "abstract": "Graph Neural Networks (GNNs) with Differential Privacy (DP) guarantees have been proposed to preserve privacy when nodes contain sensitive information that needs to be kept private but is critical for training. Existing methods deploy a fixed uniform noise generation mechanism that lacks the flexibility to adjust between nodes, leading to increasing the risk of graph information leakage and decreasing the model's overall performance. To address the above challenges, we propose NIP-GNN, a Node-level Individual Private GNN with DP guarantee based on the adaptive perturbation over sensitive components to safeguard node information. First, we propose a Topology-based Node Influence Estimation (TNIE) method to infer unknown node influence with neighborhood and centrality awareness. \n    Second, given the obtained node influence rank, an adaptive private aggregation method is proposed to perturb neighborhood embeddings directed by node-wise influence. \n    Third, we propose to privately train the graph learning algorithm over perturbed aggregations in adaptive residual connection mode over multi-layer convolution for node-wise tasks. Theoretically, analysis ensures that NIP-GNN satisfies DP guarantee. Empirical experiments over real-world graph datasets show that NIP-GNN \n    presents a better resistance over node inference attacks and \n    achieves a better trade-off between privacy and accuracy.", "title_embedding_index": 21912, "title_abs_embedding_index": 21937}, {"title": "Sitcom-Crafter: A Plot-Driven Human Motion Generation System in 3D Scenes", "link_suffix": "/forum?id=FvIASa0tau", "link": "https://openreview.net/forum?id=FvIASa0tau", "pdf_link": "https://openreview.net/pdf?id=FvIASa0tau", "keywords": "Human Motion Synthesis, Human-Human Interaction, Physically Compliant Motion, Creative Workflow Automation", "abstract": "Recent advancements in human motion synthesis have focused on specific types of motions, such as human-scene interaction, locomotion or human-human interaction, however, there is a lack of a unified system capable of generating a diverse combination of motion types. In response, we introduceSitcom-Crafter, a comprehensive and extendable system for human motion generation in 3D space, which can be guided by extensive plot contexts to enhance workflow efficiency for anime and game designers. The system is comprised of eight modules, three of which are dedicated to motion generation, while the remaining five are augmentation modules that ensure consistent fusion of motion sequences and system functionality. Central to the generation modules is our novel 3D scene-aware human-human interaction module, which addresses collision issues by synthesizing implicit 3D Signed Distance Function (SDF) points around motion spaces, thereby minimizing human-scene collisions without additional data collection costs. Complementing this, our locomotion and human-scene interaction modules leverage existing methods to enrich the system's motion generation capabilities. Augmentation modules encompass plot comprehension for command generation, motion synchronization for seamless integration of different motion types, hand pose retrieval to enhance motion realism, motion collision revision to prevent human collisions, and 3D retargeting to ensure visual fidelity. Experimental evaluations validate the system's ability to generate high-quality, diverse, and physically realistic motions, underscoring its potential for advancing creative workflows. Code and demonstration videos can be found in the supplementary files.", "title_embedding_index": 21913, "title_abs_embedding_index": 21938}, {"title": "Gap Preserving Distillation by Building Bidirectional Mappings with A Dynamic Teacher", "link_suffix": "/forum?id=PnfghHD4Pi", "link": "https://openreview.net/forum?id=PnfghHD4Pi", "pdf_link": "https://openreview.net/pdf?id=PnfghHD4Pi", "keywords": "Knowledge Distillation; Model Expansion; Reparameterization", "abstract": "Knowledge distillation aims to transfer knowledge from a large teacher model to a compact student counterpart, often coming with a significant performance gap between them. We find that a too-large performance gap can hamper the training process, which is also verified in recent studies. To address this, we propose a Gap Preserving Distillation (GPD) method that trains an additional dynamic teacher model from scratch along with training the student to bridge this gap. In this way, it becomes possible to maintain a reasonable performance gap between teacher and student during the whole distillation process. To further strengthen distillation from the dynamic teacher to the student, we develop a hard strategy by enforcing them to share parameters and encouraging parameter inheritance. Besides hard strategy, we also build the soft bidirectional mappings between them which are built on an Inverse Reparameterization (IR) method and a Channel-Branch Reparameterization (CBR) strategy. We highlight that our IR is able to initialize a larger dynamic teacher with an arbitrary expansion ratio, while preserving exactly the same accuracy as the given student model. In this way, it guarantees that the dynamic teacher and student start from the same point and avoid a too large gap in early stage of training. As for our CBR, with parameter-sharing, it directly extracts an effective student model from the well-learned dynamic teacher without any post-training, making our method highly flexible for model deployment.\nIn the experiments, GPD significantly outperforms existing distillation methods on top of both CNNs and transformers architectures, achieving up to 1.58% accuracy improvement. Interestingly, GPD also generalizes well to the scenarios without a pretrained teacher, including training from scratch and fine-tuning, yielding a large improvement of 1.80% and 0.89% on ResNet18, respectively.", "title_embedding_index": 21914, "title_abs_embedding_index": 21939}, {"title": "OracleMamba: A Dynamic Market-Guided and Time State Selection Framework for Robust Stock Prediction", "link_suffix": "/forum?id=0x8wWloW2O", "link": "https://openreview.net/forum?id=0x8wWloW2O", "pdf_link": "https://openreview.net/pdf?id=0x8wWloW2O", "keywords": "deep learning, time series", "abstract": "Stock price prediction is a complex challenge due to the inherent volatility of financial markets and the influence of diverse factors such as macroeconomic conditions, capital flows, and market sentiment. Recent joint stock forecasting models focus on extracting temporal patterns from individual stock price series and combining them to model stock correlations. However, these models face two critical limitations: first, in long-term predictions, they retain both informative and excessive states, amplifying noise and increasing complexity; second, in short-term predictions, they prioritize market indices and technical indicators, neglecting the real-time influence of market sentiment, which can drive price movements independent of traditional indicators. While state space models (SSMs) like Mamba improve efficiency and capture long-distance relationships, they still underperform compared to Transformer-based models.\nTo address these challenges, we propose OracleMamba, a novel framework that integrates a dynamic market-guided module for short-term forecasting and a SelectiveMamba module for long-term forecasting. The dynamic market-guided module fuses objective market data and subjective sentiment analysis to enhance short-term prediction accuracy. The SelectiveMamba module efficiently captures both spectral and temporal features using a 3D scan mechanism, which extracts and filters key signals from the time-series data. By integrating spectral features to identify market rhythms and temporal features to track price movements over time, the SelectiveMamba module reduces noise and preserves critical information for long-term forecasts. This framework significantly improves both model efficiency and accuracy, outperforming existing approaches across real-world stock prediction tasks.", "title_embedding_index": 21915, "title_abs_embedding_index": 21940}, {"title": "Optimizing Calibration by Gaining Aware of Prediction Correctness", "link_suffix": "/forum?id=34xYxTTiM0", "link": "https://openreview.net/forum?id=34xYxTTiM0", "pdf_link": "https://openreview.net/pdf?id=34xYxTTiM0", "keywords": "Post-hoc Model Calibration, Model Calibration Loss", "abstract": "Model calibration aims to align confidence with prediction correctness. The Cross-Entropy (CE) loss is widely used for calibrator training, which enforces the model to increase confidence on the ground truth class. However, we find the CE loss has intrinsic limitations. For example, for a narrow misclassification, a calibrator trained by the CE loss often produces high confidence on the wrongly predicted class (e.g., a test sample is wrongly classified and its softmax score on the ground truth class is around 0.4), which is undesirable. In this paper, we propose a new post-hoc calibration objective derived from the aim of calibration. Intuitively, the proposed objective function asks that the calibrator decrease model confidence on wrongly predicted samples and increase confidence on correctly predicted samples. \nBecause a sample itself has insufficient ability to indicate correctness, we use its transformed versions (e.g., rotated, greyscaled, and color-jittered) during calibrator training. Trained on an in-distribution validation set and tested with isolated, individual test samples, \nour method achieves competitive calibration performance on both in-distribution and out-of-distribution test sets compared with the state of the art. Further, our analysis points out the difference between our method and commonly used objectives such as CE loss and Mean Square Error (MSE) loss, where the latters sometimes deviates from the calibration aim.", "title_embedding_index": 21916, "title_abs_embedding_index": 21941}, {"title": "Dynamic Pre-training: Towards Efficient and Scalable All-in-One Image Restoration", "link_suffix": "/forum?id=MtoklWYQus", "link": "https://openreview.net/forum?id=MtoklWYQus", "pdf_link": "https://openreview.net/pdf?id=MtoklWYQus", "keywords": "All-in-one \u00b7 Image Restoration \u00b7 Foundation model", "abstract": "All-in-one image restoration tackles different types of degradations with a unified model instead of having task-specific, non-generic models for each degradation. The requirement to tackle multiple degradations using the same model can lead to high-complexity designs with fixed configuration that lack the adaptability to more efficient alternatives. We propose DyNet, a dynamic family of networks designed in an encoder-decoder style for all-in-one image restoration tasks. Our DyNet can seamlessly switch between its bulkier and lightweight variants, thereby offering flexibility for efficient model deployment with a single round of training. This seamless switching is enabled by our weights-sharing mechanism, forming the core of our architecture and facilitating the reuse of initialized module weights. Further, to establish robust weights initialization, we introduce a dynamic pre-training strategy that trains variants of the proposed DyNet concurrently, thereby achieving a 50% reduction in GPU hours. Our dynamic pre-training strategy eliminates the need for maintaining separate checkpoints for each variant, as all models share a common set of checkpoints, varying only in model depth. This efficient strategy significantly reduces storage overhead and enhances adaptability. To tackle the unavailability of large-scale dataset required in pre-training, we curate a high-quality, high-resolution image dataset named Million-IRD, having 2M image samples. We validate our DyNet for image denoising, deraining, and dehazing in all-in-one setting, achieving state-of-the-art results with 31.34% reduction in GFlops and a 56.75% reduction in parameters compared to baseline models. The source codes and trained models will be publicly  released.", "title_embedding_index": 21917, "title_abs_embedding_index": 21942}, {"title": "Numerical Pitfalls in Policy Gradient Updates", "link_suffix": "/forum?id=u4dORXVAnx", "link": "https://openreview.net/forum?id=u4dORXVAnx", "pdf_link": "https://openreview.net/pdf?id=u4dORXVAnx", "keywords": "Numerical stability, robustness, deep reinforcement learning, policy gradient methods, importance sampling", "abstract": "Numerical instability, such as gradient explosion, is a fundamental problem in practical deep reinforcement learning (DRL) algorithms. Beyond anecdotal debugging heuristics, there is a lack of systematic understanding of the causes for numerical sensitivity that leads to exploding gradient failures in practice. In this work, we demonstrate that the issue arises from the ill-conditioned density ratio in the surrogate objective that comes from importance sampling, which can take excessively large values during training. Perhaps surprisingly, while various policy optimization methods such as TRPO and PPO prevent excessively large policy updates, their optimization constraints on KL divergence and probability ratio cannot guarantee numerical stability. This also explains why gradient explosion often occurs during DRL training, even with code-level optimizations. We also discuss several potential approaches to ensure numerical stability and the challenges associated with them.", "title_embedding_index": 21918, "title_abs_embedding_index": 21943}, {"title": "Bridging Information Asymmetry in Text-video Retrieval: A Data-centric Approach", "link_suffix": "/forum?id=Tn6lrFbiP4", "link": "https://openreview.net/forum?id=Tn6lrFbiP4", "pdf_link": "https://openreview.net/pdf?id=Tn6lrFbiP4", "keywords": "Text-video Retrieval, Vision-Language Model, Multimodal", "abstract": "As online video content rapidly grows, the task of text-video retrieval (TVR) becomes increasingly important. A key challenge in TVR is the information asymmetry between video and text: videos are inherently richer in information, while their textual descriptions often capture only fragments of this complexity. This paper introduces a novel, data-centric framework to bridge this gap by enriching textual representations to better match the richness of video content. During training, videos are segmented into event-level clips and captioned to ensure comprehensive coverage. During retrieval, a large language model (LLM) generates semantically diverse queries to capture a broader range of possible matches. To enhance retrieval efficiency, we propose a query selection mechanism that identifies the most relevant and diverse queries, reducing computational cost while improving accuracy. Our method achieves state-of-the-art results across multiple benchmarks, demonstrating the power of data-centric approaches in addressing information asymmetry in TVR. This work paves the way for new research focused on leveraging data to improve cross-modal retrieval.", "title_embedding_index": 21919, "title_abs_embedding_index": 21944}, {"title": "Streaming Video Question-Answering with In-context Video KV-Cache Retrieval", "link_suffix": "/forum?id=8g9fs6mdEG", "link": "https://openreview.net/forum?id=8g9fs6mdEG", "pdf_link": "https://openreview.net/pdf?id=8g9fs6mdEG", "keywords": "Video Understanding, Multimodal Large Language Models, Streaming Video Question-answering", "abstract": "We propose ReKV, a novel, training-free approach that integrates seamlessly with existing Video Large Language Models (Video-LLMs) to enable efficient streaming video question-answering (StreamingVQA). Traditional VideoQA systems struggle with long videos, as they must process the entire video before responding to queries, and repeat this process for each new question. In contrast, our approach analyzes long videos in a streaming fashion, allowing for prompt responses as soon as user queries are received. Building on a common Video-LLM, we first incorporate a sliding-window attention mechanism, ensuring that input frames attend to a limited number of preceding frames, thereby reducing computational overhead. To prevent information loss, we store processed video key-value caches (KV-Caches) in RAM and disk, reloading them into GPU memory as needed. Additionally, we introduce a retrieval method that leverages an external retriever or the parameters within Video-LLMs to retrieve only query-relevant KV-Caches, ensuring both efficiency and accuracy in question answering. ReKV enables the separation of video analyzing and question-answering across different processes and GPUs, significantly enhancing the efficiency of StreamingVQA. Through comprehensive experimentation, we validate the efficacy and practicality of our approach, which significantly boosts efficiency and enhances applicability over existing VideoQA models.", "title_embedding_index": 21920, "title_abs_embedding_index": 21945}, {"title": "Is Forward Gradient an Effective Tool for Explaining Black-box Models?", "link_suffix": "/forum?id=Fq25rH3ytL", "link": "https://openreview.net/forum?id=Fq25rH3ytL", "pdf_link": "https://openreview.net/pdf?id=Fq25rH3ytL", "keywords": "explainability", "abstract": "Gradients are widely used to explain the decisions of deep neural networks. However, as models become deeper and more complex, computing gradients becomes challenging and sometimes infeasible, hindering traditional explanation methods. Recently, the forward gradient method has garnered attention for training structure-agnostic models with discontinuous objective functions. This method perturbs only the parameters of interest for gradient computation and optimization. Inspired by this, we investigate whether the forward gradient can be employed to explain black-box models. In this work, we use the likelihood ratio method to estimate output-to-input gradients and utilize them for the explanation of model decision. Additionally, we propose block-wise computation techniques to enhance estimation accuracy. Extensive experiments in black-box settings validate the effectiveness of our method, demonstrating accurate gradient estimation and improved explainability under the black-box setting.", "title_embedding_index": 21921, "title_abs_embedding_index": 21946}, {"title": "Enhancing Adversarial Transferability Through Exploiting Multiple Randomized Trajectories for Better Global Guidance", "link_suffix": "/forum?id=2ozEpaU02q", "link": "https://openreview.net/forum?id=2ozEpaU02q", "pdf_link": "https://openreview.net/pdf?id=2ozEpaU02q", "keywords": "adversarial transferability", "abstract": "Deep neural networks are well-known for their vulnerability to adversarial examples, particularly demonstrating poor performance in white-box attack settings. However, most white-box attack methods heavily depend on the target model and often get trapped in local optima, leading to limited adversarial transferability. Techniques such as momentum, variance reduction, and gradient penalty mitigate overfitting by combining historical information with local regions around adversarial examples, but exploration of the global loss landscape remains constrained, hindering further performance improvements.In this work, we find that initialization influences the optimization of adversarial examples, often guiding them toward multiple local optima, providing an opportunity to explore the loss landscape more effectively. Based on this insight, we propose two strategies: randomized global initialization and dual examples. These strategies utilize multiple trajectories from benign samples to capture global optimization directions, enhancing adversarial transferability. Our approach integrates seamlessly with existing adversarial attack methods and significantly improves transferability, as demonstrated by empirical evaluations on the standard ImageNet dataset.", "title_embedding_index": 21922, "title_abs_embedding_index": 21947}, {"title": "UrbanDiT: A Foundation Model for Open-World Urban Spatio-Temporal Learning", "link_suffix": "/forum?id=H8oCwBTDMv", "link": "https://openreview.net/forum?id=H8oCwBTDMv", "pdf_link": "https://openreview.net/pdf?id=H8oCwBTDMv", "keywords": "Foundation model, Urban Spatio-Temporal Learning, Diffusion Transformer, Prompt Learning", "abstract": "The urban environment is characterized by complex spatio-temporal dynamics arising from diverse human activities and interactions. Effectively modeling these dynamics is essential for understanding and optimizing urban systems. In this work, we introduce UrbanDiT, a foundation model for open-world urban spatio-temporal learning that successfully scale up diffusion transformers in this field. UrbanDiT pioneers a unified model that integrates diverse spatio-temporal data sources and types while learning universal spatio-temporal patterns across different cities and scenarios. This allows the model to unify both multi-data and multi-task learning, and effectively support a wide range of spatio-temporal applications. Its key innovation lies in the elaborated prompt learning framework, which adaptively generates both data-driven and task-specific prompts, guiding the model to deliver superior performance across various urban applications.UrbanDiT  offers three primary advantages: 1)  It unifies diverse data types, such as grid-based and graph-based data, into a sequential format,  allowing  to capture spatio-temporal dynamics across diverse scenarios of different cities; 2) With masking strategies and task-specific prompts, it supports a wide range of tasks, including bi-directional spatio-temporal prediction, temporal interpolation, spatial extrapolation, and spatio-temporal imputation; and 3) It generalizes effectively to open-world scenarios, with its powerful zero-shot capabilities outperforming nearly all baselines with training data. These features allow UrbanDiT  to achieves state-of-the-art performance in different domains such as transportation traffic, crowd flows, taxi demand, bike usage, and cellular traffic, across multiple cities and tasks. UrbanDiT sets up a new benchmark for foundation models in the urban spatio-temporal domain. Code and datasets are publicly available athttps://anonymous.4open.science/r/UrbanDiT.", "title_embedding_index": 21923, "title_abs_embedding_index": 21948}, {"title": "EXAGREE: Towards Explanation Agreement in Explainable Machine Learning", "link_suffix": "/forum?id=wJVZkUOUjh", "link": "https://openreview.net/forum?id=wJVZkUOUjh", "pdf_link": "https://openreview.net/pdf?id=wJVZkUOUjh", "keywords": "Explainable Machine Learning, Explainable Artificial Intelligence, Rashomon Sets", "abstract": "Explanations in machine learning are critical for trust, transparency, and fairness. Yet, complex disagreements among these explanations limit the reliability and applicability of machine learning models, especially in high-stakes environments. We formalize four fundamental ranking-based explanation disagreement problems and introduce a novel framework, EXplanation AGREEment (EXAGREE), to bridge diverse interpretations in explainable machine learning, particularly from stakeholder-centric perspectives. Our approach leverages a Rashomon set for attribution predictions and then optimizes within this set to identify Stakeholder-Aligned Explanation Models (SAEMs) that minimize disagreement with diverse stakeholder needs while maintaining predictive performance.  Rigorous empirical analysis on synthetic and real-world datasets demonstrates that EXAGREE reduces explanation disagreement and improves fairness across subgroups in various domains. EXAGREE not only provides researchers with a new direction for studying explanation disagreement problems but also offers data scientists a tool for making better-informed decisions in practical applications.", "title_embedding_index": 21924, "title_abs_embedding_index": 21949}]