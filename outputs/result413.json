[
    {
        "title": "4DEditPro: Progressively Editing 4D Scenes from Monocular Videos with Text Prompts",
        "link_suffix": "/forum?id=4dHyH42ha7",
        "link": "https://openreview.net/forum?id=4dHyH42ha7",
        "pdf_link": "https://openreview.net/pdf?id=4dHyH42ha7",
        "keywords": "4D scene editing, Diffusion model, 4D Gaussian representation",
        "abstract": "Editing 4D scenes using text prompts is a novel task made possible by advances in text-to-image diffusion models and differentiable scene representations. However, conventional approaches typically use multi-view images or videos with camera poses as input, which causes inconsistencies when editing monocular videos due to the reliance of these tools on iteratively per-image editing and the absence of multi-view supervision.\nFurthermore, these techniques usually require external Structure-from-Motion (SfM) libraries for camera pose estimation, which can be impractical for casual monocular videos. \nTo tackle these hurdles, we present 4DEditPro, a novel framework that enables consistent 4D scene editing on casual monocular videos with text prompts. \nIn our 4DEditPro, the Temporally Propagated Editing (TPE) module guides the diffusion model to ensure temporal coherence across all input frames in scene editing.\nFurthermore, the Spatially Propagated Editing (SPE) module in 4DEditPro introduces auxiliary novel views near the camera trajectory to enhance the spatial consistency of edited scenes. \n4DEditPro employs a pose-free 4D Gaussian Splatting (4DGS) approach for reconstructing dynamic scenes from monocular videos, which progressively recovers relative camera poses, reconstructs the scene, and facilitates scene editing.\nWe have conducted extensive experiments to demonstrate the effectiveness of our approach, including both quantitative measures and user studies."
    },
    {
        "title": "Masked Temporal Interpolation Diffusion for Procedure Planning in Instructional Videos",
        "link_suffix": "/forum?id=HnpDHiItd2",
        "link": "https://openreview.net/forum?id=HnpDHiItd2",
        "pdf_link": "https://openreview.net/pdf?id=HnpDHiItd2",
        "keywords": "procedure planning, diffusion, U-Net, temporal logic interpolation, action prediction, mask",
        "abstract": "In this paper, we study the problem of procedure planning in instructional videos, which involves making goal-directed plans based on current visual observations in unstructured, real-life videos. Prior research leverages different forms of supervision to bridge the gap between observed states and unobserved actions. Building on this foundation, we propose an innovative approach by introducing a latent space temporal logical interpolation module within the diffusion model framework. This module enables the intermediate supervision of temporal logical relationships that were previously nonexistent. In terms of details, we employ an interpolator to guide the intermediate process within the diffusion model, using the start and end observation features as inputs. This involves extracting latent features through an encoder and applying an interpolation strategy with transformer encoder blocks to derive the latent features. Furthermore, to ensure the accuracy of actions in the outputs, we implement a masking strategy to constrain the scope of predictions and a task-adaptive masked proximity loss for the training process. Results across these three datasets of varying scales demonstrate that our MTID model achieves state-of-the-art performance on the overwhelming majority of key metrics. The code is available athttps://anonymous.4open.science/r/MTID-E2E3/README.md."
    },
    {
        "title": "AdvLoRA: Adversarial Low-Rank Adaptation of Vision-Language Models",
        "link_suffix": "/forum?id=5ECUAQJUuq",
        "link": "https://openreview.net/forum?id=5ECUAQJUuq",
        "pdf_link": "https://openreview.net/pdf?id=5ECUAQJUuq",
        "keywords": "Vision-Language Models, Adversarial Training, Parameter-efficient Adaptation",
        "abstract": "Vision-Language Models (VLMs) are a significant technique for Artificial General Intelligence (AGI). With the fast growth of AGI, the security problem become one of the most important challenges for VLMs. In this paper, through extensive experiments, we demonstrate the vulnerability of the conventional adaptation methods for VLMs, which may bring significant security risks. In addition, as the size of the VLMs increases, performing conventional adversarial adaptation techniques on VLMs results in high computational costs. To solve these problems, we propose a parameter-efficient \\underline{Adv}ersarial adaptation method named \\underline{AdvLoRA} by \\underline{Lo}w-\\underline{R}ank \\underline{A}daptation. At first, we investigate and reveal the intrinsic low-rank property during the adversarial adaptation for VLMs. Different from LoRA, we improve the efficiency and robustness of adversarial adaptation by designing a novel reparameterizing method based on parameter clustering and parameter alignment. In addition, an adaptive parameter update strategy is proposed to further improve the robustness. By these settings, our proposed AdvLoRA alleviates the model security and high resource waste problems. Extensive experiments demonstrate the effectiveness and efficiency of the AdvLoRA."
    },
    {
        "title": "BlueSuffix: Reinforced Blue Teaming for Vision-Language Models Against Jailbreak Attacks",
        "link_suffix": "/forum?id=wwVGZRnAYG",
        "link": "https://openreview.net/forum?id=wwVGZRnAYG",
        "pdf_link": "https://openreview.net/pdf?id=wwVGZRnAYG",
        "keywords": "Adversarial Defense, Blue-Teaming, Large Vision-Language Model",
        "abstract": "Despite their superb multimodal capabilities, Vision-Language Models (VLMs) have been shown to be vulnerable to jailbreak attacks, which are inference-time attacks that induce the model to output harmful responses with tricky prompts. It is thus essential to defend VLMs against potential jailbreaks for their trustworthy deployment in real-world applications. In this work, we focus on black-box defense for VLMs against jailbreak attacks. Existing black-box defense methods are either unimodal or bimodal. Unimodal methods enhance either the vision or language module of the VLM, while bimodal methods robustify the model through text-image representation realignment. \nHowever, these methods suffer from two limitations: 1) they fail to fully exploit the cross-modal information, or 2) they degrade the model performance on benign inputs. To address these limitations, we propose a novel blue-team method BlueSuffix that defends the black-box target VLM against jailbreak attacks without compromising its performance. BlueSuffix includes three key components: 1) a visual purifier against jailbreak images, 2) a textual purifier against jailbreak texts, and 3) a blue-team suffix generator fine-tuned via reinforcement learning for enhancing cross-modal robustness. We empirically show on three VLMs (LLaVA, MiniGPT-4, and Gemini) and two safety benchmarks (MM-SafetyBench and RedTeam-2K) that BlueSuffix outperforms the baseline defenses by a significant margin. Our BlueSuffix opens up a promising direction for defending VLMs against jailbreak attacks."
    },
    {
        "title": "Active Learning for Continual Learning: Keeping the Past Alive in the Present",
        "link_suffix": "/forum?id=mnLmmtW7HO",
        "link": "https://openreview.net/forum?id=mnLmmtW7HO",
        "pdf_link": "https://openreview.net/pdf?id=mnLmmtW7HO",
        "keywords": "active learning, continual learning, Fisher information",
        "abstract": "Continual learning (CL)enables deep neural networks to adapt to ever-changing data distributions. In practice, there may be scenarios where annotation is costly, leading toactive continual learning (ACL), which performsactive learning (AL)for the CL scenarios when reducing the labeling cost by selecting the most informative subset is preferable. However, conventional AL strategies are not suitable for ACL, as they focus solely on learning the new knowledge, leading tocatastrophic forgettingof previously learned tasks. Therefore, ACL requires a new AL strategy that can balance the prevention of catastrophic forgetting and the ability to quickly learn new tasks. In this paper, we proposeAccuACL,Accumulated informativeness-basedActiveContinualLearning, by achieving an optimal balance between the two required capabilities of ACL, as well as alleviating the scalability issue of Fisher information-based AL. Extensive experiments demonstrate that AccuACL significantly outperforms AL baselines across various CL algorithms, increasing the average accuracy and forgetting by 23.8% and 17.0%, respectively, in average."
    },
    {
        "title": "Group Downsampling with Equivariant Anti-aliasing",
        "link_suffix": "/forum?id=sOte83GogU",
        "link": "https://openreview.net/forum?id=sOte83GogU",
        "pdf_link": "https://openreview.net/pdf?id=sOte83GogU",
        "keywords": "equivariance, downsampling, signal processing",
        "abstract": "Downsampling layers are crucial building blocks in CNN architectures, which help to increase the receptive field for learning high-level features and reduce the amount of memory/computation in the model. In this work, we study the generalization of the uniform downsampling layer for group equivariant architectures, e.g., $G$-CNNs. That is, we aim to downsample signals (feature maps) on general finite groupswithanti-aliasing. This involves the following:(a)Given a finite group and a downsampling rate, we present an algorithm to form a suitable choice of subgroup.(b)Given a group and a subgroup, we study the notion of bandlimited-ness and propose how to perform anti-aliasing. Notably, our method generalizes the notion of downsampling based on classical sampling theory. When the signal is on a cyclic group, i.e., periodic, our method recovers the standard downsampling of an ideal low-pass filter followed by a subsampling operation. Finally, we conducted experiments on image classification tasks demonstrating that the proposed downsampling operation improves accuracy, better preserves equivariance, and reduces model size when incorporated into $G$-equivariant networks"
    },
    {
        "title": "Visually Guided Decoding: Gradient-Free Hard Prompt Inversion with Language Models",
        "link_suffix": "/forum?id=mQ55y4s5hj",
        "link": "https://openreview.net/forum?id=mQ55y4s5hj",
        "pdf_link": "https://openreview.net/pdf?id=mQ55y4s5hj",
        "keywords": "text-to-image, inversion, gradient free hard prompt inversion, language model guidance on latent diffusion model",
        "abstract": "Text-to-image generative models like DALL-E and Stable Diffusion have revolutionized visual content creation across various applications, including advertising, personalized media, and design prototyping. \nHowever, crafting effective textual prompts to guide these models remains challenging, often requiring extensive trial and error. \nExisting prompt inversion methods, such as soft and hard prompt techniques, suffer from issues like limited interpretability and incoherent prompt generation. \nTo address these limitations, we introduce Visually Guided Decoding (VGD), a gradient-free approach that leverages large language models (LLMs) and CLIP-based guidance to generate coherent and semantically aligned prompts. \nVGD utilizes the robust text generation capabilities of LLMs to produce human-readable prompts while employing CLIP scores to ensure alignment with user-specified visual concepts. \nThis method enhances the interpretability, generalization, and flexibility of prompt generation without the need for additional training. \nOur experiments demonstrate that VGD outperforms existing prompt inversion techniques in generating understandable and contextually relevant prompts, facilitating more intuitive and controllable interactions with text-to-image models. \nVGD's compatibility with various LLMs, including LLama2, LLama3, and Mistral, makes it a versatile solution for enhancing image generation workflows."
    },
    {
        "title": "BP-Modified Local Loss for Efficient Training of Deep Neural Networks",
        "link_suffix": "/forum?id=MtW30ql5Oj",
        "link": "https://openreview.net/forum?id=MtW30ql5Oj",
        "pdf_link": "https://openreview.net/pdf?id=MtW30ql5Oj",
        "keywords": "deep learning optimization, local loss training, bias-variance balance",
        "abstract": "The training of large models is memory-constrained, one direction to relieve this is training using local loss, like GIM, LoCo, and Forward-Forward algorithm. However, the local loss methods are facing the issue of slow or non-convergence. In this paper, we propose a novel BP-modified local loss method that uses the true Backward Propagation (BP) gradient to modify the local loss gradient to improve the performance of local loss training. We use the stochastic modified equation to analyze our method and show that modified offset decreases the bias between the BP gradient and local loss gradient but introduces additional variance, which results in a bias-variance balance. Numerical experiments on full-tuning and LoKr tuning on the ResNet-50 model and LoRA tuning on the ViT-b16 model on CIFAR-100 datasets show 20-30% test top-1 accuracy improvement for the Forward-Forward algorithm, 15-25% improvement for LoCo algorithm and achieve only in average 7.7% of test accuracy loss compared to the BP algorithm up to 75% memory saving."
    },
    {
        "title": "Unifying Back-Propagation and Forward-Forward Algorithms through Model Predictive Control",
        "link_suffix": "/forum?id=1MHgMGoqsH",
        "link": "https://openreview.net/forum?id=1MHgMGoqsH",
        "pdf_link": "https://openreview.net/pdf?id=1MHgMGoqsH",
        "keywords": "deep learning optimization, model predictive control",
        "abstract": "We introduce a Model Predictive Control (MPC) framework for training deep neural networks,\n systematically unifying the Back-Propagation (BP)\n and Forward-Forward (FF) algorithms.\n At the same time, it gives rise to a range of\n intermediate training algorithms with varying look-forward horizons,\n leading to a performance-efficiency trade-off.\n We perform a precise analysis of this trade-off on\n a deep linear network, where the qualitative conclusions\n carry over to general networks.\n Based on our analysis, we propose a principled method to choose\n the optimization horizon based on given objectives and model specifications.\n Numerical results on various models and tasks\n demonstrate the versatility of our method."
    },
    {
        "title": "GTD-LLM: A Plug-and-Play LLM Reasoning Module for Gaze Target Detection",
        "link_suffix": "/forum?id=Akccupz2pP",
        "link": "https://openreview.net/forum?id=Akccupz2pP",
        "pdf_link": "https://openreview.net/pdf?id=Akccupz2pP",
        "keywords": "LLM Reasoning; Prompt Engineering; Gaze Target Detection",
        "abstract": "Gaze target detection is an important task in computer vision, aiming to predict where people in an image are looking. In our view, this task not only contains explicit image features, but also implies a large amount of prior knowledge about the correlations between human visual attention and daily activities. However, existing gaze target methods rely entirely on visual modality information to detect salient objects along the gaze direction, limiting their generalization in challenging scenarios such as activity-related, long-tailed, small-sized, or long-distance gaze targets. Inspired by the great success of LLM technology, we break away from the traditional pure-visual approaches and propose GTD-LLM, the first plug-and-play LLM reasoning module for gaze target detection in visual scenes, providing a new paradigm for traditional pure-visual approaches. Our GTD-LLM module can be plug-and-play integrated with any existing gaze target visual models and directly bring them universal performance improvements, simultaneously demonstrating strong generalizability and effectiveness. In our GTD-LLM module, we design a novel prompt engineering method GTD-Prompt, to guide LLMs like GPT-4 to perform logical reasoning on possible gaze targets, without the need for any training or fine-tuning. The proposed GTD-Prompt method can also be easily extended to downstream tasks by simply adjusting the corresponding task prompt words, further illustrating its versatility."
    },
    {
        "title": "SHARP: Splatting High-fidelity And Relightable Photorealistic 3D Gaussian Head Avatars",
        "link_suffix": "/forum?id=dfC2ji6nek",
        "link": "https://openreview.net/forum?id=dfC2ji6nek",
        "pdf_link": "https://openreview.net/pdf?id=dfC2ji6nek",
        "keywords": "3D Head avatar; monocular video; 3D Gaussian Splatting",
        "abstract": "Reconstructing animatable and high-fidelity 3D head avatars from monocular videos, especially with realistic relighting, is a valuable task. However, the limited information from single-view input, combined with the complex head poses and facial movements, makes this challenging. Previous methods achieve real-time performance by combining 3D Gaussian Splatting with a parametric head model, but the resulting head quality suffers from inaccurate face tracking and limited expressiveness of the deformation model. These methods also fail to produce realistic effects under novel lighting conditions. To address these issues, we propose SHARP, a method that reconstructs high-fidelity, relightable 3D head avatars using 3D Gaussian points. SHARP reduces tracking errors through end-to-end optimization and better captures individual facial deformations using learnable blendshapes and linear blend skinning. Additionally, it decomposes head appearance into several physical properties and incorporates physically-based shading to account for environmental lighting. Extensive experiments demonstrate that SHARP not only reconstructs superior-quality heads but also achieves realistic visual effects under varying lighting conditions."
    },
    {
        "title": "The Crucial Role of Samplers in Online Direct Preference Optimization",
        "link_suffix": "/forum?id=F6z3utfcYw",
        "link": "https://openreview.net/forum?id=F6z3utfcYw",
        "pdf_link": "https://openreview.net/pdf?id=F6z3utfcYw",
        "keywords": "direct preference optimization, online DPO, multi-armed bandit",
        "abstract": "Direct Preference Optimization (DPO) has emerged as a stable, scalable, and efficient solution for language model alignment.\nDespite its empirical success, theoptimizationproperties, particularly the impact of samplers on its convergence rates, remain underexplored. In this paper, we provide a rigorous analysis of DPO'sconvergence rateswith different sampling strategies under the exact gradient setting, revealing a surprising separation: uniform sampling achieveslinearconvergence, while our proposed online sampler achievesquadraticconvergence.\nWe further adapt the sampler to practical settings by incorporating posterior distributions andlogit mixing, demonstrating significant improvements over previous approaches.\nOn Safe-RLHF dataset, our method exhibits a $4.5$% improvement over vanilla DPO and a $3.0$% improvement over on-policy DPO;  on Iterative-Prompt, our approach outperforms vanilla DPO, on-policy DPO, and Hybrid GSHF by over $4.2$%.\nOur results not only offer insights into the theoretical standing of DPO but also pave the way for potential algorithm designs in the future."
    },
    {
        "title": "Euclid: Supercharging Multimodal LLMs with Synthetic High-Fidelity Visual Descriptions",
        "link_suffix": "/forum?id=x07rHuChwF",
        "link": "https://openreview.net/forum?id=x07rHuChwF",
        "pdf_link": "https://openreview.net/pdf?id=x07rHuChwF",
        "keywords": "Multimodal LLMs, Geometric Perception, Low-level Visual Perception",
        "abstract": "Multimodal large language models (MLLMs) have made rapid progress in recent years, yet continue to struggle with low-level visual perception\u2014particularly the ability to accurately describe the geometric details of an image. This capability is crucial for applications in areas such as robotics, medical image analysis, and manufacturing. To address this challenge, we first introduce Geoperception, a benchmark designed to evaluate an MLLM\u2019s ability to accurately transcribe 2D geometric information from an image. Using this benchmark, we demonstrate the limitations of leading MLLMs, and then conduct a comprehensive empirical study to explore strategies for improving their performance on geometric tasks. Our findings highlight the benefits of certain model architectures, training techniques, and data strategies, including the use of high-fidelity synthetic data and multi-stage training with a data curriculum. Notably, we find that a data curriculum enables models to learn challenging geometry understanding tasks which they fail to learn from scratch. Leveraging these insights, we develop Euclid, a family of models specifically optimized for strong low-level geometric perception. Although purely trained on synthetic multimodal data, Euclid shows strong generalization ability to novel geometry shapes. For instance, Euclid outperforms the best closed-source model, Gemini-1.5-Pro, by up to 54.52% on benchmark tasks."
    },
    {
        "title": "Near-Optimal Policy Identification in Robust Constrained Markov Decision Processes via Epigraph Form",
        "link_suffix": "/forum?id=G5sPv4KSjR",
        "link": "https://openreview.net/forum?id=G5sPv4KSjR",
        "pdf_link": "https://openreview.net/pdf?id=G5sPv4KSjR",
        "keywords": "Markov Decision Process, Constrained Optimization, Robust Optimization",
        "abstract": "Designing a safe policy for uncertain environments is crucial in real-world control applications. However, this challenge remains inadequately addressed within the Markov decision process (MDP) framework. This paper presents the first algorithm guaranteed to identify a near-optimal policy in a robust constrained MDP (RCMDP), where an optimal policy minimizes cumulative cost while satisfying constraints in the worst-case scenario across a set of environments. We first prove that the conventional policy gradient approach to the Lagrangian max-min formulation can become trapped in suboptimal solutions by encountering a sum of conflicting gradients from the objective and constraint functions during its inner minimization problem. To address this, we leverage the epigraph form of the RCMDP problem, which resolves the conflict by selecting a single gradient from either the objective or the constraints. Building on the epigraph form, we propose a binary search algorithm with a policy gradient subroutine and prove that it identifies an $\\varepsilon$-optimal policy in an RCMDP with $\\widetilde{\\mathcal{O}}(\\varepsilon^{-4})$ robust policy evaluations."
    },
    {
        "title": "FlipAttack: Jailbreak LLMs via Flipping",
        "link_suffix": "/forum?id=H6UMc5VS70",
        "link": "https://openreview.net/forum?id=H6UMc5VS70",
        "pdf_link": "https://openreview.net/pdf?id=H6UMc5VS70",
        "keywords": "Large Language Model, AI Safety, Red Teaming",
        "abstract": "This paper proposes a simple yet effective jailbreak attack named FlipAttack against black-box LLMs. First, from the autoregressive nature, we reveal that LLMs tend to understand the text from left to right and find that they struggle to comprehend the text when noise is added to the left side. Motivated by these insights, we propose to disguise the harmful prompt by constructing left-side noise merely based on the prompt itself, then generalize this idea to 4 flipping modes. Second, we verify the strong ability of LLMs to perform the text-flipping task, and then develop 4 variants to guide LLMs to denoise, understand, and execute harmful behaviors accurately. These designs keep FlipAttack universal, stealthy, and simple, allowing it to jailbreak black-box LLMs within only 1 query. Experiments on 8 LLMs demonstrate the superiority of FlipAttack. Remarkably, it achieves $\\sim$98% attack success rate on GPT-4o, and $\\sim$98% bypass rate against 5 guardrail models on average. The codes are available at Anonymous GitHub\\footnote{https://anonymous.4open.science/r/ICLR25-1731-FlipAttack}."
    },
    {
        "title": "MENTOR: Mixture-of-Experts Network with Task-Oriented Perturbation for Visual Reinforcement Learning",
        "link_suffix": "/forum?id=8w8d8j2FCy",
        "link": "https://openreview.net/forum?id=8w8d8j2FCy",
        "pdf_link": "https://openreview.net/pdf?id=8w8d8j2FCy",
        "keywords": "Visual Reinforcement Learning, Robotics, Mixture-of-Experts",
        "abstract": "Visual deep reinforcement learning (RL) enables robots to acquire skills from visual input for unstructured tasks. However, current algorithms suffer from low sample efficiency, limiting their practical applicability. In this work, we present MENTOR, a method that improves both the architecture and optimization of RL agents. Specifically, MENTOR replaces the standard multi-layer perceptron (MLP) with a mixture-of-experts (MoE) backbone, enhancing the agent's ability to handle complex tasks by leveraging modular expert learning to avoid gradient conflicts. Furthermore, MENTOR introduces a task-oriented perturbation mechanism, which heuristically samples perturbation candidates containing task-relevant information, leading to more targeted and effective optimization. MENTOR outperforms state-of-the-art methods across three simulation domains---DeepMind Control Suite, Meta-World, and Adroit. Additionally, MENTOR achieves an average of 83% success rate on three challenging real-world robotic manipulation tasks including peg insertion, cable routing, and tabletop golf, which significantly surpasses the success rate of 32% from the current strongest model-free visual RL algorithm. These results underscore the importance of sample efficiency in advancing visual RL for real-world robotics. Experimental videos are available athttps://mentor-vrl.github.io/."
    },
    {
        "title": "Improved Robustness and Hyperparameter Selection in the Dense Associative Memory",
        "link_suffix": "/forum?id=Tn02m7ZTch",
        "link": "https://openreview.net/forum?id=Tn02m7ZTch",
        "pdf_link": "https://openreview.net/pdf?id=Tn02m7ZTch",
        "keywords": "Dense Associative Memory, Modern Hopfield Network, Associative Memory Robustness, Stability, Hyperparameter Selection",
        "abstract": "The Dense Associative Memory generalizes the Hopfield network by allowing for sharper interaction functions. This increases the capacity of the network as an autoassociative memory as nearby learned attractors will not interfere with one another. However, the implementation of the network relies on applying large exponents to the dot product of memory vectors and probe vectors. If the dimension of the data is large the calculation can be very large and result in imprecisions and overflow when using floating point numbers in a practical implementation. We describe the computational issues in detail, modify the original network description to mitigate the problem, and show the modification will not alter the networks' dynamics during update or training. We also show our modification greatly improves hyperparameter selection for the Dense Associative Memory, removing dependence on the interaction vertex and resulting in an optimal region of hyperparameters that does not significantly change with the interaction vertex as it does in the original network. Our modifications also allow us to train a Dense Associative Memory with larger interaction vertices than have been used in any previous literature."
    },
    {
        "title": "Scaling Diffusion Models for Downstream Prediction",
        "link_suffix": "/forum?id=YryL3QIWWc",
        "link": "https://openreview.net/forum?id=YryL3QIWWc",
        "pdf_link": "https://openreview.net/pdf?id=YryL3QIWWc",
        "keywords": "Generative Models, Diffusion",
        "abstract": "In this paper, we argue that iterative computation, as exemplified by diffusion models, offers a powerful paradigm for not only image generation but also for visual perception tasks. First, we unify few of the mid-level vision tasks as image to image translations tasks ranging from depth estimation to optical flow to segmentation. Then, through extensive experiments across these tasks, we demonstrate how diffusion models scale with increased compute during both training and inference. Notably, we train various dense and Mixture of Expert models up to 2.8 billion parameters, and we utilize increased sampling steps, use various ensembling methods to increase compute at test time. Our work provides compelling evidence for the benefits of scaling compute at train and test time for diffusion models for visual perception, and by studying the scaling properties carefully, we were able to archive same performance of the state-of-the-art models with less compute."
    },
    {
        "title": "Grounding is All You Need? Dual Temporal Grounding for Video Dialog",
        "link_suffix": "/forum?id=rtUjj03qZv",
        "link": "https://openreview.net/forum?id=rtUjj03qZv",
        "pdf_link": "https://openreview.net/pdf?id=rtUjj03qZv",
        "keywords": "video dialog; multi-modal understanding; video grounding",
        "abstract": "In the realm of video dialog response generation, the understanding of video content and the temporal nuances of conversation history are paramount. While a segment of current research leans heavily on large-scale pretrained visual-language models and often overlooks temporal dynamics, another delves deep into spatial-temporal relationships within videos but demands intricate object trajectory pre-extractions and sidelines dialog temporal dynamics. \nThis paper introduces the Dual Temporal Grounding-enhanced Video Dialog model (DTGVD), strategically designed to merge the strengths of both dominant approaches.\nIt emphasizes dual temporal relationships by predicting dialog turn-specific temporal regions, filtering video content accordingly, and grounding responses in both video and dialog contexts. \nOne standout feature of DTGVD is its heightened attention to chronological interplay. By recognizing and acting upon the dependencies between different dialog turns, it captures more nuanced conversational dynamics. \nTo further bolster the alignment between video and dialog temporal dynamics, we've implemented a list-wise contrastive learning strategy. Within this framework, accurately grounded turn-clip pairings are designated as positive samples, while less precise pairings are categorized as negative. This refined classification is then funneled into our holistic end-to-end response generation mechanism. Evaluations using AVSD@DSTC-7 and AVSD@DSTC-8 datasets underscore the superiority of our methodology."
    },
    {
        "title": "Unifying Vocabulary of Large Language Model with Statistical Token-level Alignment",
        "link_suffix": "/forum?id=CP6CAqxAGJ",
        "link": "https://openreview.net/forum?id=CP6CAqxAGJ",
        "pdf_link": "https://openreview.net/pdf?id=CP6CAqxAGJ",
        "keywords": "Vocabulary Adaptation, Large Language Model, Efficient NLP",
        "abstract": "Large Language Models (LLMs) achieve great success across many general tasks, but the mismatch among different vocabularies hinders further applications like token-level distillation and inference with various models. To align the vocabularies of LLMs, we propose a simple yet effective method namedUnifyVocabto replace the vocabulary of an LLM at a limited cost. A new vocabulary alignment method is devised first to align the source vocabulary to the target one. We then rearrange the corresponding parameters like embeddings, and progressively fine-tune the model. Experimental results on models across multiple parameter scales demonstrate the effectiveness and generalization of UnifyVocab, which costs as few as 10B tokens to recover 98.02% performance of the vanilla models on average. We further find that unifying the vocabularies significantly facilitates the token-level distillation which remarkably boosts (+4.4%) the model with only 235M tokens. Moreover, our method provides a better initialization of multilingual vocabulary for LLMs to adapt to new languages."
    },
    {
        "title": "ZZEdit: ZigZag Trajectories of Inversion and Denoising for Zero-shot Image Editing",
        "link_suffix": "/forum?id=Rmm0Ohulxf",
        "link": "https://openreview.net/forum?id=Rmm0Ohulxf",
        "pdf_link": "https://openreview.net/pdf?id=Rmm0Ohulxf",
        "keywords": "ZigZag Trajectories\uff0cZero-shot \uff0cImage Editing",
        "abstract": "Editability and fidelity are two essential demands for text-driven image editing, which expects that the editing area should align with the target prompt and the rest should remain unchanged separately. The current cutting-edge editing methods usually obey an ''inversion-then-editing'' pipeline, where the input image is first inverted to an approximate Gaussian noise $z_T$ with $T$ steps, based on which a sampling process is performed using the target prompt. Nevertheless, we argue that \\textit{it is not a good choice to use a near-Gaussian noise as a pivot for further editing since it almost lost all structure fidelity.} To verify this, we conduct a pilot experiment and find that the target prompt has different guiding degrees towards those latents on the inversion trajectory. Thus, a structure-preserving while sufficient-for-editing point is a more suitable pivot. Based on this, we propose a novel editing paradigm dubbed ZZEdit, which first locates such a pivot during the inversion trajectory and then mildly strengthens target guidance via the proposed ZigZag process. Concretely, our ZigZag process fulfills denoising and inversion iteratively, which gradually approaches the target while still holding background fidelity. Afterwards, to achieve the same number of inversion and denoising steps, we perform a pure sampling process under the target prompt. Extensive experiments highlight the effectiveness of our ZZEdit paradigm in diverse image editing scenarios compared with the existing ''inversion-then-editing'' pipeline."
    },
    {
        "title": "Low Rank Quantization Adaptation for Large Language Model",
        "link_suffix": "/forum?id=aJnKjvTtPq",
        "link": "https://openreview.net/forum?id=aJnKjvTtPq",
        "pdf_link": "https://openreview.net/pdf?id=aJnKjvTtPq",
        "keywords": "Quantization, Low-Rank Adaptation, LLM",
        "abstract": "As the parameters of Large Language Models (LLMs) increase, quantization has emerged as a potent strategy for model compression and acceleration. Concurrently, Low-Rank Adaptation (LoRA) has been recognized as an effective method for enhancing LLM performance. However, integrating LoRA with quantization presents significant challenges, particularly in preserving the quantization format after model optimization. In this paper, we introduce Low rank Quantization Adaptation (LoQA) for LLM, a novel approach that effectively fine-tunes holistic quantization parameters. Specifically, we first propose a new perspective of quantization operator, which is compatiable with LoRA and mathematically equivalent to the original operator. In this way, all the parameters (scale and zero point) are finetuned simultaneously, and thus yields notable improvements in model performance.Thanks to the expanded optimization landscape, LoQA is broadly applicabile to various Post-Training Quantization (PTQ) techniques, ensuring better generalizability in practical deployments. To maintain the stability of the optimization, we further propose a LoRA scaling strategy that leverages quantization data to adjust the norm of the low rank adaptation, regulating the speed of convergence in optimization and preventing inappropriate LoRA scaling, which could lead to overfitting or underfitting. Compared to existing methods, LoQA consistently achieves performance gains across a wide range of models, proving its effectiveness and adaptability."
    },
    {
        "title": "Neural Manifold Regularization: Aligning 2D Latent Dynamics with Stereotyped, Natural, and Attempted Movements",
        "link_suffix": "/forum?id=TVnkjz4MqV",
        "link": "https://openreview.net/forum?id=TVnkjz4MqV",
        "pdf_link": "https://openreview.net/pdf?id=TVnkjz4MqV",
        "keywords": "dimensionality reduction, brain-machine interfaces, motor control, neural coding, self-supervised learning",
        "abstract": "Mapping neural activity to behavior is a fundamental goal in both neuroscience and brain-machine interfaces. Traditionally, at least three-dimensional (3D) latent dynamics have been required to represent two-dimensional (2D) movement trajectories. In this work, we introduce Neural Manifold Regularization (NMR), a method that embeds neural dynamics into a 2D latent space and regularizes the manifold based on the distances and densities of continuous movement labels. NMR pulls together positive pairs of neural embeddings (corresponding to closer labels) and pushes apart negative pairs (representing more distant labels). Additionally, NMR applies greater force to infrequent labels to prevent them from collapsing into dominant labels. We benchmarked NMR against other dimensionality reduction techniques using neural activity from four signal modalities: single units, multiunit threshold crossings, unsorted events, and local field potentials. These latent dynamics were mapped to three types of movements: stereotyped center-out reaching and natural random target reaching in monkeys, as well as attempted handwriting in a paralyzed patient. NMR consistently outperforms other methods by over 50% across four signal modalities and three movement types, evaluated over 68 sessions. Our code is uploaded."
    },
    {
        "title": "Reducing class-wise confusion for incremental learning with disentangled manifolds",
        "link_suffix": "/forum?id=mZvzvwIu8f",
        "link": "https://openreview.net/forum?id=mZvzvwIu8f",
        "pdf_link": "https://openreview.net/pdf?id=mZvzvwIu8f",
        "keywords": "Class incremental learning; Auto-encoder; Manifold",
        "abstract": "Class incremental learning (CIL) aims to enable models to continuously learn new classes without catastrophically forgetting old ones. A promising direction is to learn and use prototypes of classes during incremental updates. Despite simplicity and intuition, we find that such methods suffer from inadequate representation capability and unsatisfied confusion caused by distribution drift. In this paper, we develop a Confusion-REduced AuTo-Encoder classifier (CREATE) for CIL. Specifically, our method employs a lightweight auto-encoder module to learn each compact class manifold in latent subspace, constraining samples well reconstructed only on the semantically correct auto-encoder. Thus, the representation stability and capability of class distributions are enhanced, alleviating the potential class-wise confusion problem. To further distinguish the drifted features, we propose a confusion-aware latent space separation loss that ensures exemplars are closely distributed in their corresponding low-dimensional manifold while keeping away from the distributions of drifted features from other classes. Our method demonstrates stronger representational capacity by learning disentangled manifolds and reduces class confusion caused by drift. Extensive experiments on multiple datasets and settings show that CREATE outperforms other state-of-the-art methods up to 5.41%."
    },
    {
        "title": "Robust Video Moment Retrieval with Introspective Knowledge Distillation",
        "link_suffix": "/forum?id=npBrvlYftk",
        "link": "https://openreview.net/forum?id=npBrvlYftk",
        "pdf_link": "https://openreview.net/pdf?id=npBrvlYftk",
        "keywords": "video moment retrieval, diverse query types, model versatility",
        "abstract": "With the huge requirement of video content understanding and editing, Video moment retrieval (VMR) is becoming more and more critical, necessitating models that are adept at correlating video contents with textual queries. The effectiveness of prevailing VMR models, however, is often compromised by their reliance on training data biases, which significantly hampers their generalization capabilities when faced with out-of-distribution (OOD) content. This challenge underscores the need for innovative approaches that can adeptly navigate the intricate balance between leveraging in-distribution (ID) data for learning and maintaining robustness against OOD variations. Addressing this critical need, we introduce Reflective Knowledge Distillation (RefKD), a novel and comprehensive training methodology that integrates the dual processes of Introspective Learning and Extrospective Adjustment. This methodology is designed to refine the model's ability to internalize and apply learned correlations in a manner that is both contextually relevant and resilient to bias-induced distortions. By employing a dual-teacher framework, RefKD encapsulates and contrasts the distinct bias perspectives prevalent in VMR datasets, facilitating a dynamic and reflective learning dialogue with the student model. This interaction is meticulously structured to encourage the student model to engage in a deeper introspection of learned biases and to adaptively recalibrate its learning focus in response to evolving content landscapes. Through this reflective learning process, the model develops a more nuanced and comprehensive understanding of content-query correlations, significantly enhancing its performance across both ID and OOD scenarios. Our extensive evaluations, conducted across several standard VMR benchmarks, demonstrate the unparalleled efficacy of RefKD. The methodology not only aligns with the OOD performance benchmarks set by existing debiasing methods but also, in many instances, significantly surpasses their ID performance metrics. By effectively bridging the gap between ID and OOD learning, RefKD sets a new standard for building VMR systems that are not only more adept at understanding and interpreting video content in a variety of contexts but also more equitable and reliable across diverse operational scenarios. This work not only contributes to the advancement of VMR technology but also paves the way for future research in the domain of bias-aware and robust multimedia content analysis."
    }
]