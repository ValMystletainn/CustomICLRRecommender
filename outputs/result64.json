[
    {
        "title": "Unraveling Neural Cellular Automata for Lightweight Image Compression",
        "link_suffix": "/forum?id=gIrVoQEDQv",
        "link": "https://openreview.net/forum?id=gIrVoQEDQv",
        "pdf_link": "https://openreview.net/pdf?id=gIrVoQEDQv",
        "keywords": "image compression, automata, deep learning",
        "abstract": "Neural Cellular Automata (NCA) are computational models inspired by cellular growth, capable of learning complex behaviors through local interactions. While NCAs have been applied to various tasks like image restoration and synthesis, their potential for image compression remains largely unexplored. This paper aims to unravel the capabilities of NCAs for lightweight image compression by introducing a Grid Neural Cellular Automata (GNCA) training strategy. Unlike traditional methods that depend on large deep learning models, NCAs offer a low-cost compact and highly parallelizable alternative with intrinsic robustness to noise. Through experiments on the COCO 2017 dataset, we compare the compression performance of NCAs against JPEG, JPEG-2000 and WebP, using the metrics  PSNR, SSIM, and MSE and Compression Rate. Our results demonstrate that NCAs achieve competitive compression rates and image quality reconstruction, highlighting their potential as a lightweight solution for efficient image compression. The code will be available upon acceptance."
    },
    {
        "title": "Q-GaLore: Quantized GaLore with INT4 Projection and Layer-Adaptive Low-Rank Gradients",
        "link_suffix": "/forum?id=rBzvEEbrF7",
        "link": "https://openreview.net/forum?id=rBzvEEbrF7",
        "pdf_link": "https://openreview.net/pdf?id=rBzvEEbrF7",
        "keywords": "Large Language Models; Memory Efficient Training; Low Rank",
        "abstract": "Training Large Language Models (LLMs) is memory-intensive due to the large number of parameters and associated optimization states. GaLore, a recent method, reduces memory usage by projecting weight gradients into a low-rank subspace without compromising performance. However, GaLore relies on time-consuming Singular Value Decomposition (SVD) operations to identify the subspace, and the frequent subspace updates lead to significant training time overhead. Moreover, GaLore offers minimal improvements in accuracy and efficiency compared to LoRA in more accessible fine-tuning scenarios. To address these limitations, we introduce Q-GaLore, a novel approach that substantially reduces memory usage by combining quantization and low-rank projection, surpassing the benefits of GaLore. Our method is based on two key observations: (i) the gradient subspace exhibits diverse properties, with some layers converging early in training while others are subject to frequent changes; (ii) the projection matrices are highly resilient to low-bit quantization. Leveraging these insights, Q-GaLore adaptively updates the gradient subspace based on its convergence statistics, achieving comparable performance while significantly reducing the number of SVD operations. We maintain the projection matrices in INT4 format for aggressive memory conservation and preserve weights in INT8 format, incorporating stochastic rounding to capture accumulated gradient information. This approach enables a high-precision training trajectory using only low-precision weights. We demonstrate that Q-GaLore achieves highly competitive pre-training and fine-tuning performance with exceptional memory efficiency. At pre-training, Q-GaLore facilitates training a LLaMA-7B model from scratch on a single NVIDIA RTX 4060 Ti with only 16 GB memory, showcasing its exceptional memory efficiency and practicality. At fine-tuning, it reduces memory consumption by up to 50% compared to LoRA and GaLore, while consistently outperforming QLoRA (by up to 5.19 on MMLU) at the same memory cost. Codes will be released upon acceptance."
    },
    {
        "title": "ROLoRA: Rank Optimization  for Low-Rank Adaptation under Memory Constraints",
        "link_suffix": "/forum?id=GDjwSBZy6l",
        "link": "https://openreview.net/forum?id=GDjwSBZy6l",
        "pdf_link": "https://openreview.net/pdf?id=GDjwSBZy6l",
        "keywords": "low rank adaptation, fine-tuning",
        "abstract": "Low-Rank Adaptation (LoRA) has emerged as a prominent technique for fine-tuning large language models (LLMs) with limited computational resources. However, by injecting low-rank adapters with a rank identical across all layers, standard LoRA overlooks the varying importance of the weight matrices, often leading to suboptimal performance. Therefore, discovering an optimal rank configuration that efficiently utilizes limited training resources remains an open question. Existing solutions typically compromises computational constraints for performance gains, limiting their practical usage in resource-constrained scenarios. To address these issues, in this paper, we propose a novel method named ROLoRA to efficiently discover an effective rank configuration for low-rank adaptation, while strictly adhering to a constrained computational budget during training. In particular, our method iteratively prunes saturated adapters and expands under-fitted ones to increase their capacity until they converge to a highly optimized configuration. Our approach is delicately designed within the Frank-Wolfe algorithmic framework, which offers potential theoretical guarantees. Experimentally, we demonstrate that ROLoRA outperforms standard LoRA on common natural language processing tasks, including the GLUE and SQuAD benchmarks. Additionally, we provide a comprehensive analysis to explain why ROLoRA surpasses competing state-of-the-arts."
    },
    {
        "title": "Everyone Deserves Recourse: Feasible Recourse Paths Using Data Augmentation",
        "link_suffix": "/forum?id=opSPgPIwAD",
        "link": "https://openreview.net/forum?id=opSPgPIwAD",
        "pdf_link": "https://openreview.net/pdf?id=opSPgPIwAD",
        "keywords": "Recourse, Explainability",
        "abstract": "Decisions made using machine learning models can negatively impact individuals\nin critical applications such as healthcare and finance by denying essential services\nor access to opportunity. Algorithmic recourse supplements a negative AI decision\nby providing rejected individuals with advice on the changes they can make to their\nprofiles, so that they may eventually achieve the desired outcome. Most existing\nrecourse methods provide single-step changes by using counterfactual explanations.\nThese counterfactual explanations are computed assuming a fixed (not learned)\ndistance function. Further, few works consider providing more realistic multi-step\nchanges in the form of recourse paths. However, such methods may fail to provide\nany recourse path for some individuals or provide paths that might not be feasible,\nsince intermediate steps needed to reach the counterfactual explanation may not\nbe realizable. We introduce a framework for learning an optimal distance function\nand threshold to compute multi-step recourse paths for all. First, we formalize the\nproblem of finding multi-step recourse paths. Given a set of feasible transitions, we\npropose a data-driven framework for learning the optimal distance and threshold\nfor each step with PAC (Probably Approximately Correct) guarantees. Finally,\nwe provide a data augmentation algorithm to ensure that a solution exists for all\nindividuals. Experiments on several datasets show that the proposed method learns\nfeasible recourse paths for all individuals."
    },
    {
        "title": "Cauchy-Schwarz Regularizers",
        "link_suffix": "/forum?id=KZu3xhPhke",
        "link": "https://openreview.net/forum?id=KZu3xhPhke",
        "pdf_link": "https://openreview.net/pdf?id=KZu3xhPhke",
        "keywords": "Regularizer, optimization, discretization, quantization",
        "abstract": "We introduce a novel class of regularization functions, called Cauchy–Schwarz~(CS) regularizers, which can be designed to induce a wide range of properties in solution vectors of optimization problems. To demonstrate the versatility of CS regularizers, we derive concrete regularization functions that promote discrete-valued vectors, eigenvectors of a given matrix, and orthogonal matrices. The resulting CS regularizers are simple, differentiable, and can be free of spurious critical points, making them suitable for gradient-based solvers and large-scale optimization problems. In addition, CS regularizers automatically adapt to the appropriate scale, which is, for example, beneficial when discretizing weights of neural networks. To demonstrate the efficacy of CS regularizers, we provide results for solving underdetermined systems of linear equations and weight quantization in neural networks. Furthermore, we discuss specializations, variations, and generalizations, which lead to an even broader class of new and possibly more powerful regularizers."
    },
    {
        "title": "Optimizing Attention",
        "link_suffix": "/forum?id=vnp2LtLlQg",
        "link": "https://openreview.net/forum?id=vnp2LtLlQg",
        "pdf_link": "https://openreview.net/pdf?id=vnp2LtLlQg",
        "keywords": "transfomers, attention, efficiency",
        "abstract": "The attention mechanism is an important part of transformer architectures. It en-\nables the network to compare samples within a sequence. Before the comparison\nis performed, tokens are multiplied by trainable matrices. These matrices can\nconstitute a significant part of the total number of parameters. Their size creates\nproblems on systems with limited cache in the compute unit, especially if there\nis limited bandwidth between compute unit and memory. In particular, GPUs on\nmobile devices suffer from this double bottleneck.\nPrior works mitigate this problem for instance by storing low-rank approxima-\ntions, quantization or minimizing the amount of data that needs to be transferred.\nIn this paper, an alternative to the traditional attention mechanism is proposed\nwhich does not require any trainable matrices to perform the attention. The idea\nrests upon solving optimization problems, whereby memory is substituted for\ncompute. It will be shown however, that the computational demand can be re-\nduced such that auto-differentiation becomes possible. An experimental evalua-\ntion shows that the proposed algorithm performs favorable compared with several\nbaselines."
    },
    {
        "title": "Adversarially Robust Anomaly Detection through Spurious Negative Pair Mitigation",
        "link_suffix": "/forum?id=t8fu5m8R5m",
        "link": "https://openreview.net/forum?id=t8fu5m8R5m",
        "pdf_link": "https://openreview.net/pdf?id=t8fu5m8R5m",
        "keywords": "Anomaly Detection, Adversarially Robust Anomaly Detection, Mitigating Spurious Negative Pairs, Anomaly Aware Contrastive Learning",
        "abstract": "Despite significant progress in Anomaly Detection (AD), the robustness of existing detection methods against adversarial attacks remains a challenge, compromising their reliability in critical real-world applications such as autonomous driving. This issue primarily arises from the AD setup, which assumes that training data is limited to a group of unlabeled normal samples, making the detectors vulnerable to adversarial anomaly samples during testing. Additionally, implementing adversarial training as a safeguard encounters difficulties, such as formulating an effective objective function without access to labels. An ideal objective function for adversarial training in AD should promote strong perturbations both within and between the normal and anomaly groups to maximize margin between normal and anomaly distribution. To address these issues, we first propose crafting a pseudo-anomaly group derived from normal group samples. Then, we demonstrate that adversarial training with contrastive loss could serve as an ideal objective function, as it creates both inter- and intra-group perturbations. However, we notice that spurious negative pairs compromise the conventional contrastive loss for achieving robust AD. Spurious negative pairs are those that should be mapped closely but are erroneously separated. These pairs introduce noise and misguide the direction of inter-group adversarial perturbations. To overcome the effect of spurious negative pairs, we define opposite pairs and adversarially pull them apart to strengthen inter-group perturbations. Experimental results demonstrate our superior performance in both clean and adversarial scenarios, with a 26.1% improvement in robust detection across various challenging benchmark datasets."
    },
    {
        "title": "Holographic Node Representations: Pre-training Task-Agnostic Node Embeddings",
        "link_suffix": "/forum?id=tGYFikNONB",
        "link": "https://openreview.net/forum?id=tGYFikNONB",
        "pdf_link": "https://openreview.net/pdf?id=tGYFikNONB",
        "keywords": "GNN, symmetries, pretraining",
        "abstract": "Large general purpose pre-trained models have revolutionized computer vision and natural language understanding. However, the development of general purpose pre-trained Graph Neural Networks (GNNs) lags behind other domains due to the lack of suitable generalist node representations. Existing GNN architectures are often tailored to specific task orders, such as node-level, link-level, or higher-order tasks, because different tasks require distinct permutation symmetries, which are difficult to reconcile within a single model. In this paper, we proposeholographic node representations, a new blueprint for node representations capable of solving tasks of any order. Holographic node representations have two key components: (1) a task-agnostic expansion map, which produces highly expressive, high-dimensional embeddings, free from node-permutation symmetries, to be fed into (2) a reduction map that carefully reintroduces the relevant permutation symmetries to produce low-dimensional, task-specific embeddings. We show that well-constructed expansion maps enable simple and efficient reduction maps, which can be adapted for any task order. Empirical results show that holographic node representations can be effectively pre-trained and reused across tasks of varying orders, yielding up to 100% relative performance improvement, including in cases where prior methods fail entirely."
    },
    {
        "title": "Enhancing LLM's interpretability for time series via multi-level aligned embeddings",
        "link_suffix": "/forum?id=bnmhMxz7PO",
        "link": "https://openreview.net/forum?id=bnmhMxz7PO",
        "pdf_link": "https://openreview.net/pdf?id=bnmhMxz7PO",
        "keywords": "time series, Large Language Models, cross modality",
        "abstract": "The adaptation of large language models (LLMs) to time series forecasting poses unique challenges, as time series data is continuous in nature, while LLMs operate on discrete tokens. Despite the success of LLMs in natural language processing (NLP) and other structured domains, aligning time series data with language-based representations while maintaining both predictive accuracy and interpretability remains a significant hurdle. Existing methods have attempted to reprogram time series data into text-based forms, but these often fall short in delivering meaningful, interpretable results.\nIn this paper, we propose a multi-text alignment framework for time series forecasting using LLMs that not only improves prediction accuracy but also enhances the interpretability of time series representations. Our method decomposes time series into trend, seasonality, and residual components, which are then reprogrammed into component-specific text representations. \nWe introduce a multi-level alignment mechanism, where component-specific embeddings are aligned with pre-trained word tokens, enabling more interpretable forecasts. \nExperiments on multiple datasets demonstrate that our method outperforms state-of-the-art models in accuracy while providing good interpretability."
    },
    {
        "title": "Link Prediction with Relational Hypergraphs",
        "link_suffix": "/forum?id=DAEXilQHYU",
        "link": "https://openreview.net/forum?id=DAEXilQHYU",
        "pdf_link": "https://openreview.net/pdf?id=DAEXilQHYU",
        "keywords": "link prediction, relational hypergraphs, expressivity study",
        "abstract": "Link prediction with knowledge graphs has been thoroughly studied in graph machine learning, leading to a rich landscape of graph neural network architectures with successful applications. \nNonetheless, it remains challenging to transfer the success of these architectures to link prediction withrelational hypergraphs, where the task is over$k$-ary relations, substantially harder than link prediction on knowledge graphs with binary relations only.\nIn this paper, we propose a framework for link prediction with relational hypergraphs, empowering applications of graph neural networks onfully relationalstructures. Theoretically, we conduct a thorough analysis of the expressive power of the resulting model architectures via corresponding relational Weisfeiler-Leman algorithms and also via logical expressiveness.  Empirically, we validate the power of the proposed model architectures on various relational hypergraph benchmarks. The resulting model architectures substantially outperform every baseline for inductive link prediction, and also lead to state-of-the-art results for transductive link prediction."
    },
    {
        "title": "LLaVA-Read: Enhancing Reading Ability of Multimodal Large Language Models",
        "link_suffix": "/forum?id=pPK6sNbFWV",
        "link": "https://openreview.net/forum?id=pPK6sNbFWV",
        "pdf_link": "https://openreview.net/pdf?id=pPK6sNbFWV",
        "keywords": "Multimodal Large Language Models, Text-rich Images, Visual Text Understanding",
        "abstract": "Multimodal large language models have demonstrated impressive capabilities in understanding and manipulating images. However, many of these models struggle with comprehending intensive textual contents embedded within the images, primarily due to the limited text recognition and layout understanding ability. To understand the sources of these limitations, we perform an exploratory analysis showing the drawbacks of classical visual encoders on visual text understanding. Hence, we present LLaVA-Read, a multimodal large language model that utilizes dual visual encoders along with a visual text encoder. Our model surpasses existing state-of-the-art models in various text-rich image understanding tasks, showcasing enhanced comprehension of textual content within images. Together, our research suggests visual text understanding remains an open challenge and an \\textit{efficient} visual text encoder is crucial for future successful multimodal systems."
    },
    {
        "title": "Multi-modal graph neural networks for localized off-grid weather forecasting",
        "link_suffix": "/forum?id=CN328Aw03P",
        "link": "https://openreview.net/forum?id=CN328Aw03P",
        "pdf_link": "https://openreview.net/pdf?id=CN328Aw03P",
        "keywords": "Weather forecasting, Graph Neural Network, Multi-modal, off-grid weather forecasting, heterogeneous graph neural network, climate, climate change, sustainability",
        "abstract": "Urgent applications like wildfire management and renewable energy generation require precise, localized weather forecasts near the Earth's surface.\nHowever, weather forecast products from machine learning or numerical weather models are currently generated on a global regular grid, on which a naive interpolation cannot accurately reflect fine-grained weather patterns close to the ground.\nIn this work, we train a heterogeneous graph neural network (GNN) end-to-end to downscale gridded forecasts to off-grid locations of interest.\nThis multi-modal GNN takes advantage of local historical weather observations (e.g., wind vector, temperature) to correct the gridded weather forecast at different lead times towards locally accurate forecasts.\nEach data modality is modeled as a different type of node in the graph.\nUsing message passing, the node at the prediction location aggregates information from its heterogeneous neighbor nodes.\nExperiments using weather stations across the Northeastern United States show that our model outperforms a range of data-driven and non-data-driven off-grid forecasting methods.\nOur approach demonstrates how the gap between global large-scale weather models and locally accurate predictions can be bridged to inform localized decision-making."
    },
    {
        "title": "The Ramanujan Library - Automated Discovery on the Hypergraph of Integer Relations",
        "link_suffix": "/forum?id=EyaH1wzmao",
        "link": "https://openreview.net/forum?id=EyaH1wzmao",
        "pdf_link": "https://openreview.net/pdf?id=EyaH1wzmao",
        "keywords": "Continued Fractions, Mathematical Constants, Integer Relations, Experimental Mathematics, Riemann Zeta Function, Irrational Number, PSLQ, AI In Mathematics, Automated Conjecture Generation",
        "abstract": "Fundamental mathematical constants appear in nearly every field of science, from physics to biology. \nFormulas that connect different constants often bring great insight by hinting at connections between previously disparate fields.\nDiscoveries of such relations, however, have remained scarce events, relying on sporadic strokes of creativity by human mathematicians.\nRecent developments of algorithms for automated conjecture generation have accelerated the discovery of formulas for specific constants.\nYet, the discovery of connections between constants has not been addressed.\nIn this paper, we present the first library dedicated to mathematical constants and their interrelations. This library can serve as a central repository of knowledge for scientists from different areas, and as a collaborative platform for development of new algorithms.\nThe library is based on a new representation that we propose for organizing the formulas of mathematical constants: \na hypergraph, with each node representing a constant and each edge representing a formula involving two or more constants, corresponding to a regular edge or a hyperedge, respectively.\nUsing this representation, we propose and demonstrate a systematic approach for automatically enriching this library using PSLQ, an integer relation algorithm based on QR decomposition and lattice construction. During its development and testing, our strategy led to the discovery of 75 previously unknown connections between constants, including a new formula for the `first continued fraction' constant $C_1$, novel formulas for natural logarithms, and new formulas connecting $\\pi$ and $e$.\nThe latter formulas generalize a century-old relation between $\\pi$ and $e$ by Ramanujan, which until now was considered a singular formula and is now found to be part of a broader mathematical structure. \nThe code supporting this library is a public, open-source API that can serve researchers in experimental mathematics and other fields of science."
    },
    {
        "title": "Fast and Noise-Robust Diffusion Solvers for Inverse Problems: A Frequentist Approach",
        "link_suffix": "/forum?id=Z9Odi09Rv9",
        "link": "https://openreview.net/forum?id=Z9Odi09Rv9",
        "pdf_link": "https://openreview.net/pdf?id=Z9Odi09Rv9",
        "keywords": "diffusion models, inverse problems, maximum likelihood",
        "abstract": "Diffusion models have been firmly established as principled zero-shot solvers for linear and nonlinear inverse problems, owing to their powerful image prior and ease of formulation as Bayesian posterior samplers. However, many existing solvers struggle in the noisy measurement regime, either overfitting or underfitting to the measurement constraint, resulting in poor sample quality and inconsistent performance across noise levels. Moreover, existing solvers rely on an approximation of Tweedie's formula, where an intractable $\\textit{conditional}$ score is replaced by an $\\textit{unconditional}$ score network, introducing a fundamental source of error in the resulting solution. In this work, we propose a novel frequentist's approach to diffusion-based inverse solvers, where each diffusion step can be seen as the maximum likelihood solution to a simple single-parameter conditional likelihood model, derived by an adjusted application of Tweedie's formula to the forward measurement model. We demonstrate that this perspective is not only scalable and fast, but also allows for a noise-aware maximization scheme with a likelihood-based stopping criterion that promotes the proper noise-adapted fit given knowledge of the measurement noise $\\sigma_\\mathbf{y}$. Finally, we demonstrate comparable or improved performance against a wide selection of contemporary inverse solvers across multiple datasets, tasks, and noise levels."
    },
    {
        "title": "Decoupled SGDA for Games with Intermittent Strategy Communication",
        "link_suffix": "/forum?id=swqZKDoMJA",
        "link": "https://openreview.net/forum?id=swqZKDoMJA",
        "pdf_link": "https://openreview.net/pdf?id=swqZKDoMJA",
        "keywords": "optimization, minimax optimization, distributed games, distributed optimization",
        "abstract": "We focus on reducing communication overhead in multiplayer games, where frequently exchanging strategies between players is not feasible and players have noisy or outdated strategies of the other players.\nWe propose \\textit{Decoupled SGDA}, an extension of Stochastic Gradient Descent Ascent (SGDA), where players perform independent updates using outdated strategies of opponents, with periodic strategy synchronization.\nFor Strongly-Convex-Strongly-Concave (SCSC) games, we demonstrate that Decoupled SGDA achieves near-optimal communication complexity comparable to the best-known GDA rates.\nFor \\emph{weakly coupled} games where the interaction between players is lower relative to non-interactive part of the game, Decoupled SGDA significantly reduces communication costs compared to standard SGDA. \nOur findings extend to multi-player games. To provide insights into the effect of communication frequency and convergence, we extensively study the convergence of Decoupled SGDA for quadratic minimax problems. \nLastly, in settings where the noise over the players is imbalanced, Decoupled SGDA significantly outperforms federated minimax methods."
    },
    {
        "title": "NeuZip: Memory-Efficient Training and Inference with Dynamic Compression of Neural Networks",
        "link_suffix": "/forum?id=N23g8eGOiP",
        "link": "https://openreview.net/forum?id=N23g8eGOiP",
        "pdf_link": "https://openreview.net/pdf?id=N23g8eGOiP",
        "keywords": "Compression, Quantization, Memory-efficient training, Memory-efficient inference",
        "abstract": "The performance of neural networks improves when more parameters are used. However, the model sizes are constrained by the available on-device memory during training and inference. Although applying techniques like quantization can alleviate the constraint, they suffer from performance degradation. In this work, we introduce NeuZip, a new weight compression scheme based on the entropy of floating-point numbers in neural networks. With NeuZip, we are able to achieve memory-efficient training and inference without sacrificing performance. Notably, we significantly reduce the memory footprint of training a Llama-3 8B model from 31GB to less than 16GB, while keeping the training dynamics fully unchanged. In inference, our method can reduce memory usage by more than half while maintaining near-lossless performance."
    },
    {
        "title": "Profiler: Black-box AI-generated Text Origin Detection via Context-aware Inference Pattern Analysis",
        "link_suffix": "/forum?id=7Ab1Uck1Pq",
        "link": "https://openreview.net/forum?id=7Ab1Uck1Pq",
        "pdf_link": "https://openreview.net/pdf?id=7Ab1Uck1Pq",
        "keywords": "AI-generated Text Detection, Large Language Models",
        "abstract": "With the increasing capabilities of Large Language Models (LLMs), the proliferation of AI-generated texts has become a serious concern. Given the diverse range of organizations providing LLMs, it is crucial for governments and third-party entities to identify the origin LLM of a given text to enable accurate infringement and mitigation of potential misuse. However, existing detection methods, primarily designed to distinguish between human-generated and LLM-generated texts, often fail to accurately identify the origin LLM due to the high similarity of AI-generated texts from different sources. In this paper, we propose a novel black-box AI-generated text origin detection method, dubbed Profiler, which accurately predicts the origin of an input text by extracting distinct context inference patterns through calculating and analyzing novel context losses between the surrogate model's output logits and the adjacent input context. Extensive experimental results show that Profiler outperforms 10 state-of-the-art baselines, achieving more than a 25% increase in AUC score on average across both natural language and code datasets when evaluated against five of the latest commercial LLMs under both in-distribution and out-of-distribution settings."
    },
    {
        "title": "Atmospheric Radiation Parameterization by Neural Ordinary Differential Equations and Related Models",
        "link_suffix": "/forum?id=otXB6odSG8",
        "link": "https://openreview.net/forum?id=otXB6odSG8",
        "pdf_link": "https://openreview.net/pdf?id=otXB6odSG8",
        "keywords": "regression models, neural ODEs, numerical weather prediction, neural networks, radiative transfer",
        "abstract": "Radiation parameterization schemes are crucial components of weather and climate models, however, they are known to be computationally intensive. Alternatively, they can be emulated with machine learning (ML) regression models. Mainly vertical energy propagation motivates the usage of ML models featuring sequential data processing. We investigate these and related models for radiation parameterization using atmospheric data modeled within an Arctic region. We observe that NeuralODE performs best in predicting both the long- and short-wave heating rates. Furthermore, we substitute the architecture with its discrete form to boost its efficiency while preserving competitive performance. The practical applicability of the models was studied for different model sizes. Finally, We linked the trained neural network to the operational weather forecast model and assessed its performance versus the conventional radiation parametrization. We received a speedup of four times of the radiation steps without significant loss of accuracy. The proposed parametrization substitute can strongly reduce the computational burden and the carbon footprint of the weather forecasting."
    },
    {
        "title": "Computationally Efficient RL under Linear Bellman Completeness for Deterministic Dynamics",
        "link_suffix": "/forum?id=hyfe5q5TD0",
        "link": "https://openreview.net/forum?id=hyfe5q5TD0",
        "pdf_link": "https://openreview.net/pdf?id=hyfe5q5TD0",
        "keywords": "reinforcement learning theory, linear function approximation",
        "abstract": "We study computationally and statistically efficient Reinforcement Learning algorithms for thelinear Bellman Completesetting, a setting that uses linear function approximation to capture value functions and unifies existing models like linear Markov Decision Processes (MDP) and Linear Quadratic Regulators (LQR).  While it is known from the prior works that this setting is statistically tractable, it remained open whether a computationally efficient algorithm exists. Our work provides a computationally efficient algorithm for the linear Bellman complete setting that works for MDPs with large action spaces, random initial states, and random rewards but relies on the underlying dynamics to be deterministic. Our approach is based on randomization: we inject random noise into least square regression problems to perform optimistic value iteration. Our key technical contribution is to carefully design the noise to only act in the null space of the training data to ensure optimism while circumventing a subtle error amplification issue."
    },
    {
        "title": "HELMET: How to Evaluate Long-context Models Effectively and Thoroughly",
        "link_suffix": "/forum?id=293V3bJbmE",
        "link": "https://openreview.net/forum?id=293V3bJbmE",
        "pdf_link": "https://openreview.net/pdf?id=293V3bJbmE",
        "keywords": "long-context language models, benchmarking",
        "abstract": "Many benchmarks exist for evaluating long-context language models (LCLMs), but developers often rely on synthetic tasks like needle-in-a-haystack (NIAH) or arbitrarily selected subsets of datasets. It remains unclear whether these evaluations translate to the diverse downstream applications of LCLMs, and the inconsistency further complicates model comparison. We investigate the underlying reasons behind current practices and find that existing benchmarks often provide noisy signals due to low coverage of long-context applications, insufficient dataset lengths, unreliable metrics, and incompatibility with base models. In this work, we present HELMET (How to Evaluate Long-context Models Effectively and Thoroughly), a comprehensive benchmark encompassing seven diverse, application-centric categories. We also address many issues in previous benchmarks by adding controllable lengths up to 128k tokens, model-based evaluation for reliable metrics, and few-shot prompting in all tasks for evaluating base models. Consequently, we demonstrate that HELMET offers more reliable and distinct rankings of frontier LCLMs. Through a comprehensive study of 51 LCLMs, we find that (1) synthetic tasks like NIAH are not good predictors of downstream performance; (2) the diverse categories in HELMET exhibit distinct trends that do not correlate well with each other; and (3) while most LCLMs achieve perfect NIAH scores, open-source models significantly lag behind closed ones when the task requires full-context reasoning or following complex instructions---the gap widens with increased lengths. Finally, we recommend using our RAG tasks for fast model developments, as they are easy to run and more predictive of downstream applications than existing synthetic tasks; but ultimately, we advocate for a holistic evaluation across diverse tasks. We hope HELMET serves as a valuable resource for future long-context model development."
    },
    {
        "title": "EquiJump: Protein Dynamics Simulation via SO(3)-Equivariant Stochastic Interpolants",
        "link_suffix": "/forum?id=i3QV4XgsLA",
        "link": "https://openreview.net/forum?id=i3QV4XgsLA",
        "pdf_link": "https://openreview.net/pdf?id=i3QV4XgsLA",
        "keywords": "biomolecules, proteins, molecular dynamics, generative models, neural transport, stochastic interpolants",
        "abstract": "Mapping the conformational dynamics of proteins is crucial for elucidating their functional mechanisms. While Molecular Dynamics (MD) simulation enables detailed time evolution of protein motion, its computational toll hinders its use in practice. To address this challenge, multiple deep learning models for reproducing and accelerating MD have been proposed drawing on transport-based generative methods. However, existing work focuses on generation through transport of samples from prior distributions, that can often be distant from the data manifold. The recently proposed framework of stochastic interpolants, instead, enables transport between arbitrary distribution endpoints. Building upon this work, we introduce EquiJump, a transferable SO(3)-equivariant model that bridges all-atom protein dynamics simulation time steps directly. Our approach unifies diverse sampling methods and is benchmarked against existing models on trajectory data of fast folding proteins. EquiJump achieves state-of-the-art results on dynamics simulation with a transferable model on all of the fast folding proteins."
    },
    {
        "title": "Efficient Physics-Constrained Diffusion Models for Solving Inverse Problems",
        "link_suffix": "/forum?id=Da3j02cHe0",
        "link": "https://openreview.net/forum?id=Da3j02cHe0",
        "pdf_link": "https://openreview.net/pdf?id=Da3j02cHe0",
        "keywords": "physics-constraints inverse problem, diffusion model, PDE, generative modeling",
        "abstract": "Solving inverse problems in scientific and engineering domains often involves complex, nonlinear forward physics and ill-posed conditions. \nRecent advancements in diffusion model have shown promise for general inverse problems, yet their application to scientific domains remains less explored and is hindered by the complexity and high non-linearity of physics constraints. We present a novel framework called physics-constrained diffusion model (PCDM) that integrates pre-trained diffusion models and physics-constrained objectives, providing plausible solutions to physics-constrained inverse problems within a feasible time. \nWe leverage accelerated diffusion sampling to enable a practical generation process while strictly adhering to physics constraints by solving optimization problems at each timestep. By decoupling the likelihood optimization from the reverse diffusion steps, we ensure that the solutions remain physically consistent, even when employing fewer sampling steps.\nThis approach provides physically plausible solutions without requiring excessive inference time. \nWe validate our method on a wide range of challenging physics-constrained inverse problems, including data assimilation, topology optimization, and full-waveform inversion. Experimental results show that our approach significantly outperforms existing methods in both efficiency and precision, making it practical for real-world applications."
    },
    {
        "title": "Uncovering Latent Memories in Large Language Models",
        "link_suffix": "/forum?id=KSBx6FBZpE",
        "link": "https://openreview.net/forum?id=KSBx6FBZpE",
        "pdf_link": "https://openreview.net/pdf?id=KSBx6FBZpE",
        "keywords": "Large Language Models, Memorization, Empirical Study, Data Leakage, Privacy, LLMs, Dynamics, Interpretability, Mechanistic",
        "abstract": "Frontier AI systems are making transformative impacts across society, but such benefits are not without costs: models trained on web-scale datasets containing personal and private data raise profound concerns about data privacy and security. Language models are trained on extensive corpora including potentially sensitive or proprietary information, and the risk of data leakage, where the model response reveals pieces of such information, remains inadequately understood. Prior work has investigated that sequence complexity and the number of repetitions are the primary drivers of memorization. In this work, we examine the most vulnerable class of data: highly complex sequences that are presented only once during training. These sequences often contain the most sensitive information and pose considerable risk if memorized. By analyzing the progression of memorization for these sequences throughout training, we uncover a striking observation: many memorized sequences persist in the model's memory, exhibiting resistance to catastrophic forgetting even after just one encounter. Surprisingly, these sequences may not appear memorized immediately after their first exposure but can later be “uncovered” during training, even in the absence of subsequent exposures - a phenomenon we call \"latent memorization.\" Latent memorization presents a serious challenge for data privacy, as sequences that seem hidden at the final checkpoint of a model may still be easily recoverable. We demonstrate how these hidden sequences can be revealed through random weight perturbations, and we introduce a diagnostic test based on cross-entropy loss to accurately identify latent memorized sequences."
    },
    {
        "title": "Episodic Memories Generation and Evaluation Benchmark for Large Language Models",
        "link_suffix": "/forum?id=6ycX677p2l",
        "link": "https://openreview.net/forum?id=6ycX677p2l",
        "pdf_link": "https://openreview.net/pdf?id=6ycX677p2l",
        "keywords": "Episodic Memory Modeling, Large Language Models, Synthetic Benchmark Generation, Cue-based Retrieval, Temporal-Spatial Reasoning, Long-context Understanding, Human-inspired AI",
        "abstract": "Episodic memory -- the ability to recall specific events grounded in time and space -- is a cornerstone of human cognition, enabling not only coherent storytelling, but also planning and decision-making. Despite their remarkable capabilities, Large Language Models (LLMs) lack a robust mechanism for episodic memory: we argue that integrating episodic memory capabilities into LLM is essential for advancing AI towards human-like cognition, increasing their potential to reason consistently and ground their output in real-world episodic events, hence avoiding confabulations. To address this challenge, we introduce a comprehensive framework to model and evaluate LLM episodic memory capabilities. Drawing inspiration from cognitive science, we develop a structured approach to represent episodic events, encapsulating temporal and spatial contexts, involved entities, and detailed descriptions. We synthesize a unique episodic memory benchmark, free from contamination,\nand release open source code and datasets to assess LLM performance across various recall and episodic reasoning tasks. Our evaluation of state-of-the-art models, including GPT-4 and Claude variants, in addition to the recent o1-mini, reveals that even the most advanced LLMs struggle with episodic memory tasks, particularly when dealing with multiple related events or complex spatio-temporal relationships --- even in contexts as short as 10k-100k tokens."
    },
    {
        "title": "Just How Flexible are Neural Networks in Practice?",
        "link_suffix": "/forum?id=xImTb8mNOr",
        "link": "https://openreview.net/forum?id=xImTb8mNOr",
        "pdf_link": "https://openreview.net/pdf?id=xImTb8mNOr",
        "keywords": "Neural networks, approximation theory, model complexity, generalization",
        "abstract": "Although overparameterization theory suggests that neural networks can fit any dataset with up to as many samples as they have parameters, practical limitations often prevent them from reaching this capacity. In this study, we empirically investigate the practical flexibility of neural networks and uncover several surprising findings. Firstly, we observe that standard optimizers, such as stochastic gradient descent (SGD), often converge to solutions that fit significantly fewer samples than the model's parameter count, highlighting a gap between theoretical and practical capacity. Secondly, we find that convolutional neural networks (CNNs) are substantially more parameter-efficient than multi-layer perceptrons (MLPs) and Vision Transformers (ViTs), even when trained on randomly labeled data, emphasizing the role of architectural inductive biases. Thirdly, we demonstrate that the difference in a network's ability to fit correctly labeled data versus incorrectly labeled data is a strong predictor of generalization performance, offering a novel metric for predicting generalization. Lastly, we show that stochastic training methods like SGD enable networks to fit more data than full-batch gradient descent, suggesting that stochasticity enhances flexibility beyond regularization effects. These findings highlight the importance of understanding practical capacity limits and their implications for model generalization, providing new insights into neural network training and architectural design."
    }
]