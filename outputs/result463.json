[
    {
        "title": "PerLDiff: Controllable Street View Synthesis Using Perspective-Layout Diffusion Model",
        "link_suffix": "/forum?id=MjhTb4gwFP",
        "link": "https://openreview.net/forum?id=MjhTb4gwFP",
        "pdf_link": "https://openreview.net/pdf?id=MjhTb4gwFP",
        "keywords": "Controllable generation;3D annotation;Geometric priors;",
        "abstract": "Controllable generation is considered a potentially vital approach to address the challenge of annotating 3D data, and the precision of such controllable generation becomes particularly imperative in the context of data production for autonomous driving. Existing methods focus on the integration of diverse generative information into controlling inputs, utilizing frameworks such as GLIGEN or ControlNet, to produce commendable outcomes in controllable generation. However, such approaches intrinsically restrict generation performance to the learning capacities of predefined network architectures. In this paper, we explore the integration of controlling information and introduce PerLDiff (\\textbf{Per}spective-\\textbf{L}ayout \\textbf{Diff}usion Models), a method for effective street view image generation that fully leverages perspective 3D geometric information. Our PerLDiff employs 3D geometric priors to guide the generation of street view images with precise object-level control within the network learning process, resulting in a more robust and controllable output. Moreover, it demonstrates superior controllability compared to alternative layout control methods. Empirical results justify that our PerLDiff markedly enhances the precision of generation on the NuScenes and KITTI datasets."
    },
    {
        "title": "Unsolvable Problem Detection: Evaluating Trustworthiness of Large Multimodal Models",
        "link_suffix": "/forum?id=K4YMFdx2Z2",
        "link": "https://openreview.net/forum?id=K4YMFdx2Z2",
        "pdf_link": "https://openreview.net/pdf?id=K4YMFdx2Z2",
        "keywords": "Unsolvable Problem Detection: Evaluating Trustworthiness of Large Multimodal Models",
        "abstract": "This paper introduces a novel and well-defined challenge for Large Multimodal Models (LMMs), termed Unsolvable Problem Detection (UPD). UPD examines the LMM's ability to withhold answers when faced with unsolvable problems. UPD encompasses three problems: Absent Answer Detection (AAD), Incompatible Answer Set Detection (IASD), and Incompatible Visual Question Detection (IVQD), covering unsolvable cases like answer-lacking or incompatible choices and image-question mismatches. In this paper, we introduce the MM-UPD Bench, a benchmark for assessing performance across various ability dimensions. Our experiments reveal that even most LMMs, which demonstrate adequate performance on existing benchmarks, struggle significantly with MM-UPD, underscoring a novel aspect of trustworthiness that current benchmarks have overlooked. To deepen the understanding of the UPD, we explore various solutions, including chain of thought, self-reflection, and instruction tuning, and demonstrate each approach's efficacy and limitations. We hope our insights, together with future efforts within the proposed UPD settings, will enhance the broader understanding and development of more practical and reliable LMMs."
    },
    {
        "title": "Weak Supervision from Vision-Language Models to Self-Improve on Downstream Tasks",
        "link_suffix": "/forum?id=XCg9YcSKCZ",
        "link": "https://openreview.net/forum?id=XCg9YcSKCZ",
        "pdf_link": "https://openreview.net/pdf?id=XCg9YcSKCZ",
        "keywords": "Semi-supervised Learning, Vision-language Model",
        "abstract": "We present SelfPrompt, a novel prompt-tuning approach for vision-language models (VLMs) in a semi-supervised learning setup. Existing methods for tuning VLMs in semi-supervised setups struggle with the efficient use of the limited label-set budget, the negative impact of the miscalibrated VLMs on pseudo-labelling, and the accumulation of noisy pseudo-labels. SelfPrompt addresses these challenges by introducing (a) a weakly-supervised sampling technique that selects a diverse and representative labelled set, (b) a cluster-guided pseudo-labelling method that improves pseudo-label accuracy, and (c) a confidence-aware semi-supervised learning module that maximizes the utilization of unlabelled data by combining supervised learning and weakly-supervised learning. We conduct extensive evaluations across 13 datasets, significantly surpassing state-of-the-art performances with average improvements of 7.92% in semi-supervised learning and 4.9% in base-to-novel generalization, using a 2-shot setup. Furthermore, SelfPrompt shows excellent generalization in single-shot settings, achieving an average improvement of 11.78%. Our ablation studies and sensitivity analyses highlight the robustness of our method."
    },
    {
        "title": "Clip Body and Tail Separately: High Probability Guarantees for DP-SGD with Heavy Tails",
        "link_suffix": "/forum?id=du7iixIeke",
        "link": "https://openreview.net/forum?id=du7iixIeke",
        "pdf_link": "https://openreview.net/pdf?id=du7iixIeke",
        "keywords": "Privacy, DPSGD, Gradient Clipping, High Probability Bounds",
        "abstract": "Differentially Private Stochastic Gradient Descent (DPSGD) is widely utilized to preserve training data privacy in deep learning, which first clips the gradients to a predefined norm and then injects calibrated noise into the training procedure. Existing DPSGD works typically assume the gradients follow sub-Gaussian distributions and design various gradient clipping mechanisms to optimize training performance. However, recent studies have shown that the gradients in deep learning exhibit a heavy-tail phenomenon, that is, the tails of the gradient may have infinite variance, which leads to excessive clipping loss with existing mechanisms. To address this problem, we propose a novel approach, Discriminative Clipping~(DC)-DPSGD, with two key designs. First, we introduce a subspace identification technique to distinguish between body and tail gradients. Second, we present a discriminative clipping mechanism that applies different clipping thresholds separately for body and tail gradients to reduce the clipping loss. Under the non-convex condition and heavy-tailed sub-Weibull gradient noise assumption, DC-DPSGD reduces the empirical risk from ${\\mathbb{O}\\left(\\log^{\\max(0,\\theta-1)}(T/\\delta)\\log^{2\\theta}(\\sqrt{T})\\right)}$ to ${\\mathbb{O}\\left(\\log(\\sqrt{T})\\right)}$ with heavy-tailed index $\\theta> 1/2$, iterations $T$, and high probability $1-\\delta$. Extensive experiments on five real-world datasets demonstrate that our approach outperforms three baselines by up to 9.72% in terms of accuracy."
    },
    {
        "title": "GAUSSIANFLOW: SPLATTING GAUSSIAN DYNAMICS FOR 4D CONTENT CREATION",
        "link_suffix": "/forum?id=okD9dbifxa",
        "link": "https://openreview.net/forum?id=okD9dbifxa",
        "pdf_link": "https://openreview.net/pdf?id=okD9dbifxa",
        "keywords": "optical flow, neural rendering, Gaussian Splatting",
        "abstract": "Creating 4D fields of Gaussian Splatting from images or videos is a challenging task due to its under-constrained nature. While the optimization can draw photometric reference from the input videos or be regulated by generative models, directly supervising Gaussian motions remains underexplored. In this paper, we introduce a novel concept, Gaussian flow, which connects the dynamics of 3D Gaussians and pixel velocities between consecutive frames. The Gaussian flow can be efficiently obtained by splatting Gaussian dynamics into the image space. This differentiable process enables direct dynamic supervision from optical flow. Our method significantly benefits 4D dynamic content generation and 4D novel view synthesis with Gaussian Splatting, especially for contents with rich motions that are hard to be handled by existing methods. The common color drifting issue that happens in 4D generation is also resolved with improved Guassian dynamics. Superior visual quality on extensive experiments demonstrates our method\u2019s effectiveness. As shown in our evaluation, Gaussian Flow can drastically improve both quantitative and qualitative results for 4D Generation and 4D novel view synthesis."
    },
    {
        "title": "3DTrajMaster: Mastering 3D Trajectory for Multi-Entity Motion in Video Generation",
        "link_suffix": "/forum?id=Gx04TnVjee",
        "link": "https://openreview.net/forum?id=Gx04TnVjee",
        "pdf_link": "https://openreview.net/pdf?id=Gx04TnVjee",
        "keywords": "Controllable Video Generation, 3D Motion Control",
        "abstract": "This paper aims to manipulate multi-entity 3D motions in video generation. Previous methods on controllable video generation primarily leverage 2D control signals to manipulate object motions and have achieved remarkable synthesis results. However, 2D control signals are inherently limited in expressing the 3D nature of object motions. To overcome this problem, we introduce 3DTrajMaster, a robust controller that regulates multi-entity dynamics in 3D space, given user-desired 6DoF pose (location and rotation) sequences of entities.  At the core of our approach is a plug-and-play 3D-motion grounded object injector that fuses multiple input entities with their respective 3D trajectories through a gated self-attention mechanism. In addition, we exploit the ControlNet-like architecture to preserve the video diffusion prior, which is crucial for generalization ability. To mitigate video quality degradation, we introduce a domain adaptor during training and employ an annealed sampling strategy during inference.  To address the lack of suitable training data, we construct a 360-Motion Dataset, which first correlates collected 3D human and animal assets with GPT-generated trajectory and then captures their motion with 12 evenly-surround cameras.  Extensive experiments show that 3DTrajMaster sets a new state-of-the-art in both accuracy and generalization for controlling multi-entity 3D motions."
    },
    {
        "title": "Adaptive In-conversation Team Building for Language Model Agents",
        "link_suffix": "/forum?id=uPwe2w78Wx",
        "link": "https://openreview.net/forum?id=uPwe2w78Wx",
        "pdf_link": "https://openreview.net/pdf?id=uPwe2w78Wx",
        "keywords": "Natural Language Processing, Large Language Model, Agent, Multi-agent",
        "abstract": "Leveraging multiple large language model (LLM) agents has shown to be a promising approach for tackling complex tasks, while the effective design of multiple agents for a particular application remains an art. It is thus intriguing to answer a critical question: Given a task, how can we build a team of LLM agents to solve it effectively? Our new adaptive team-building paradigm offers a flexible solution, realized through a novel agent design named Captain Agent. It dynamically forms and manages teams for each step of a task-solving process, utilizing nested group conversations and reflection to ensure diverse expertise and prevent stereotypical outputs, allowing for a flexible yet structured approach to problem-solving. A comprehensive evaluation across six real-world scenarios demonstrates that Captain Agent significantly outperforms existing multi-agent methods with 21.94% improvement in average accuracy, providing outstanding performance without requiring task-specific prompt engineering. Our exploration of different backbone LLM and cost analysis further shows that Captain Agent can improve the conversation quality of weak LLM and achieve competitive performance with extremely low cost, which illuminates the application of multi-agent systems."
    },
    {
        "title": "UniMatch: Universal Matching from Atom to Task for Few-Shot Drug Discovery",
        "link_suffix": "/forum?id=v9EjwMM55Y",
        "link": "https://openreview.net/forum?id=v9EjwMM55Y",
        "pdf_link": "https://openreview.net/pdf?id=v9EjwMM55Y",
        "keywords": "Few-shot molecular representation learning, maching learning",
        "abstract": "Drug discovery is crucial for identifying candidate drugs for various diseases. However, its low success rate often results in a scarcity of annotations, posing a few-shot learning problem. Existing methods primarily focus on single-scale features, overlooking the hierarchical molecular structures that determine different molecular properties. To address these issues, we introduce Universal Matching Networks (UniMatch), a dual matching framework that integrates explicit hierarchical molecular matching with implicit task-level matching via meta-\nlearning, bridging multi-level molecular representations and task-level generalization. Specifically, our approach explicitly captures structural features across multiple levels\u2014atoms, substructures, and molecules\u2014via hierarchical pooling and matching, facilitating precise molecular representation and comparison. Additionally, we employ a meta-learning strategy for implicit task-level matching, allowing the model to capture shared patterns across tasks and quickly adapt to new ones. This unified matching framework ensures effective molecular alignment while leveraging shared meta-knowledge for fast adaptation. Our experimental results demonstrate that UniMatch outperforms state-of-the-art methods on the MoleculeNet and FS-Mol benchmarks, achieving improvements of 2.87% in AUROC and 6.52% in \u2206AUPRC. UniMatch also shows excellent generalization ability on the Meta-MolNet benchmark"
    },
    {
        "title": "Safe Multi-agent Reinforcement Learning with Protection Motivation Theory",
        "link_suffix": "/forum?id=37f8b1ZDzS",
        "link": "https://openreview.net/forum?id=37f8b1ZDzS",
        "pdf_link": "https://openreview.net/pdf?id=37f8b1ZDzS",
        "keywords": "Safety, Multi-agent Reinforcement Learning, Protection Motivation Theory",
        "abstract": "A challenging problem for implementing multi-agent reinforcement learning (MARL) in real-world applications is ensuring the safety of cooperative strategies. According to the Protection Motivation Theory (PMT), threat appraisals result in negative emotions and elicit protective behaviors, which are instrumental for coping with security threats. Drawing inspiration from the PMT, we focus on two discrete emotions--fear and regret--to evaluate threat severity and facilitate multiple agents to learn protective behaviors. These can promote cooperative decision-making with fewer safety violations. Specifically, we propose two safety guarantee methods with PMT: fear for safety guarantee (F4SG) and regret for safety guarantee (R4SG), utilizing the active inference technique to model the emotions of fear and regret separately. The threat severity evaluated by these emotions influences the state value and the executed action respectively, which avoids the potential threat of visiting certain states or taking certain actions. Experimental results demonstrate that our proposed methods are safer and more efficient than state-of-the-art baselines on challenging tasks in safe MARL benchmarks."
    },
    {
        "title": "EcoAct: Economic Agent Determines When to Register What Action",
        "link_suffix": "/forum?id=OyWreBlvIE",
        "link": "https://openreview.net/forum?id=OyWreBlvIE",
        "pdf_link": "https://openreview.net/pdf?id=OyWreBlvIE",
        "keywords": "LLM, efficiency",
        "abstract": "Recent advancements have enabled Large Language Models (LLMs) to function as agents that can perform actions using external tools. \nThis requires registering, i.e. integrating tool information into the LLM context prior to taking actions.\nCurrent methods indiscriminately incorporates all candidate tools into the agent\u2019s context and retains them across multiple reasoning steps. This process remains opaque to LLM agents and is not integrated into their reasoning procedures, leading to inefficiencies due to increased context length from irrelevant tools. \nTo address this, we introduce EcoAct, a tool-using algorithm that allows LLMs to selectively register tools as needed, optimizing context use. By integrating the tool registration process into the reasoning procedure, EcoAct reduces computational costs by over 50% in multi-step reasoning tasks while maintaining performance, as demonstrated through extensive experiments. Moreover, it can be plugged into any reasoning pipeline with only minor modifications to the prompt, making it universally applicable to LLM agents now and in the future."
    },
    {
        "title": "SeaDiff: Delve into Underwater Image Generation with Symmetrical Parameter Control",
        "link_suffix": "/forum?id=K7qurARDi1",
        "link": "https://openreview.net/forum?id=K7qurARDi1",
        "pdf_link": "https://openreview.net/pdf?id=K7qurARDi1",
        "keywords": "diffusion model, controllable generation, underwater",
        "abstract": "With the advancement of diffusion models, the controllability of image generation has significantly improved. However, due to the refraction and absorption of light in water, underwater images often exhibit notable variations in luminance and color cast. This leads to challenges for generative models pre-trained on terrestrial images, as they struggle to produce underwater images with a diverse range of these variations, severely limiting the appearance diversity of generated underwater images. To address this issue, we focus on the precise control of appearance in underwater images. We model the appearance of underwater images using three attributes: luminance, dynamic range, and color cast. We propose a new method, SeaDiff, which introduces a Symmetrical Parameter Control structure to achieve precise control over the appearance of underwater images. The proposed method comprises two modules: Appearance Writer, which encodes and injects appearance attributes into the U-Net encoder, and Appearance Reader, which ensures that the generated images align with the desired appearance by analyzing the feature maps. Experimental results demonstrate that the proposed SeaDiff method significantly improves control over underwater image appearance while maintaining image quality, validating its effectiveness in underwater image generation."
    },
    {
        "title": "Unconstrained Salient and Camouflaged Object Detection",
        "link_suffix": "/forum?id=AQqOC3FKPO",
        "link": "https://openreview.net/forum?id=AQqOC3FKPO",
        "pdf_link": "https://openreview.net/pdf?id=AQqOC3FKPO",
        "keywords": "Salient object detection, Camouflaged object detection, SAM, Benchmark",
        "abstract": "Visual Salient Object Detection (SOD) and Camouflaged Object Detection (COD) are two interrelated yet distinct tasks. Both tasks model the human visual system's ability to perceive the presence of objects. The traditional SOD datasets and methods are designed for scenes where only salient objects are present, similarly, COD datasets and methods are designed for scenes where only camouflaged objects are present. Scenes where both salient and camouflaged objects coexist, or where neither is present, are not considered. This simplifies the existing research on SOD and COD. In this paper, to explore a more generalized approach to SOD and COD, we introduce a benchmark called Unconstrained Salient and Camouflaged Object Detection \\textbf{(USCOD)}, which supports the simultaneous detection of salient and camouflaged objects in unconstrained scenes, regardless of their presence. Towards this, we construct a large-scale dataset, \\textbf{CS12K}, that encompasses a variety of scenes, including four distinct types: scenes containing only salient objects, scenes with only camouflaged objects, scenes where both salient and camouflaged objects coexist, and scenes without any objects. In our benchmark experiments, we find that a major challenge in USCOD is distinguishing salient objects from camouflaged objects within the same model. To address this, we propose a USCOD baseline called \\textbf{\\ourmodel}, which freezes the SAM mask decoder for mask reconstruction, allowing the model to focus on distinguishing between salient and camouflaged objects. Furthermore, to evaluate models\u2019 ability to distinguish between salient and camouflaged objects, we design a metric called Camouflage-Saliency Confusion Score (\\textbf{CSCS}). The proposed method achieves state-of-the-art performance on the newly introduced USCOD task. The code and dataset will be publicly available."
    },
    {
        "title": "Animate-X: Universal Character Image Animation with Enhanced Motion Representation",
        "link_suffix": "/forum?id=1IuwdOI4Zb",
        "link": "https://openreview.net/forum?id=1IuwdOI4Zb",
        "pdf_link": "https://openreview.net/pdf?id=1IuwdOI4Zb",
        "keywords": "Animation, Anthropomorphic, Video Generation, Pose",
        "abstract": "Character image animation, which generates high-quality videos from a reference image and target pose sequence, has seen significant progress in recent years. However, most existing methods only apply to human figures, which usually do not generalize well on anthropomorphic characters commonly used in industries like gaming and entertainment. Our in-depth analysis suggests to attribute this limitation to their insufficient modeling of motion, which is unable to comprehend the movement pattern of the driving video, thus imposing a pose sequence rigidly onto the target character. To this end, this paper proposes $\\texttt{Animate-X}$, a universal animation framework based on LDM for various character types (collectively named $\\texttt{X}$), including anthropomorphic characters. To enhance motion representation, we introduce the Pose Indicator, which captures comprehensive motion pattern from the driving video through both implicit and explicit manner. The former leverages CLIP visual features of a driving video to extract its gist of motion, like the overall movement pattern and temporal relations among motions, while the latter strengthens the generalization of LDM by simulating possible inputs in advance that may arise during inference. Moreover, we introduce a new Animated Anthropomorphic Benchmark ($\\texttt{$A^2$Bench}$) to evaluate the performance of $\\texttt{Animate-X}$ on universal and widely applicable animation images. Extensive experiments demonstrate the superiority and effectiveness of $\\texttt{Animate-X}$ compared to state-of-the-art methods. Please use any web browser to open the $\\textit{.html}$ file in the $\\textit{Supplementary Materials}$ to view the generated videos."
    },
    {
        "title": "Inner Information Analysis Algorithm for Deep Neural Network based on Community",
        "link_suffix": "/forum?id=awz1JPyXNK",
        "link": "https://openreview.net/forum?id=awz1JPyXNK",
        "pdf_link": "https://openreview.net/pdf?id=awz1JPyXNK",
        "keywords": "inner information analysis, transparency, knowledge mining",
        "abstract": "Deep learning has achieved advancements across a variety of forefront fields. However, its inherent 'black box' characteristic poses challenges to the comprehension and trustworthiness of the decision-making processes within neural networks. To mitigate these challenges, we introduce InnerSightNet, an inner information analysis algorithm designed to illuminate the inner workings of deep neural networks through the perspectives of community. This approach is aimed at deciphering the intricate patterns of neurons within deep neural networks, thereby shedding light on the networks' information processing and decision-making pathways. InnerSightNet operates in three primary phases, 'neuronization-aggregation-evaluation'. Initially, it transforms learnable units into a structured network of neurons. Subsequently, these neurons are aggregated into distinct communities according to representation attributes. The final phase involves the evaluation of these communities' roles and functionalities, to unpick the information flow and decision-making. By transcending focus on single-layer or individual neuron, InnerSightNet broadens the horizon for deep neural network interpretation. InnerSightNet offers a unique vantage point, enabling insights into the collective behavior of communities within the overarching architecture, thereby enhancing transparency and trust in deep learning systems."
    },
    {
        "title": "SoundMorpher: Perceptually-Uniform Sound Morphing with Diffusion Model",
        "link_suffix": "/forum?id=h3n7al4mhk",
        "link": "https://openreview.net/forum?id=h3n7al4mhk",
        "pdf_link": "https://openreview.net/pdf?id=h3n7al4mhk",
        "keywords": "Sound Morphing, Perceptual Consistency, Diffusion Model",
        "abstract": "We present SoundMorpher, a sound morphing method that generates perceptually uniform morphing trajectories using a diffusion model. Traditional sound morphing methods models the intractable relationship between morph factor and perception of the stimuli for resulting sounds under a linear assumption, which oversimplifies the complex nature of sound perception and limits their morph quality. In contrast, SoundMorpher explores an explicit proportional mapping between the morph factor and the perceptual stimuli of morphed sounds based on Mel-spectrogram. This approach enables smoother transitions between intermediate sounds and ensures perceptually consistent transformations, which can be easily extended to diverse sound morphing tasks. Furthermore, we present a set of quantitative metrics to comprehensively assess sound morphing systems based on three objective criteria, namely, correspondence, perceptual intermediateness, and smoothness. We provide extensive experiments to demonstrate the effectiveness and versatility of SoundMorpher in real-world scenarios, highlighting its potential impact on various applications such as creative music composition, film post-production and interactive audio technologies."
    },
    {
        "title": "BrainSF: A Foundation Model for Whole Brain Functional Signals Forecasting",
        "link_suffix": "/forum?id=SDG0EBoqpp",
        "link": "https://openreview.net/forum?id=SDG0EBoqpp",
        "pdf_link": "https://openreview.net/pdf?id=SDG0EBoqpp",
        "keywords": "Foundation Model, Brain Functional Signal, Time Series Forecasting",
        "abstract": "Foundational models hold significant potential for advancing brain function research, particularly with recent technological advancements enabling the capture of spatiotemporal dynamics of brain signals. However, existing methods are primarily limited to characterizing observed brain signals and cannot infer continuous future signals\u2014an essential component for understanding the brain's causal structure and its role in various cognitive states. Current research leaves a substantial gap in forecasting whole-brain signal sequences. To address this, we propose a self-supervised model that embeds momentary whole-brain fMRI signals into vector representations and predicts continuous future signals. Our model is trained on a large-scale fMRI dataset, encompassing both resting-state and naturalistic stimuli conditions. Experimental results demonstrate that the model performs effectively in zero-shot forecasting of future whole-brain signals on unseen data and excels in downstream tasks such as task-based functional state decoding. To the best of our knowledge, this is the first approach to forecast and model whole-brain signals at such a large scale. The experimental results validate the feasibility of our method, offering new directions for theoretical research on brain signal time series and potential applications in diagnosing and treating brain disorders."
    },
    {
        "title": "Crafting Layered Designs from Pixels",
        "link_suffix": "/forum?id=D8fk2fY0lh",
        "link": "https://openreview.net/forum?id=D8fk2fY0lh",
        "pdf_link": "https://openreview.net/pdf?id=D8fk2fY0lh",
        "keywords": "Graphic Design, Layered Image Generation",
        "abstract": "Graphic designs play a vital role in communicating ideas, values, and messages.\nDuring the design process, designers typically organize their work into layers of\ntext, objects, and backgrounds to facilitate easier editing and customization. However, creating design in such a format requires significant effort and expertise. On\nthe other hand, with the advancement of GenAI technologies, high quality graphic\ndesigns created in pixel format have become more popular and accessible, while\nwith the inherent limitation of editability. Despite this limitation, we recognize\nthe significant reference value of these non-layered designs, as human designers\noften derive inspiration from these images to determine layouts or text styles. Motivated by this observation, we propose Accordion, a graphic design generation\nframework built around a vision language model playing distinct roles in three\nkey stages: (1) reference creation, (2) design planning, and (3) layer generation.\nBy using the reference image as global design guidance, distinct from existing\nmethods, our approach ensures that elements within the design are visually harmonious. Moreover, through this three-stage framework, Accordion can benefit\nfrom an unlimited supply of AI-generated references. The stage-wise design of\nour framework allows for flexible configuration and various applications, such as\nstarting from user provided references directly with the later two stages. Additionally, it leverages multiple vision experts such as SAM and element removal\nmodels to facilitate the creation of editable graphic layers. Experimental results\nshow that Accordion generates favourable results on the DesignIntention benchmark, including tasks such as text-to-template, adding text on background, and\ntext de-rendering. Furthermore, we fully explore the potential of Accordion to\nfacilitate the creation of design variations, validating its versatility and flexibility\nin the whole design workflow."
    },
    {
        "title": "Adaptive Second-Order Stochastic Optimization",
        "link_suffix": "/forum?id=gBT6rAEqvx",
        "link": "https://openreview.net/forum?id=gBT6rAEqvx",
        "pdf_link": "https://openreview.net/pdf?id=gBT6rAEqvx",
        "keywords": "MachinSecond-order optimization, stochastic Newton methods, stochastic quasi-Newton methods, adaptive graident descent, convergence analysis",
        "abstract": "As a much possible way of improving first-order stochastic optimization ($\\mathcal{FSO}$), the role of second-order information in stochastic optimization is receiving an increasing attention especially for solving the model with large-scale datasets in recent years, resulting in various second-order stochastic optimization ($\\mathcal{SSO}$) methods, e.g., the stochastic Newton (SN) method, the stochastic quasi-Newton (SQN) method, etc. However, the question of how to set an appropriate update rule of the learning rate for SSO methods is still an extremely intractable task, and surprisingly there is quite less literature to tackle this issue. To bridge the gap between the SSO methods and the learning rate, this work develops a class of adaptive SSO methods from the perspective of adaptive gradient methods. Concretely, a general adaptive gradient (GAG) method with the quasi-hyperbolic momentum (QHM) strategy that encompasses Adam, AdaGrad, RMSProp, etc., as the special case of GAG, is incorporated into SN and SQN, respectively, which leads to two methods: SN-GAG and SQN-GAG. In addition, we establish a unified analysis for different adaptive SSO methods, covering their convergence behavior and computational complexity for different backgrounds, such as the strongly convex (SC) case and the Polyak-{\\L}ojasiewicz (P{\\L}) case, where, particularly, the latter is missing in current studies. Finally, numerical tests on different applications of machine learning demonstrate the superiority and the robustness of the resulting methods."
    },
    {
        "title": "3DRealCar: An In-the-wild RGB-D Car Dataset with 360-degree Views",
        "link_suffix": "/forum?id=zo049dh2r9",
        "link": "https://openreview.net/forum?id=zo049dh2r9",
        "pdf_link": "https://openreview.net/pdf?id=zo049dh2r9",
        "keywords": "3D reconstruction, Car reconstruction, Car dataset",
        "abstract": "3D cars are commonly used in self-driving systems, virtual/augmented reality, and games. However, existing 3D car datasets are either synthetic or low-quality, presenting a significant gap toward the high-quality real-world 3D car datasets and limiting their applications in practical scenarios. In this paper, we propose the first large-scale 3D real car dataset, termed 3DRealCar, offering three distinctive features. (1) \\textbf{High-Volume}: 2,500 cars are meticulously scanned by 3D scanners, obtaining car images and point clouds with real-world dimensions; (2) \\textbf{High-Quality}: Each car is captured in an average of 200 dense, high-resolution 360-degree RGB-D views, enabling high-fidelity 3D reconstruction; (3) \\textbf{High-Diversity}: The dataset contains various cars from over 100 brands, collected under three distinct lighting conditions, including reflective, standard, and dark. Additionally, we offer detailed car parsing maps for each instance to promote research in car parsing tasks. Moreover, we remove background point clouds and standardize the car orientation to a unified axis for the reconstruction only on cars without background and controllable rendering. We benchmark 3D reconstruction results with state-of-the-art methods across each lighting condition in 3DRealCar. Extensive experiments demonstrate that the standard lighting condition part of 3DRealCar can be used to produce a large number of high-quality 3D cars, improving various 2D and 3D tasks related to cars. Notably, our dataset brings insight into the fact that recent 3D reconstruction methods face challenges in reconstructing high-quality 3D cars under reflective and dark lighting conditions. \n\\textcolor{red}{\\href{https://3drealcar.github.io/}{Ourdataset is available here.}}"
    },
    {
        "title": "GPU-Accelerated Counterfactual Regret Minimization",
        "link_suffix": "/forum?id=dWsBrgaNzU",
        "link": "https://openreview.net/forum?id=dWsBrgaNzU",
        "pdf_link": "https://openreview.net/pdf?id=dWsBrgaNzU",
        "keywords": "counterfactual regret minimization, extensive-form games, graphical processing units, imperfect information games, Nash equilibrium",
        "abstract": "Counterfactual regret minimization is a family of algorithms of no-regret learning dynamics capable of solving large-scale imperfect information games. We propose implementing this algorithm as a series of dense and sparse matrix and vector operations, thereby making it highly parallelizable for a graphical processing unit, at a cost of higher memory usage. Our experiments show that our implementation performs up to about 352.5 times faster than OpenSpiel's Python implementation and up to about 22.2 times faster than OpenSpiel's C++ implementation and the speedup becomes more pronounced as the size of the game being solved grows."
    },
    {
        "title": "MVGS: Multi-view-regulated Gaussian Splatting for Novel View Synthesis",
        "link_suffix": "/forum?id=X7XgNI0Eym",
        "link": "https://openreview.net/forum?id=X7XgNI0Eym",
        "pdf_link": "https://openreview.net/pdf?id=X7XgNI0Eym",
        "keywords": "3D Reconstruction; Gaussian Splatting; Neural Radiance Feild",
        "abstract": "Recent works in volume rendering, \\textit{e.g.} NeRF and 3D Gaussian Splatting (3DGS), significantly advance the rendering quality and efficiency with the help of the learned implicit neural radiance field or 3D Gaussians. \nRendering on top of an explicit representation, the vanilla 3DGS and its variants deliver real-time efficiency by optimizing the parametric model with single-view supervision per iteration during training which is adopted from NeRF. Consequently, certain views are overfitted, leading to unsatisfying appearance in novel-view synthesis and imprecise 3D geometries.\nTo solve aforementioned problems, we propose a new 3DGS optimization method embodying four key novel contributions:We transform the conventional single-view training paradigm into a multi-view training strategy. With our proposed multi-view regulation, 3D Gaussian attributes are further optimized without overfitting certain training views. As a general solution, we improve the overall accuracy in a variety of scenarios and different Gaussian variants.Inspired by the benefit introduced by additional views, we further propose a cross-intrinsic guidance scheme, leading to a coarse-to-fine training procedure concerning different resolutions.Built on top of our multi-view regulated training, we further propose a cross-ray densification strategy, densifying more Gaussian kernels in the ray-intersect regions from a selection of views.By further investigating the densification strategy, we found that the effect of densification should be enhanced when certain views are distinct dramatically.\nAs a solution, we propose a novel multi-view augmented densification strategy, where 3D Gaussians are encouraged to get densified to a sufficient number accordingly, resulting in improved reconstruction accuracy.\nWe conduct extensive experiments to demonstrate that our proposed method is capable of improving novel view synthesis of the Gaussian-based explicit representation methods about 1 dB PSNR for various tasks. \\href{https://mvgs666.github.io/}{\\textcolor{magenta}{Codesare available.}}"
    },
    {
        "title": "CTGC: Cluster-Aware Transformer for Graph Clustering",
        "link_suffix": "/forum?id=bew8KfCw7g",
        "link": "https://openreview.net/forum?id=bew8KfCw7g",
        "pdf_link": "https://openreview.net/pdf?id=bew8KfCw7g",
        "keywords": "Graph Clustering, Graph Transformer",
        "abstract": "Graph clustering is a fundamental unsupervised task in graph mining. However, mainstream clustering methods are built on graph neural networks, thus inevitably suffer from the difficulty in long-range dependencies capturing. Moreover, current two-stage clustering scheme, consisting of representation learning and clustering, limits the ability of the graph encoder to fully exploit task-related information, resulting in suboptimal embeddings. In this work, we propose CTGC ($\\textbf{C}$luster-Aware $\\textbf{T}$ransformer for $\\textbf{G}$raph $\\textbf{C}$lustering) to mitigate these issues. Specifically, considering the excellence of transformer in long-range dependencies modeling, we first introduce transformer to graph clustering as the crucial graph encoder. To further enhance the task awareness of encoder during representation learning, we presents two mechanisms: momentum cluster-aware attention and cluster-aware regularization. In momentum cluster-aware attention, previous clustering results are adopted to guide the node embedding production with specially designed cluster-aware queries. Cluster-aware regularization is designed to fuse the cluster information into bordering nodes through minimizing the overlap between different clusters while maximizing the completeness of each cluster. We evaluate our method on seven real-world graph datasets and achieve superior results compared to existing state-of-the-art methods, demonstrating its effectiveness in improving the quality of graph clustering."
    },
    {
        "title": "AniSDF: Fused-Granularity Neural Surfaces with Anisotropic Encoding for High-Fidelity 3D Reconstruction",
        "link_suffix": "/forum?id=v1f6c7wVBm",
        "link": "https://openreview.net/forum?id=v1f6c7wVBm",
        "pdf_link": "https://openreview.net/pdf?id=v1f6c7wVBm",
        "keywords": "Surface Reconstruction, Neural Radiance Field",
        "abstract": "Neural radiance fields have recently revolutionized novel-view synthesis and achieved high-fidelity renderings. \nHowever, these methods sacrifice the geometry for the rendering quality, limiting their further applications including relighting and deformation. \nHow to synthesize photo-realistic rendering while reconstructing accurate geometry remains an unsolved problem. In this work, we present AniSDF, a novel approach that learns fused-granularity neural surfaces with physics-based encoding for high-fidelity 3D reconstruction. Different from previous neural surfaces, our fused-granularity geometry structure balances the overall structures and fine geometric details, producing accurate geometry reconstruction. \nTo disambiguate geometry from reflective appearance, we introduce blended radiance fields to model diffuse and specularity following the anisotropic spherical Gaussian encoding, a physics-based rendering pipeline. With these designs, AniSDF can reconstruct objects with complex structures and produce high-quality renderings. \nFurthermore, our method is a unified model that does not require complex hyperparameter tuning for specific objects. \nExtensive experiments demonstrate that our method boosts the quality of SDF-based methods by a great scale in both geometry reconstruction and novel-view synthesis."
    },
    {
        "title": "Look Twice Before You Answer: Memory-Space Visual Retracing for Hallucination Mitigation in Multimodal Large Language Models",
        "link_suffix": "/forum?id=tkg9XMFo0H",
        "link": "https://openreview.net/forum?id=tkg9XMFo0H",
        "pdf_link": "https://openreview.net/pdf?id=tkg9XMFo0H",
        "keywords": "Hallucination mitigation, MLLMs, visual retracing, training-free",
        "abstract": "Despite their impressive capabilities, Multimodal Large Language Models (MLLMs) are susceptible to hallucinations, especially assertively fabricating content not present in the visual inputs. To address the aforementioned challenge, we follow a common cognitive process - \\textit{when one's initial memory of critical on-sight details fades, replenishing visual memory is essential to seek a factual and accurate answer.} Therefore, we introduce $\\textbf{Mem}$ory-space $\\textbf{V}$isual $\\textbf{R}$etracing ($\\textbf{MemVR}$), a novel hallucination mitigation paradigm that without the need for external knowledge retrieval or additional fine-tuning. In particular, we treat visual prompts as supplementary evidence to be reinjected into MLLMs via Feed Forward Network (FFN) as \u201ckey-value memory\u201d, when the model is uncertain or even amnesic about question-relevant visual memories. Comprehensive experimental evaluations demonstrate that \\modelname significantly mitigates hallucination issues across various MLLMs and excels in general benchmarks without incurring added time overhead, thus emphasizing its potential for widespread applicability."
    },
    {
        "title": "A Computational Framework for Modeling Emergence of Color Vision in the Human Brain",
        "link_suffix": "/forum?id=g3xuCtrG6H",
        "link": "https://openreview.net/forum?id=g3xuCtrG6H",
        "pdf_link": "https://openreview.net/pdf?id=g3xuCtrG6H",
        "keywords": "color vision, computational neuroscience, retina simulation, cortical learning, self-supervised learning, color blindness",
        "abstract": "It is a mystery how the brain decodes color vision purely from the optic nerve signals it receives, with a core inferential challenge being how it disentangles internal perception with the correct color dimensionality from the unknown encoding properties of the eye. In this paper, we introduce a computational framework for modeling this emergence of human color vision by simulating both the eye and the cortex. Existing research often overlooks how the cortex develops color vision or represents color space internally, assuming that the color dimensionality is known a priori; however, we argue that the visual cortex has the capability and the challenge of inferring the color dimensionality purely from fluctuations in the optic nerve signals. To validate our theory, we introduce a simulation engine for biological eyes based on established vision science and generate optic nerve signals resulting from looking at natural images. Further, we propose a model of cortical learning based on self-supervised principle and show that this model naturally learns to generate color vision by disentangling retinal invariants from the sensory signals. When the retina contains $N$ types of color photoreceptors, our simulation shows that $N$-dimensional color vision naturally emerges, verified through formal colorimetry. Using this framework, we also present the first simulation work that successfully boosts the color dimensionality, as observed in gene therapy on squirrel monkeys, and demonstrates the possibility of enhancing human color vision from 3D to 4D."
    }
]