[{"title": "PARSE-Ego4D: Personal Action Recommendation Suggestions for Ego-Centric Videos", "link_suffix": "/forum?id=Kh5OS3oNlg", "link": "https://openreview.net/forum?id=Kh5OS3oNlg", "pdf_link": "https://openreview.net/pdf?id=Kh5OS3oNlg", "keywords": "augmented reality, virtual reality, human annotation, recommender systems, perception, large language models", "abstract": "Intelligent assistance involves not only understanding but also action. Existing ego-centric video datasets contain rich annotations of the videos, but not of actions that an intelligent assistant could perform in the moment. To address this gap, we release PARSE-Ego4D, a new set of personal action recommendation annotations for the Ego4D dataset. We take a multi-stage approach to generating and evaluating these annotations. First, we used a prompt-engineered large language model (LLM) to generate context-aware action suggestions and identified over 18,000 action suggestions. While these synthetic action suggestions are valuable, the inherent limitations of LLMs necessitate human evaluation. To ensure high-quality and user-centered recommendations, we conducted a large-scale human annotation study that provides grounding in human preferences for all of PARSE-Ego4D. We analyze the inter-rater agreement and evaluate subjective preferences of participants. Based on our synthetic dataset and complete human annotations, we propose several new tasks for action suggestions based on ego-centric videos. We encourage novel solutions that improve latency and energy requirements. The annotations in PARSE-Ego4D will support researchers and developers who are working on building action recommendation systems for augmented and virtual reality systems.", "title_embedding_index": 350, "title_abs_embedding_index": 375}, {"title": "Black-Box Detection of Language Model Watermarks", "link_suffix": "/forum?id=E4LAVLXAHW", "link": "https://openreview.net/forum?id=E4LAVLXAHW", "pdf_link": "https://openreview.net/pdf?id=E4LAVLXAHW", "keywords": "llm, watermarking", "abstract": "Watermarking has emerged as a promising way to detect LLM-generated text, by augmenting LLM generations with later detectable signals. Recent work has proposed multiple families of watermarking schemes, several of which focus on preserving the LLM distribution. This distribution-preservation property is motivated by the fact that it is a tractable proxy for retaining LLM capabilities, as well as the inherently implied undetectability of the watermark by downstream users. Yet, despite much discourse around undetectability, no prior work has investigated the practical detectability of any of the current watermarking schemes in a realistic black-box setting. In this work we tackle this for the first time, developing rigorous statistical tests to detect the presence, and estimate parameters, of all three popular watermarking scheme families, using only a limited number of black-box queries. We experimentally confirm the effectiveness of our methods on a range of schemes and a diverse set of open-source models. Further, we validate the feasibility of our tests on real-world APIs. Our findings indicate that current watermarking schemes are more detectable than previously believed.", "title_embedding_index": 351, "title_abs_embedding_index": 376}, {"title": "FredNormer: Frequency Domain Normalization for Non-stationary Time Series Forecasting", "link_suffix": "/forum?id=5VK1UulEbE", "link": "https://openreview.net/forum?id=5VK1UulEbE", "pdf_link": "https://openreview.net/pdf?id=5VK1UulEbE", "keywords": "time series forecasting, deep learning", "abstract": "Recent normalization-based methods have shown great success in tackling the distribution shift issue, facilitating non-stationary time series forecasting.\nSince these methods operate in the time domain, they may fail to fully capture the dynamic patterns that are more apparent in the frequency domain, leading to suboptimal results.\nThis paper first theoretically analyzes how normalization methods affect frequency components.\nWe prove that the current normalization methods that operate in the time domain uniformly scale non-zero frequencies, and thus, they struggle to determine components that contribute to more robust forecasting.\nTherefore, we propose FredNormer, which observes datasets from a frequency perspective and adaptively up-weights the key frequency components.\nTo this end, FredNormer consists of two components: a statistical metric that normalizes the input samples based on their frequency stability and a learnable weighting layer that adjusts stability and introduces sample-specific variations. Notably, FredNormer is a plug-and-play module, which does not compromise the efficiency compared to existing normalization methods. \nExtensive experiments show that FredNormer improves the averaged MSE of backbone forecasting models by 33.3% and 55.3% on the ETTm2 dataset.\nCompared to the baseline normalization methods, FredNormer achieves 18 top-1 results and 6 top-2 results out of 28 settings.\nOur code is available at:https://anonymous.4open.science/r/ICLR2025-13956-8F84", "title_embedding_index": 352, "title_abs_embedding_index": 377}, {"title": "ProAdvPrompter: A Two-Stage Journey to Effective Adversarial Prompting for LLMs", "link_suffix": "/forum?id=tpHqsyZ3YX", "link": "https://openreview.net/forum?id=tpHqsyZ3YX", "pdf_link": "https://openreview.net/pdf?id=tpHqsyZ3YX", "keywords": "jailbreaking attacks; large language model", "abstract": "As large language models (LLMs) are increasingly being integrated into various real-world applications, the identification of their vulnerabilities to jailbreaking attacks becomes an essential component of ensuring the safety and reliability of LLMs. \nPrevious studies have developed LLM assistants, known as the adversarial prompter, to automatically generate suffixes that manipulate target LLMs into generating harmful and undesirable outputs.\nHowever, these approaches often suffer from low performance or generate semantically meaningless prompts, which can be easily identified by perplexity-based defenses.\nIn this paper, we introduce a novel two-stage method, $\\texttt{ProAdvPrompter}$, that significantly improves the performance of adversarial prompters.\nIn $\\texttt{ProAdvPrompter}$, the first stage (Exploration) utilizes the loss information to guide the adversarial prompter in generating suffixes that are more likely to elicit harmful responses.\nThen the second stage (Exploitation) iteratively fine-tunes the prompter using high-quality generated adversarial suffixes to further boost performance.\nAdditionally, we incorporate the prompt template to aid in the Exploration stage and propose a filtering mechanism to accelerate the training process in the Exploitation stage.\nWe evaluate $\\texttt{ProAdvPrompter}$ against the well-aligned LLMs (i.e., Llama2-Chat-7B and Llama3-chat-8B), achieving attack success rates of 99.68% and 97.12% respectively after 10 trials on the AdvBench dataset, thereby enhancing performance by $\\sim 2$ times compared to previous works.\nMoreover, $\\texttt{ProAdvPrompter}$ reduces training time by 20% on Llama3-Instruct-8B, generates more generalized adversarial suffixes, and demonstrates resilience against the perplexity defense.\nAn ablation study further evaluates the effects of key components in $\\texttt{ProAdvPrompter}$ (the prompt template and the filtering mechanism).", "title_embedding_index": 353, "title_abs_embedding_index": 378}, {"title": "Commute Your Domains: Trajectory Optimality Criterion for Multi-Domain Learning", "link_suffix": "/forum?id=LOAfGVdL2G", "link": "https://openreview.net/forum?id=LOAfGVdL2G", "pdf_link": "https://openreview.net/pdf?id=LOAfGVdL2G", "keywords": "Multi-domain learning, Lie bracket, Gradient dynamics, Domain interaction", "abstract": "In multi-domain learning, a single model is trained on diverse data domains to leverage shared knowledge and improve generalization. The order in which the data from these domains is used for training can significantly affect the model's performance on each domain. However, this dependence is under-studied. In this paper, we investigate the influence of training order (or data mixing) in multi-domain learning using the concept of Lie bracket of gradient vector fields. By analyzing the infinitesimal effects of changing the training order, we identify regions in the parameter space where altering the order between two training domains can benefit the target loss. We validate the predictions of our theoretical framework on the influence of training order (or data mixing) both on a toy example and  bilingual LLM pre-training.", "title_embedding_index": 354, "title_abs_embedding_index": 379}, {"title": "BALSA: Benchmarking Active Learning Strategies for Autonomous laboratories", "link_suffix": "/forum?id=PHkUNcno9n", "link": "https://openreview.net/forum?id=PHkUNcno9n", "pdf_link": "https://openreview.net/pdf?id=PHkUNcno9n", "keywords": "active learning, experimental design, AI for science", "abstract": "Accelerating the scientific discoveries holds significant potential to address some of the most pressing challenges facing society, from mitigating climate change to combating public health crises, such as the growing resistance to existing antibiotics. The vast and complex nature of design parameter spaces makes identifying promising candidates both time-consuming and resource-intensive, rendering conventional exhaustive searches impractical. However, recent advancements in data-driven methods, particularly within the framework of \"active learning,\" have led to more efficient strategies for scientific discovery. By iteratively identifying and labeling the most informative data points, these methods operate within a closed loop, guiding experiments or simulations to accelerate the identification of optimal candidates while reducing the demand of data labeling. Despite this progress, the lack of standardized benchmarks in this emerging field of autonomous scientific discovery impedes progress and limits its potential translational impact. To address this, we introduce BALSA: a comprehensive benchmark specifically designed for evaluating various search algorithms applied in autonomous laboratories within the active learning framework. BALSA offers a standardized evaluation protocol, provides a metric to characterize high-dimensional objective functions, and includes reference implementations of recent methodologies, with a focus on minimizing the data required to reach optimal results. It provides not only a suite of synthetic functions, but also real-world active learning tasks in biology and materials science \u2014 each presenting unique challenges for autonomous laboratory tasks.", "title_embedding_index": 355, "title_abs_embedding_index": 380}, {"title": "Robust Heterogeneous Treatment Effect Estimation under Covariate Perturbation", "link_suffix": "/forum?id=glgvpS1dD1", "link": "https://openreview.net/forum?id=glgvpS1dD1", "pdf_link": "https://openreview.net/pdf?id=glgvpS1dD1", "keywords": "causal inference, treatment effect estimation, robust estimation", "abstract": "Heterogeneous treatment effect estimation has important applications in fields such as healthcare, economics, and education, attracting increasing attention from both research and the industrial community. However, most existing causal machine learning methods may not perform well in practice due to the lack of robustness of the treatment effect estimation predicted by deep neural networks when an imperceptible perturbation has been added to the covariate. In this paper, we alleviate this problem using the idea of adversarial machine learning. We first show that our loss of interest, the adversarial loss, is partly bounded by the Lipschitz constant of the casual model. Next, we propose a representation learning framework called RHTE which estimates heterogeneous treatment effect under covariate perturbation by controlling the empirical loss, Lipschitz constant, and distance metric simulta neously. Theories are then derived to guarantee the performance and robustness of our estimation. To the best of our knowledge, this is the first work proposing robust representation learning methods under variable perturbation. Extensive experiments on both synthetic examples and standard benchmarks demonstrate the effectiveness and generality of our framework.", "title_embedding_index": 356, "title_abs_embedding_index": 381}, {"title": "GFLAgent: Green Federated Learning Agent for Alleviating Heterogeneity", "link_suffix": "/forum?id=ArJikvI6xo", "link": "https://openreview.net/forum?id=ArJikvI6xo", "pdf_link": "https://openreview.net/pdf?id=ArJikvI6xo", "keywords": "green computing, federated learning, statistical heterogeneity, LLM agent", "abstract": "Federated Learning (FL), as a privacy-preserving distributed machine learning paradigm, faces significant challenges in terms of data and device heterogeneity in practical applications. In this paper, we present a novel Large Language Model Agent decision system, called Green Federated Learning Agent (GFLAgent), for alleviating the challenges arising from data and device heterogeneity within the FL tasks. GFLAgent is efficient and energy friendly, and meets the requirements of green computing. GFLAgent dynamically monitors the status of each client, selects and reasonably allocates them to different layers to achieve efficient asynchronous training, and responds to unexpected situations during training. Furthermore, to optimize overall system expenditure, we implement a strategy that minimizes local training overhead and the updates  costs for clients with historically subpar performance. The experimental results show that GFLAgent outperforms SOTA methods and can be quickly ported to other distributed machine learning frameworks to improve efficiency.", "title_embedding_index": 357, "title_abs_embedding_index": 382}, {"title": "Ward: Provable RAG Dataset Inference via LLM Watermarks", "link_suffix": "/forum?id=kVrwHLAb20", "link": "https://openreview.net/forum?id=kVrwHLAb20", "pdf_link": "https://openreview.net/pdf?id=kVrwHLAb20", "keywords": "llm, watermarks, dataset inference, rag", "abstract": "Retrieval-Augmented Generation (RAG) improves LLMs by enabling them to incorporate external data during generation. This raises concerns for data owners regarding unauthorized use of their content in RAG systems. Despite its importance, the challenge of detecting such unauthorized usage remains underexplored, with existing datasets and methodologies from adjacent fields being ill-suited for its study. In this work, we take several steps to bridge this gap. First, we formalize this problem as (black-box) RAG Dataset Inference (RAG-DI). To facilitate research on this challenge, we further introduce a novel dataset specifically designed for benchmarking RAG-DI methods under realistic conditions, and propose a set of baseline approaches. Building on this foundation, we introduce Ward, a RAG-DI method based on LLM watermarks that enables data owners to obtain rigorous statistical guarantees regarding the usage of their dataset in a RAG system. In our experimental evaluation, we show that Ward consistently outperforms all baselines across many challenging settings, achieving higher accuracy, superior query efficiency and robustness. Our work provides a foundation for future studies of RAG-DI and highlights LLM watermarks as a promising approach to this problem.", "title_embedding_index": 358, "title_abs_embedding_index": 383}, {"title": "From Imitation to Introspection: Probing Self-Consciousness in Language Models", "link_suffix": "/forum?id=NH47cNdgNz", "link": "https://openreview.net/forum?id=NH47cNdgNz", "pdf_link": "https://openreview.net/pdf?id=NH47cNdgNz", "keywords": "self-consciousness, evaluation, probing, large language model, causality", "abstract": "Self-consciousness, the introspection of one's existence and thoughts, represents a high-level cognitive process. As language models advance at an unprecedented pace, a critical question arises: Are these models becoming self-conscious?\nDrawing upon insights from psychological and neural science, this work presents a practical definition of self-consciousness for language models and refines ten core concepts. Our work pioneers an investigation into self-consciousness in language models by, for the first time, leveraging causal structural games to establish the functional definitions of the ten core concepts. \nBased on our definitions, we conduct a comprehensive four-stage experiment: quantification (evaluation of ten leading models), representation (visualization of self-consciousness within the models), manipulation (modification of the models' representation), and acquisition (fine-tuning the models on core concepts). \nOur findings indicate that although models are in the early stages of developing self-consciousness, there is a discernible representation of certain concepts within their internal mechanisms. However, these representations of self-consciousness are hard to manipulate positively at the current stage, yet they can be acquired through targeted fine-tuning.", "title_embedding_index": 359, "title_abs_embedding_index": 384}, {"title": "Multimodal LLM-guided Query Optimization for Visual-Language Retrieval", "link_suffix": "/forum?id=sjGmiI49sd", "link": "https://openreview.net/forum?id=sjGmiI49sd", "pdf_link": "https://openreview.net/pdf?id=sjGmiI49sd", "keywords": "vision-language retrieval, cross-modal retrieval, prompt engineering, query rewriting, large language model", "abstract": "Vision-language retrieval (VLR), involving the use of text (or images) as queries to retrieve corresponding images (or text), has been widely used in multimedia and computer vision tasks. However, ambiguous or complex concepts contained in queries often confuse retrievers, making it difficult to effectively align these concepts with visual content, thereby limiting their performance. Existing query optimization methods neglect the feedback of retrievers' preferences, thus resulting in sub-optimal performance. Inspired by the powerful ability of Multimodal Large Language Models (MLLMs), we propose a Multimodal LLM-Guided Query Rewriter (MGQRe) for query optimization. Specifically, MGQRe first utilizes MLLM to explore the retriever's weakness and perform targeted iterative optimizations to capture the retriever's expressive preferences. Subsequently, we develop a trainable rewriter that learns this preference knowledge through a three-step tuning strategy: supervised fine-tuning, preference learning, and reinforcement learning. This ensures that the queries generated by the rewriter align with the retriever\u2019s preferences, thereby enhancing the retriever's performance. Extensive VLR benchmark experiments have demonstrated the superiority of MGQRe, as well as its generalizability and transferability. This work showcases the potential of using advanced language models to overcome the inherent limitations in current VLR technology.", "title_embedding_index": 360, "title_abs_embedding_index": 385}, {"title": "Diffusion on language model encodings for protein sequence generation", "link_suffix": "/forum?id=LoXJlAW3gU", "link": "https://openreview.net/forum?id=LoXJlAW3gU", "pdf_link": "https://openreview.net/pdf?id=LoXJlAW3gU", "keywords": "diffusion, protein language models, protein generation", "abstract": "Protein design necessitates a profound understanding of the intricate nature of the protein universe. While many efforts focus on conditional generation or specific protein families, the foundational task of unconditional generation remains underexplored and underappreciated.  Existing models still struggle to achieve both high quality and diversity in generated protein sequences. To address this gap, this research introduces DiMA, a novel model that leverages latent diffusion on representations derived from the protein language model, ESM-2, to generate amino acid sequences. We quantitatively investigate the impact of components of the latent diffusion model, revealing their contributions to superior protein generation performance. Extensive evaluations using multiple metrics across two protein modalities showcase DiMA's superior quality, diversity, and distribution matching capabilities compared to leading autoregressive transformer-based and discrete diffusion models, while utilizing ten times fewer parameters. Our approach consistently produces novel, diverse protein sequences that accurately reflect the inherent structural and functional diversity of the protein space. Furthermore, we demonstrate the conditional generation capabilities of our method. Our work advances the field of protein design by providing a robust framework for scalable and high-quality protein sequence generation.", "title_embedding_index": 361, "title_abs_embedding_index": 386}, {"title": "ToG-I: Progressively Instructed Knowledge Graph-based Large Language Model Reasoning", "link_suffix": "/forum?id=oW3XIIHaOn", "link": "https://openreview.net/forum?id=oW3XIIHaOn", "pdf_link": "https://openreview.net/pdf?id=oW3XIIHaOn", "keywords": "LLM Reasoning\uff1bKnowledge Graph; Instructed\uff1bProgressively", "abstract": "Large language models (LLMs) reasoning based on knowledge graphs (KGs), by integrating structured knowledge from the KGs, provide a significant solution to alleviate the hallucination problem in complex reasoning tasks. Current techniques mainly focus on the retrieval of explicit knowledge from KGs. LLMs directly use the specific facts and relationships retrieved to construct a reasoning chain to answer the questions. However, these methods often overlook the significance of comprehending implicit knowledge when dealing with problems involving logical reasoning or ambiguous intentions. This could potentially lead to deviations in the reasoning path, hindering their applicability in real-world applications. In this paper, we propose a progressive instructed reasoning framework, ToG-I. The framework identifies core elements, discerns latent intentions, and integrates necessary commonsense reasoning by analyzing the problem from multiple perspectives and levels. Based on this, ToG-I transforms these analysis results into specific reasoning instructions, guiding the LLMs to carry out a progressive reasoning process from a global perspective. This not only ensures the accuracy of the reasoning process but also effectively avoids unnecessary consumption of reasoning resources. Extensive experiments on multiple public datasets show that ToG-I achieves state-of-the-art performance in KG reasoning tasks based on information retrieval and demonstrates superiority in knowledge-intensive tasks.", "title_embedding_index": 362, "title_abs_embedding_index": 387}, {"title": "Black-Box Adversarial Attacks on LLM-Based Code Completion", "link_suffix": "/forum?id=h2Q3gOIz8q", "link": "https://openreview.net/forum?id=h2Q3gOIz8q", "pdf_link": "https://openreview.net/pdf?id=h2Q3gOIz8q", "keywords": "code completion, security, code security, adversarial attacks, black-box, large language models", "abstract": "Modern code completion engines, powered by large language models (LLMs), assist millions of developers with their impressive capabilities to generate functionally correct code. As such it is crucial to investigate their security implications. In this work, we present INSEC, the first black-box adversarial attack designed to manipulate modern LLM-based code completion engines into generating vulnerable code. INSEC works by injecting an attack string as a short comment in the completion input. The attack string is crafted through a query-based optimization procedure starting from a set of initialization schemes. We demonstrate INSEC's broad applicability and effectiveness by evaluating it on various state-of-the-art open-source models and black-box commercial services (e.g., OpenAI API and GitHub Copilot). We show that on a diverse set of security-critical test cases covering 16 CWEs across 5 programming languages, INSEC significantly increases the rate of generated insecure code by ~50%, while upholding the engines' capabilities of producing functionally correct code. Moreover, due to its black-box nature, developing INSEC does not require expensive local compute and costs less than 10 USD by querying remote APIs, thereby enabling the threat of widespread attacks.", "title_embedding_index": 363, "title_abs_embedding_index": 388}, {"title": "Rethinking MUSHRA: Addressing Modern Challenges in Text-to-Speech Evaluation", "link_suffix": "/forum?id=n6YVISFrcN", "link": "https://openreview.net/forum?id=n6YVISFrcN", "pdf_link": "https://openreview.net/pdf?id=n6YVISFrcN", "keywords": "evaluation methodologies, evaluation, speech technologies, datasets for low resource languages", "abstract": "Despite rapid advancements in TTS models, a consistent and robust human evaluation framework is still lacking. For example, MOS tests fail to differentiate between similar models, and CMOS's pairwise comparisons are time-intensive. The MUSHRA test is a promising alternative for evaluating multiple TTS systems simultaneously, but in this work we show that its reliance on matching human reference speech unduly penalises the scores of modern TTS systems that can exceed human speech quality. More specifically, we conduct a comprehensive assessment of the MUSHRA test, focusing on its sensitivity to factors such as rater variability, listener fatigue, and reference bias. Based on our extensive evaluation involving 471 human listeners across Hindi and Tamil we identify two primary shortcomings: (i) \\textit{reference-matching bias}, where raters are unduly influenced by the human reference, and (ii) \\textit{judgement ambiguity}, arising from a lack of clear fine-grained guidelines. To address these issues, we propose two refined variants of the MUSHRA test. The first variant enables fairer ratings for synthesized samples that surpass human reference quality. The second variant reduces ambiguity, as indicated by the relatively lower variance across raters. By combining these approaches, we achieve both more reliable and more fine-grained assessments. We also release MANGO, a massive dataset of 47,100 human ratings, the first-of-its-kind collection for Indian languages, aiding in analyzing human preferences and developing automatic metrics for evaluating TTS systems.", "title_embedding_index": 364, "title_abs_embedding_index": 389}, {"title": "Scaling Laws for Task-Optimized Models of the Primate Visual Ventral Stream", "link_suffix": "/forum?id=4fyg68nmd7", "link": "https://openreview.net/forum?id=4fyg68nmd7", "pdf_link": "https://openreview.net/pdf?id=4fyg68nmd7", "keywords": "scaling laws, neural alignment, behavioral alignment, computer vision, primate visual ventral stream", "abstract": "When trained on large-scale object classification datasets, certain artificial neural network models begin to approximate core object recognition (COR) behaviors and neural response patterns in the primate visual ventral stream (VVS). While recent machine learning advances suggest that scaling model size, dataset size, and compute resources improve task performance, the impact of scaling on brain alignment remains unclear. In this study, we explore scaling laws for modeling the primate VVS by systematically evaluating over 600 models trained under controlled conditions on benchmarks spanning V1, V2, V4, IT and COR behaviors. \nWe observe that while behavioral alignment continues to scale with larger models, neural alignment saturates. \nThis observation remains true across model architectures and training datasets, even though models with stronger inductive bias and datasets with higher-quality images are more compute-efficient. %demonstrate better sample efficiency at lower scales, especially for neural alignment. \nIncreased scaling is especially beneficial for higher-level visual areas, such that small models trained on few samples exhibit only poor alignment.\nFinally, we develop a scaling recipe, suggesting that a greater proportion of compute should be allocated to data samples over model size. \nOur results suggest that while scaling alone might suffice for alignment with human core object recognition behavior, it will not yield improved models of the brain's visual ventral stream with current architectures and datasets, warranting a rethinking in the way we build brain-like models.", "title_embedding_index": 365, "title_abs_embedding_index": 390}, {"title": "Cayley Maze: Universal Open-Ended Reinforcement Learning Environment", "link_suffix": "/forum?id=fvyuKLTC3s", "link": "https://openreview.net/forum?id=fvyuKLTC3s", "pdf_link": "https://openreview.net/pdf?id=fvyuKLTC3s", "keywords": "Reinforcement Learning, Unsupervised Environment Design, Open-Ended Learning", "abstract": "Parametrizable environments with variable complexity are crucial for advancing fields such as Unsupervised Environment Design (UED), Open-Ended Learning, Curriculum Learning, and Meta Reinforcement Learning. However, the selection of environments in evaluation procedures, along with their complexities, is often either neglected or lacks formal justification. We propose the formal definition of complexity for Markov Decision Processes using Finite Automata and Group Theory machinery. We introduce Cayley Maze, a novel open-ended reinforcement learning environment that naturally generalizes problems like solving the Rubik's Cube, sorting, and integer factorization. Cayley Maze is universal: every deterministic sparse MDP is an MDP of a certain instance of Cayley Maze. We demonstrate how Cayley Maze enables control over complexity, simplification and combination of its instances. Finally, we evaluate UED algorithms on various instances of the Cayley Maze and analyze their capacity to produce agents with robust generalization capabilities.", "title_embedding_index": 366, "title_abs_embedding_index": 391}, {"title": "SCOPE: A Self-supervised framework for Improving Faithfulness in Conditional Text Generation", "link_suffix": "/forum?id=dTkqaCKLPp", "link": "https://openreview.net/forum?id=dTkqaCKLPp", "pdf_link": "https://openreview.net/pdf?id=dTkqaCKLPp", "keywords": "faithfulness, hallucination, conditional text generation, natural language processing, large language models", "abstract": "Large Language Models (LLMs), when used for conditional text generation, often produce hallucinations, i.e., information that is unfaithful or not grounded in the input context. This issue arises in typical conditional text generation tasks, such as text summarization and data-to-text generation, where the goal is to produce fluent text based on contextual input. When fine-tuned on specific domains, LLMs struggle to provide faithful answers to a given context, often adding information or generating errors. One underlying cause of this issue is that LLMs rely on statistical patterns learned from their training data. This reliance can interfere with the model\u2019s ability to stay faithful to a provided context, leading to the generation of ungrounded information. We build upon this observation and introduce a novel self-supervised method for generating a training set of unfaithful samples. We then refine the model using a training process that encourages the generation of grounded outputs over unfaithful ones, drawing on preference-based training. Our approach leads to significantly more grounded text generation, outperforming existing self-supervised techniques in faithfulness, as evaluated through automatic metrics, LLM-based assessments, and human evaluations.", "title_embedding_index": 367, "title_abs_embedding_index": 392}, {"title": "Principle Counterfactual Fairness", "link_suffix": "/forum?id=TLgDQ0Rr2Z", "link": "https://openreview.net/forum?id=TLgDQ0Rr2Z", "pdf_link": "https://openreview.net/pdf?id=TLgDQ0Rr2Z", "keywords": "Counterfactual Fairness", "abstract": "Fairness in human and algorithmic decision-making is crucial in areas such as criminal justice, education, and social welfare. Recently, counterfactual fairness has drawn increasing research interest, suggesting that decision-making for individuals should remain the same when intervening with different values on the protected attributes. Nevertheless, the question of \"which attributes and individuals should be protected\" is rarely discussed in the existing counterfactual fairness literature. For example, when considering leg disability as a protected attribute, the algorithms should not treat individuals with leg disabilities differently in college admissions, but one may naturally take into this factor for the purpose of selecting runner athletes. In other words, when and how to enforce fairness is expected to depend on the causal relation between the protected attribute and the outcome of interest. Formally, this paper proposes principal counterfactual fairness using the concept of principal stratification from the causal inference literature, focusing on whether an algorithm is counterfactually fair for individuals whose protected attribute has no individual causal effect on the outcome of interest. To examine whether an algorithm satisfies principal counterfactual fairness, we derive the statistical bounds, and propose a post-processing approach to achieving principal counterfactual fairness with minimal individual decision changes. Experiments are conducted using synthetic and real-world datasets to verify the effectiveness of our methods.", "title_embedding_index": 368, "title_abs_embedding_index": 393}, {"title": "Clique Number Estimation via Differentiable Functions of Adjacency Matrix Permutations", "link_suffix": "/forum?id=DFSb67ksVr", "link": "https://openreview.net/forum?id=DFSb67ksVr", "pdf_link": "https://openreview.net/pdf?id=DFSb67ksVr", "keywords": "Graph neural network, distant supervision", "abstract": "Estimating the clique number in a graph is central to various applications, e.g., community detection, graph retrieval, etc. \nExisting estimators often rely on non-differentiable combinatorial components. Here, we propose a full differentiable estimator for clique number estimation, which can be trained from distant supervision of clique numbers, rather than demonstrating actual cliques.\nOur key insight is a formulation of the maximum clique problem (MCP) as a maximization of the size of fully dense square submatrix, within a suitably row-column-permuted adjacency matrix.\nWe design a differentiable mechanism to search for permutations that lead to the discovery of such dense blocks.\nHowever, the optimal permutation is not unique, which leads to the learning of spurious permutations. To tackle this problem, we view the MCP problem as a sequence of subgraph matching tasks, each detecting progressively larger cliques in a nested manner. This allows effective navigation through suitable node permutations.\nThese steps result in MxNet, an end-to-end differentiable model, which learns to predict clique number without explicit clique demonstrations, with the added benefit of interpretability.  Experiments on eight datasets show the superior accuracy of our approach.", "title_embedding_index": 369, "title_abs_embedding_index": 394}, {"title": "Auto-RAG: Autonomous Retrieval-Augmented Generation for Large Language Models", "link_suffix": "/forum?id=jkVQ31GeIA", "link": "https://openreview.net/forum?id=jkVQ31GeIA", "pdf_link": "https://openreview.net/pdf?id=jkVQ31GeIA", "keywords": "Retrieval-Augmented Generation, Large Language Models, Autonomous Decision-Making", "abstract": "Iterative retrieval refers to the process in which the model continuously queries the retriever during generation to enhance the relevance of the retrieved knowledge, thereby improving the performance of Retrieval-Augmented Generation (RAG).\nExisting work typically employs few-shot prompting or manually constructed rules to implement iterative retrieval.\nThis introduces additional inference overhead and overlooks the remarkable reasoning capabilities of Large Language Models (LLMs).\nIn this paper, we introduce Auto-RAG, an autonomous iterative retrieval model centered on the LLM's powerful decision-making capabilities. Auto-RAG engages in multi-turn dialogues with the retriever, systematically planning retrievals and refining queries to acquire valuable knowledge. This process continues until sufficient external information is gathered, at which point the results are presented to the user. To this end, we develop a method for autonomously synthesizing reasoning-based decision-making instructions in iterative retrieval and fine-tuned the latest open-source LLMs.The experimental results indicate that Auto-RAG is capable of autonomous iterative interaction with the retriever, effectively leveraging the remarkable reasoning and decision-making abilities of LLMs, which lead to outstanding performance across six benchmarks. Further analysis reveals that Auto-RAG can autonomously adjust the number of iterations based on the difficulty of the questions and the utility of the retrieved knowledge, without requiring any human intervention. \nMoreover, Auto-RAG expresses the iterative retrieval process in natural language, enhancing interpretability while providing users with a more intuitive experience.", "title_embedding_index": 370, "title_abs_embedding_index": 395}, {"title": "Assessing Vulnerabilities of Large Language Models to Social Bias Attacks", "link_suffix": "/forum?id=5V8d2dVF1F", "link": "https://openreview.net/forum?id=5V8d2dVF1F", "pdf_link": "https://openreview.net/pdf?id=5V8d2dVF1F", "keywords": "Language model, Bias, Attack", "abstract": "Large Language Models (LLMs) have become foundational in human-computer interaction, demonstrating remarkable linguistic capabilities across various tasks. However, there is a growing concern about their potential to perpetuate social biases present in their training data. In this paper, we comprehensively investigate the vulnerabilities of contemporary LLMs to various social bias attacks, including prefix injection, refusal suppression, and learned attack prompts. We evaluate popular models such as LLaMA2, GPT-3.5, and GPT-4 across gender, racial, and religious bias types. Our findings reveal that models are generally more susceptible to gender bias attacks compared to racial or religious biases. We also explore novel aspects such as cross-bias and multiple-bias attacks, finding varying degrees of transferability across bias types. Additionally, our results show that larger models and pretrained base models often exhibit higher susceptibility to bias attacks. These insights contribute to the development of more inclusive and ethically responsible LLMs, emphasizing the importance of understanding and mitigating potential bias vulnerabilities. We offer recommendations for model developers and users to enhance the robustness of LLMs against social bias attacks.", "title_embedding_index": 371, "title_abs_embedding_index": 396}, {"title": "Bridging PCA and Neural Networks: New Insights into Class Bias", "link_suffix": "/forum?id=qcyn7ESaM8", "link": "https://openreview.net/forum?id=qcyn7ESaM8", "pdf_link": "https://openreview.net/pdf?id=qcyn7ESaM8", "keywords": "hardness, class bias, class-level hardness identifiers, instance-level hardness identifiers", "abstract": "Understanding class-level hardness is essential for addressing class bias in machine learning. Traditionally, class bias has been explored with two primary approaches: analyzing raw input data to improve preprocessing strategies or examining neural network latent representations to refine model training. In this work, we find that PCA-transformed spaces\u2014despite being produced through linear transformations\u2014still contain substantial information about class-level hardness. This suggests that, despite their distinct goals and methodologies, both PCA and neural networks may encode similar features related to class bias, offering new insights into the nature of class bias and how data representations are formed in both PCA and neural networks.Analyzing class bias commonly involves Pearson Correlation, which assumes stable inputs. However, we find that class bias is a highly unstable phenomenon with respect to variables such as training time and model initialization, with class-level variability often exceeding the differences between classes. Together with increased variability in class accuracies over dataset-level ones, this suggests that current methods for addressing dataset-level variability may be inadequate for handling class bias.", "title_embedding_index": 372, "title_abs_embedding_index": 397}, {"title": "Train once and generalize: Zero-shot quantum state preparation with RL", "link_suffix": "/forum?id=zHeHIIFQVF", "link": "https://openreview.net/forum?id=zHeHIIFQVF", "pdf_link": "https://openreview.net/pdf?id=zHeHIIFQVF", "keywords": "Quantum State Preparation, Deep Reinforcement Learning, Zero-shot Inference, Off-the-shelf Algorithms, Generalization", "abstract": "Quantum state preparation is an essential cornerstone of quantum information science and quantum algorithms. Notwithstanding worst-case hardness results, designing efficient and scalable methods for approximate state preparation on near-term quantum devices remains a significant challenge. In this work, we present a deep reinforcement learning approach to quantum state preparation that allows for the zero-shot preparation of any state at a fixed system size. We scale significantly beyond previous works by designing a novel reward function with provable guarantees. In our experiments on stabilizer states up to nine qubits, we achieve generalization to unseen states by training on less than $10^{-3}$% of the state space. We prepare target states with varying degrees of entanglement content and obtain insight into the quantum dynamics generated by our trained agent. Benchmarking shows our model produces stabilizer circuits up to $60$% shorter than existing algorithms, setting a new state of the art. To our knowledge, this is the first work to prepare arbitrary stabilizer states on more than two qubits without re-training.", "title_embedding_index": 373, "title_abs_embedding_index": 398}, {"title": "Nonmyopic Bayesian Optimization in Dynamic Cost Settings", "link_suffix": "/forum?id=IiAckbuccF", "link": "https://openreview.net/forum?id=IiAckbuccF", "pdf_link": "https://openreview.net/pdf?id=IiAckbuccF", "keywords": "nonmyopic Bayesian optimization, dynamic cost settings, language model policy optimization", "abstract": "Bayesian optimization (BO) is a popular framework for optimizing black-box functions, leveraging probabilistic models such as Gaussian processes. However, conventional BO assumes static query costs, which limits its applicability to real-world problems with dynamic cost structures, such as geological surveys or biological sequence design, where query costs vary based on previous actions. To address this, we propose a cost-constrained nonmyopic BO algorithm that incorporates dynamic cost models. Our method employs a neural network policy for variational optimization over multi-step lookahead horizons to plan ahead in dynamic cost environments. Empirically, we benchmark our method on synthetic functions exhibiting a variety of dynamic cost structures. Furthermore, we apply our method to a real-world application in protein sequence design using a large language model-based policy, demonstrating its scalability and effectiveness in handling multi-step planning in a large and complex query space. Our nonmyopic BO algorithm consistently outperforms its myopic counterparts in both synthetic and real-world settings, achieving significant improvements in both efficiency and solution quality.", "title_embedding_index": 374, "title_abs_embedding_index": 399}]