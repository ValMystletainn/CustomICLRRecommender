[{"title": "Safeguard User Privacy in LLM Cloud Services", "link_suffix": "/forum?id=INXZOxYsLd", "link": "https://openreview.net/forum?id=INXZOxYsLd", "pdf_link": "https://openreview.net/pdf?id=INXZOxYsLd", "keywords": "large language model, privacy leakage, defense, cloud service", "abstract": "Large language models (LLMs) have witnessed substantial growth in recent years. To leverage convenient LLM cloud services, users are inevitable to upload their prompts. Further, for tasks such as translation, reading comprehension, and summarization, related files or contexts are inherently required to be uploaded, whether they contain user privacy or not. Despite the rapid advancement of LLM capability, there has been a scarcity of research focusing on preserving user privacy during inference. To this end, this paper conducts a comprehensive study in this domain. Firstly, we demonstrate that (1) the embedding space of tokens is remarkably sparse, and (2) LLMs primarily function in the orthogonal subspace of embedding space, these two factors making privacy extremely vulnerable. Then, we analyze the structural characteristics of LLMs and design a distributed privacy-preserving inference paradigm which can effectively resist privacy attacks. Finally, we conduct a comprehensive evaluation of the defended models on mainstream tasks and find that low-bit quantization techniques can be well combined with our inference paradigm, achieving a balance between privacy, utility, and runtime memory efficiency.", "title_embedding_index": 22700, "title_abs_embedding_index": 22725}, {"title": "DenseGrounding: Improving Dense Language-Vision Semantics for Ego-centric 3D Visual Grounding", "link_suffix": "/forum?id=iGafR0hSln", "link": "https://openreview.net/forum?id=iGafR0hSln", "pdf_link": "https://openreview.net/pdf?id=iGafR0hSln", "keywords": "3D Visual Grounding", "abstract": "Enabling intelligent agents to comprehend and interact with 3D environments through natural language is crucial for advancing robotics and human-computer interaction. A fundamental task in this field is ego-centric 3D visual grounding, where agents locate target objects in real-world 3D spaces based on verbal descriptions. However, this task faces two significant challenges: (1) loss of fine-grained visual semantics due to sparse fusion of point clouds with ego-centric multi-view images, (2) limited textual semantic context due to arbitrary language descriptions. We propose DenseGrounding, a novel approach designed to address these issues by enhancing both visual and textual semantics. For visual features, we introduce the Hierarchical Scene Semantic Enhancer, which retains dense semantics by capturing fine-grained global scene features and facilitating cross-modal alignment. For text descriptions, we propose a Language Semantic Enhancer that leverage large language models to provide rich context and diverse language descriptions with additional context during model training. Extensive experiments show that DenseGrounding significantly outperforms existing methods in overall accuracy, achieving improvements of5.81%and7.56%when trained on the comprehensive full training dataset and smaller mini subset, respectively, further advancing the SOTA in ego-centric 3D visual grounding. Our method also achieves1st placeand receivesInnovation Awardin the 2024 Autonomous Grand Challenge Multi-view 3D Visual Grounding Track, validating its effectiveness and robustness.", "title_embedding_index": 22701, "title_abs_embedding_index": 22726}, {"title": "Bayesian Analysis of Combinatorial Gaussian Process Bandits", "link_suffix": "/forum?id=50cmx4SrkM", "link": "https://openreview.net/forum?id=50cmx4SrkM", "pdf_link": "https://openreview.net/pdf?id=50cmx4SrkM", "keywords": "Multi-armed bandits, Combinatorial bandits, Contextual bandits, Gaussian processes, Energy-efficient navigation", "abstract": "We consider the combinatorial volatile Gaussian process (GP) semi-bandit problem. Each round, an agent is provided a set of available base arms and must select a subset of them to maximize the long-term cumulative reward. We study the Bayesian setting and provide novel Bayesian cumulative regret bounds for three GP-based algorithms: GP-UCB, GP-BayesUCB and GP-TS. Our bounds extend previous results for GP-UCB and GP-TS to the \\emph{infinite}, \\emph{volatile} and \\emph{combinatorial} setting, and to the best of our knowledge, we provide the first regret bound for GP-BayesUCB. Volatile arms encompass other widely considered bandit problems such as contextual bandits.\nFurthermore, we employ our framework to address the challenging real-world problem of online energy-efficient navigation, where we demonstrate its effectiveness compared to the alternatives.", "title_embedding_index": 22702, "title_abs_embedding_index": 22727}, {"title": "ParCon: Noise-Robust Collaborative Perception via Multi-module Parallel Connection", "link_suffix": "/forum?id=fPQZd3p1De", "link": "https://openreview.net/forum?id=fPQZd3p1De", "pdf_link": "https://openreview.net/pdf?id=fPQZd3p1De", "keywords": "Collaborative perception, Vehicle-Infrastructure cooperative driving, 3D Object Detection", "abstract": "In this paper, we investigate improving the perception performance of autonomous vehicles through communication with other vehicles and road infrastructures. To this end, we introduce a novel collaborative perception architecture, called $\\textbf{ParCon}$, which connects multiple modules in parallel, as opposed to the sequential connections used in most other collaborative perception methods. Through extensive experiments, we demonstrate that ParCon inherits the advantages of parallel connection. Specifically, ParCon is robust to noise, as the parallel architecture allows each module to manage noise independently and compensate for the limitations of other modules. \nAs a result, ParCon achieves state-of-the-art accuracy, particularly in noisy environments, such as real-world datasets, increasing detection accuracy by 6.91%. Additionally, ParCon is computationally efficient, reducing floating-point operations (FLOPs) by 11.46%.", "title_embedding_index": 22703, "title_abs_embedding_index": 22728}, {"title": "SPEAR: Receiver-to-Receiver Acoustic Neural Warping Field", "link_suffix": "/forum?id=zq1zTgSBro", "link": "https://openreview.net/forum?id=zq1zTgSBro", "pdf_link": "https://openreview.net/pdf?id=zq1zTgSBro", "keywords": "Spatial Acoustic Effects, Receiver-to-Receiver, Neural Warping Field", "abstract": "We present SPEAR, a continuous receiver-to-receiver acoustic neural warping field for spatial acoustic effects prediction in an acoustic 3D space with a single stationary audio source. Unlike traditional source-to-receiver modelling methods that require prior space acoustic properties knowledge to rigorously model audio propagation from source to receiver, we propose to predict by warping the spatial acoustic effects from one reference receiver position to another target receiver position, so that the warped audio essentially accommodates all spatial acoustic effects belonging to the target position. SPEAR can be trained in a data much more readily accessible manner, in which we simply ask two robots to independently record spatial audio at different positions. We further theoretically prove the universal existence of the warping field if and only if one audio source presents. Three physical principles are incorporated to guide SPEAR network design, leading to the learned warping field physically meaningful. We demonstrate SPEAR superiority through detailed experiments on both synthetic, photo-realistic and real-world dataset.", "title_embedding_index": 22704, "title_abs_embedding_index": 22729}, {"title": "Learning Monotonic Attention in Transducer for Streaming Generation", "link_suffix": "/forum?id=pbJMPo4HwR", "link": "https://openreview.net/forum?id=pbJMPo4HwR", "pdf_link": "https://openreview.net/pdf?id=pbJMPo4HwR", "keywords": "streaming generation, simultaneous translation, Transducer", "abstract": "Streaming generation models are increasingly utilized across various fields, with the Transducer architecture being particularly popular in industrial applications. However, its input-synchronous decoding mechanism presents challenges in tasks requiring non-monotonic alignments, such as simultaneous translation, leading to suboptimal performance in these contexts. In this research, we address this issue by tightly integrating Transducer's decoding with the history of input stream via a learnable monotonic attention mechanism. Our approach leverages the forward-backward algorithm to infer the posterior probability of alignments between the predictor states and input timestamps, which is then used to estimate the context representations of monotonic attention in training. This allows Transducer models to adaptively adjust the scope of attention based on their predictions, avoiding the need to enumerate the exponentially large alignment space. Extensive experiments demonstrate that our MonoAttn-Transducer significantly enhances the handling of non-monotonic alignments in streaming generation, offering a robust solution for Transducer-based frameworks to tackle more complex streaming generation tasks. Codes are publicly available in supplementary materials.", "title_embedding_index": 22705, "title_abs_embedding_index": 22730}, {"title": "SaMoye: Zero-shot Singing Voice Conversion Model Based on Feature Disentanglement and Enhancement", "link_suffix": "/forum?id=kbSU5bwoRv", "link": "https://openreview.net/forum?id=kbSU5bwoRv", "pdf_link": "https://openreview.net/pdf?id=kbSU5bwoRv", "keywords": "singing voice conversion, zero-shot, feature disentanglement", "abstract": "Singing voice conversion (SVC) aims to convert a singer's voice to another singer's from a reference audio while keeping the original semantics. However, existing SVC methods can hardly perform zero-shot due to incomplete feature disentanglement or dependence on the speaker look-up table. We propose the first open-source high-quality zero-shot SVC model SaMoye that can convert singing to human and non-human timbre. SaMoye disentangles the singing voice's features into content, timbre, and pitch features, where we combine multiple ASR models and compress the content features to reduce timbre leakages. Besides, we enhance the timbre features by unfreezing the speaker encoder and mixing the speaker embedding with top-3 similar speakers. We also establish an unparalleled large-scale dataset to guarantee zero-shot performance, which comprises more than 1,815 hours of pure singing voice and 6,367 speakers. We conduct objective and subjective experiments to find that SaMoye outperforms other models in zero-shot SVC tasks even under extreme conditions like converting singing to animals' timbre.", "title_embedding_index": 22706, "title_abs_embedding_index": 22731}, {"title": "Quantum Neural Fields", "link_suffix": "/forum?id=gnexAe3kjx", "link": "https://openreview.net/forum?id=gnexAe3kjx", "pdf_link": "https://openreview.net/pdf?id=gnexAe3kjx", "keywords": "quantum neural fields, representation learning, Neuro-deterministic data encoding, quantum ansatz", "abstract": "This paper introduces a new type of neural field for visual computing with components compatible with gate-based quantum hardware or simulators thereof. Our Quantum Neural Field Network (QNF-Net) expects as input a query coordinate and, optionally, a latent variable value, and outputs the corresponding field value. QNF-Net includes a new feature map for classical data encoding and a parametrised quantum circuit. The proposed neuro-deterministic data encoding converts, into qubit amplitudes, an energy spectrum of the Gibbs-Boltzmann distribution corresponding to the learned problem energy manifold. We provide a theoretical analysis of the model and its components and perform experiments on a simulator of a gate-based quantum computer with 2D images and 3D shapes (and their collections as learnt priors) and compare results with several classical baselines. QNF-Net consistently outperforms the classical baselines with a comparable number of parameters and achieves faster convergence speed, therefore showing its potential quantum advantages, even for relatively large-scale problems compared to what has been demonstrated in quantum machine learning so far. We will release the source code to facilitate method reproducibility.", "title_embedding_index": 22707, "title_abs_embedding_index": 22732}, {"title": "Test-time Adaptation for Image Compression with Distribution Regularization", "link_suffix": "/forum?id=bsnRUkVn63", "link": "https://openreview.net/forum?id=bsnRUkVn63", "pdf_link": "https://openreview.net/pdf?id=bsnRUkVn63", "keywords": "test-time adaptation, image compression, entropy coding", "abstract": "Current test- or compression-time adaptation image compression (TTA-IC) approaches, which leverage both latent and decoder refinements as a two-step adaptation scheme, have potentially enhanced the rate-distortion (R-D) performance of learned image compression models on cross-domain compression tasks, \\textit{e.g.,} from natural to screen content images.  However, compared with the emergence of various decoder refinement variants, the latent refinement, as an inseparable ingredient, is barely\n tailored to cross-domain scenarios. To this end, we are interested in developing an advanced latent refinement method by extending the effective hybrid latent refinement (HLR) method, which is designed for \\textit{in-domain} inference improvement but shows noticeable degradation of the rate cost in \\textit{cross-domain} tasks. Specifically, we first provide theoretical analyses, in a cue of marginalization approximation from in- to cross-domain scenarios,  to uncover that the vanilla HLR suffers from an underlying mismatch between refined Gaussian conditional and hyperprior distributions, leading to deteriorated joint probability approximation of marginal distribution with increased rate consumption. To remedy this issue, we introduce a simple Bayesian approximation-endowed \\textit{distribution regularization} to encourage learning a better joint probability approximation in a plug-and-play manner. Extensive experiments on six in- and cross-domain datasets demonstrate that our proposed method not only improves the R-D performance compared with other latent refinement counterparts, but also can be flexibly integrated into existing TTA-IC methods with incremental benefits.", "title_embedding_index": 22708, "title_abs_embedding_index": 22733}, {"title": "KAC: Kolmogorov-Arnold Classifier for Continual Learning", "link_suffix": "/forum?id=qFeeJ2ZQiH", "link": "https://openreview.net/forum?id=qFeeJ2ZQiH", "pdf_link": "https://openreview.net/pdf?id=qFeeJ2ZQiH", "keywords": "continual learning, class incremental learning", "abstract": "Continual learning requires models to train continuously across consecutive tasks without forgetting. Most existing methods utilize linear classifiers, which struggle to maintain a stable classification space while learning new tasks. Inspired by the success of Kolmogorov-Arnold Networks (KAN) in preserving learning stability during simple continual regression tasks, we set out to explore their potential in more complex continual learning scenarios. In this paper, we introduce the Kolmogorov-Arnold Classifier (KAC), a novel classifier developed for continual learning based on the KAN structure. We delve into the impact of KAN's spline functions and introduce Radial Basis Functions (RBF) for improved compatibility with continual learning. We replace linear classifiers with KAC in several recent approaches and conduct experiments across various continual learning benchmarks, all of which demonstrate performance improvements, highlighting the effectiveness and robustness of KAC in continual learning.", "title_embedding_index": 22709, "title_abs_embedding_index": 22734}, {"title": "PSHead: 3D Head Reconstruction from a Single Image with Diffusion Prior and Self-Enhancement", "link_suffix": "/forum?id=0KFwhDqTQ6", "link": "https://openreview.net/forum?id=0KFwhDqTQ6", "pdf_link": "https://openreview.net/pdf?id=0KFwhDqTQ6", "keywords": "Diffusion models, Text to 3D, Image to 3D, 3D Avatar", "abstract": "In this work, we investigate the problem of creating high-fidelity photorealistic 3D avatars from only a single face image. This task is inherently challenging due to the limited 3D cues and ambiguities present in a single viewpoint, further complicated by the intricate details of the human face (e.g., wrinkles, facial hair). To address these challenges, we introduce PSHead, a coarse-to-fine framework that optimizes 3D Gaussian Splatting for a single image, guided by a mixture of object and face prior to generate high-quality 3D avatars while preserving faithfulness to the original image.  At the coarse stage, we leverage diffusion models trained on general objects to predict coarse representation by applying score distillation sampling losses at novel views. This marks the first attempt to integrate text-to-image, image-to-image, and text-to-video diffusion priors, ensuring consistency across multiple views and robustness to variations in face size. In the fine stage, we utilize pretrained face generation models to denoise the rendered noisy images,  and use them as supervision to refine the 3D representation. Our method outperforms existing approaches on in-the-wild images, proving its robustness and ability to capture intricate details without the need for extensive 3D supervision.", "title_embedding_index": 22710, "title_abs_embedding_index": 22735}, {"title": "FreeGaussian: Guidance-free Controllable 3D Gaussian Splats with Flow Derivatives", "link_suffix": "/forum?id=rQV33MVNWs", "link": "https://openreview.net/forum?id=rQV33MVNWs", "pdf_link": "https://openreview.net/pdf?id=rQV33MVNWs", "keywords": "3D Gaussian Splatting, Controllable View Synthesis", "abstract": "Reconstructing controllable Gaussian splats from monocular video is a challenging task due to its inherently insufficient constraints. Widely adopted approaches supervise complex interactions with additional masks and control signal annotations, limiting their real-world applications. In this paper, we propose an annotation guidance-free method, dubbed FreeGaussian, that mathematically derives dynamic Gaussian motion from optical flow and camera motion using novel dynamic Gaussian constraints. By establishing a connection between 2D flows and 3D Gaussian dynamic control, our method enables self-supervised optimization and continuity of dynamic Gaussian motions from flow priors. Furthermore, we introduce a 3D spherical vector controlling scheme, which represents the state with a 3D Gaussian trajectory, thereby eliminating the need for complex 1D control signal calculations and simplifying controllable Gaussian modeling. Quantitative and qualitative evaluations on extensive experiments demonstrate the state-of-the-art visual performance and control capability of our method.", "title_embedding_index": 22711, "title_abs_embedding_index": 22736}, {"title": "AnyAttack: Self-supervised Generation of Targeted Adversarial Attacks for Vision-Language Models", "link_suffix": "/forum?id=bQ0sbMLYFj", "link": "https://openreview.net/forum?id=bQ0sbMLYFj", "pdf_link": "https://openreview.net/pdf?id=bQ0sbMLYFj", "keywords": "targeted adversarial attack, Vision-Language Models, self-supervised", "abstract": "Due to their multimodal capabilities, Vision-Language Models (VLMs) have found numerous impactful applications in real-world scenarios. However, recent studies have revealed that VLMs are vulnerable to image-based adversarial attacks, particularly targeted adversarial images that manipulate the model to generate harmful content specified by the adversary. \nCurrent attack methods rely on predefined target labels to create targeted adversarial attacks, which limits their scalability and applicability for large-scale robustness evaluations. In this paper, we proposeAnyAttack, a self-supervised framework that generates targeted adversarial images for VLMs without label supervision, allowinganyimage to serve as a target for theattack.\nTo address the limitation of existing methods that require label supervision, we introduce a contrastive loss that trains a generator on a large-scale unlabeled image dataset, LAION-400M\ndataset, for generating targeted adversarial noise. \nThis large-scale pre-training endows our method with powerful transferability across a wide range of VLMs.\nExtensive experiments on five mainstream open-source VLMs (CLIP, BLIP, BLIP2, InstructBLIP, and MiniGPT-4) across three multimodal tasks (image-text retrieval, multimodal classification, and image captioning) demonstrate the effectiveness of our attack.\nAdditionally, we successfully transfer AnyAttack to multiple commercial VLMs, including Google's Gemini, Claude's Sonnet, and Microsoft's Copilot.\nThese results reveal an unprecedented risk to VLMs, highlighting the need for effective countermeasures.\nUpon publication, we will release the pre-trained generator to support further research in addressing this challenge.", "title_embedding_index": 22712, "title_abs_embedding_index": 22737}, {"title": "Controllable Blur Data Augmentation Using 3D-Aware Motion Estimation", "link_suffix": "/forum?id=Wvi8c0tgvt", "link": "https://openreview.net/forum?id=Wvi8c0tgvt", "pdf_link": "https://openreview.net/pdf?id=Wvi8c0tgvt", "keywords": "Blur synthesis, Data augmentation, Blind motion deblurring, 3D motion modeling", "abstract": "Existing realistic blur datasets provide insufficient variety in scenes and blur patterns to be trained, while expanding data diversity demands considerable time and effort due to complex dual-camera systems. To address the challenge, data augmentation can be an effective way to artificially increase data diversity. However, existing methods on this line are typically designed to estimate motions from a 2D perspective, e.g., estimating 2D non-uniform kernels disregarding physical blur modeling, which leads to unrealistic motion patterns due to the fact that camera and object motions inherently arise in 3D space. In this paper, we propose a 3D-aware blur synthesizer capable of generating diverse and realistic blur images for blur data augmentation. Specifically, we estimate 3D camera positions within the motion blur interval, generate the corresponding scene images, and aggregate them to synthesize a physically-driven blur image. Since the 3D camera positions projected onto the 2D image plane inherently lie in 2D space, we can represent the 3D transformation as a combination of 2D transformation and projected 3D residual component. This allows for 3D transformation without requiring explicit depth measurements, as the 3D residual component is directly estimated via a neural network. Furthermore, our blur synthesizer allows for controllable blur data augmentation by modifying blur magnitude, direction, and scenes, resulting in diverse blur images. As a result, our method significantly improves deblurring performance, making it more practical for real-world scenarios.", "title_embedding_index": 22713, "title_abs_embedding_index": 22738}, {"title": "SyntheOcc: Synthesize Geometric-Controlled Street View Images through 3D Semantic MPIs", "link_suffix": "/forum?id=Dq9VrVuLzV", "link": "https://openreview.net/forum?id=Dq9VrVuLzV", "pdf_link": "https://openreview.net/pdf?id=Dq9VrVuLzV", "keywords": "Autonomous Driving, Image Generation, Data-centric AI, 3D Vision", "abstract": "The advancement of autonomous driving is increasingly reliant on high-quality annotated datasets, especially in the task of 3D occupancy prediction, where the occupancy labels require dense 3D annotation with significant human effort. In this paper, we propose SyntheOcc, which denotes a diffusion model that Synthesize photorealistic and geometric-controlled images by conditioning Occupancy labels in driving scenarios. This yields an unlimited amount of diverse, annotated, and controllable datasets for applications like training perception models and simulation. SyntheOcc addresses the critical challenge of how to efficiently encode 3D geometric information as conditional input to a 2D diffusion model. Our approach innovatively incorporates 3D semantic multi-plane images (MPIs) to provide comprehensive and spatially aligned 3D scene descriptions for conditioning. As a result, SyntheOcc can generate photorealistic multi-view images and videos that faithfully align with the given geometric labels (semantics in 3D voxel space). Extensive qualitative and quantitative evaluations of SyntheOcc on the nuScenes dataset prove its effectiveness in generating controllable occupancy datasets that serve as an effective data augmentation to perception models.", "title_embedding_index": 22714, "title_abs_embedding_index": 22739}, {"title": "MaxCutPool: differentiable feature-aware Maxcut for pooling in graph neural networks", "link_suffix": "/forum?id=xlbXRJ2XCP", "link": "https://openreview.net/forum?id=xlbXRJ2XCP", "pdf_link": "https://openreview.net/pdf?id=xlbXRJ2XCP", "keywords": "Graph neural networks, graph pooling, graph coarsening, maxcut", "abstract": "We propose a novel approach to compute the MAXCUT in attributed graphs, i.e., graphs with features associated with nodes and edges. Our approach is robust to the underlying graph topology and is fully differentiable, making it possible to find solutions that jointly optimize the MAXCUT along with other objectives.\nBased on the obtained MAXCUT partition, we implement a hierarchical graph pooling layer for Graph Neural Networks, which is sparse, differentiable, and particularly suitable for downstream tasks on heterophilic graphs.", "title_embedding_index": 22715, "title_abs_embedding_index": 22740}, {"title": "OmniBooth: Learning Latent Control for Image Synthesis with Multi-modal Instruction", "link_suffix": "/forum?id=uBcx1aFpXy", "link": "https://openreview.net/forum?id=uBcx1aFpXy", "pdf_link": "https://openreview.net/pdf?id=uBcx1aFpXy", "keywords": "Image Synthesis, Diffusion Model, Controllable Generation, Spatial Control, Multi-modal Instruction", "abstract": "We present OmniBooth, an image generation framework that enables spatial control with instance-level multi-modal customization. For all instances, the multi-modal instruction can be described through text prompts or image references. Given a set of user-defined masks and associated text or image guidance, our objective is to generate an image, where multiple objects are positioned at specified coordinates and their attributes are precisely aligned with the corresponding guidance. This approach significantly expands the scope of text-to-image generation, and elevates it to a more versatile and practical dimension in controllability. In this paper, our core contribution lies in the proposed latent control signals, a high-dimensional spatial feature that provides a unified representation to integrate the spatial, textual, and image conditions seamlessly. The text condition extends ControlNet to provide instance-level open-vocabulary generation. The image condition further enables fine-grained control with personalized identity. In practice, our method empowers users with more flexibility in controllable generation, as users can choose multi-modal conditions from text or images as needed. Furthermore, thorough experiments demonstrate our enhanced performance in image synthesis fidelity and alignment across different tasks and datasets.", "title_embedding_index": 22716, "title_abs_embedding_index": 22741}, {"title": "Analyzing and Mitigating Inconsistency in Discrete Audio Tokens for Neural Codec Language Models", "link_suffix": "/forum?id=KFLtFSOtdj", "link": "https://openreview.net/forum?id=KFLtFSOtdj", "pdf_link": "https://openreview.net/pdf?id=KFLtFSOtdj", "keywords": "TTS, speech generation, neural audio codec, neural codec lanugage model", "abstract": "Building upon advancements in Large Language Models (LLMs), the field of audio processing has seen increased interest in training audio generation tasks with discrete audio token sequences. However, directly discretizing audio by\nneural audio codecs often results in sequences that fundamentally differ from text sequences. Unlike text, where text token sequences are deterministic, discrete audio tokens can exhibit significant variability based on contextual factors, while still producing perceptually identical audio segments. We refer to this phenomenon as \\textbf{Discrete Representation Inconsistency (DRI)}. This inconsistency can lead to a single audio segment being represented by multiple divergent sequences, which creates confusion in neural codec language models and results in omissions and repetitions during speech generation. In this paper, we quantitatively analyze the DRI phenomenon within popular audio tokenizers such as EnCodec. Our approach effectively mitigates the DRI phenomenon of the neural audio codec. Furthermore, extensive experiments on the neural codec language model over LibriTTS and large-scale MLS datasets (44,000 hours) demonstrate the effectiveness and generality of our method. The demo of audio samples is available online~\\footnote{\\url{https://consistencyinneuralcodec.github.io}}.", "title_embedding_index": 22717, "title_abs_embedding_index": 22742}, {"title": "\u03f5-VAE: Denoising as Visual Decoding", "link_suffix": "/forum?id=8ROIRnKloJ", "link": "https://openreview.net/forum?id=8ROIRnKloJ", "pdf_link": "https://openreview.net/pdf?id=8ROIRnKloJ", "keywords": "Diffusion Model, VAE, Image Tokenizer, Rectified Flow", "abstract": "In generative modeling, tokenization simplifies complex data into compact, structured representations, creating a more efficient, learnable space. For high-dimensional visual data, it reduces redundancy and emphasizes key features for high-quality generation. Current visual tokenization methods rely on a traditional autoencoder framework, where the encoder compresses data into latent representations, and the decoder reconstructs the original input. In this work, we offer a new perspective by proposing denoising as decoding, shifting from single-step reconstruction to iterative refinement. Specifically, we replace the decoder with a diffusion process that iteratively refines noise to recover the original image, guided by the latents provided by the encoder. We evaluate our approach by assessing both reconstruction (rFID) and generation quality (FID), comparing it to state-of-the-art autoencoding approach. We hope this work offers new insights into integrating iterative generation and autoencoding for improved compression and generation.", "title_embedding_index": 22718, "title_abs_embedding_index": 22743}, {"title": "UMVMap: Improving Vectorized Map Construction via Multi-vehicle Perspectives", "link_suffix": "/forum?id=ZjvUcCAEK8", "link": "https://openreview.net/forum?id=ZjvUcCAEK8", "pdf_link": "https://openreview.net/pdf?id=ZjvUcCAEK8", "keywords": "multi-vehicle, autonomous driving, vectorized map construction", "abstract": "Prevalent vectorized map construction pipelines predominantly follow an end-to-end DETR-based paradigm. While these methods have achieved significant advancements, they are limited by their reliance on data from a single ego vehicle, which restricts their effectiveness and can lead to perceptual uncertainty in handling complex environmental scenarios. To address this limitation, we introduce a novel framework: Uncertainty-aware Multi-Vehicle Vectorized Map Construction (UMVMap). This framework effectively mitigates uncertainties by leveraging relevant non-ego information. UMVMap comprises two essential components: the Uncertainty-aware Multi-Vehicle Vectorized Map Construction Network (UMVMap-Net), which optimally integrates data from multiple vehicles, and the Uncertainty-aware Non-ego Vehicle Selection (UNVS) strategy, which identifies and incorporates the most informative non-ego data to minimize uncertainty. Comprehensive evaluations on the nuScenes dataset demonstrate that UMVMap significantly outperforms the single-vehicle MapTRv2 baseline by a margin of 9.1% and 9.9% respectively on the full and partial validation sets, with each of its components proving to be both effective and robust.", "title_embedding_index": 22719, "title_abs_embedding_index": 22744}, {"title": "Prediction Via Shapley Value Regression", "link_suffix": "/forum?id=eBVCZj3RZN", "link": "https://openreview.net/forum?id=eBVCZj3RZN", "pdf_link": "https://openreview.net/pdf?id=eBVCZj3RZN", "keywords": "Explainable Machine Learning, Neural Networks, Kolmogorov\u2013Arnold Networks", "abstract": "Shapley values have several desirable properties for explaining black-box model predictions, which come with strong theoretical support. Traditionally, Shapley values are computed post-hoc, leading to additional computational cost at inference time. To overcome this, we introduce ViaSHAP, a novel approach that learns a function to compute Shapley values, from which the predictions can be derived directly by summation. We explore two learning approaches based on the universal approximation theorem and the Kolmogorov-Arnold representation theorem. Results from a large-scale empirical investigation are presented, in which the predictive performance of ViaSHAP is compared to state-of-the-art algorithms for tabular data, where the implementation using Kolmogorov-Arnold Networks showed a superior performance. It is also demonstrated that the explanations of ViaSHAP are accurate, and that the accuracy is controllable through the hyperparameters.", "title_embedding_index": 22720, "title_abs_embedding_index": 22745}, {"title": "FakeShield: Explainable Image Forgery Detection and Localization via Multi-modal Large Language Models", "link_suffix": "/forum?id=pAQzEY7M03", "link": "https://openreview.net/forum?id=pAQzEY7M03", "pdf_link": "https://openreview.net/pdf?id=pAQzEY7M03", "keywords": "Image Forgery Detection and Localization, Multi-modal Large Language Model, Tamper Detection", "abstract": "The rapid development of generative AI is a double-edged sword, which not only facilitates content creation but also makes image manipulation easier and more difficult to detect. Although current image forgery detection and localization (IFDL) methods are generally effective, they tend to face two challenges: \\textbf{1)} black-box nature with unknown detection principle, \\textbf{2)} limited generalization across diverse tampering methods (e.g., Photoshop, DeepFake, AIGC-Editing). To address these issues, we propose the explainable IFDL task and design FakeShield, a multi-modal framework capable of evaluating image authenticity, generating tampered region masks, and providing a judgment basis based on pixel-level and image-level tampering clues. Additionally, we leverage GPT-4o to enhance existing IFDL datasets, creating the Multi-Modal Tamper Description dataSet (MMTD-Set) for training FakeShield's tampering analysis capabilities. Meanwhile, we incorporate a Domain Tag-guided Explainable Forgery Detection Module (DTE-FDM) and a Multi-modal Forgery Localization Module (MFLM) to address various types of tamper detection interpretation and achieve forgery localization guided by detailed textual descriptions. Extensive experiments demonstrate that FakeShield effectively detects and localizes various tampering techniques, offering an explainable and superior solution compared to previous IFDL methods.", "title_embedding_index": 22721, "title_abs_embedding_index": 22746}, {"title": "SecureGS: Boosting the Security and Fidelity of 3D Gaussian Splatting Steganography", "link_suffix": "/forum?id=H4FSx06FCZ", "link": "https://openreview.net/forum?id=H4FSx06FCZ", "pdf_link": "https://openreview.net/pdf?id=H4FSx06FCZ", "keywords": "3DGS steganography, copyright protection, watermarking", "abstract": "3D Gaussian splatting (3DGS) has emerged as a premier method for 3D representation due to its real-time rendering and high-quality outputs, underscoring the critical need to protect the copyright and privacy of 3D assets. Traditional NeRF steganography methods fail to address the explicit nature of 3DGS since its point cloud files are publicly accessible. Existing GS steganography solutions mitigate some issues but still struggle with reduced rendering fidelity, increased computational demands, and security flaws, especially in the security of the geometric structure of the visualized point cloud. To address these challenges, we propose a SecureGS, a secure and efficient 3DGS steganography framework inspired by Scaffold-GS's anchor point design and neural decoding. SecureGS utilizes a hybrid decoupled Gaussian encryption mechanism to embed offsets, scales, rotations, and RGB attributes of the hidden 3D Gaussian points within anchor point features, retrievable only by authorized users through privacy-preserving neural networks. To further enhance security, we propose a density region-aware anchor growing and pruning strategy that adaptively locates optimal hiding regions without exposing hidden information. Extensive experiments demonstrate that SecureGS significantly surpasses existing GS steganography methods in rendering fidelity, speed, and security, effectively concealing and accurately extracting 3D objects, images, and bits within original 3D scenes.", "title_embedding_index": 22722, "title_abs_embedding_index": 22747}, {"title": "VideoDiT: Bridging Image Diffusion Transformers for Streamlined Video Generation", "link_suffix": "/forum?id=lvgsPjRtLM", "link": "https://openreview.net/forum?id=lvgsPjRtLM", "pdf_link": "https://openreview.net/pdf?id=lvgsPjRtLM", "keywords": "Text-to-Video Generation, Diffusion Models, Image Diffusion Transformer", "abstract": "We present VideoDiT, a streamlined video generation framework adapted from pre-trained image generation models. Unlike previous methods that simply add temporal layers to image diffusion models, we enhance both the tokenizer, implemented with the variational autoencoder (VAE), and the diffusion model. We emphasize the importance of combining 3D VAE compression with knowledge from pre-trained image diffusion models to achieve efficient video generation, though the tight coupling between image diffusion models and 2D VAEs poses significant challenges. To address this, we introduce the Distribution-Preserving VAE (DP-VAE), which encodes key frames in a video clip using the original 2D VAE while compressing non-key frames with a 3D VAE for spatiotemporal modeling. A regularization term ensures alignment between the 3D video latent space and the 2D image latent space, facilitating seamless transfer of pre-trained diffusion models. Leveraging the Diffusion Image Transformers (DiT) architecture and incorporating 3D positional embeddings, we extend 2D attention into 3D with negligible increased parameters. Furthermore, leveraging our proposed DP-VAE, VideoDiT supports joint image-video training, preserving the spatial modeling capabilities of the base model while excelling in both image and video generation. Extensive experiments validate the effectiveness of our approach.", "title_embedding_index": 22723, "title_abs_embedding_index": 22748}, {"title": "TimeBase: The Power of Minimalism in  Long-term Time Series Forecasting", "link_suffix": "/forum?id=HksnKo0iV9", "link": "https://openreview.net/forum?id=HksnKo0iV9", "pdf_link": "https://openreview.net/pdf?id=HksnKo0iV9", "keywords": "Long-term Time Series Forecasting", "abstract": "Long-term time series forecasting (LTSF) has traditionally relied on models with large parameters to capture extended temporal dependencies. However, time series data, unlike high-dimensional images or text, often exhibit strong periodicity and  low-rank structures, especially in long forecasting horizons. This characteristic can lead many models  focusing on redundant patterns, resulting in inefficient use of computational resources. In this paper, we introduce TimeBase, an ultra-lightweight network with fewer than 0.4$k$ parameters, designed to harness the power of minimalism in LTSF. TimeBase extracts core periodic features by leveraging full-rank typical period representations under orthogonality constraints, enabling accurate prediction of future cycles. Extensive experiments on real-world datasets demonstrate that TimeBase not only achieves minimalism in both model size and computational cost, reducing MACs by 35x and parameter counts by over 1000 times compared to standard linear models, but also wins state-of-the-art forecasting performance, ranking Top1-Top5 in all 28 prediction settings. Moreover, TimeBase exhibits robust generalization, maintaining high accuracy in limited-data, zero-shot, and low-quality scenarios. Code is available at \\url{https://anonymous.4open.science/r/TimeBase/}.", "title_embedding_index": 22724, "title_abs_embedding_index": 22749}]