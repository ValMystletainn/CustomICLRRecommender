[
    {
        "title": "EmbodiedCity: A Benchmark Platform for Embodied Agent in Real-world City Environment",
        "link_suffix": "/forum?id=y15LAM4u0A",
        "link": "https://openreview.net/forum?id=y15LAM4u0A",
        "pdf_link": "https://openreview.net/pdf?id=y15LAM4u0A",
        "keywords": "Embodied intelligence, real-world city environment, large language model agent, benchmark",
        "abstract": "Embodied artificial intelligence (EmbodiedAI) emphasizes the role of an agent's body in generating human-like behaviors. The recent efforts on  EmbodiedAI pay a lot of attention to building up machine learning models to possess perceiving, planning, and acting abilities, thereby enabling real-time interaction with the world. However, most works focus on bounded indoor environments, such as navigation in a room or manipulating a device, with limited exploration of embodying the agents in open-world scenarios. That is, embodied intelligence in the open and outdoor environment is less explored, for which one potential reason is the lack of high-quality simulators, benchmarks, and datasets. To address it, in this paper, we construct a benchmark platform for embodied intelligence evaluation in real-world city environments. Specifically, we first construct a highly realistic 3D simulation environment based on the real buildings, roads, and other elements in a real city. In this environment, we combine historically collected data and simulation algorithms to conduct simulations of pedestrian and vehicle flows with high fidelity. Further, we designed a set of evaluation tasks covering different EmbodiedAI abilities. Moreover, we provide a complete set of input and output interfaces for access, enabling embodied agents to easily take task requirements and current environmental observations as input and then make decisions and obtain performance evaluations. On the one hand, it expands the capability of existing embodied intelligence to higher levels. On the other hand, it has a higher practical value in the real world and can support more potential applications for artificial general intelligence. Based on this platform, we evaluate some popular large language models for embodied intelligence capabilities of different dimensions and difficulties. The executable program of this platform is available for download, and we have also released an easy-to-use Python library and detailed tutorial documents. All of the software, Python library, codes, datasets, tutorials, and real-time online service are available on this anonymous website:https://embodied-ai.city."
    },
    {
        "title": "Multi-Label Test-Time Adaptation with Bound Entropy Minimization",
        "link_suffix": "/forum?id=75PhjtbBdr",
        "link": "https://openreview.net/forum?id=75PhjtbBdr",
        "pdf_link": "https://openreview.net/pdf?id=75PhjtbBdr",
        "keywords": "Vision-Language Models, Zero-Shot Multi-Label Generalization, Test-Time Adaptation",
        "abstract": "Mainstream test-time adaptation (TTA) techniques endeavor to mitigate distribution shifts via entropy minimization for multi-class classification, inherently increasing the probability of the most confident class. However, when encountering multi-label instances, the primary challenge stems from the varying number of labels per image, and prioritizing only the highest probability class inevitably undermines the adaptation of other positive labels. To address this issue, we investigate TTA within multi-label scenario (ML--TTA), developing Bound Entropy Minimization (BEM) objective to simultaneously increase the confidence of multiple top predicted labels. Specifically, to determine the number of labels for each augmented view, we retrieve a paired caption with yielded textual labels for that view. These labels are allocated to both the view and caption, called weak label set and strong label set with the same size k. Following this, the proposed BEM considers the highest top-k predicted labels from view and caption as a single entity, respectively, learning both view and caption prompts concurrently. By binding top-k predicted labels, BEM overcomes the limitation of vanilla entropy minimization, which exclusively optimizes the most confident class. Across the MSCOCO, VOC, and NUSWIDE multi-label datasets, our ML--TTA framework equipped with BEM exhibits superior performance compared to the latest SOTA methods, across various model architectures, prompt initialization, and varying label scenarios. The code is available athttps://anonymous.4open.science/r/ML-TTA-10BE."
    },
    {
        "title": "Action-Constrained Imitation Learning",
        "link_suffix": "/forum?id=UlAkM88Vum",
        "link": "https://openreview.net/forum?id=UlAkM88Vum",
        "pdf_link": "https://openreview.net/pdf?id=UlAkM88Vum",
        "keywords": "action-constrained reinforcement learning, imitation learning, safety",
        "abstract": "Policy learning under action constraints plays a central role in ensuring safe behaviors in various robot control and resource allocation applications.\nIn this paper, we study a new problem setting termed Action-Constrained Imitation Learning (ACIL), where an action-constrained imitator aims to learn from a demonstrative expert with larger action space.\nThe fundamental challenge of ACIL lies in the unavoidable mismatch of occupancy measure between the expert and the imitator caused by the action constraints. We tackle this mismatch through $\\textit{trajectory alignment}$ and propose DTWIL, which replaces the original expert demonstrations with a surrogate dataset that follows similar state trajectories while adhering to the action constraints. Specifically, we recast trajectory alignment as a planning problem and solve it via Model Predictive Control, which aligns the surrogate trajectories with the expert trajectories based on the Dynamic Time Warping (DTW) distance. Through extensive experiments, we demonstrate that learning from the dataset generated by DTWIL significantly enhances performance across multiple robot control tasks and outperforms various benchmark imitation learning algorithms in terms of sample efficiency."
    },
    {
        "title": "LMRL Gym: Benchmarks for Multi-Turn Reinforcement Learning with Language Models",
        "link_suffix": "/forum?id=EdKSI2ijUY",
        "link": "https://openreview.net/forum?id=EdKSI2ijUY",
        "pdf_link": "https://openreview.net/pdf?id=EdKSI2ijUY",
        "keywords": "benchmarks, LLMs, RL",
        "abstract": "Large language models (LLMs) provide excellent text-generation capabilities, but standard prompting and generation methods generally do not lead to intentional or goal-directed agents and might necessitate considerable prompt tuning. Even the best current LLMs rarely ask clarifying questions, engage in explicit information gathering, or take actions that lead to better decisions after multiple turns. Reinforcement learning has the potential to leverage the powerful modeling capabilities of LLMs, as well as their internal representation of textual interactions, to create capable goal-directed language agents. This can enable intentional and temporally extended interactions, such as with humans, the emergence of complex skills such as persuasion, and long-horizon strategic behavior, such as in the context of games. Enabling this requires the community to develop reliable reinforcement learning algorithms for training LLMs. Developing such algorithms requires tasks that can gauge progress on algorithm design, provide accessible and reproducible evaluations for multi-turn interactions, and cover a range of task properties and challenges in improving reinforcement learning algorithms. Our paper introduces the LMRL-Gym benchmark for evaluating multi-turn RL for LLMs, together with an open-source research framework for getting started on multi-turn RL with offline value-based and online policy-based RL methods. Our benchmark consists of 3 Interactive Dialogue tasks and 5 RL Capability tests for a total of 8 tasks, which require multiple rounds of language interaction and cover a range of tasks in open-ended dialogue and text games."
    },
    {
        "title": "ToolGen: Unified Tool Retrieval and Calling via Generation",
        "link_suffix": "/forum?id=XLMAMmowdY",
        "link": "https://openreview.net/forum?id=XLMAMmowdY",
        "pdf_link": "https://openreview.net/pdf?id=XLMAMmowdY",
        "keywords": "Agent, Tool Learning, Virtual Token",
        "abstract": "As large language models (LLMs) advance, their inability to autonomously execute tasks by directly interacting with external tools remains a critical limitation. Traditional methods rely on inputting tool descriptions as context, which is constrained by context length and requires separate, often inefficient, retrieval mechanisms. We introduce ToolGen, a paradigm shift that integrates tool knowledge directly into the LLM\u2019s parameters by representing each tool as a unique token. This enables the LLM to generate tool calls and arguments as part of its next token prediction capabilities, seamlessly blending tool invocation with language generation.  Our framework allows the LLM to access and utilize a vast amount of tools with no additional retrieval step, significantly enhancing both performance and scalability. Experimental results with over 47,000 tools show that ToolGen not only achieves superior results in both tool retrieval and autonomous task completion but also sets the stage for a new era of AI agents that can adapt to tools across diverse domains.  By fundamentally transforming tool retrieval into a generative process, ToolGen paves the way for more versatile, efficient, and autonomous AI systems. ToolGen enables end-to-end tool learning and opens opportunities for integration with other advanced techniques such as chain-of-thought and reinforcement learning, thereby expanding the practical capabilities of LLMs"
    },
    {
        "title": "No Training Data, No Cry: Model Editing  without Training Data or Fine-tuning",
        "link_suffix": "/forum?id=wLR9d5ZFpY",
        "link": "https://openreview.net/forum?id=wLR9d5ZFpY",
        "pdf_link": "https://openreview.net/pdf?id=wLR9d5ZFpY",
        "keywords": "pruning, model editing, classwise unlearning",
        "abstract": "Model Editing(ME)--such as classwise unlearning and structured pruning--is a nascent field that deals with identifying editable components that, when modified, significantly change the model's behaviour, typically requiring fine-tuning to regain performance.\nThe challenge of model editing increases when dealing with multi-branch networks(e.g. ResNets) in the data-free regime, where the training data and the loss function are not available.\nIdentifying editable components is more difficult in multi-branch networks due to the coupling of individual components across layers through skip connections. \nThis paper addresses these issues through the following contributions.\nFirst, we hypothesize that in a well-trained model, there exists a small set of channels, which we call HiFi channels, whose input contributions strongly correlate with the output feature map of that layer.\nFinding such subsets can be naturally posed as an expected reconstruction error problem. To solve this, we provide an efficient heuristic called RowSum.\nSecond, to understand how to regain accuracy after editing, we prove, for the first time, an upper bound on the loss function post-editing in terms of the change in the stored BatchNorm(BN) statistics.  With this result, we derive BNFix, a simple algorithm to restore accuracy by updating the BN statistics using distributional access to the data distribution.\nWith these insights, we propose retraining free algorithms for structured pruning and classwise unlearning, CoBRA-P and CoBRA-U, that identify HiFi components and retains(structured pruning) or discards(classwise unlearning) them. CoBRA-P achieves at least 50% larger reduction in FLOPS and at least 10% larger reduction in parameters for similar drop in accuracy in the training free regime. In the training regime, for ImageNet, it achieves 60% larger parameter reduction. CoBRA-U achieves, on average, a 94% reduction in forget-class accuracy with a minimal drop in remaining class accuracy."
    },
    {
        "title": "Investigating Self-Attention: Its Impact on Sample Efficiency in Deep Reinforcement Learning",
        "link_suffix": "/forum?id=J5s6EG6ual",
        "link": "https://openreview.net/forum?id=J5s6EG6ual",
        "pdf_link": "https://openreview.net/pdf?id=J5s6EG6ual",
        "keywords": "self-attention, sample efficiency, reinforcement learning",
        "abstract": "Improving the sample efficiency of deep reinforcement learning (DRL) agents has been an ongoing challenge in research and real-world applications. Self-attention, a mechanism originally popularized in natural language processing, has shown great potential in enhancing sample efficiency when integrated with traditional DRL algorithms. However, the influence of self-attention mechanisms on the sample efficiency of DRL models has not been fully explored. In this paper, we ponder the fundamental operation of the self-attention mechanism in visual-based DRL settings and systematically investigate how different types of scaled dot-product attention impact the sample efficiency of the DRL algorithms. We design and evaluate the performance of our self-attention DRL models in the Arcade Learning Environment. Our results indicate that the design of the self-attention module influences the sample complexity of the DRL agent across various environments. To understand how self-attention modules influence the learning process, we perform an interpretability study from the perspectives of state representation and exploration. From our initial findings, we hypothesize that the interplay between feature extraction, action selection, and reward could be influenced subtly by the inductive biases of the proposed self-attention modules. This work contributes to the ongoing efforts to optimize DRL architectures, offering insights into the mechanisms that can enhance their performance in data-scarce scenarios."
    },
    {
        "title": "C-Adam: Confidence-Based Optimization for Online Learning",
        "link_suffix": "/forum?id=GamwMdPj0y",
        "link": "https://openreview.net/forum?id=GamwMdPj0y",
        "pdf_link": "https://openreview.net/pdf?id=GamwMdPj0y",
        "keywords": "Optimization Algorithm, Online Learning, Recommendation Systems",
        "abstract": "Modern recommendation systems frequently employ online learning to dynamically update their models with freshly collected data. The most commonly used optimizer for updating neural networks in these contexts is the Adam optimizer, which integrates momentum ($m_t$) and adaptive learning rate ($v_t$). However, the volatile nature of online learning data, characterized by its frequent distribution shifts and presence of noises, poses significant challenges to Adam's standard optimization process: (1) Adam may use outdated momentum and the average of squared gradients, resulting in slower adaptation to distribution changes, and (2) Adam's performance is adversely affected by data noise. To mitigate these issues, we introduce CAdam, a confidence-based optimization strategy that assesses the consistence between the momentum and the gradient for each parameter dimension before deciding on updates. If momentum and gradient are in sync, CAdam proceeds with parameter updates according to Adam's original formulation; if not, it temporarily withholds updates and monitors potential shifts in data distribution in subsequent iterations. This method allows CAdam to distinguish between the true distributional shifts and mere noise, and adapt more quickly to new data distributions. Our experiments with both synthetic and real-world datasets demonstrate that CAdam surpasses other well-known optimizers, including the original Adam, in efficiency and noise robustness. Furthermore, in large-scale A/B testing within a live recommendation system, CAdam significantly enhances model performance compared to Adam, leading to substantial increases in the system's gross merchandise volume (GMV)."
    },
    {
        "title": "Value Residual Learning For Alleviating  Attention Concentration In Transformers",
        "link_suffix": "/forum?id=kn3GT7LbxT",
        "link": "https://openreview.net/forum?id=kn3GT7LbxT",
        "pdf_link": "https://openreview.net/pdf?id=kn3GT7LbxT",
        "keywords": "Transformer, Self-Attention, Cross-Layer Attention, Residual Learning, Language model, KV cache",
        "abstract": "Transformers can capture long-range dependencies using self-attention, allowing tokens to attend to all others directly. However, stacking multiple attention layers leads to attention concentration. One natural way to address this issue is to use cross-layer attention, allowing information from earlier layers to be directly accessible to later layers. However, this approach is computationally expensive. To address this problem, we propose Transformer with residual value (ResFormer) which approximates cross-layer attention through adding a residual connection from the values of the the first layer to all subsequent layers. Based on this method, one variant is the Transformer with single layer value (SVFormer), where all layers share the same value embedding from first layer, reducing the $KV$ cache by nearly 50%.  Comprehensive empirical evidence demonstrates that ResFormer mitigates attention concentration problem in deeper layers and enhances representation across most layers, outperforming the vanilla Transformer, DenseFormer, and NeuTRENO in training error as well as downstream tasks. SVFormer trains significantly faster than the vanilla Transformer and performs better than other methods like GQA and CLA, with performance influenced by sequence length and cumulative learning rate."
    },
    {
        "title": "Re-examining learning linear functions in context",
        "link_suffix": "/forum?id=CCUrU4A92S",
        "link": "https://openreview.net/forum?id=CCUrU4A92S",
        "pdf_link": "https://openreview.net/pdf?id=CCUrU4A92S",
        "keywords": "In context learning, GPT, limitations",
        "abstract": "In context learning (ICL) is an attractive method of solving a wide range of problems.  Inspired by Garg et al., we look closely at ICL in a variety of train and test settings for several transformer models of different sizes trained from scratch.  Our study complements prior work by pointing out several systematic failures of these  models to generalize to data not in the training distribution, thereby showing some limitations of ICL."
    },
    {
        "title": "Improving Transformer Interpretability with Activation Contrast-Based Attribution",
        "link_suffix": "/forum?id=irCuIdCdAl",
        "link": "https://openreview.net/forum?id=irCuIdCdAl",
        "pdf_link": "https://openreview.net/pdf?id=irCuIdCdAl",
        "keywords": "Transformer, Interpretability, XAI, Attention, Contrast-based",
        "abstract": "Transformers have revolutionized AI research, particularly in natural language processing (NLP). However, understanding the decisions made by transformer-based models remains challenging, which impedes trust and safe deployment in real-world applications. While activation-based attribution methods have proven effective in explaining transformer-based text classification models, our findings suggest that they may suffer from class-irrelevant features within activations, potentially degrading the quality of their interpretations. To address this issue, we introduce Contrast-CAT, a novel activation contrast-based attribution method that improves token-level attribution by filtering out class-irrelevant features from activations. Contrast-CAT enhances interpretability by contrasting the activations of input sequences with reference activations, allowing for the generation of clearer and more faithful attribution maps. Our experiments demonstrate that Contrast-CAT consistently outperforms state-of-the-art methods across various datasets and models, achieving significant gains over the second-best methods with average improvements in AOPC and LOdds by $\\times 1.30$ and $\\times 2.25$, respectively, under the MoRF setting. Contrast-CAT provides a promising step forward in enhancing the interpretability and transparency of transformer-based models."
    },
    {
        "title": "Exploring New Frontiers in Vertical Federated Learning: the Role of Saddle Point Reformulation",
        "link_suffix": "/forum?id=5KgKa96PUG",
        "link": "https://openreview.net/forum?id=5KgKa96PUG",
        "pdf_link": "https://openreview.net/pdf?id=5KgKa96PUG",
        "keywords": "convex optimization, saddle point problem, vertical federated learning",
        "abstract": "Distributed learning problems have gained significant popularity due to the increasing need for cluster training and the emergence of novel paradigms like Federated Learning (FL). One variant of FL, called Vertical Federated Learning (VFL), partitions data based on features across devices. The objective is to collectively train a model using the information available on each user's device. This paper focuses on solving the VFL problem using the saddle point reformulation via the classical Lagrangian function. We first demonstrate how this formulation can be solved using deterministic methods. But more importantly, the paper explores various stochastic modifications to adapt to practical scenarios, such as employing compression techniques for efficient information transmission, enabling partial participation for asynchronous communication, and utilizing coordinate selection for faster local computation. We show that the saddle point reformulation plays a key role and opens up possibilities to use mentioned extension that seem to be impossible in the standard minimization formulation. Convergence estimates are provided for each algorithm, demonstrating their effectiveness in addressing the VFL problem. Additionally, alternative reformulations of the VFL problem are investigated, and numerical experiments are conducted to validate the proposed methods' performance and effectiveness."
    },
    {
        "title": "ICConv: A Large-Scale Intent-Oriented and Context-Aware Conversational Search Dataset",
        "link_suffix": "/forum?id=Rp3DjldbSc",
        "link": "https://openreview.net/forum?id=Rp3DjldbSc",
        "pdf_link": "https://openreview.net/pdf?id=Rp3DjldbSc",
        "keywords": "conversaitonal search, multi-intent",
        "abstract": "In recent years, search engines have made significant advancements. Yet, traditional ad-hoc search engines often struggle with complex search scenarios (e.g. multi-turn information seeking). This challenge has shifted the focus towards conversational search, an approach enabling search engines to interact directly with users to obtain more precise results. Progress in conversational search has been slow due to a lack of data and difficulties in gathering real-world conversational search data. To address these hurdles, we embarked on a journey to autonomously create a large-scale, high-quality conversational search dataset. Previous efforts to create such datasets often overlooked the multi-intent aspect and contextual information, or resulted in a biased dataset, where all dialogue queries linked to a single positive passage. In our study, we have incorporated multi-intent based on the existing search sessions and converted each keyword-based query into multiple natural language queries based on different latent intents present in the related passage. We then contextualized these natural language queries within the same session and organized them into a conversational search tree. A carefully designed dialogue discriminator was utilized to ensure the consistency and coherence of all generated conversations, assessing their quality and filtering out any substandard ones.\nAfter extensive data cleaning, we are proud to introduce the \\textbf{I}ntent-oriented and \\textbf{C}ontext-aware \\textbf{Conv}ersational search dataset (ICConv), a large-scale synthetic dataset comprising over 100,000 high-quality, information-seeking conversations. Our human annotators have evaluated ICConv based on six dialogue and search related criteria and it has performed admirably. We further explore the statistical characteristics of ICConv and validate the effectiveness of various conversational search methods using it as a standard for comparison."
    },
    {
        "title": "Efficiently Scanning and Resampling Spatio-Temporal Tasks with Irregular Observations",
        "link_suffix": "/forum?id=TySMCLoGVl",
        "link": "https://openreview.net/forum?id=TySMCLoGVl",
        "pdf_link": "https://openreview.net/pdf?id=TySMCLoGVl",
        "keywords": "sequence modeling, efficient training, efficient inference, spatio-temporal, multi-agent task",
        "abstract": "Various works have aimed at combining the inference efficiency of recurrent models and training parallelism of multi-head attention for sequence modeling. However, most of these works focus on tasks with fixed-dimension observation spaces, such as individual tokens in language modeling or pixels in image completion. To handle an observation space of varying size, we propose a novel algorithm that alternates between cross-attention between a 2D latent state and observation, and a discounted cumulative sum over the sequence dimension to efficiently accumulate historical information. We find this resampling cycle is critical for performance. To evaluate efficient sequence modeling in this domain, we introduce two multi-agent intention tasks: simulated agents chasing bouncing particles and micromanagement analysis in professional StarCraft II games. Our algorithm achieves comparable accuracy with a lower parameter count, faster training and inference compared to existing methods."
    },
    {
        "title": "Activation Gradient based Poisoned Sample Detection Against Backdoor Attacks",
        "link_suffix": "/forum?id=VNMJfBBUd5",
        "link": "https://openreview.net/forum?id=VNMJfBBUd5",
        "pdf_link": "https://openreview.net/pdf?id=VNMJfBBUd5",
        "keywords": "Backdoor Defense, Poisoned Sample Detection, AI security",
        "abstract": "This work studies the task of poisoned sample detection for defending against data poisoning based backdoor attacks. Its core challenge is finding a generalizable and discriminative metric to distinguish between clean and various types of poisoned samples (e.g., various triggers, various poisoning ratios). Inspired by a common phenomenon in backdoor attacks that the backdoored model tend to map significantly different poisoned and clean samples within the target class to similar activation areas, we introduce a novel perspective of the circular distribution of the gradients w.r.t. sample activation, dubbed gradient circular distribution (GCD). And, we find two interesting observations based on GCD. One is that the GCD of samples in the target class is much more dispersed than that in the clean class. The other is that in the GCD of target class, poisoned and clean samples are clearly separated. Inspired by above two observations, we develop an innovative three-stage poisoned sample detection approach, called Activation Gradient based Poisoned sample Detection (AGPD). First, we calculate GCDs of all classes from the model trained on the untrustworthy dataset. Then, we identify the target class(es) based on the difference on GCD dispersion between target and clean classes. Last, we filter out poisoned samples within the identified target class(es) based on the clear separation between poisoned and clean samples. Extensive experiments under various settings of backdoor attacks demonstrate the superior detection performance of the proposed method to existing poisoned detection approaches according to sample activation-based metrics."
    },
    {
        "title": "Causally Motivated Sycophancy Mitigation for Large Language Models",
        "link_suffix": "/forum?id=yRKelogz5i",
        "link": "https://openreview.net/forum?id=yRKelogz5i",
        "pdf_link": "https://openreview.net/pdf?id=yRKelogz5i",
        "keywords": "Large Language Model; Sycophancy; Causal Modeling",
        "abstract": "Incorporating user preferences into large language models (LLMs) can enhance the personalization and reliability of model outputs and facilitate the application of LLMs to real-world scenarios. However, leveraging user preferences can be a double-edged sword. Recent studies have found that improper utilization can incur sycophancy, where LLMs prioritize alignment with user preferences over the correctness of their outputs. To address sycophancy in LLMs, we analyze and model the problem through the lens of structured causal models (SCMs). We attribute sycophancy to LLMs' reliance on spurious correlations between user preferences and model outputs in this paper. Based on the proposed SCMs, we develop a novel framework to mitigate sycophancy in LLMs by exploiting a significant causal signature. Specifically, we eliminate the spurious correlations embedded in the intermediate layers of LLMs through head reweighting, and then calibrate the intra-head knowledge along the causal representation direction. Extensive experiments are conducted across diverse language tasks, and the empirical results demonstrate the superiority of our method over state-of-the-art competitors in mitigating sycophancy in LLMs."
    },
    {
        "title": "LVLM-COUNT: Enhancing the Counting Ability of Large Vision-Language Models",
        "link_suffix": "/forum?id=GsCMKwyfWm",
        "link": "https://openreview.net/forum?id=GsCMKwyfWm",
        "pdf_link": "https://openreview.net/pdf?id=GsCMKwyfWm",
        "keywords": "Counting, Large vision-language models",
        "abstract": "Counting is a fundamental skill for various visual tasks in real-life applications, requiring both object recognition and robust counting capabilities. Despite their advanced visual perception, large vision-language models (LVLMs) struggle with counting tasks, especially when the number of objects exceeds those commonly encountered during training. We enhance LVLMs\u2019 counting abilities using a divide-and conquer approach, breaking counting problems into sub-counting tasks. Unlike prior methods, which do not generalize well to counting datasets on which they have not been trained, our method performs well on new datasets without any additional training or fine-tuning. We demonstrate that our approach enhances counting capabilities across various datasets and benchmarks."
    },
    {
        "title": "CGD: Modifying the Loss Landscape by Gradient Regularization",
        "link_suffix": "/forum?id=qotIZREPZf",
        "link": "https://openreview.net/forum?id=qotIZREPZf",
        "pdf_link": "https://openreview.net/pdf?id=qotIZREPZf",
        "keywords": "optimization, gradient regularization",
        "abstract": "Line-search methods are commonly used to solve optimization problems. The simplest line search method is the steepest descent where we always move in the direction of the negative gradient. Newton\u2019s method on the other hand is a second-order method that uses the curvature information in the Hessian to pick the descent direction. In this work, we propose a new line-search method called Constrained Gradient Descent (CGD) that implicitly changes the landscape of the objective function for efficient optimization. CGD is formulated as a solution to the constrained version of the original problem where the constraint is on a function of the gradient. We optimize the corresponding Lagrangian function thereby favourably changing the landscape of the objective function. This results in a line search procedure where the Lagrangian penalty acts as a control over the descent direction and can therefore be used to iterate over points that have smaller gradient values, compared to iterates of vanilla steepest descent. We reinterpret and draw parallels with the Explicit Gradient Regularization (EGR) method, discussing its drawbacks and potential enhancements. Numerical experiments are conducted on synthetic test functions to illustrate the performance of CGD and its variants."
    },
    {
        "title": "Cross-modal Mitigation of Spurious Correlation for Prompt-tuning in VLMs with Causally Motivated Logic Alignment",
        "link_suffix": "/forum?id=BlzBcWYmdB",
        "link": "https://openreview.net/forum?id=BlzBcWYmdB",
        "pdf_link": "https://openreview.net/pdf?id=BlzBcWYmdB",
        "keywords": "Vision-Language Models, Prompt Tuning, Spurious Correlations, Out-of-Distribution Generalization, Causality, Probability of Necessity and Sufficiency",
        "abstract": "Recent studies have shown that pre-trained vision-language models can effectively adapt to diverse downstream tasks through parameter-efficient prompt tuning. Unfortunately, the tuned models can exploit spurious correlations during prediction, resulting in a failure to generalize to out-of-distribution test data, especially when the tuning dataset exhibits bias. How to achieve cross-modal mitigation of spurious correlations during prompt tuning of vision-language models remains an open question. In this paper, the challenging problem is tackled by leveraging the stable relationship between necessary and sufficient causal features and the corresponding label. On the one hand, we constrain the learning process of prompt by reinforcing the necessary and sufficient connection between the textual labels and textual features. On the other hand, the probability of necessity and sufficiency between the textual features and the filtered visual features is measured and maximized to enhance cross-modal feature alignment. By iteratively optimizing these two objectives, we can achieve cross-modal mitigation of spurious correlations because the logic equivalence between textual labels and visual features is bolstered. The theoretical analysis on generalization error indicates that our method can achieve a tighter generalization error bound than existing approaches. We evaluate the proposed method on several commonly adopted out-of-distribution datasets, and the empirical results demonstrate the superiority of our method over the state-of-the-art competitors."
    },
    {
        "title": "ROSARL: Reward-Only Safe Reinforcement Learning",
        "link_suffix": "/forum?id=ayT4e9C3Gd",
        "link": "https://openreview.net/forum?id=ayT4e9C3Gd",
        "pdf_link": "https://openreview.net/pdf?id=ayT4e9C3Gd",
        "keywords": "Reinforcement Learning, Deep Reinforcement Learning, Safety, Safe AI, Safe RL",
        "abstract": "An important problem in reinforcement learning is designing agents that learn to solve tasks safely in an environment. A common solution is to define either a penalty in the reward function or a cost to be minimised when reaching unsafe states. However, designing reward or cost functions is non-trivial and can increase with the complexity of the problem. To address this, we investigate the concept of a Minmax penalty, the smallest penalty for unsafe states that leads to safe optimal policies, regardless of task rewards. We derive an upper and lower bound on this penalty by considering both environment diameter and solvability. Additionally, we propose a simple algorithm for agents to estimate this penalty while learning task policies. Our experiments demonstrate the effectiveness of this approach in enabling agents to learn safe policies in high-dimensional continuous control environments."
    },
    {
        "title": "Mitigating Reward Over-optimization in Direct Alignment Algorithms with Adaptive Importance Sampling",
        "link_suffix": "/forum?id=H9dNX6TaRE",
        "link": "https://openreview.net/forum?id=H9dNX6TaRE",
        "pdf_link": "https://openreview.net/pdf?id=H9dNX6TaRE",
        "keywords": "Reinforcement Learning From Human Feedback, Direct Preference Optimization, Reward Hacking",
        "abstract": "Recently, Direct Alignment Algorithms (DAAs) such as Direct Preference Optimization (DPO) have emerged as alternatives to the standard Reinforcement learning from human feedback (RLHF) for aligning large language models (LLMs) with human values. Surprisingly, while DAAs do not use a separate proxy reward model as in RLHF, their performance can still deteriorate due to over-optimization \u2013 a phenomenon found in RLHF where the policy can exploit failures of the reward model to achieve high rewards but the actual quality of the model begins to degrade. Recent studies find that DAAs tend to increase probability mass on out-of-distribution responses and the training objective in DAAs is heavily under-constrained on these out-of-distribution (OOD) responses due to a mismatch between offline distribution and the LM policy. In this paper, we propose a method to mitigate the distribution shift between the offline distribution and the LM policy by multiplying with an importance weight to reflect the policy distribution. The resulting method, called Adaptive Importance Sampling (AIS), relies on importance sampling techniques and resolves the high variance issue in importance sampling without extra hyper-parameters. Our experiment results showed Adaptive IS can improve win rates by 15% while maintaining a lower KL budget compared to DAAs."
    },
    {
        "title": "GETMusic: Generating Music Tracks with a Unified Representation and Diffusion Framework",
        "link_suffix": "/forum?id=ErpRu7qMq1",
        "link": "https://openreview.net/forum?id=ErpRu7qMq1",
        "pdf_link": "https://openreview.net/pdf?id=ErpRu7qMq1",
        "keywords": "Symbolic Music Generation, Symbolic Music Representation, Diffusion Model",
        "abstract": "Symbolic music generation aims to create musical notes, which can help users compose music, such as generating target instrument tracks based on provided source tracks. In practical scenarios where there\u2019s a predefined ensemble of tracks and various composition needs, an efficient and effective generative model that can generate any target tracks based on the other tracks becomes crucial. However, previous efforts have fallen short in addressing this necessity due to limitations in their music representations and models. In this paper, we introduce a framework known as GETMusic, with \u201cGET\u201d standing for \u201cGEnerate music Tracks.\u201d This framework encompasses a novel music representation \u201cGETScore\u201d and a diffusion model \u201cGETDiff.\u201d GETScore represents musical notes as tokens and organizes tokens in a 2D structure, with tracks stacked vertically and progressing horizontally over time. At a training step, each track of a music piece is randomly selected as either the target or source. The training involves two processes: In the forward process, target tracks are corrupted by masking their tokens, while source tracks remain as the ground truth; in the denoising process, GETDiff is trained to predict the masked target tokens conditioning on the source tracks. Our proposed representation, coupled with the non-autoregressive generative model, empowers GETMusic to generate music with any arbitrary source-target track combinations. Our experiments demonstrate that the versatile GETMusic outperforms prior works proposed for certain specific composition tasks."
    },
    {
        "title": "Multiple-play Stochastic Bandits with Prioritized Resource Sharing",
        "link_suffix": "/forum?id=9e5syenoVE",
        "link": "https://openreview.net/forum?id=9e5syenoVE",
        "pdf_link": "https://openreview.net/pdf?id=9e5syenoVE",
        "keywords": "Multiple-play stochastic bandit, prioritized resource sharing, regret bounds",
        "abstract": "This paper proposes a variant of  multiple-play stochastic bandits tailored to resource allocation problems arising from LLM applications, \nedge intelligence applications, etc.  The proposed model is composed of $M$ arms and $K$ plays.  Each arm has a stochastic number of capacities, and each unit of capacity is associated with a reward function.  Each play is associated with a priority weight.When multiple plays compete for the arm capacity, the arm capacity is allocated in a larger priority weight first  manner.  Instance independent and instance dependent regret lower bounds of $\\Omega( \\alpha_1 \\sigma \\sqrt{KM T} )$ and $\\Omega(\\alpha_1 \\sigma^2 \\frac{MK}{\\Delta} \\ln T)$  are proved,  where $\\alpha_1$ is the largest priority weight and $\\sigma$ characterizes the reward tail.When model parameters are given, we design an algorithm named \\texttt{MSB-PRS-OffOpt} to locate the optimal play allocation policy with a computational complexity of $O(M^3K^3)$.   Utilizing \\texttt{MSB-PRS-OffOpt} as a subroutine, an approximate upper confidence bound (UCB) based algorithm is designed, which has instance independent and instance dependent regret upper bounds matching the corresponding lower bound up to factors of $K \\sqrt{ \\ln KT }$ and $\\alpha_1 K$ respectively.   To this end, we address nontrivial technical challenges arising from optimizing and learning under a special nonlinear combinatorial utility function induced by the prioritized resource sharing mechanism."
    },
    {
        "title": "Binary Spiking Neural Networks as causal models",
        "link_suffix": "/forum?id=NNBAzdF7Cg",
        "link": "https://openreview.net/forum?id=NNBAzdF7Cg",
        "pdf_link": "https://openreview.net/pdf?id=NNBAzdF7Cg",
        "keywords": "Explainability, Causal reasoning, Spiking Neural Networks, White-box",
        "abstract": "In this paper, we provide a causal analysis of  binary spiking neural networks (BSNNs)\naimed at explaining their behaviors. \nWe formally define a BSNN \nand   represent its  spiking activity\n  as a binary causal model.\nThanks to this causal  representation, \nwe are able to explain the output of the network\nby leveraging  logic-based  methods. \nIn particular,\nwe show that we  can successfully \nuse a SAT  (Boolean satisfiability) solver to  compute \n  abductive explanations from this  binary causal model. \nTo illustrate our approach, \nwe trained the BSNN on the standard MNIST\ndataset and applied our SAT-based  method  to\nfinding  abductive  explanations of  the network's classifications\nbased on pixel-level features. We also compared the found explanations against SHAP,  a popular \nmethod used in the area of explainable\nAI to explain ``black box'' classifiers.\nWe show that, unlike SHAP,\nour method guarantees that a found  explanation  does\nnot contain completely irrelevant features."
    },
    {
        "title": "Compositional simulation-based inference for time series",
        "link_suffix": "/forum?id=uClUUJk05H",
        "link": "https://openreview.net/forum?id=uClUUJk05H",
        "pdf_link": "https://openreview.net/pdf?id=uClUUJk05H",
        "keywords": "Simulation-based inference, Bayesian inference, time series, markovian simulators, Amortized Bayesian inference",
        "abstract": "Amortized simulation-based inference (SBI) methods train neural networks on simulated data to perform Bayesian inference. While this approach avoids the need for tractable likelihoods, it often requires a large number of simulations and has been challenging to scale to time-series data. Scientific simulators frequently emulate real-world dynamics through thousands of single-state transitions over time. We propose an SBI framework that can exploit such Markovian simulators by locally identifying parameters consistent with individual state transitions. We then compose these local results to obtain a posterior over parameters that align with the entire time series observation. We focus on applying this approach to neural posterior score estimation but also show how it can be applied, e.g., to neural likelihood (ratio) estimation. We demonstrate that our approach is more simulation-efficient than directly estimating the global posterior on several synthetic benchmark tasks and simulators used in ecology and epidemiology. Finally, we validate scalability and simulation efficiency of our approach by applying it to a high-dimensional Kolmogorov flow simulator with around one million dimensions in the data domain."
    }
]