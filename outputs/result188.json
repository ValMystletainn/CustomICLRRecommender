[
    {
        "title": "Towards Autonomous Agents: Adaptive-planning, Reasoning, and Acting in Language Models",
        "link_suffix": "/forum?id=cb4etlGvOY",
        "link": "https://openreview.net/forum?id=cb4etlGvOY",
        "pdf_link": "https://openreview.net/pdf?id=cb4etlGvOY",
        "keywords": "llm agents, in-context learning, autonomous agents",
        "abstract": "We propose a novel in-context learning algorithm for building autonomous decision-making language agents. The language agent continuously attempts to solve the same task by reasoning, acting, observing and then self-correcting each time the task fails. Our selected language agent demonstrates the ability to solve tasks in a text-based game environment. Our results show that the gemma-2-9b-it language model, using our proposed method, can successfully complete two of six tasks that failed in the first attempt. This highlights the effectiveness of our approach in enhancing the problem-solving capabilities of a single language model through self-correction, paving the way for more advanced autonomous agents. The code is publicly available athttps://anonymous.4open.science/r/AutonomousLLMAgentwithAdaptingPlanning-D613/."
    },
    {
        "title": "MatMamba: A Matryoshka State Space Model",
        "link_suffix": "/forum?id=RfrdbJVvVf",
        "link": "https://openreview.net/forum?id=RfrdbJVvVf",
        "pdf_link": "https://openreview.net/pdf?id=RfrdbJVvVf",
        "keywords": "matryoshka, mamba, state space models, adaptive inference, representation learning, neural network architecture, efficient, visual",
        "abstract": "State Space Models (SSMs) like Mamba2 are a promising alternative to Transformers, with faster theoretical training and inference times -- especially for long context lengths. Recent work on Matryoshka Representation Learning -- and its application to Transformer backbones in works like MatFormer --  showed how to introduce nested granularities of smaller submodels in one universal elastic model. In this work, we present MatMamba: a state space model which combines Matryoshka-style learning with Mamba2, by modifying the block to contain nested dimensions to enable joint training and adaptive inference. MatMamba allows for efficient and adaptive deployment across various model sizes. We train a single large MatMamba model and are able to get a number of smaller nested models for free -- while maintaining or improving upon the performance of a baseline smaller model trained from scratch. We train language and image models at a variety of parameter sizes from 35M to 1.4B. Our results on ImageNet and FineWeb show that MatMamba models scale comparably to Transformers, while having more efficient inference characteristics. This makes MatMamba a practically viable option for deploying large-scale models in an elastic way based on the available inference compute."
    },
    {
        "title": "Efficient Gun Detection in Real-World Videos: Challenges and Solutions",
        "link_suffix": "/forum?id=ys3eqxzkeN",
        "link": "https://openreview.net/forum?id=ys3eqxzkeN",
        "pdf_link": "https://openreview.net/pdf?id=ys3eqxzkeN",
        "keywords": "Image-augmented training, transfer learning",
        "abstract": "Object detection in videos is a crucial task in the computer vision domain. Existing methods have explored different approaches to detect objects and classify the videos. However, detecting tiny objects (e.g., gun) in videos has always been a challenging and rigorous task. Moreover, the existing video analysis (detection and classification) models may not achieve high accuracy for gun detection in videos in real-world scenarios due to the lack of a large amount of labeled data. Thus, it is imperative to develop an efficient method to capture the features of tiny objects and train models that can perform accurate gun detection. To address this challenge, we make three contributions. First, we perform an empirical study of several existing video classification methods to identify the presence of guns in videos. Our extensive analysis shows that these methods may not achieve high accuracy in detecting guns in videos. Second, we propose a novel gun detection method with image-augmented training and evaluate the technique in real-world settings with different evaluation metrics. Third, our experimental results demonstrate that our proposed domain-specific method can achieve significant performance improvements in real-world settings compared to the other popular methods. We also discuss emerging challenges and critical aspects of detecting tiny objects, e.g., guns, using existing computer vision techniques, their limitations, and future research opportunities."
    },
    {
        "title": "Leveraging System-Prompt Attention to Counteract Novel Jailbreak Attacks",
        "link_suffix": "/forum?id=MV5j4Qpq7N",
        "link": "https://openreview.net/forum?id=MV5j4Qpq7N",
        "pdf_link": "https://openreview.net/pdf?id=MV5j4Qpq7N",
        "keywords": "jailbreaks, agents, safeguards, latent representations",
        "abstract": "In the past few years, Language Models (LMs) have shown par-human capabilities in several domains. \nDespite their practical applications and exceeding user consumption, they are susceptible to jailbreaks when malicious inputs exploit the LM's weaknesses, causing it to deviate from its intended behavior. Current defensive strategies either classify the input prompt as adversarial or prevent LMs from generating harmful outputs. The primary challenge is that the current defense techniques are built against known and established jailbreaking patterns while work poorly against novel attacks. In this research, we propose an end-to-end framework for generating novel attack patterns and demonstrate how the proposed defense approach can generalize over known and unknown attack patterns. Attack patterns are generated using a novel self-learning large language model (LLM)-based multi-agent system with closed loop feedback called ALMAS, which stands for Attack using LLM-based Multi-Agent Systems. We demonstrate that system-prompt attention from Small Language Models (SLMs) can be used to characterize adversarial prompts providing a novel explainable and cheaper defense approach called AttentionDefense. The proposed AttentionDefense is evaluated against existing jailbreak benchmark datasets as well as the novel jailbreaks generated using ALMAS. Ablation studies demonstrate that SLM-based AttentionDefense has equivalent or better jailbreak detection performance as compared to text embedding based classifiers and GPT-4 zero-shot detectors. Our research suggests that the attention mechanism is an integral component in understanding and explaining how LMs respond to malicious inputs that is not captured in the semantic meaning of text embeddings. Additionally, for practical purposes AttentionDefense is an ideal solution as it has the computation requirements of a small LM but the performance of a LLM detector."
    },
    {
        "title": "Towards Making Linear Attention Usable",
        "link_suffix": "/forum?id=y59zhBNKGZ",
        "link": "https://openreview.net/forum?id=y59zhBNKGZ",
        "pdf_link": "https://openreview.net/pdf?id=y59zhBNKGZ",
        "keywords": "Linear Attention, Kernel Separation, Transformers, Memory Reduction",
        "abstract": "The original Transformer attention mechanism, based on Softmax, has time and memory complexities of $O(N^2D)$ and $O(N^2)$, where $N$ is the number of tokens and $D$ the dimension per attention head. As current LLM applications trend towards processing larger token sequences, and Transformers gain popularity in image, video, and audio processing, addressing this quadratic cost becomes imperative. Since the introduction of Transformers, numerous approaches have been proposed to linearize this scaling. One such method is Linear Attention, which captures all-to-all token pair attention in $O(ND^2)$ time. However, its drawback lies in its high memory footprint of $O(ND^2)$. While Linear Attention has shown promise in small-scale benchmarks, the high memory demand has prevented Linear Attention to be studied in context of large benchmarks and practical use cases. In this work, we demonstrate how to reduce the memory complexity to $O(ND)$ by approaching calculations from a novel perspective. Additionally, since Linear Attention does not compute the attention matrix directly, it precludes the use of traditional dropout. To address this, we introduce an alternative dropout mechanism. Our study confirms linear scaling in both wall-clock time and memory usage. We also compare our method with Flash Attention and conduct an ablation study on our proposed dropout alternative."
    },
    {
        "title": "Round and Round We Go! What makes Rotary Positional Encodings useful?",
        "link_suffix": "/forum?id=GtvuNrk58a",
        "link": "https://openreview.net/forum?id=GtvuNrk58a",
        "pdf_link": "https://openreview.net/pdf?id=GtvuNrk58a",
        "keywords": "Large Language Models, Transformers, Positional Encodings, Rotary Positional Encodings",
        "abstract": "Positional Encodings (PEs) are a critical component of Transformer-based Large Language Models (LLMs), providing the attention mechanism with important sequence-position information. One of the most popular types of encoding used today in LLMs are Rotary Positional Encodings (RoPE), that rotate the queries and keys based on their relative distance. A common belief is that RoPE is useful because it helps to decay token dependency as relative distance increases. In this work, we argue that this is unlikely to be the core reason. We study the internals of a trained Gemma 7B model to understand how RoPE is being used at a mechanical level. We find that Gemma learns to use RoPE to construct robust `positional' attention patterns by exploiting the highest frequencies. We also find that, in general, Gemma greatly prefers to use the lowest frequencies of RoPE, which we suspect are used to carry semantic information. We mathematically prove interesting behaviours of RoPE and conduct experiments to verify our findings, proposing a modification of RoPE that fixes some highlighted issues and improves performance. We believe that this work represents an interesting step in better understanding PEs in LLMs, which we believe holds crucial value for scaling LLMs to large sizes and context lengths."
    },
    {
        "title": "On the self-verification limitations of large language models on reasoning and planning tasks",
        "link_suffix": "/forum?id=4O0v4s3IzY",
        "link": "https://openreview.net/forum?id=4O0v4s3IzY",
        "pdf_link": "https://openreview.net/pdf?id=4O0v4s3IzY",
        "keywords": "Large Language Models, Reasoning, Planning, Self-Critique, Verification",
        "abstract": "There has been considerable divergence of opinion on the reasoning abilities of Large Language Models (LLMs).\nWhile the initial optimism that reasoning might emerge automatically with scale has been tempered thanks to a slew of counterexamples--ranging from multiplication to simple planning--there persists a wide spread belief that LLMs can self-critique and improve their own solutions in an iterative fashion.\nThis belief seemingly rests on the assumption that verification of correctness should be easier than generation--a rather classical argument from computational complexity--which should be irrelevant to LLMs to the extent that what they are doing is approximate retrieval.\nIn this paper, we set out to systematically investigate the effectiveness of iterative prompting in the context of reasoning and planning.\nWe present a principled empirical study of the performance of GPT-4 in three domains: Game of 24, Graph Coloring, and STRIPS planning.\nWe experiment both with the model critiquing its own answers and with an external correct reasoner verifying proposed solutions.\nIn each case, we analyze whether the content of criticisms actually affects bottom line performance, and whether we can ablate elements of the augmented system without losing performance. We observe significant performance collapse\nwith self-critique and significant performance gains with sound external verification.\nWe also note that merely re-prompting with a sound verifier maintains most of the benefits of more involved setups."
    },
    {
        "title": "Simulation-Based Inference with Uncertainty Quantification using Generative Models in Quantum Chromodynamics",
        "link_suffix": "/forum?id=Zy7zGe5YfE",
        "link": "https://openreview.net/forum?id=Zy7zGe5YfE",
        "pdf_link": "https://openreview.net/pdf?id=Zy7zGe5YfE",
        "keywords": "Non-differentiable Parameter Inference, GANs, Quantum Correlation Functions",
        "abstract": "Generative and adversarial machine learning methods have been used for parameter inference of physical models from observed data in various works. However, many real-world problems of interest involve non-differentiable models, a context in which many approaches cease to be sufficient. An example of this can be found in quantum chromodynamics, where inferring quantum correlation functions from observed data is hindered by the problem's intrinsic non-differentiability and stochasticity. To overcome this, we present a framework based fundamentally on generative adversarial networks in which parameters are iteratively optimized to generate realistic samples. This framework is novel compared to related works in that it simultaneously circumvents non-differentiability, enables uncertainty quantification, and is free of assumptions on parameters. We demonstrate the utility of this framework in learning synthetic distributions and simulated quantum correlation functions."
    },
    {
        "title": "Parameter Monte Carlo Tree Search: Efficient Chip Placement via Transfer Learning",
        "link_suffix": "/forum?id=iv6Sw43IMl",
        "link": "https://openreview.net/forum?id=iv6Sw43IMl",
        "pdf_link": "https://openreview.net/pdf?id=iv6Sw43IMl",
        "keywords": "Transfer learning, chip placement, reinforcement learning, Monte Carlo Tree Search, finetuning",
        "abstract": "Automated chip placement is an important problem in enhancing the design and effectiveness of computer chips. Previous approaches have employed transfer learning to adapt knowledge obtained via machine learning from one chip placement task to another. However, these approaches have not notably reduced the necessary chip design time, which is crucial for minimizing the total resource utilization. This paper introduces a novel transfer learning approach called Parameter Monte Carlo Tree Search (PMCTS) that utilizes MCTS to transfer the learned knowledge from deep reinforcement learning (RL) models trained on one chip design task to another chip design by searching directly over the model parameters to generate models for efficient chip placement. We employ MCTS to escape the local optima reached by training from scratch and fine-tuning methods. We evaluate our methodology on four chip design tasks from the literature: Ariane, Ariane133, IBM01, and IBM02. Through extensive experiments, we find that our approach can generate models for optimized chip placement in less time than training from scratch and fine-tuning methods when transferring knowledge from complex chip designs to simpler ones."
    },
    {
        "title": "Broadening Target Distributions for Accelerated Diffusion Models via a Novel Analysis Approach",
        "link_suffix": "/forum?id=reZKq6hjOZ",
        "link": "https://openreview.net/forum?id=reZKq6hjOZ",
        "pdf_link": "https://openreview.net/pdf?id=reZKq6hjOZ",
        "keywords": "generative models, denoising diffusion probabilistic model (DDPM), convergence analysis, accelerated methods",
        "abstract": "Accelerated diffusion models hold the potential to significantly enhance the efficiency of standard diffusion processes. Theoretically, these models have been shown to achieve faster convergence rates than the standard $\\mathcal O(1/\\epsilon^2)$ rate of vanilla diffusion models, where $\\epsilon$ denotes the target accuracy. However, current theoretical studies have established the acceleration advantage only for restrictive target distribution classes, such as those with smoothness conditions imposed along the entire sampling path or with bounded support. In this work, we significantly broaden the target distribution classes with a new accelerated stochastic DDPM sampler. In particular, we show that it achieves accelerated performance for three broad distribution classes not considered before. Our first class relies on the smoothness condition posed only to the target density $q_0$, which is far more relaxed than the existing smoothness conditions posed to all $q_t$ along the entire sampling path. Our second class requires only a finite second moment condition, allowing for a much wider class of target distributions than the existing finite-support condition. Our third class is Gaussian mixture, for which our result establishes the first acceleration guarantee. Moreover, among accelerated DDPM type samplers, our results specialized for bounded-support distributions show an improved dependency on the data dimension $d$. Our analysis introduces a novel technique for establishing performance guarantees via constructing a tilting factor representation of the convergence error and utilizing Tweedie's formula to handle Taylor expansion terms. This new analytical framework may be of independent interest."
    },
    {
        "title": "Performing Interpretability Analysis in Federated Learning Context",
        "link_suffix": "/forum?id=agocj3HTTd",
        "link": "https://openreview.net/forum?id=agocj3HTTd",
        "pdf_link": "https://openreview.net/pdf?id=agocj3HTTd",
        "keywords": "Federated Learning, Interpretability, Neural Additive Models, Optimization",
        "abstract": "Federated learning continues to evolve but faces challenges in interpretability and explainability. We introduce a creative approach employing Neural Additive Models (NAMs) within a federated learning framework to address these challenges. These models referred to as Federated Neural Additive Models (FedNAMs), merge the advantages of NAMs, where individual networks concentrate on specific input features, with the decentralized approach of federated learning, ultimately producing interpretable analysis results. This integration enhances privacy by training on local data across multiple devices, thereby minimizing the risks of data centralization and enhancing model robustness and generalizability. FedNAMs maintain detailed feature-specific learning, making them especially valuable in sectors like finance and healthcare. They facilitate training client-specific models that integrate local updates, preserving privacy and reducing centralization concerns. Our studies on various text and image classification tasks, using datasets such as OpenFetch ML Wine, UCI Heart Disease, and Iris, show that FedNAMs deliver strong interpretability with minimal accuracy loss compared to traditional Federated Deep Neural Networks (DNNs). The research involves notable findings, including the identification of key predictive features at the client level as well as at the global level. Volatile acidity, sulfates, and chlorides for wine quality. Chest pain type, maximum heart rate, and number of vessels for heart disease. Petal length and width for iris classification. This approach strengthens privacy and model efficiency and improves interpretability and robustness across diverse datasets. Finally, FedNAMs generate insights on causes of highly and low interpretable features."
    },
    {
        "title": "Rethinking Behavior Regularization in Offline Safe RL: A Region-Based Approach",
        "link_suffix": "/forum?id=GVhfWu5L8D",
        "link": "https://openreview.net/forum?id=GVhfWu5L8D",
        "pdf_link": "https://openreview.net/pdf?id=GVhfWu5L8D",
        "keywords": "Offline Reinforcement Learning; Safe Reinforcement Learning",
        "abstract": "Behavior regularization is a widely adopted technique in offline reinforcement learning (RL) to control distributional shift and mitigate extrapolation errors from out-of-distribution (OOD) actions by keeping the learned policy close to the behavior policy used to collect the dataset. However, directly applying behavior regularization to offline safe RL presents several issues. The optimal policy in safe RL should not only favor actions that prevent the agent from entering unsafe regions but also identify the shortest escape path when the agent finds itself in unsafe states. Enforcing safety and behavior regularization constraints simultaneously is inherently difficult and can often lead to infeasible solutions, especially when multiple constraints are involved. Furthermore, adding behavior regularization may cause the learned policy to imitate the behavior policy, even in states where the behavior policy performs poorly (not safe). This issue becomes particularly severe in offline safe RL, where the quality of the dataset collected by the behavior policy heavily impacts the learned policy\u2019s effectiveness. To address these challenges, we propose $\\textit{BARS}$ ($\\underline{B}$ehavior-$\\underline{A}$ware $\\underline{R}$egion-Based $\\underline{S}$afe offline RL), a novel algorithm that distinguishes between safe and unsafe states and applies region-specific, selective behavior regularization to optimize the policy. Extensive experiments show that BARS significantly outperforms several state-of-the-art baselines in terms of both rewards and safety, particularly in scenarios where the behavior policy is far from optimal. Notably, when dataset quality is low, BARS continues to perform well and ensure safety, while all other baselines fail to guarantee a safe policy in most of the environments. Our work has great potential to address a previously overlooked issue in offline safe RL."
    },
    {
        "title": "Nonlinear Sequence Embedding by Monotone Variational Inequality",
        "link_suffix": "/forum?id=U834XHJuqk",
        "link": "https://openreview.net/forum?id=U834XHJuqk",
        "pdf_link": "https://openreview.net/pdf?id=U834XHJuqk",
        "keywords": "Monotone Variational Inequality, Convex Optimization, Sequence data, Time Series, Representation Learning",
        "abstract": "In the wild, we often encounter collections of sequential data such as electrocardiograms, motion capture, genomes, and natural language, and sequences may be multichannel or symbolic with nonlinear dynamics. We introduce a method to learn low-dimensional representations of nonlinear sequence and time-series data without supervision which has provable recovery guarantees. The learned representation can be used for downstream machine-learning tasks such as clustering and classification. The method assumes that the observed sequences arise from a common domain, with each sequence following its own autoregressive model, and these models are related through low-rank regularization. We cast the problem as a convex matrix parameter recovery problem using monotone variational inequalities (VI) and encode the common domain assumption via low-rank constraint across the learned representations, which can capture the geometry for the entire domain as well as faithful representations for the dynamics of each individual sequence using the domain information in totality. We show the competitive performance of our method on real-world time-series data with baselines and demonstrate its effectiveness for symbolic text modeling and RNA sequence clustering."
    },
    {
        "title": "Star Attention: Efficient LLM Inference over Long Sequences",
        "link_suffix": "/forum?id=KVLnLKjymq",
        "link": "https://openreview.net/forum?id=KVLnLKjymq",
        "pdf_link": "https://openreview.net/pdf?id=KVLnLKjymq",
        "keywords": "Large Language Model, Transformers, Long Context, Efficient Inference, Local and Global Attention",
        "abstract": "Transformer-based Large Language Models (LLMs) with extended context capabilities are becoming increasingly prevalent, with recent models supporting contexts of up to 2 million tokens. However, inference on such long sequences poses significant challenges due to the quadratic complexity of global self-attention. In this paper, we introduce Star Attention, a novel two-phase inference approach designed to handle long sequences more efficiently. In the first phase, the context is encoded using local attention by distributing the self-attention and feedforward computations across multiple devices in blockwise fashion, where each block is prefixed with an \"anchor\" block. In the second phase, global attention is applied to process the query and for auto-regressive output token generation, with all tokens attending to the key-value cache generated during Phase 1. Star Attention can be integrated with most Transformer-based LLMs that utilize global attention, leading to substantial reductions in computational and memory requirements for long-sequence inference. Experiments on Llama-based models show that Star Attention reduces inference time by up to 11x, while maintaining 95-100% of the accuracy achieved by full attention."
    },
    {
        "title": "Single-agent Poisoning Attacks Suffice to Ruin Multi-Agent Learning",
        "link_suffix": "/forum?id=46xYl55hdc",
        "link": "https://openreview.net/forum?id=46xYl55hdc",
        "pdf_link": "https://openreview.net/pdf?id=46xYl55hdc",
        "keywords": "Multi-agent learning, reward poisoning attack, Nash equilibrium, monotone game, convergence, robustness",
        "abstract": "We investigate the robustness of multi-agent learning in strongly monotone games with bandit feedback. While previous research has developed learning algorithms that achieve last-iterate convergence to the unique Nash equilibrium (NE) at a polynomial rate, we demonstrate that all such algorithms are vulnerable to adversaries capable of poisoning even a single agent's utility observations. Specifically, we propose an attacking strategy such that for any given time horizon $T$, the adversary can mislead any multi-agent learning algorithm to converge to a point other than the unique NE with a corruption budget that grows sublinearly in $T$. To further understand the inherent robustness of these algorithms, we characterize the fundamental trade-off between convergence speed and the maximum tolerable total utility corruptions for two example algorithms, including the state-of-the-art one. Our theoretical and empirical results reveal an intrinsic efficiency-robustness trade-off: the faster an algorithm converges, the more vulnerable it becomes to utility poisoning attacks. To the best of our knowledge, this is the first work to identify and characterize such a trade-off in the context of multi-agent learning."
    },
    {
        "title": "Communication-Efficient Federated Low-Rank Update Algorithm and its Connection to Implicit Regularization",
        "link_suffix": "/forum?id=DdPeCRVyCd",
        "link": "https://openreview.net/forum?id=DdPeCRVyCd",
        "pdf_link": "https://openreview.net/pdf?id=DdPeCRVyCd",
        "keywords": "Federated Learning, Communication-Efficient Federated Learning, Low-Rank Nature, Cross-Device Federated Learning",
        "abstract": "Federated Learning (FL) faces significant challenges related to communication efficiency and heterogeneity. To address these issues, we explore the potential of using low-rank updates. Our theoretical analysis reveals that client's loss exhibits a higher rank structure (gradients span higher rank subspaces of Hessian) compared to the server's loss. Based on this insight, we hypothesize that constraining client-side optimization to a low-rank subspace could provide an implicit regularization effect. Consequently, we propose FedLoRU, a general low-rank update framework for FL. Our framework enforces low-rank client-side updates and accumulates these updates to form a higher-rank model. Additionally, variants of FedLoRU can adapt to environments with statistical and model heterogeneity by employing multiple or hierarchical low-rank updates. Experimental results demonstrate that FedLoRU performs comparably to full-rank algorithms and exhibits robustness to heterogeneous and large numbers of clients."
    },
    {
        "title": "Improving Pretraining Data Using Perplexity Correlations",
        "link_suffix": "/forum?id=huuKoVQnB0",
        "link": "https://openreview.net/forum?id=huuKoVQnB0",
        "pdf_link": "https://openreview.net/pdf?id=huuKoVQnB0",
        "keywords": "pretraining data; data selection; natural language processing; statistics; large language models",
        "abstract": "Quality pretraining data is often seen as the key to high-performance language models. However, progress in understanding pretraining data has been slow due to the costly pretraining runs required for data selection experiments. We present a framework that avoids these costs and selects high-quality pretraining data without any LLM training of our own. Our work is based on a simple observation: LLM losses on many pretraining texts are correlated with downstream benchmark performance, and selecting high-correlation documents is an effective pretraining data selection method. We build a new statistical framework for data selection centered around estimates of perplexity-benchmark correlations and perform data selection using a sample of 90 LLMs taken from the Open LLM Leaderboard on texts from tens of thousands of web domains. In controlled pretraining experiments at the 160M parameter scale on 8 benchmarks, our approach outperforms DSIR on every benchmark, while matching the best data selector found in DataComp-LM, a hand-engineered bigram classifier."
    },
    {
        "title": "Counterfactual Learning under Rank Preservation",
        "link_suffix": "/forum?id=8YsP0pBgKA",
        "link": "https://openreview.net/forum?id=8YsP0pBgKA",
        "pdf_link": "https://openreview.net/pdf?id=8YsP0pBgKA",
        "keywords": "Counterfactual Inference, Causal Inference, Identifiability",
        "abstract": "Counterfactual inference aims to estimate the counterfactual outcome given knowledge of an observed treatment and the factual outcome, with broad applications in fields such as epidemiology, econometrics, and management science. In this paper, we propose a principled approach for identifying and estimating the counterfactual outcome.  Specifically, we introduce a simple and intuitive rank preservation assumption to identify the counterfactual outcome without relying on a known structural causal model. Building on this, we propose a novel ideal loss for theoretically unbiased learning of the counterfactual outcome and further develop a kernel-based estimator for its empirical estimation. Our theoretical analysis shows that the proposed ideal loss is convex, and the proposed estimator is unbiased. Extensive semi-synthetic and real-world experiments are conducted to demonstrate the effectiveness of the proposed method."
    },
    {
        "title": "Logic Agent: Enhancing Validity with Logic Rule Invocation",
        "link_suffix": "/forum?id=OZG6MuD3pT",
        "link": "https://openreview.net/forum?id=OZG6MuD3pT",
        "pdf_link": "https://openreview.net/pdf?id=OZG6MuD3pT",
        "keywords": "reasoning, large language models, logic, logic reasoning",
        "abstract": "Chain-of-Thought (CoT) prompting has become a key strategy for enhancing the inferential abilities of large language models (LLMs) in reasoning tasks. However, it often struggles with ensuring reasoning validity and maintaining informativeness. This paper presents the Logic Agent (LA), a novel framework designed to boost the validity of reasoning in LLMs through strategic logic function calls. Distinct from traditional methods, LA converts LLMs into dynamic agents that apply propositional logic rules, transforming natural language inputs into structured logical forms. The agent utilizes a robust suite of predefined functions to guide the reasoning process effectively. This approach can enhance the structured and coherent generation of reasoning outputs, improving their interpretability and logical consistency. Through detailed experiments, we showcase LA's ability to adapt across different LLM sizes, significantly enhancing the accuracy of complex reasoning tasks across various domains."
    },
    {
        "title": "Decoding Intelligence: A Framework for Certifying Knowledge Comprehension in LLMs",
        "link_suffix": "/forum?id=3UB4NaEb1g",
        "link": "https://openreview.net/forum?id=3UB4NaEb1g",
        "pdf_link": "https://openreview.net/pdf?id=3UB4NaEb1g",
        "keywords": "Large Language Models, Reasoning, Information Extraction, Certification",
        "abstract": "Knowledge comprehension capability is an important aspect of human intelligence. As Large Language Models (LLMs) are being envisioned as superhuman\nagents, it is crucial for them to be proficient at knowledge comprehension. However, existing benchmarking studies do not provide consistent, generalizable, and\nformal guarantees on the knowledge comprehension capabilities of LLMs. In\nthis work, we propose the first framework to certify knowledge comprehension in\nLLMs with formal probabilistic guarantees. Our certificates are quantitative - they consist of high-confidence, tight bounds on the probability that a target LLM\ngives the correct answer on any knowledge comprehension prompt sampled from\na distribution. We design and certify novel specifications that precisely represent\ndistributions of knowledge comprehension prompts leveraging knowledge graphs.\nWe certify SOTA LLMs for specifications over the Wikidata5m knowledge graph.\nWe find that the knowledge comprehension capability improves significantly with\nscaling the size of the models."
    },
    {
        "title": "MultiMedia-Agent: A Multimodal Agent for Multimedia Content Generation",
        "link_suffix": "/forum?id=2JN73Z8f9Q",
        "link": "https://openreview.net/forum?id=2JN73Z8f9Q",
        "pdf_link": "https://openreview.net/pdf?id=2JN73Z8f9Q",
        "keywords": "multimodal agent, video generation",
        "abstract": "With the advancement of AIGC (AI-generated content) technologies, an increasing number of generative models are revolutionizing fields such as video editing, music generation, and even film production. However, due to the limitations of current AIGC models, most models can only serve as individual components within specific application scenarios and are not capable of completing tasks end-to-end in real-world applications. In real-world applications, editing experts often work with a wide variety of images and video inputs, producing multimodal outputs---a video typically includes audio, text, and other elements. This level of integration across multiple modalities is something current models are unable to achieve effectively. However, the rise of agent-based systems has made it possible to use AI tools to tackle complex content generation tasks.\nTo deal with the complex scenarios, in this paper, we propose a multimedia content generation agent system designed to automate complex content creation. Our agent system includes a data generation pipeline, a tool library for content creation, and a set of metrics for evaluating preference alignment. Notably, we introduce the skill acquisition theory to model the training data curation and agent training. We designed a two-stage correlation strategy for plan optimization, including self-correlation and model preference correlation. \nAdditionally, we utilized the generated plans to train the MultiMedia-Agent via a three stage approach including base/success plan finetune and preference optimization. The comparison results demonstrate that the our approaches are effective and the MultiMedia-Agent can generate better multimedia content compared to GPT4o."
    },
    {
        "title": "Self-Play Preference Optimization for Language Model Alignment",
        "link_suffix": "/forum?id=a3PmRgAB5T",
        "link": "https://openreview.net/forum?id=a3PmRgAB5T",
        "pdf_link": "https://openreview.net/pdf?id=a3PmRgAB5T",
        "keywords": "self play, preference optimization, large language model, RLHF",
        "abstract": "Standard reinforcement learning from human feedback (RLHF) approaches relying on parametric models like the Bradley-Terry model fall short in capturing the intransitivity and irrationality in human preferences. Recent advancements suggest that directly working with preference probabilities can yield a more accurate reflection of human preferences, enabling more flexible and accurate language model alignment. In this paper, we propose a self-play-based method for language model alignment, which treats the problem as a constant-sum two-player game aimed at identifying the Nash equilibrium policy. Our approach, dubbedSelf-Play Preference Optimization(SPPO), utilizes iterative policy updates to provably approximate the Nash equilibrium. \nAdditionally, we propose a new SPPO objective which is both strongly motivated by theory and is simple and effective in practice.\nIn our experiments, using only 60k prompts (without responses) from the UltraFeedback dataset and without any prompt augmentation, by leveraging a pre-trained preference model PairRM with only 0.4B parameters, SPPO can obtain a model from fine-tuning Mistral-7B-Instruct-v0.2 that achieves the state-of-the-art length-controlled win-rate of 28.53% against GPT-4-Turbo on AlpacaEval 2.0. It also outperforms the (iterative) DPO and IPO on MT-Bench, Arena-Hard, and the Open LLM Leaderboard.\nStarting from a stronger base model Llama-3-8B-Instruct, we are able to achieve a length-controlled win rate of 38.77%.\nNotably, the strong performance of SPPO is achieved without additional external supervision (e.g., responses, preferences, etc.) from GPT-4 or other stronger language models."
    },
    {
        "title": "Protein Language Model Fitness is a Matter of Preference",
        "link_suffix": "/forum?id=UvPdpa4LuV",
        "link": "https://openreview.net/forum?id=UvPdpa4LuV",
        "pdf_link": "https://openreview.net/pdf?id=UvPdpa4LuV",
        "keywords": "protein language models, zero-shot fitness prediction",
        "abstract": "Leveraging billions of years of evolution, scientists have trained protein language models (pLMs) to understand the sequence and structure space of proteins aiding in the design of more functional proteins. Although they have shown ability to improve efficiency in engineering, it remains unclear under what conditions they will succeed or fail. We aim to predict the circumstances in which pLMs can successfully perform zero-shot fitness estimation. Our work demonstrates the trends observed over hundreds of deep mutational scans across multiple different fitness objectives. We find that the likelihood, or abstractly, implicit preference of a certain protein sequence imbued during pretraining is predictive fitness prediction capabilities. Both over-preferred and under-preferred wild type sequences harm performance. Generating a causal link between training data and likelihood, we show a power law tail over what data increases protein likelihood which is tied to training sequence homology. Lastly, proteins of low likelihood can be remedied by unsupervised finetuning. In sum, the zero-shot fitness estimation abilities of pLMs can be predicted by the likelihood of the engineered sequence, thus suggesting when pLMs should be deployed in protein maturation campaigns and a way to improve their performance under circumstances of low likelihood."
    },
    {
        "title": "Optimization Proxies using Limited Labeled Data and Training Time - A Semi-Supervised Bayesian Neural Network Approach",
        "link_suffix": "/forum?id=EXGahWDp1E",
        "link": "https://openreview.net/forum?id=EXGahWDp1E",
        "pdf_link": "https://openreview.net/pdf?id=EXGahWDp1E",
        "keywords": "Optimization Proxy, Semi-supervised Bayesian Neural Networks, Constrained Optimization",
        "abstract": "Constrained optimization problems arise in various engineering system operations such as inventory management and electric power grids. However, the requirement to repeatedly solve such optimization problems with uncertain parameters poses a significant computational challenge. This work introduces a learning scheme using Bayesian Neural Networks (BNNs) to solve constrained optimization problems under limited labeled data and restricted model training times. We propose a semi-supervised BNN for this practical but complex regime, wherein training commences in a sandwiched fashion, alternating between a supervised  learning step (using labeled data) for minimizing cost, and an unsupervised learning step (using unlabeled data) for enforcing constraint feasibility. Both supervised and unsupervised steps use a Bayesian approach, where Stochastic Variational Inference is employed for approximate Bayesian inference. We show that the proposed semi-supervised learning method outperforms conventional BNN and deep neural network (DNN) architectures on important non-convex constrained optimization problems from energy network operations, achieving up to a tenfold reduction in expected maximum equality gap and halving the optimality and inequality (feasibility) gaps, without requiring any correction or projection step. By leveraging the BNN's ability to provide posterior samples at minimal computational cost, we demonstrate that a Selection via Posterior (SvP) scheme can further reduce equality gaps by more than 10%. We also provide tight and practically meaningful probabilistic confidence bounds that can be constructed using a low number of labeled testing data and readily adapted to other applications."
    },
    {
        "title": "Dual-Pathway Neural Networks: Harnessing Scene and Object Pathways for Enhanced Visual Understanding",
        "link_suffix": "/forum?id=gbruScKTJ2",
        "link": "https://openreview.net/forum?id=gbruScKTJ2",
        "pdf_link": "https://openreview.net/pdf?id=gbruScKTJ2",
        "keywords": "scene and object learning, disentangled representations, generalization",
        "abstract": "Standard artificial neural networks (ANNs) often struggle with generalization due to their reliance on surface-level cues, which can lead to suboptimal performance. Drawing inspiration from the distinct processing pathways for scenes and objects in the human brain, we explore the interactions between scene and object and introduce a dual-modality architecture aimed at emulating this cognitive processing mechanism within ANNs. Our approach features separate encodings for scene and object modalities, which are fused to facilitate enhanced visual understanding. By optimizing object recognition and scene reconstruction objectives, our architecture efficiently encodes scene and object information crucial for holistic representation learning. Empirical validation demonstrates significant improvements in generalization, lifelong learning, and adversarial robustness compared to conventional architectures. These findings underscore the potential of integrating biological insights into AI systems to bridge the gap between artificial and biological intelligence."
    }
]