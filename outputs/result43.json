[
    {
        "title": "Value-Incentivized Preference Optimization: A Unified Approach to Online and Offline RLHF",
        "link_suffix": "/forum?id=SQnitDuow6",
        "link": "https://openreview.net/forum?id=SQnitDuow6",
        "pdf_link": "https://openreview.net/pdf?id=SQnitDuow6",
        "keywords": "preference optimization, the principle of optimism/pessimism, RLHF theory",
        "abstract": "Reinforcement learning from human feedback (RLHF) has demonstrated great promise in aligning large language models (LLMs) with human preference. Depending on the availability of preference data, both online and offline RLHF are active areas of investigation. A key bottleneck is understanding how to incorporate uncertainty estimation in the reward function learned from the preference data for RLHF, regardless of how the preference data is collected. While the principles of optimism or pessimism under uncertainty are well-established in standard reinforcement learning (RL), a practically-implementable and theoretically-grounded form amenable to large language models is not yet available, as standard techniques for constructing confidence intervals become intractable under arbitrary policy parameterizations.In this paper, we introduce a unified approach to online and offline RLHF --- value-incentivized preference optimization (VPO) --- which regularizes the maximum-likelihood estimate of the reward function with the corresponding value function, modulated by a sign to indicate whether the optimism or pessimism is chosen. VPO also directly optimizes the policy with implicit reward modeling, and therefore shares a simpler RLHF pipeline similar to direct preference optimization. Theoretical guarantees of VPO are provided for both online and offline settings, matching the rates of their standard RL counterparts. Moreover, experiments on text summarization, dialogue, and standard benchmarks verify the practicality and effectiveness of VPO."
    },
    {
        "title": "Beyond-Expert Performance with Limited Demonstrations: Efficient Imitation Learning with Double Exploration",
        "link_suffix": "/forum?id=FviefuxmeW",
        "link": "https://openreview.net/forum?id=FviefuxmeW",
        "pdf_link": "https://openreview.net/pdf?id=FviefuxmeW",
        "keywords": "Reinforcement learning, imitation learning",
        "abstract": "learning where the goal is to learn a policy that mimics the expert's behavior. In practice, it is often challenging to learn the expert policy from a limited number of demonstrations accurately due to the complexity of the state space. Moreover, it is essential to explore the environment and collect data to achieve beyond-expert performance. To overcome these challenges, we propose a novel imitation learning algorithm namely Imitation Learning with Double Exploration (ILDE), which implements exploration in two aspects: (1) optimistic policy optimization via an exploration bonus that rewards state-action pairs with high uncertainty to potentially improve the convergence to the expert policy, and (2) curiosity-driven exploration of the states that deviate from the demonstration trajectories to potentially yield beyond-expert performance. Empirically, we demonstrate that ILDE outperforms the state-of-the-art imitation learning algorithms in terms of sample efficiency and achieves beyond-expert performance on Atari and MuJoCo tasks with fewer demonstrations than those in previous work. We also provide theoretical justification of ILDE as an uncertainty-regularized policy optimization method with optimistic exploration, leading to a regret growing sublinearly in the number of episodes."
    },
    {
        "title": "Quantile Activation: Correcting a failure mode of ML models",
        "link_suffix": "/forum?id=ZpcQfTNtKv",
        "link": "https://openreview.net/forum?id=ZpcQfTNtKv",
        "pdf_link": "https://openreview.net/pdf?id=ZpcQfTNtKv",
        "keywords": "Machine Learning Foundations, Quantiles, Distribution Shift",
        "abstract": "An established failure mode for machine learning models occurs when the same features are equally likely to belong to class $0$ and class $1$.. In such cases, any ML model cannot to correctly classify the sample. However, a solvable case emerges when the probabilities of class $0$ and $1$ vary with the \"context distribution\". To the best of our knowledge, standard neural network architectures like MLPs or CNNs are not equipped to handle this.In this article, we propose a simple activation function, quantile activation (QACT), that addresses this problem without significantly increasing computational costs. The core idea is to \"adapt\" the outputs of each neuron to itscontext distribution. The proposed quantile activation, QACT, produces the \"relative quantile\" of the sample in its context distribution, rather than the actual values, as in traditional networks.A practical example where the same sample can have different labels arises in cases of inherent distribution shift. We validate the proposed activation function under such shifts, using datasets designed to test robustness against distortionsâ€”CIFAR10C, CIFAR100C, MNISTC, TinyImagenetC. Our results demonstrate significantly better generalization across distortions compared to conventional classifiers, across various architectures. Although this paper presents a proof of concept, we find that this approach unexpectedly outperforms DINOv2 (small) under large distortions, despite DINOv2 being trained with a much larger network and dataset."
    },
    {
        "title": "No Need to Talk: Asynchronous Mixture of Language Models",
        "link_suffix": "/forum?id=pHOH8FVrTp",
        "link": "https://openreview.net/forum?id=pHOH8FVrTp",
        "pdf_link": "https://openreview.net/pdf?id=pHOH8FVrTp",
        "keywords": "language models, distributed learning, divide and conquer, efficient inference",
        "abstract": "We introduce SMALLTALK LM, an innovative method for training a mixture of language models in an almost asynchronous manner. Each\nmodel of the mixture specializes in distinct parts of the data distribution, without the need of high-bandwidth communication between the nodes training each model. At inference, a lightweight router directs a given sequence to a single expert, according to a short prefix. This inference scheme naturally uses a fraction of the parameters from the overall mixture model. Our experiments on language modeling demonstrate that SMALLTALK LM achieves significantly lower perplexity than dense model baselines for the same total training FLOPs and an almost identical inference cost. Finally, in our downstream evaluations we outperform the dense baseline on 75% of the tasks."
    },
    {
        "title": "Self-Informed Generative Active Learning",
        "link_suffix": "/forum?id=0YkZe9nwiC",
        "link": "https://openreview.net/forum?id=0YkZe9nwiC",
        "pdf_link": "https://openreview.net/pdf?id=0YkZe9nwiC",
        "keywords": "Active Learning, Large Language Model, Synthetic Data, Reinforcement Learning",
        "abstract": "Active learning has been a cost-efficient approach to obtaining high-performance AI models with fewer selective annotations. In scenarios where the acquisition of original unlabeled data poses significant challenges, active learning harnessing synthesized data instances is more promising than traditional pool-based methods. In this paper, we propose the Self-Informed Generative Active Learning (SIGnAL) framework as an effective solution to actively generate and select data instances for annotation and downstream model training. In SIGnAL, we propose to guide the data generation based on a reinforcement learning policy, where the generator is self-informed by the reward to generate more informative instances. In addition, we introduce an acquisition function that measures both the informativeness and relevance of instances. Such acquisition function can be transformed to the reward seamlessly for generator optimization. Our experiments on the text classification task validate the effectiveness of our framework, especially when the original data scale is limited."
    },
    {
        "title": "Beyond Cosine Similarity: Introducing the Unified semantic Similarity Metric Benchmark (USMB) for Text Similarity Measurement",
        "link_suffix": "/forum?id=EwRxk3Ho1V",
        "link": "https://openreview.net/forum?id=EwRxk3Ho1V",
        "pdf_link": "https://openreview.net/pdf?id=EwRxk3Ho1V",
        "keywords": "Deep Learning or Neural Networks, Similarity and Distance Learning, (Application) Natural Language and Text Processing, (Cognitive/Neuroscience) Language",
        "abstract": "Text embedding models are increasingly utilized in production across various applications, from Information Retrieval (IR) to document parsing, but relatively little research has been focused on how to best utilize these embeddings for downstream tasks. While cosine similarity, a popular measure of embedding and text similarity, is widely used, it may not be the strongest metric choice for all tasks.  In this work, we introduce the Unified semantic Similarity Metric Benchmark (USMB), a novel leaderboard for text similarity metrics composed of 5 unique tasks and 30+ datasets with the goal of providing a standardized means of measuring the effectiveness of a text similarity metric on a suite of challenging tasks encompassing the nuances of semantic understanding. Additionally, we demonstrate that while cosine similarity achieves the highest score on our benchmark of any pre-existing metric, developing a task-specific ensembled model using our metrics leads to a 40.3% increase in benchmark performance relative to cosine similarity. We hope that through this work, greater attention can be given to potential performance gains through metric selection and that the field's ability to measure semantic similarity advances as a result."
    },
    {
        "title": "Splitted Wavelet Differential Inclusion for neural signal processing",
        "link_suffix": "/forum?id=0gGPVbRqOE",
        "link": "https://openreview.net/forum?id=0gGPVbRqOE",
        "pdf_link": "https://openreview.net/pdf?id=0gGPVbRqOE",
        "keywords": "Wavelet smoothing, differential inclusion, weak signal, signal reconstruction, Parkinson's disease, burst activity",
        "abstract": "Wavelet shrinkage is a powerful tool in neural signal processing. It has been applied to various types of neural signals, such as non-invasive signals and extracellular recordings. For example, in Parkinson's disease (PD), $\\beta$ burst activities in local field potentials (LFP) signals indicated pathological information, which corresponds to \\emph{strong signal} with higher wavelet coefficients. However, it has been found that there also exists \\emph{weak signal} that should not be ignored. This weak signal refers to the set of small coefficients, which corresponds to the non-burst/tonic activity in PD. While it lacks the interpretability of the strong signal, neglecting it may result in the omission of movement-related information during signal reconstruction. However, most existing methods mainly focused on strong signals, while ignoring weak signals. In this paper, we propose \\emph{Splitted Wavelet Differential Inclusion}, which is provable to achieve better estimation of both the strong signal and the whole signal. Equipped with an $\\ell_2$ splitting mechanism, we derive the solution path of a couple of parameters in a newly proposed differential inclusion, of which the sparse one can remove bias in estimating the strong signal and the dense parameter can additionally capture the weak signal with the $\\ell_2$ shrinkage. The utility of our method is demonstrated by the improved accuracy in a numerical experiment and additional findings of tonic activity in PD."
    },
    {
        "title": "Inheritune: Training Smaller Yet More Attentive Language Models",
        "link_suffix": "/forum?id=ob7UrZOJve",
        "link": "https://openreview.net/forum?id=ob7UrZOJve",
        "pdf_link": "https://openreview.net/pdf?id=ob7UrZOJve",
        "keywords": "Large Language Models, Small Language Models, Attention degeneration, Efficient training, Model Initialization",
        "abstract": "Large Language Models (LLMs) have achieved remarkable performance across various natural language processing tasks, primarily due to the transformer architecture and its self-attention mechanism. However, we observe that in standard decoder-style LLMs attention matrices degenerate to single-column for deeper layers. Layers in this state unable to learn anything meaningful and mostly redundant; we refer to these as lazy layers. The goal of this paper is to train smaller models by eliminating this structural inefficiency without compromising performance.Motivated by this observation, we propose Inheritune, a simple yet effective training recipe for developing smaller, high-performing language models. Smaller models trained with Inheritune inherits early transformer layers from a larger pre-trained model, then retrains and progressively expands the smaller model until it matches or exceeds the performance of the larger model. We demonstrate that Inheritune enables the training of various sizes of GPT-2 models on datasets like OpenWebText-9B and FineWeb_Edu. Models trained with Inheritune, despite having significantly fewer layers, match or even surpass the performance of their larger counterparts. For instance, our 16-layer GPT-2 medium variant achieves comparable performance to the standard 24-layer GPT-2 medium model."
    },
    {
        "title": "Relative-Translation Invariant Wasserstein Distance",
        "link_suffix": "/forum?id=CrOHzVtWmH",
        "link": "https://openreview.net/forum?id=CrOHzVtWmH",
        "pdf_link": "https://openreview.net/pdf?id=CrOHzVtWmH",
        "keywords": "Optimal transport theory, Wasserstein distance, Distribution shift",
        "abstract": "In many real-world applications, data distributions are often subject to translation shifts caused by various factors such as changes in environmental conditions, sensor settings, or shifts in data collection practices. These distribution shifts pose a significant challenge for measuring the similarity between probability distributions, particularly in tasks like domain adaptation or transfer learning. To address this issue, we introduce a new family of distances, relative-translation invariant Wasserstein distances ($RW_p$), to measure the similarity of two probability distributions under distribution shift. Generalizing it from the classical optimal transport model, we show that $RW_p$ distances are also real distance metrics defined on the quotient set $\\mathcal{P}_p(\\mathbb{R}^n)/\\sim$ and invariant to distribution translations, which forms a family of new metric spaces. When $p=2$, the $RW_2$ distance enjoys more exciting properties, including decomposability of the optimal transport model and translation-invariance of the $RW_2$ distance. Based on these properties, we show that a distribution shift, measured by $W_2$ distance, can be explained in the bias-variance perspective. In addition, we propose two algorithms: one algorithm is a two-stage optimization algorithm for computing the general case of $RW_p$ distance, and the other is a variant of the Sinkhorn algorithm, named $RW_2$ Sinkhorn algorithm, for efficiently calculating $RW_2$ distance, coupling solutions, as well as $W_2$ distance. We also provide the analysis of numerical stability and time complexity for the proposed algorithms. Finally, we validate the $RW_p$ distance metric and the algorithm performance with two experiments. We conduct one numerical validation for the $RW_2$ Sinkhorn algorithm and demonstrate the effectiveness of using $RW_p$ under distribution shift for similar thunderstorm detection. The experimental results report that our proposed algorithm significantly improves the computational efficiency of Sinkhorn in practical applications, and the $RW_p$ distance is robust to distribution translations."
    },
    {
        "title": "ORSO: Accelerating Reward Design via Online Reward Selection and Policy Optimization",
        "link_suffix": "/forum?id=0uRc3CfJIQ",
        "link": "https://openreview.net/forum?id=0uRc3CfJIQ",
        "pdf_link": "https://openreview.net/pdf?id=0uRc3CfJIQ",
        "keywords": "Reinforcement Learning, Reward Design, Reward Selection",
        "abstract": "Reward shaping is a critical component in reinforcement learning (RL), particularly for complex tasks where sparse rewards can hinder learning. While shaping rewards have been introduced to provide additional guidance, selecting effective shaping functions remains challenging and computationally expensive. This paper introduces Online Reward Selection and Policy Optimization (ORSO), a novel approach that frames shaping reward selection as an online model selection problem. ORSO employs principled exploration strategies to automatically identify promising shaping reward functions without human intervention, balancing exploration and exploitation with provable regret guarantees. We demonstrate ORSO's effectiveness across various continuous control tasks using the Isaac Gym simulator. Compared to traditional methods that fully evaluate each shaping reward function, ORSO significantly improves sample efficiency, reduces computational time, and consistently identifies high-quality reward functions that produce policies comparable to those generated by domain experts through hand-engineered rewards."
    },
    {
        "title": "Risk-Sensitive Variational Actor-Critic: A Model-Based Approach",
        "link_suffix": "/forum?id=irrtPRFksw",
        "link": "https://openreview.net/forum?id=irrtPRFksw",
        "pdf_link": "https://openreview.net/pdf?id=irrtPRFksw",
        "keywords": "reinforcement learning, variational inference, risk sensitive RL",
        "abstract": "Risk-sensitive reinforcement learning (RL) with an entropic risk measure typically requires knowledge of the transition kernel or performs unstable updates w.r.t. exponential Bellman equations. As a consequence, algorithms that optimize this objective have been restricted to tabular or low-dimensional continuous environments. In this work we leverage the connection between the entropic risk measure and the RL-as-inference framework to develop a risk-sensitive variational actor-critic algorithm (rsVAC). Our work extends the variational framework to incorporate stochastic rewards and proposes a variational model-based actor-critic approach that modulates policy risk via a risk parameter.  We consider, both, the risk-seeking and risk-averse regimes and present rsVAC learning variants for each setting.  Our experiments demonstrate that this approach produces risk-sensitive policies and yields improvements in both tabular and risk-aware variants of complex continuous control tasks in MuJoCo."
    },
    {
        "title": "Learning Large Skillsets in Stochastic Settings with Empowerment",
        "link_suffix": "/forum?id=rxeh2tZ8lW",
        "link": "https://openreview.net/forum?id=rxeh2tZ8lW",
        "pdf_link": "https://openreview.net/pdf?id=rxeh2tZ8lW",
        "keywords": "Empowerment, Unsupervised Skill Learning, Unsupervised Reinforcement Learning, Self-Supervised Reinforcement Learning",
        "abstract": "General purpose agents need to be able to execute large skillsets in stochastic settings.  Given that the mutual information between skills and states measures the number of distinct skills in a skillset, a compelling objective for learning a diverse skillset is to find the skillset with the largest mutual information between skills and states.  The problem is that the two main unsupervised approaches for maximizing this mutual information objective, Empowerment-based skill learning and Unsupervised Goal-Conditioned Reinforcement Learning, only maximize loose lower bounds on the mutual information, which can impede diverse skillset learning.  We propose a new empowerment objective, Skillset Empowerment, that maximizes a tighter bound on the mutual information between skills and states.  For any proposed skillset, the tighter bound on mutual information is formed by replacing the posterior distribution of the proposed skillset with a variational distribution that is conditioned on the proposed skillset and trained to match the posterior of the proposed skillset.  Maximizing our mutual information lower bound objective is a bandit problem in which actions are skillsets and the rewards are our mutual information objective, and we optimize this bandit problem with a new actor-critic architecture.  We show empirically that our approach is able to learn large abstract skillsets in stochastic domains, including ones with high-dimensional observations, in contrast to existing approaches."
    },
    {
        "title": "Enhancing Multi-Step Reasoning Abilities of Language Models through Direct Q-Function Optimization",
        "link_suffix": "/forum?id=k2q0rUX2lx",
        "link": "https://openreview.net/forum?id=k2q0rUX2lx",
        "pdf_link": "https://openreview.net/pdf?id=k2q0rUX2lx",
        "keywords": "Alignment, Multi-step RL",
        "abstract": "Reinforcement Learning (RL) plays a crucial role in aligning large language models (LLMs) with human preferences and improving their ability to perform complex tasks. However, current approaches either require significant computational resources due to the use of multiple models and extensive online sampling for training (e.g., PPO) or are framed as bandit problems (e.g., DPO, DRO), which often struggle with multi-step reasoning tasks, such as math problem-solving and complex reasoning that involve long chains of thought. \nTo overcome these limitations, we introduce Direct Q-function Optimization (DQO), which formulates the response generation process as a Markov Decision Process (MDP) and utilizes the soft actor-critic (SAC) framework to optimize a Q-function directly parameterized by the language model. The MDP formulation of DQO offers structural advantages over bandit-based methods, enabling more effective process supervision. \nExperimental results on two math problem-solving datasets, GSM8k and MATH, demonstrate that DQO outperforms previous methods, establishing it as a promising offline reinforcement learning approach for aligning language models."
    },
    {
        "title": "Personalized Federated Fine-tuning for Heterogeneous Data: a Two-Level Low Rank Adaptation Approach",
        "link_suffix": "/forum?id=bWTuOf7ZDR",
        "link": "https://openreview.net/forum?id=bWTuOf7ZDR",
        "pdf_link": "https://openreview.net/pdf?id=bWTuOf7ZDR",
        "keywords": "Federated Learning, Low Rank Adaptation, Heterogenoeus Data, Language Model, Foundation Model",
        "abstract": "We study the personalized federated fine-tuning task with heterogeneous client data in the context of foundation models, where clients collaboratively fine-tune a foundation model (e.g., BERT, GPT) without sharing their local data, achieving personalized models simultaneously. While recent efforts have applied parameter-efficient fine-tuning techniques like low-rank adaptation (LoRA) or training prompts in federated settings, they often overlook data heterogeneity and model personalization.  The primary challenge is that a single common adapter or prompt learner may not suffice for the diverse data of all clients. To address this issue, we propose PF2LoRA, a new personalized federated fine-tuning algorithm based on a novel \\emph{ two-level low rank adaptation framework} on top of LoRA. Given the pretrained foundation model whose weight is frozen, our algorithm aims to learn two levels of adaptation simultaneously: the first level aims to learn a common adapter for all clients, while the second level fosters individual client personalization. This framework explicitly accommodates variations in adapter matrix ranks across clients and introduces minimal additional memory overhead, as the second-level adaptation comprises a small number of parameters compared to the first level. Our experiments on natural language understanding and generation tasks demonstrate that PF2LoRA significantly outperforms existing federated fine-tuning methods."
    },
    {
        "title": "Stochastic Flow Matching for Resolving Small-Scale Physics",
        "link_suffix": "/forum?id=HZxJfzs3w6",
        "link": "https://openreview.net/forum?id=HZxJfzs3w6",
        "pdf_link": "https://openreview.net/pdf?id=HZxJfzs3w6",
        "keywords": "flow matching, diffusion models, multiscale dynamics, misaligned data distributions, superresolution",
        "abstract": "Conditioning diffusion and flow models have proven effective for super-resolving small-scale details in natural images. However, in physical sciences such as weather, super-resolving small-scale details poses significant challenges due to: $(i)$ misalignment between input and output distributions (i.e., solutions to distinct partial differential equations (PDEs) follow different trajectories), $(ii)$ multi-scale dynamics, deterministic dynamics at large scales vs. stochastic at small scales, and $(iii)$ limited data, increasing the risk of overfitting. To address these challenges, we propose encoding the inputs to a \\textit{latent} base distribution that is closer to the target distribution, followed by flow matching to generate small-scale physics. The encoder captures the deterministic components, while flow matching adds stochastic small-scale details. To account for uncertainty in the deterministic part, we inject noise into the encoder's output using an adaptive noise scaling mechanism, which is dynamically adjusted based on maximum-likelihood estimates of the encoderâ€™s predictions. We conduct extensive experiments on both the real-world CWA weather dataset and the PDE-based Kolmogorov dataset, with the CWA task involving super-resolving the weather variables for the region of Taiwan from 25 km to 2 km scales. Our results show that the proposed stochastic flow matching (SFM) framework significantly outperforms existing methods such as conditional diffusion and flows."
    },
    {
        "title": "A Spectral Framework for Assessing the Geodesic Distance Between Graphs",
        "link_suffix": "/forum?id=OPKBPz6Qnz",
        "link": "https://openreview.net/forum?id=OPKBPz6Qnz",
        "pdf_link": "https://openreview.net/pdf?id=OPKBPz6Qnz",
        "keywords": "Graph Theory, Graph Neural Network, Graph Laplacian, Riemannian Manifold, Geodesic, Graph Classification",
        "abstract": "This paper introduces a spectral framework for assessing the generalization of Graph Neural Networks (GNNs) through a new Graph Geodesic Distance (GGD) metric. For two different graphs with the same number of nodes, our framework leverages a spectral graph matching procedure to find node correspondence so that the geodesic distance between them can be subsequently computed by solving a generalized eigenvalue problem associated with their Laplacian matrices. For graphs of different sizes, a resistance-based spectral graph coarsening scheme is introduced to reduce the size of the larger graph while preserving the original spectral properties. We show that the proposed GGD metric can effectively quantify dissimilarities between two graphs by encapsulating their differences in key structural (spectral) properties, such as effective resistances between nodes, cuts, the mixing time of random walks, etc. Through extensive experiments comparing with the state-of-the-art metrics, such as the latest Tree-Mover's Distance (TMD) metric, the proposed GGD metric shows significantly improved performance for graph classification and stability evaluation of GNNs, especially when only partial node features are available."
    },
    {
        "title": "NNetscape Navigator: Complex Demonstrations for Web Agents Without a Demonstrator",
        "link_suffix": "/forum?id=hHF5AayC7O",
        "link": "https://openreview.net/forum?id=hHF5AayC7O",
        "pdf_link": "https://openreview.net/pdf?id=hHF5AayC7O",
        "keywords": "language model agents, large language models, demonstrations for sequential decision making, language conditioned RL, grounded instruction following",
        "abstract": "We introduce NNetscape Navigator (NNetnav), a method for training web agents entirely through synthetic demonstrations. These demonstrations are collected by first interacting with a browser to generate trajectory rollouts, which are then retroactively labeled into instructions using a language model.  Most work on training browser agents has relied on expensive human supervision, and the limited previous work on such \\emph{interaction-first} synthetic data techniques has failed to provide effective search through the exponential space of exploration. In contrast, NNetnav exploits the hierarchical structure of language instructions to make this search more tractable: complex instructions are typically decomposable into simpler subtasks, allowing NNetnav to automatically prune interaction episodes when an intermediate trajectory cannot be annotated with a meaningful sub-task. We use NNetnav demonstrations from a language model for supervised fine-tuning of a smaller language model policy, and find improvements of 6 points on WebArena and over 20 points on MiniWoB++, two popular environments for web-agents. Notably, on WebArena, we observe that language model policies can be further enhanced when fine-tuned with NNetnav demonstrations derived from the \\emph{same} language model. Finally, we collect and release a dataset of over 6k NNetnav demonstrations on WebArena, spanning a diverse and complex set of instructions."
    },
    {
        "title": "Unveiling Causal Relationships Among Candidate Output Tokens in Large Language Models: Towards Interpretability and Control",
        "link_suffix": "/forum?id=6o9QUqUq9f",
        "link": "https://openreview.net/forum?id=6o9QUqUq9f",
        "pdf_link": "https://openreview.net/pdf?id=6o9QUqUq9f",
        "keywords": "large language model (LLM), causal effect, decoding",
        "abstract": "Understanding how large language models (LLMs) generate tokens is crucial for enhancing their performance and interpretability. We hypothesize that cause-effect relationships exist among candidate output tokens during next token prediction in LLMs. Specifically, we propose that certain candidate output tokens---termed \"effect tokens\"---are causally influenced by other candidate tokens activated in earlier layers, referred to as \"cause tokens\". To test this hypothesis, we develop a causal analysis methodology that uncovers these relationships within open-source LLMs. We find that while cause tokens are essential for generating effect tokens, including them in the final output can degrade model performance.Building on these findings, we introduce a decoding algorithm that employs two heuristics: Critical Layer Ablation (CLA), which approximates causal relationships by selectively removing transformer layers and observing their impact on token generation, and Causally-Informed Decoding (CID), which uses the relationships identified by CLA to adjust token probabilities. Specifically, CID increases the probability of selecting effect tokens while decreasing that of cause tokens during generation. Our method achieves measurable accuracy improvements across various benchmark datasets, demonstrating its potential to enhance both the controllability and performance of LLM-generated text."
    },
    {
        "title": "Rethinking Dataset Quantization: Efficient Core Set Selection via Semantically-Aware Data Augmentation",
        "link_suffix": "/forum?id=xajif1l65R",
        "link": "https://openreview.net/forum?id=xajif1l65R",
        "pdf_link": "https://openreview.net/pdf?id=xajif1l65R",
        "keywords": "Coreset Selection, Dataset Quantization, Data Augmentation, Efficient Deep Learning, Semantically-Aware Augmentation",
        "abstract": "Dataset quantization (DQ) is an innovative coreset selection method to choose representative subsets from large-scale datasets, such as ImageNet. Although DQ has made significant progress, it heavily relies on large pre-trained models (like MAEs), leading to substantial additional computational overhead. We first identify that removing this pre-trained MAE model degrades DQâ€™s performance and increases the variance in model training. Where MAE plays a crucial role in introducing prior knowledge and implicit regularization into the training process. Second, we investigate a data augmentation scheme that can simulate the steps of pixel compression and reconstruction in DQ by simply using a randomly initialized ResNet model. This randomly initialized ResNet model can take advantage of the inductive bias of CNNs to locate the semantic object region and then replace the other region with other images. Therefore, we can use a random model or trained model in the early training stage to enhance semantic diversity while selecting important samples. We remove the module that contains the pre-trained MAE model and integrate the data augmentation scheme into the DQ pipeline, which formulates a new simple but efficient method, called DQ v2. Our method achieves performance improvements across multiple datasets, such as ImageNette, CUB-200, and Food-101."
    },
    {
        "title": "L3Ms â€” Lagrange Large Language Models",
        "link_suffix": "/forum?id=ULGbw2URE3",
        "link": "https://openreview.net/forum?id=ULGbw2URE3",
        "pdf_link": "https://openreview.net/pdf?id=ULGbw2URE3",
        "keywords": "LLM, alignment",
        "abstract": "Supervised fine-tuning (SFT) and alignment of large language models (LLMs) are key steps in providing a good user experience. However, the concept of an appropriate alignment is inherently application-dependent, and current methods often rely on heuristic choices to drive the optimization. In this work, we formulate SFT and alignment as a constrained optimization problem, where the LLM is trained on a task while being required to meet application-specific requirements, without resorting to heuristics. To solve this, we propose Lagrange large language models (L3Ms), which employ logarithmic barriers to enforce the constraints. This approach allows for the customization of L3Ms across diverse applications while avoiding heuristic-driven processes. We demonstrate experimentally the versatility and efficacy of L3Ms in achieving tailored alignments for various applications."
    },
    {
        "title": "Dragonfly: Multi-Resolution Zoom-In Encoding Enhances Vision-Language Models",
        "link_suffix": "/forum?id=4FRUNLuY54",
        "link": "https://openreview.net/forum?id=4FRUNLuY54",
        "pdf_link": "https://openreview.net/pdf?id=4FRUNLuY54",
        "keywords": "Multimodel Language Model, Visual Instruction Tuning, Biomedical multimodal model, foundation model",
        "abstract": "Recent advancements in vision-language models (VLMs) have highlighted the benefits of processing images at higher resolutions and leveraging multi-crop features to retain native resolution details. However, current vision transformers (ViTs) often struggle to capture fine-grained details from non-dominant objects, charts, and embedded text, limiting their effectiveness in certain tasks. In this paper, we push beyond the conventional high-resolution and multi-crop techniques by not only preserving but also zooming in past the native resolution of images. This enhancement allows our model to better extract fine-grained details, overcoming the limitations of current ViTs. To manage the increased token count and computational complexity, we show that a simple mean-pooling aggregation over tokens is effective. Our model, Dragonfly, achieves competitive performance on general tasks such as ScienceQA and AI2D, and excels in tasks requiring fine-grained image understanding, including TextVQA and ChartQA. On average, across ten general-domain benchmarks, Dragonfly ranks at the top, outperforming models that are significantly larger or trained on much larger datasets. Notably, Dragonfly sets new benchmarks on several biomedical tasks, achieving 91.6% accuracy on the SLAKE (compared to 84.8% for Med-Gemini) and a 67.1% token F1 score on Path-VQA (compared to 62.7% for Med-PaLM M). On biomedical image captioning tasks, {\\model} attains state-of-the-art results majority of the performance metrics."
    },
    {
        "title": "Locality Alignment Improves Vision-Language Models",
        "link_suffix": "/forum?id=qssVptHTPN",
        "link": "https://openreview.net/forum?id=qssVptHTPN",
        "pdf_link": "https://openreview.net/pdf?id=qssVptHTPN",
        "keywords": "Multimodal language models, vision-language models, locality, alignment, vision transformers",
        "abstract": "Vision language models (VLMs) have seen growing adoption in recent years, but many still struggle with basic spatial reasoning. We hypothesize that this is due to VLMs adopting pre-trained vision backbones, specifically vision transformers (ViTs) trained with image-level supervision and minimal inductive biases. Such models may fail to encode the class contents at each position in the image, and our goal is to resolve this by ensuring that the vision backbone effectively captures both local and global image semantics. Our main insight is that we do not require new supervision to learn this capability -- pre-trained models contain significant knowledge of local semantics that we can extract and use for scalable self-supervision. We propose a new efficient post-training stage for ViTs called locality alignment, and a specific fine-tuning procedure called MaskEmbed that uses a masked reconstruction loss to learn semantic contributions for each image patch. We first evaluate locality alignment with a vision-only benchmark, and we find that it improves a model's performance at a patch-level semantic segmentation task, especially for strong backbones trained with image-caption pairs (e.g., CLIP and SigLIP). We then train a series of VLMs with and without locality alignment, and we find that locality-aligned backbones improve performance across a range of benchmarks, particularly ones that involve spatial understanding (e.g., RefCOCO, OCID-Ref, TallyQA, VSR, AI2D). Overall, we show that we can efficiently learn local semantic extraction via a locality alignment stage, and that this procedure complements existing VLM training recipes that use off-the-shelf vision backbones."
    },
    {
        "title": "FactCheckmate: Preemptively Detecting and Mitigating Hallucinations in LMs",
        "link_suffix": "/forum?id=4ExwvWAy9b",
        "link": "https://openreview.net/forum?id=4ExwvWAy9b",
        "pdf_link": "https://openreview.net/pdf?id=4ExwvWAy9b",
        "keywords": "Large Language Models, Hallucination Detection, Hallucination Mitigation, Factuality",
        "abstract": "Language models (LMs) hallucinate. We inquire: Can we detect and mitigate hallucinations before they happen? This work answers this research question in the positive, by showing that the internal representations of LMs provide rich signals that can be used for this purpose. We introduce FactCheckMate, which preemptively detects hallucinations by learning a classifier that predicts whether the LM will hallucinate, based on the model's hidden states produced over the inputs, before decoding begins. If a hallucination is detected, FactCheckMate then intervenes, by adjusting the LM's hidden states such that the model will produce more factual outputs. FactCheckMate provides fresh insights that the inner workings of LMs can be revealed by their hidden states. Practically, both the detection and mitigation models in FactCheckMate are lightweight, adding little inference overhead; FactCheckMate proves a more efficient approach for mitigating hallucinations compared to many post-hoc alternatives. We evaluate FactCheckMate over LMs of different scales and model families (including Llama, Mistral, and Gemma), across a variety of QA datasets from different domains. Our results demonstrate the effectiveness of leveraging internal representations for early hallucination detection and mitigation, achieving over 70% preemptive detection accuracy. On average, outputs generated by LMs with intervention are 34.4% more factual compared to those without intervention. The average overhead difference in the inference time introduced by FactCheckMate is around 3.16 seconds."
    },
    {
        "title": "Constrained Multi-Objective Optimization",
        "link_suffix": "/forum?id=u6Y0GdTEYp",
        "link": "https://openreview.net/forum?id=u6Y0GdTEYp",
        "pdf_link": "https://openreview.net/pdf?id=u6Y0GdTEYp",
        "keywords": "constrained multi-objective optimization, multi-gradient descent algorithms",
        "abstract": "There is more and more attention on constrained multi-objective optimization (CMOO) problems, however, most of them are based on gradient-free methods. This paper proposes a constraint gradient-based algorithm for multi-objective optimization (MOO) problems based on multi-gradient descent algorithms. We first establish a framework for the CMOO problem. Then, we provide a Moreau envelope-based Lagrange Multiplier (MLM-CMOO) algorithm to solve the formulated CMOO problem, and the convergence analysis shows that the proposed algorithm convergence to Pareto stationary solutions with a rate of $\\mathcal{O}(\\frac{1}{\\sqrt{T}})$. Finally, the MLM-CMOO algorithm is tested on several CMOO problems and has shown superior results compared to some chosen state-of-the-art designs."
    },
    {
        "title": "VerifierQ: Enhancing LLM Test Time Compute with Q-Learning-based Verifiers",
        "link_suffix": "/forum?id=OD9pwKQzXl",
        "link": "https://openreview.net/forum?id=OD9pwKQzXl",
        "pdf_link": "https://openreview.net/pdf?id=OD9pwKQzXl",
        "keywords": "LLM, test time compute, Reinforcement Learning, Q-Learning, verifier",
        "abstract": "Recent advancements in test time compute, particularly through the use of verifier models, have significantly enhanced the reasoning capabilities of Large Language Models (LLMs). This generator-verifier approach closely resembles the actor-critic framework in reinforcement learning (RL). However, current verifier models in LLMs often rely on supervised fine-tuning without temporal difference learning such as Q learning. This paper introduces VerifierQ, a novel approach that integrates Offline Q-learning into LLM verifier models. We address three key challenges in applying Q-learning to LLMs: (1) handling utterance-level Markov Decision Processes (MDPs), (2) managing large action spaces, and (3) mitigating overestimation bias. VerifierQ introduces a modified Bellman update for bounded Q-values, incorporates Implicit Q-learning (IQL) for efficient action space management, and integrates a novel Conservative Q-learning (CQL) formulation for balanced Q-value estimation. Our method enables parallel Q-value computation and improving training efficiency. While recent work has explored RL techniques like MCTS for generators, VerifierQ is among the first to investigate the verifier (critic) aspect in LLMs through Q-learning. This integration of RL principles into verifier models complements existing advancements in generator techniques, potentially enabling more robust and adaptive reasoning in LLMs. Experimental results on mathematical reasoning tasks demonstrate VerifierQ's superior performance compared to traditional supervised fine-tuning approaches, with improvements in accuracy and robustness.  By enhancing the synergy between generation and evaluation capabilities, VerifierQ contributes to the ongoing evolution of AI systems in addressing complex cognitive tasks across various domains."
    }
]