[{"title": "Automatic Task-aware Instruction Optimizer for Black-box LLMs", "link_suffix": "/forum?id=m0ddLnNvXS", "link": "https://openreview.net/forum?id=m0ddLnNvXS", "pdf_link": "https://openreview.net/pdf?id=m0ddLnNvXS", "keywords": "Large Language Models; Instruction Optimization", "abstract": "Large Language Models (LLMs) have demonstrated superior capabilities in terms of solving various real-world tasks. However, their performance and generated content quality heavily depend on task-relevant instructions, which makes instruction optimization a challenging but critical direction to explore. In particular, as practitioners generally cannot access black-box (or API) LLMs' internal parameters and gradient information, it consequently makes instruction optimization for black-box LLMs especially non-trivial. Existing methods for optimizing black-box LLM instructions mainly focus on in-context learning using manually designed or heuristic disciplines, which can be insufficient due to the extreme complexity of modern black-box LLMs that can contain hundreds of billions of parameters. \nTo address these challenges, we propose a novel automatic instruction optimization framework named Automatic Instruction Optimizer (AIO). AIO is designed to perceive target task information and adaptively adjust its task-aware instructing strategy for a task-solver black-box LLM. \nBy leveraging a white-box LLM with parameter fine-tuning for enhanced representation power, AIO can automatically update its instructing strategy based on the feedback from task-solver black-box LLM. \nTo achieve this goal, AIO adopts a novel LLM parameter fine-tuning process powered by zeroth-order gradient approximation and Contextual Bandit techniques, which can effectively and efficiently help address the challenge of inaccessible black-box LLM internal parameters and gradients, as well as help alleviate expensive API cost concerns by flexibly reusing collected black-box LLM feedback.\nExtensive empirical evaluations are presented to demonstrate properties of our proposed AIO, and its effectiveness in comparison with strong baselines.", "title_embedding_index": 15450, "title_abs_embedding_index": 15475}, {"title": "Efficient Privacy-Preserving Federated Learning With Selective Parameter Encryption", "link_suffix": "/forum?id=VT2R3UCcBL", "link": "https://openreview.net/forum?id=VT2R3UCcBL", "pdf_link": "https://openreview.net/pdf?id=VT2R3UCcBL", "keywords": "Federated learning, privacy, homomorphic encryption, inversion attack", "abstract": "Federated learning trains machine learning models on distributed devices by aggregating local model updates instead of local data. However, privacy concerns arise as aggregating local model updates on the server may reveal sensitive personal information by inversion attacks. Privacy-preserving methods, such as homomorphic encryption (HE), then become necessary for FL training. Despite HE's privacy advantages, its applications suffer from impractical overheads, especially for foundation models. In this paper, we present the first practical privacy-preserving federated learning work with efficient HE-based secure model aggregation. Our approach proposes to selectively encrypt sensitive parameters, significantly reducing both computation and communication overheads during training while providing quantifiable privacy guarantee. Our optimization shows considerable overhead reduction, particularly for large foundation models (e.g. \n100x reduction for GPT-2), demonstrating the potential for scalable HE-based FL deployment.", "title_embedding_index": 15451, "title_abs_embedding_index": 15476}, {"title": "Robust Training of Neural Networks at Arbitrary Precision and Sparsity", "link_suffix": "/forum?id=i880EAXJ2x", "link": "https://openreview.net/forum?id=i880EAXJ2x", "pdf_link": "https://openreview.net/pdf?id=i880EAXJ2x", "keywords": "quantization, sparse, quantized matrix multiplication", "abstract": "The discontinuous operations inherent in quantization and sparsification introduce obstacles to backpropagation. This is particularly challenging when training deep neural networks in ultra-low precision and sparse regimes. We propose a novel, robust, and universal solution: a denoising affine transform that stabilizes training under these challenging conditions. By formulating quantization and sparsification as perturbations during training, we derive a perturbation-resilient approach based on ridge regression. Our solution employs a piecewise constant backbone model to ensure a performance lower bound and features an inherent noise reduction mechanism to mitigate perturbation-induced corruption. This formulation allows existing models to be trained at arbitrarily low precision and sparsity levels with off-the-shelf recipes. Furthermore, our method provides a novel perspective on training temporal binary neural networks, contributing to ongoing efforts to narrow the gap between artificial and biological neural networks.", "title_embedding_index": 15452, "title_abs_embedding_index": 15477}, {"title": "Learning-Augmented Robust Algorithmic Recourse", "link_suffix": "/forum?id=KCr2eoSiKF", "link": "https://openreview.net/forum?id=KCr2eoSiKF", "pdf_link": "https://openreview.net/pdf?id=KCr2eoSiKF", "keywords": "Algorithmic Recourse, Learning-Augmented, AI Explainability", "abstract": "The widespread use of machine learning models in high-stakes domains can have a major negative impact, especially on individuals who receive undesirable outcomes. Algorithmic recourse provides such individuals with suggestions of minimum-cost improvements they can make to achieve a desirable outcome in the future. However, machine learning models often get updated over time and this can cause a recourse to become invalid (i.e., not lead to the desirable outcome). The robust recourse literature aims to choose recourses less sensitive, even against adversarial model changes, but this comes at a higher cost. To overcome this obstacle, we initiate the study of algorithmic recourse through the learning-augmented framework and evaluate the extent to which a designer equipped with a prediction regarding future model changes can reduce the cost of recourse when the prediction is accurate (consistency) while also limiting the cost even when the prediction is inaccurate (robustness). We propose a novel algorithm for this problem, study the robustness-consistency trade-off, and analyze how prediction accuracy affects performance.", "title_embedding_index": 15453, "title_abs_embedding_index": 15478}, {"title": "Enhancing the Scalability and Applicability of Kohn-Sham Hamiltonians for Molecular Systems", "link_suffix": "/forum?id=twEvvkQqPS", "link": "https://openreview.net/forum?id=twEvvkQqPS", "pdf_link": "https://openreview.net/pdf?id=twEvvkQqPS", "keywords": "AI for Science, Quantum Chemistry, EGNN", "abstract": "Density Functional Theory (DFT) is a pivotal method within quantum chemistry and materials science, with its core involving the construction and solution of the Kohn-Sham Hamiltonian. Despite its importance, the application of DFT is frequently limited by the substantial computational resources required to construct the Kohn-Sham Hamiltonian. In response to these limitations, current research has employed deep learning models to efficiently predict molecular and solid Hamiltonians, with roto-translational symmetries encoded in their neural networks. However, the robustness of prior models may be problematic, especially when applied to molecules with large numbers of atoms, resulting in non-physical predictions of ground-state properties. In this study, we generate a substantially larger training data set (PubChemQH) and use it to create a scalable model for DFT calculations attempting to predict molecular Hamiltonians with physical accuracy. For this model, we introduce a loss function derived from physical principles, which we call Wavefunction Alignment Loss (WALoss). WALoss involves performing a basis change on the predicted Hamiltonian to align it with the observed one; thus, the resulting differences can serve as a surrogate for orbital energy differences, allowing us to make more precise predictions of total energies and molecular orbitals than previously possible. WALoss also substantially accelerates self-consistent-field (SCF) calculations. Here, we show it achieves a reduction in total energy prediction error by a factor of 1347 and an SCF calculation speed-up by a factor of 18%. These substantial improvements set new benchmarks for achieving accurate and applicable predictions in larger molecular systems.", "title_embedding_index": 15454, "title_abs_embedding_index": 15479}, {"title": "Selective Task Group Updates for Multi-Task Optimization", "link_suffix": "/forum?id=EdNSQHaaMR", "link": "https://openreview.net/forum?id=EdNSQHaaMR", "pdf_link": "https://openreview.net/pdf?id=EdNSQHaaMR", "keywords": "Multi-Task Learning, Multi-Task Optimization, Proximal Inter-Task Affinity", "abstract": "Multi-task learning enables the acquisition of task-generic knowledge by training multiple tasks within a unified architecture. However, training all tasks together in a single architecture can lead to performance degradation, known as negative transfer, which is a main concern in multi-task learning. Previous works have addressed this issue by optimizing the multi-task network through gradient manipulation or weighted loss adjustments. However, their optimization strategy focuses on addressing task imbalance in shared parameters, neglecting the learning of task-specific parameters. As a result, they show limitations in mitigating negative transfer, since the learning of shared space and task-specific information influences each other during optimization. To address this, we propose a different approach to enhance multi-task performance by selectively grouping tasks and updating them for each batch during optimization. We introduce an algorithm that adaptively determines how to effectively group tasks and update them during the learning process. To track inter-task relations and optimize multi-task networks simultaneously, we propose proximal inter-task affinity, which can be measured during the optimization process. We provide a theoretical analysis on how dividing tasks into multiple groups and updating them sequentially significantly affects multi-task performance by enhancing the learning of task-specific parameters. Our methods substantially outperform previous multi-task optimization approaches and are scalable to different architectures and various numbers of tasks.", "title_embedding_index": 15455, "title_abs_embedding_index": 15480}, {"title": "Toto: Time Series Optimized Transformer for Observability", "link_suffix": "/forum?id=ayupWYA1qD", "link": "https://openreview.net/forum?id=ayupWYA1qD", "pdf_link": "https://openreview.net/pdf?id=ayupWYA1qD", "keywords": "Time Series Forecasting, Proportional Factorized Attention, Observability Metrics", "abstract": "We introduce the Time Series Optimized Transformer for Observability (Toto), a foundation model designed for time series forecasting with a focus on observability metrics. Toto features a novel proportional factorized attention mechanism and a Student-T mixture model head, enabling it to efficiently handle high-dimensional, sparse, and non-stationary data. Trained on one trillion time series data points, including 75% proprietary observability data, Toto demonstrates state-of-the-art zero-shot performance on standard benchmarks such as electricity and weather forecasting. Furthermore, it significantly outperforms existing models in observability-specific tasks, making it an ideal solution for real-time system monitoring and anomaly detection. Toto\u2019s architectural innovations make it a versatile tool for both general-purpose forecasting and domain-specific applications, setting a new benchmark for scalability and accuracy in time series analysis.", "title_embedding_index": 15456, "title_abs_embedding_index": 15481}, {"title": "AugGen: Generative Synthetic Augmentation Can Boost Face Recognition", "link_suffix": "/forum?id=hWRc2L2hc5", "link": "https://openreview.net/forum?id=hWRc2L2hc5", "pdf_link": "https://openreview.net/pdf?id=hWRc2L2hc5", "keywords": "Generative Model, Face Recognition, Synthetic Data", "abstract": "As machine learning increasingly relies on large amounts of data, concerns about privacy and ethics have grown. Recently, methods for generating synthetic data to augment or replace real datasets have emerged to mitigate these concerns. In this paper, we demonstrate improved performance on a discriminative task when training on a mix of real and synthetic data, compared to training solely on the original real data. Our synthetic data is generated using a novel sampling method based on a conditional generative model and a discriminator, both trained exclusively on the original data, with no need for auxiliary data nor pre-trained foundation models. We consider the challenging task of face recognition, which is well known for its privacy and ethical issues. Using our augmented dataset, we demonstrate consistent improvements over the model trained on the original dataset, on various benchmarks including IJB-C and IJB-B by up to 5% while performing competitively with state-of-the-art synthetic data generation.", "title_embedding_index": 15457, "title_abs_embedding_index": 15482}, {"title": "Speculative RAG: Enhancing Retrieval Augmented Generation through Drafting", "link_suffix": "/forum?id=xgQfWbV6Ey", "link": "https://openreview.net/forum?id=xgQfWbV6Ey", "pdf_link": "https://openreview.net/pdf?id=xgQfWbV6Ey", "keywords": "generative model, retrieval augmented generation", "abstract": "Retrieval augmented generation (RAG) combines the generative abilities of large language models (LLMs) with external knowledge sources to provide more accurate and up-to-date responses. Recent RAG advancements focus on improving retrieval outcomes through iterative LLM refinement or self-critique capabilities acquired through additional instruction tuning of LLMs. In this work, we introduce Speculative RAG - a framework that leverages a larger generalist LM to efficiently verify multiple RAG drafts produced in parallel by a smaller, distilled specialist LM. Each draft is generated from a distinct subset of retrieved documents, offering diverse perspectives on the evidence while reducing input token counts per draft. This approach enhances comprehension of each subset and mitigates potential position bias over long context. Our method accelerates RAG by delegating drafting to the smaller specialist LM, with the larger generalist LM performing a single verification pass over the drafts. Extensive experiments demonstrate that Speculative RAG achieves state-of-the-art performance with reduced latency on TriviaQA, MuSiQue, PopQA, PubHealth, and ARC-Challenge benchmarks. It notably enhances accuracy by up to 12.97% while reducing latency by 50.83% compared to conventional RAG systems on PubHealth.", "title_embedding_index": 15458, "title_abs_embedding_index": 15483}, {"title": "Lottery Ticket Adaptation: Mitigating Destructive Interference in LLMs", "link_suffix": "/forum?id=qOqCXEXsX4", "link": "https://openreview.net/forum?id=qOqCXEXsX4", "pdf_link": "https://openreview.net/pdf?id=qOqCXEXsX4", "keywords": "catastrophic forgetting, adapter, finetuning, model merging, sparsity, lottery ticket", "abstract": "Existing methods for adapting large language models (LLMs) to new tasks are not suited to multi-task adaptation because they modify all the model weights--causing destructive interference between tasks. The resulting effects, such as catastrophic forgetting of earlier tasks, make it challenging to obtain good performance on multiple tasks at the same time. \nTo mitigate this, we propose Lottery Ticket Adaptation (LoTA), a sparse adaptation method that identifies and optimizes only a sparse subnetwork of the model. We evaluate LoTA on a wide range of challenging tasks such as instruction following, reasoning, math, and summarization. LoTA obtains better performance than full fine-tuning and low-rank adaptation (LoRA), and maintains good performance even after training on other tasks -- thus, avoiding catastrophic forgetting. By extracting and fine-tuning over \\emph{lottery tickets} (or \\emph{sparse task vectors}), LoTA also enables model merging over highly dissimilar tasks.", "title_embedding_index": 15459, "title_abs_embedding_index": 15484}, {"title": "Collaborative Hybrid Propagator for Temporal Misalignment in Audio-Visual Segmentation", "link_suffix": "/forum?id=yqJoqtUwSI", "link": "https://openreview.net/forum?id=yqJoqtUwSI", "pdf_link": "https://openreview.net/pdf?id=yqJoqtUwSI", "keywords": "audio-visual video segmentation", "abstract": "Audio-visual video segmentation (AVVS) aims to generate pixel-level maps of sound-producing objects that accurately align with the corresponding audio. However, existing methods often face temporal misalignment, where audio cues and segmentation results are not temporally coordinated. Audio provides two critical pieces of information: i) target object-level details and ii) the timing of when objects start and stop producing sounds. Current methods focus more on object-level information but neglect the boundaries of audio semantic changes, leading to temporal misalignment. To address this issue, we propose a Collaborative Hybrid Propagator Framework~(Co-Prop). This framework includes two main steps: Preliminary Audio Boundary Anchoring and Frame-by-Frame Audio-Insert Propagation. To Anchor the audio boundary, we employ retrieval-assist prompts with Qwen large language models to identify control points of audio semantic changes. These control points split the audio into semantically consistent audio portions. After obtaining the control point lists, we propose the Audio Insertion Propagator to process each audio portion using a frame-by-frame audio insertion propagation and matching approach. We curated a compact dataset comprising diverse source conversion cases and devised a metric to assess alignment rates. Compared to traditional simultaneous processing methods, our approach reduces memory requirements and facilitates frame alignment. Experimental results demonstrate the effectiveness of our approach across three datasets and two backbones. Furthermore, our method can be integrated with existing AVVS approaches, offering plug-and-play functionality to enhance their performance.", "title_embedding_index": 15460, "title_abs_embedding_index": 15485}, {"title": "Diffusion Models Learn Low-Dimensional Distributions via Subspace Clustering", "link_suffix": "/forum?id=Bon3TPZOG0", "link": "https://openreview.net/forum?id=Bon3TPZOG0", "pdf_link": "https://openreview.net/pdf?id=Bon3TPZOG0", "keywords": "diffusion models, mixture of low-rank Gaussians, denoising autoencoder, phase transition", "abstract": "Recent empirical studies have demonstrated that diffusion models can effectively learn the image distribution and generate new samples. Remarkably, these models can achieve this even with a small number of training samples despite a large image dimension, circumventing the curse of dimensionality. In this work, we provide theoretical insights into this phenomenon by leveraging key empirical observations: (i) the low intrinsic dimensionality of image data, (ii) a union of manifold structure of image data, and (iii) the low-rank property of the denoising autoencoder in trained diffusion models. These observations motivate us to assume the underlying data distribution of image data as a mixture of low-rank Gaussians and to parameterize the denoising autoencoder as a low-rank model according to the score function of the assumed distribution. With these setups, we rigorously show that optimizing the training loss of diffusion models is equivalent to solving the canonical subspace clustering problem over the training samples. Based on this equivalence, we further show that the minimal number of samples required to learn the underlying distribution scales linearly with the intrinsic dimensions under the above data and model assumptions. This insight sheds light on why diffusion models can break the curse of dimensionality and exhibit the phase transition in learning distributions. Moreover, we empirically establish a correspondence between the subspaces and the semantic representations of image data, facilitating image editing. We validate these results with corroborated experimental results on both simulated distributions and image datasets.", "title_embedding_index": 15461, "title_abs_embedding_index": 15486}, {"title": "Algorithmic Stability Based Generalization Bounds for Adversarial Training", "link_suffix": "/forum?id=2GwMazl9ND", "link": "https://openreview.net/forum?id=2GwMazl9ND", "pdf_link": "https://openreview.net/pdf?id=2GwMazl9ND", "keywords": "algorithmic stability, generalization, adversarial training", "abstract": "In this paper, we present a novel stability analysis of adversarial training and prove generalization upper bounds in terms of an expansiveness property of adversarial perturbations used during training and used for evaluation. These expansiveness parameters appear not only govern the vanishing rate of the generalization error but also govern its scaling constant. Our proof techniques do not rely on artificial assumptions of the adversarial loss, as are typically used in previous works. Our bound attributes the robust overfitting in PGD-based adversarial training to the sign function used in the PGD attack, resulting in a bad expansiveness parameter. The peculiar choice of sign function in the PGD attack appears to impact adversarial training both in terms of (inner) optimization and in terms of generalization, as shown in this work. This aspect has been largely overlooked to date. Going beyond the sign-function based PGD attacks, we further show that poor expansiveness properties exist in a wide family of PGD-like iterative attack algorithms, which may highlight an intrinsic difficulty in adversarial training.", "title_embedding_index": 15462, "title_abs_embedding_index": 15487}, {"title": "Guaranteed Neural PDE Boundary Control with Neural Barrier Function", "link_suffix": "/forum?id=LKUVlhjgOw", "link": "https://openreview.net/forum?id=LKUVlhjgOw", "pdf_link": "https://openreview.net/pdf?id=LKUVlhjgOw", "keywords": "PDE boundary control, safe control, learning for control", "abstract": "The physical world dynamics are generally governed by underlying partial derivative equations (PDEs) with unknown analytical forms in science and engineering problems. Neural network based data-driven approaches have been heavily studied in simulating and solving PDE problems in recent years, but it is still challenging to move forward from understanding to controlling the unknown PDE dynamics. PDE boundary control instantiates a simplified but important problem by only focusing on PDE boundary conditions as the control input and output. However, current model-free PDE controllers cannot ensure the boundary output satisfies some given user-specified safety constraint. To this end, we propose a safety filtering framework to guarantee the boundary output stays within the safe set for current model-free controllers. Specifically, we first introduce a general neural boundary control barrier function (BCBF) to ensure the feasibility of the trajectory-wise constraint satisfaction of boundary output. Based on a neural operator modeling the transfer function from boundary control input to output trajectories, we show that the change in the BCBF depends linearly on the change in input boundary, so \nquadratic programming-based safety filtering can be done for pre-trained model-free controllers. Extensive experiments under challenging hyperbolic, parabolic and Navier-Stokes PDE dynamics environments validate the effectiveness of the proposed method in achieving better general performance and boundary constraint satisfaction compared to the model-free controller baselines.", "title_embedding_index": 15463, "title_abs_embedding_index": 15488}, {"title": "Tracking objects that change in appearance with phase synchrony", "link_suffix": "/forum?id=m2gVfgWYDO", "link": "https://openreview.net/forum?id=m2gVfgWYDO", "pdf_link": "https://openreview.net/pdf?id=m2gVfgWYDO", "keywords": "Object tracking, human psychophysics, computational neuroscience", "abstract": "Objects we encounter often change appearance as we interact with them. Changes in illumination (shadows), object pose, or movement of nonrigid objects can drastically alter available image features. How do biological visual systems track objects as they change? It may involve specific attentional mechanisms for reasoning about the locations of objects independently of their appearances \u2014 a capability that prominent neuroscientific theories have associated with computing through neural synchrony. We computationally test the hypothesis that the implementation of visual attention through neural synchrony underlies the ability of biological visual systems to track objects that change in appearance over time. We first introduce a novel deep learning circuit that can learn to precisely control attention to features separately from their location in the world through neural synchrony: the complex-valued recurrent neural network (CV-RNN). Next, we compare object tracking in humans, the CV-RNN, and other deep neural networks (DNNs), using FeatureTracker: a large-scale challenge that asks observers to track objects as their locations and appearances change in precisely controlled ways. While humans effortlessly solved FeatureTracker, state-of-the-art DNNs did not. In contrast, our CV-RNN behaved similarly to humans on the challenge, providing a computational proof-of-concept for the role of phase synchronization as a neural substrate for tracking appearance-morphing objects as they move about.", "title_embedding_index": 15464, "title_abs_embedding_index": 15489}, {"title": "GSE: Group-wise Sparse and Explainable Adversarial Attacks", "link_suffix": "/forum?id=d54fIsAbff", "link": "https://openreview.net/forum?id=d54fIsAbff", "pdf_link": "https://openreview.net/pdf?id=d54fIsAbff", "keywords": "Sparse Adversarial Attack, Quasinorm Regularization, Nesterov Accelerated Gradient", "abstract": "Sparse adversarial attacks fool deep neural networks (DNNs) through minimal pixel perturbations, often regularized by the $\\ell_0$ norm. Recent efforts have replaced this norm with a structural sparsity regularizer, such as the nuclear group norm, to craft group-wise sparse adversarial attacks. The resulting perturbations are thus explainable and hold significant practical relevance, shedding light on an even greater vulnerability of DNNs. However, crafting such attacks poses an optimization challenge, as it involves computing norms for groups of pixels within a non-convex objective. We address this by presenting a two-phase algorithm that generates group-wise sparse attacks within semantically meaningful areas of an image. Initially, we optimize a quasinorm adversarial loss using the $1/2$-quasinorm proximal operator tailored for non-convex programming. Subsequently, the algorithm transitions to a projected Nesterov's accelerated gradient descent with $2$-norm regularization applied to perturbation magnitudes. Rigorous evaluations on CIFAR-10 and ImageNet datasets demonstrate a remarkable increase in group-wise sparsity, e.g., $50.9%$ on CIFAR-10 and $38.4%$ on ImageNet (average case, targeted attack). This performance improvement is accompanied by significantly faster computation times, improved interpretability, and a $100%$ attack success rate.", "title_embedding_index": 15465, "title_abs_embedding_index": 15490}, {"title": "Unleashing the Potential of Text-attributed Graphs: Automatic Relation Decomposition via Large Language Models", "link_suffix": "/forum?id=kJ2dAv7jKy", "link": "https://openreview.net/forum?id=kJ2dAv7jKy", "pdf_link": "https://openreview.net/pdf?id=kJ2dAv7jKy", "keywords": "large language models (LLM), pretrained language models (PLM), text attributed graphs (TAG), graph neural networks (GNN)", "abstract": "Text-attributed graphs (TAGs) integrate textual information with graph structures, offering unique opportunities for leveraging language models to enhance node feature quality. However, our extensive analysis reveals that the downstream task performance on TAGs is hindered by the graph structure itself; treating diverse semantics(e.g. \u201cadvised by\u201d, \u201cparticipates in\u201d) as a singular relation (e.g. hyperlinks). By decomposing conventional edges into distinctive semantic relations, we discover significant improvement in GNNs\u2019 downstream task performance. Motivated by this, we presentRoSE(Relation-oriented Semantic Edge-decomposition), a novel framework that leverages large language models (LLMs) to automatically decompose graph structures into different semantic relations without requiring expensive human labelling or domain expertise.RoSEconsists of two stages: (1) identifying semantic relations via an LLM-based generator and discriminator, and (2) decomposing each edge into corresponding relations by analyzing raw textual contents associated with connected nodes via an LLM-based decomposer. The decomposed edges provided by our framework can be applied in a model-agnostic, plug-and-play manner, enhancing its versatility. Moreover,RoSEachieve state-of-the-art node classification results on various benchmarks and GNN architectures.", "title_embedding_index": 15466, "title_abs_embedding_index": 15491}, {"title": "Dense Video Object Captioning from Disjoint Supervision", "link_suffix": "/forum?id=auZZ2gN0ZN", "link": "https://openreview.net/forum?id=auZZ2gN0ZN", "pdf_link": "https://openreview.net/pdf?id=auZZ2gN0ZN", "keywords": "object captioning, video, tracking", "abstract": "We propose a new task and model for dense video object captioning -- detecting, tracking and captioning trajectories of objects in a video. This task unifies spatial and temporal localization in video, whilst also requiring fine-grained visual understanding that is best described by natural language. We propose a unified model, and demonstrate how our end-to-end approach is more accurate and temporally coherent than a multi-stage pipeline combining state-of-the-art detection, tracking, and captioning models. Moreover, we propose a training strategy based on a mixture of disjoint tasks, which allows us to leverage diverse, large-scale datasets which supervise different parts of our model. Although each pretraining task only provides weak supervision, they are complementary and, when combined, result in noteworthy zero-shot ability and serve as strong initialization for additional finetuning to further improve accuracy. We carefully design new metrics capturing all components of our task, and show how we can repurpose existing video grounding datasets (e.g. VidSTG and VLN) for our new task. We show that our model improves upon a number of strong baselines for this new task. Furthermore, we can apply our model to the task of spatial grounding, outperforming prior state-of-the-art on VidSTG and VLN, without explicitly training for it. Full code is provided in the supplement.", "title_embedding_index": 15467, "title_abs_embedding_index": 15492}, {"title": "Faster Cascades via Speculative Decoding", "link_suffix": "/forum?id=vo9t20wsmd", "link": "https://openreview.net/forum?id=vo9t20wsmd", "pdf_link": "https://openreview.net/pdf?id=vo9t20wsmd", "keywords": "Cascades, Speculative Decoding, Speculative execution, LLM, Inference, Adaptive Inference", "abstract": "Cascades and speculative decoding are two common approaches to improving language models' inference efficiency.  Both approaches involve interleaving models of different sizes,  but via fundamentally distinct mechanisms: cascades employ a deferral rule that invokes the larger model only for \"hard\" inputs, while speculative decoding uses speculative execution to primarily invoke the larger model in parallel verification mode. These mechanisms offer different benefits: empirically, cascades offer better cost-quality trade-offs, often even outperforming the large model, while theoretically, speculative decoding offers a guarantee of quality-neutrality. In this paper, we leverage the best of both these approaches by designing new speculative cascading techniques that implement their deferral rule through speculative execution. We characterize the optimal deferral rule for our speculative cascades, and employ a plug-in approximation to the optimal rule.  Experiments with Gemma and T5 models on a range of language benchmarks show that our approach yields better cost quality trade-offs than cascading and speculative decoding baselines.", "title_embedding_index": 15468, "title_abs_embedding_index": 15493}, {"title": "CoTFormer: A Chain of Thought Driven Architecture with Budget-Adaptive Computation Cost at Inference", "link_suffix": "/forum?id=7igPXQFupX", "link": "https://openreview.net/forum?id=7igPXQFupX", "pdf_link": "https://openreview.net/pdf?id=7igPXQFupX", "keywords": "language models, adaptive compute, chain of thought", "abstract": "Scaling language models to larger and deeper sizes has led to significant boosts in performance. Even though the size of these models limits their application in compute-constrained environments, the race to continually develop ever larger and deeper foundational models is underway. At the same time---regardless of the model size---task-specific techniques continue to play a pivotal role in achieving optimal downstream performance. One of these techniques, called Chain-of-Thought (CoT), is particularly interesting since, as we point out in this work, it resembles employing a deeper transformer through re-applying the model multiple times. However, a key subtlety in computing the attention of past tokens differentiates CoT from simply applying the model several times. Based on this insight, we propose CoTFormer, a novel architecture which closely mimics CoT at the token level, allowing us to obtain significantly improved accuracies close to much larger models. While applying CoT introduces additional computation costs, we compensate for it by leveraging CoTFormer's special compatibility with token-wise variable depth. Through a compute adaptive model---which automatically allocates the compute to tokens that need it most---we show that it is possible to reduce the computation cost significantly without any reduction in accuracy, and with further compute cost reductions possible while maintaining a competitive accuracy.", "title_embedding_index": 15469, "title_abs_embedding_index": 15494}, {"title": "Adaptive Pruning of Pretrained Transformer via Differential Inclusions", "link_suffix": "/forum?id=WA84oMWHaH", "link": "https://openreview.net/forum?id=WA84oMWHaH", "pdf_link": "https://openreview.net/pdf?id=WA84oMWHaH", "keywords": "Differential inclusion, Mask-based pruning, Sparse optimization.", "abstract": "Large transformers have demonstrated remarkable success, making it necessary to compress these models to reduce inference costs while preserving their performance. Current compression algorithms prune transformers at fixed compression ratios, requiring a unique pruning process for each ratio, which results in high computational costs. In contrast, we propose pruning of pretrained transformers at any desired ratio within a single pruning stage, based on a differential inclusion for a mask parameter. This dynamic can generate the whole regularization solution path of the mask parameter, whose support set identifies the network structure. Therefore, the solution path identifies a Transformer weight family with various sparsity levels, offering greater flexibility and customization.In this paper, weintroduce such an effective pruning method, termed SPP (Solution Path Pruning). To achieve effective pruning, we segment the transformers into paired modules, including query-key pairs, value-projection pairs, and sequential linear layers, and apply low-rank compression to these pairs, maintaining the output structure while enabling structural compression within the inner states. Extensive experiments conducted on various well-known transformer backbones have demonstrated the efficacy of SPP.", "title_embedding_index": 15470, "title_abs_embedding_index": 15495}, {"title": "CASE-Bench: Context-Aware Safety Evaluation Benchmark for Large Language Models", "link_suffix": "/forum?id=y9tQNJ2n1y", "link": "https://openreview.net/forum?id=y9tQNJ2n1y", "pdf_link": "https://openreview.net/pdf?id=y9tQNJ2n1y", "keywords": "safety, benchmark, context, large language model, contextual integrity", "abstract": "Aligning large language models (LLMs) with human values is essential for their safe deployment and widespread adoption. Current LLM safety benchmarks often focus solely on the refusal of individual problematic queries, which overlooks the importance of the context where the query occurs and may cause undesired refusal of queries under safe contexts that diminish user experience. Addressing this gap, we introduce CASE-Bench, a Context-Aware Safety Evaluation Benchmark that integrates context into safety assessments of LLMs. CASE-Bench assigns distinct, formally described contexts to categorized queries based on Contextual Integrity theory. Additionally, in contrast to previous studies which mainly rely on majority voting from just a few annotators, we recruited a sufficient number of annotators necessary to ensure the detection of statistically significant differences among the experimental conditions based on power analysis. Our extensive analysis using CASE-Bench on various open-source and commercial LLMs reveals a substantial and significant influence of context on human judgments ($p<$0.0001 from a z-test), underscoring the necessity of context in safety evaluations. We also identify notable mismatches between human judgments and LLM responses, particularly in commercial models within safe contexts. Code and data used in the paper are available athttps://anonymous.4open.science/r/CASEBench-D5DB.", "title_embedding_index": 15471, "title_abs_embedding_index": 15496}, {"title": "Pseudo- vs. True-Randomness: Rethinking Distortion-Free Watermarks of Language Models under Watermark Key Collisions", "link_suffix": "/forum?id=jln7IcheW6", "link": "https://openreview.net/forum?id=jln7IcheW6", "pdf_link": "https://openreview.net/pdf?id=jln7IcheW6", "keywords": "LLM watermarking", "abstract": "Language model (LM) watermarking techniques inject a statistical signal into LM-generated content by substituting the random sampling process with pseudo-random sampling, using watermark keys as the random seed. Among these statistical watermarking approaches, distortion-free watermarks are particularly crucial because they embed watermarks into LM-generated content without compromising generation quality. However, one notable limitation of pseudo-random sampling compared to true-random sampling is that, under the same watermark keys (i.e., key collision), the results of pseudo-random sampling exhibit correlations. This limitation could potentially undermine the distortion-free property. Our studies reveal that key collisions are inevitable due to the limited availability of watermark keys, and existing distortion-free watermarks exhibit a significant distribution bias toward the original LM distribution in the presence of key collisions. Moreover, we go beyond the key collision condition and prove that achieving a perfect distortion-free watermark is impossible. To study the trade-off between watermark strength and its distribution bias, we introduce a new family of distortion-free watermarks--beta-watermark. Experimental results support that the  beta-watermark can effectively reduce the distribution bias under key collisions.", "title_embedding_index": 15472, "title_abs_embedding_index": 15497}, {"title": "Kernel Banzhaf: A Fast and Robust Estimator for Banzhaf Values", "link_suffix": "/forum?id=7JlL8ECPJ7", "link": "https://openreview.net/forum?id=7JlL8ECPJ7", "pdf_link": "https://openreview.net/pdf?id=7JlL8ECPJ7", "keywords": "Banzhaf values, Shapley values, Kernel SHAP, Leverage Scores, Least Squares Regression", "abstract": "Banzhaf values offer a simple and interpretable alternative to the widely-used Shapley values. We introduce Kernel Banzhaf, a novel algorithm inspired by KernelSHAP, that leverages an elegant connection between Banzhaf values and linear regression. Through extensive experiments on feature attribution tasks, we demonstrate that Kernel Banzhaf substantially outperforms other algorithms for estimating Banzhaf values in both sample efficiency and robustness to noise. Furthermore, we prove theoretical guarantees on the algorithm's performance, establishing Kernel Banzhaf as a valuable tool for interpretable machine learning.", "title_embedding_index": 15473, "title_abs_embedding_index": 15498}, {"title": "Markovian Compression: Looking to the Past Helps Accelerate the Future", "link_suffix": "/forum?id=H9oYYou34X", "link": "https://openreview.net/forum?id=H9oYYou34X", "pdf_link": "https://openreview.net/pdf?id=H9oYYou34X", "keywords": "stochastic optimization, distributed optimization, compressed communications, Markovian noise", "abstract": "This paper deals with distributed optimization problems that use compressed communication to achieve efficient performance and mitigate the communication bottleneck. We propose a family of compression schemes in which operators transform vectors fed to their input according to a Markov chain, i.e., the stochasticity of the compressors depends on previous iterations. Intuitively, this should accelerate the convergence of optimization methods, as considering previous iterations seems more natural and robust. The compressors are implemented in the vanilla Quantized Stochastic Gradient Descent (QSGD) algorithm. To further improve efficiency and convergence rate, we apply the momentum acceleration method. We prove convergence results for our algorithms with Markovian compressors and show theoretically that the accelerated method converges faster than the basic version. The analysis covers non-convex, Polyak-Lojasiewicz (PL), and strongly convex cases. Experiments are conducted to demonstrate the applicability of the results to distributed data-parallel optimization problems. Practical results demonstrate the superiority of methods utilizing our compressors design over several existing optimization algorithms.", "title_embedding_index": 15474, "title_abs_embedding_index": 15499}]