[{"title": "xFinder: Large Language Models as Automated Evaluators for Reliable Evaluation", "link_suffix": "/forum?id=7UqQJUKaLM", "link": "https://openreview.net/forum?id=7UqQJUKaLM", "pdf_link": "https://openreview.net/pdf?id=7UqQJUKaLM", "keywords": "Large Language Models; Reliable Evaluation", "abstract": "The continuous advancement of large language models (LLMs) has brought increasing attention to the critical issue of developing fair and reliable methods for evaluating their performance. Particularly, the emergence of cheating phenomena, such as test set leakage and prompt format overfitting, poses significant challenges to the reliable evaluation of LLMs. As evaluation frameworks commonly use Regular Expression (RegEx) for answer extraction, models may adjust their responses to fit formats easily handled by RegEx. Nevertheless, the key answer extraction module based on RegEx frequently suffers from extraction errors. Furthermore, recent studies proposing fine-tuned LLM as judge models for automated evaluation face challenges in terms of generalization ability and fairness. This paper comprehensively analyzes the entire LLM evaluation chain and demonstrates that optimizing the key answer extraction module improves extraction accuracy and enhances evaluation reliability. Our findings suggest that improving the key answer extraction module can lead to higher judgment accuracy and improved evaluation efficiency compared to the judge models. To address these issues, we propose xFinder, a novel evaluator for answer extraction and matching in LLM evaluation. As part of this process, we create a specialized dataset, the \\textbf{K}ey \\textbf{A}nswer \\textbf{F}inder (KAF) dataset, to ensure effective model training and evaluation. Generalization tests and real-world evaluations show that the smallest xFinder model, with only 500 million parameters, achieves an average extraction accuracy of 93.42%. In contrast, RegEx accuracy in the best evaluation framework is 74.38%. The final judgment accuracy of xFinder reaches 97.61%, outperforming existing evaluation frameworks and judge models.", "title_embedding_index": 14000, "title_abs_embedding_index": 14025}, {"title": "Regret Bounds and Reinforcement Learning Exploration of EXP-based Algorithms", "link_suffix": "/forum?id=f0cGihOlgH", "link": "https://openreview.net/forum?id=f0cGihOlgH", "pdf_link": "https://openreview.net/pdf?id=f0cGihOlgH", "keywords": "Bandit, reinforcement learning, EXP-based algorithms", "abstract": "We study the challenging exploration incentive problem in both bandit and reinforcement learning, where the rewards are scale-free and potentially unbounded, driven by real-world scenarios and differing from existing work. Past works in reinforcement learning either assume costly interactions with an environment or propose algorithms finding potentially low quality local maxima. Motivated by EXP-type methods that integrate multiple agents (experts) for exploration in bandits with the assumption that rewards are bounded, we propose new algorithms, namely EXP4.P and EXP4-RL for exploration in the unbounded reward case, and demonstrate their effectiveness in these new settings. Unbounded rewards introduce challenges as the regret cannot be limited by the number of trials, and selecting suboptimal arms may lead to infinite regret.  Specifically, we establish EXP4.P's regret upper bounds in both bounded and unbounded linear and stochastic contextual bandits. Surprisingly, we also find that by including one sufficiently competent expert, EXP4.P can achieve global optimality in the linear case. This unbounded reward result is also applicable to a revised version of EXP3.P in the Multi-armed Bandit scenario. In EXP4-RL, we extend EXP4.P from bandit scenarios to reinforcement learning to incentivize exploration by multiple agents, including one high-performing agent, for both efficiency and excellence. This algorithm has been tested on difficult-to-explore games and shows significant improvements in exploration compared to state-of-the-art.", "title_embedding_index": 14001, "title_abs_embedding_index": 14026}, {"title": "Block-to-Scene Pre-training for Point Cloud Hybrid-Domain Masked Autoencoders", "link_suffix": "/forum?id=KJkbmBcZRx", "link": "https://openreview.net/forum?id=KJkbmBcZRx", "pdf_link": "https://openreview.net/pdf?id=KJkbmBcZRx", "keywords": "Point Cloud, Self-Supervised Pre-training", "abstract": "Point clouds, as a primary representation of 3D data, can be categorized into scene domain point clouds and object domain point clouds based on the modeled content. Masked autoencoders (MAE) have become the mainstream paradigm in point clouds self-supervised learning. However, existing MAE-based methods are domain-specific, limiting the model's generalization. In this paper, we propose to pre-train a general Point cloud Hybrid-Domain Masked AutoEncoder (PointHDMAE) via a block-to-scene pre-training strategy. We first propose a hybrid-domain masked autoencoder consisting of an encoder and decoder belonging to the scene domain and object domain, respectively. The object domain encoder specializes in handling object point clouds and multiple shared object encoders assist the scene domain encoder in analyzing the scene point clouds.  Furthermore, we propose a block-to-scene strategy to pre-train our hybrid-domain model. Specifically, we first randomly select point blocks within a scene and apply a set of transformations to convert each point block coordinates from the scene space to the object space. Then, we employ an object-level mask and reconstruction pipeline to recover the masked points of each block, enabling the object encoder to learn a universal object representation. Finally, we introduce a scene-level block position regression pipeline, which utilizes the blocks' features in the object space to regress these blocks' initial positions within the scene space, facilitating the learning of scene representations.  Extensive experiments across different datasets and tasks demonstrate the generalization and superiority of our hybrid-domain model.", "title_embedding_index": 14002, "title_abs_embedding_index": 14027}, {"title": "RATE: Score Reward Models with Imperfect Rewrites of Rewrites", "link_suffix": "/forum?id=UnpxRLMMAu", "link": "https://openreview.net/forum?id=UnpxRLMMAu", "pdf_link": "https://openreview.net/pdf?id=UnpxRLMMAu", "keywords": "causality, causal inference, LLM alignment", "abstract": "This paper concerns the evaluation of reward models used in language modeling. A reward model is a function that takes a prompt and a response and assigns a score indicating how ``good'' that response is for the prompt. A key challenge is that reward models are usually imperfect proxies for actual preferences. For example, we may worry that a model trained to reward helpfulness learns to instead prefer longer responses. \n  In this paper, we develop an evaluation method, RATE (Rewrite-based Attribute Treatment Estimators), that allows us to measure the \\emph{causal} effect of a given attribute of a response (e.g., length) on the reward assigned to that response. \n  The core idea is to use large language models to rewrite responses to produce imperfect counterfactuals, and to adjust for rewriting error by rewriting \\emph{twice}. We show that the RATE estimator is consistent under reasonable assumptions. We demonstrate the effectiveness of RATE on synthetic and real-world data, showing that it can accurately estimate the effect of a given attribute on the reward model.", "title_embedding_index": 14003, "title_abs_embedding_index": 14028}, {"title": "Scalable Simulation-free Entropic Unbalanced Optimal Transport", "link_suffix": "/forum?id=py34636XvR", "link": "https://openreview.net/forum?id=py34636XvR", "pdf_link": "https://openreview.net/pdf?id=py34636XvR", "keywords": "Optimal Transport, Unbalanced Optimal Transport, Entropic Optimal Transport, Generative models, Image-to-image Translation", "abstract": "The Optimal Transport (OT) problem investigates a transport map that connects two distributions while minimizing a given cost function. Finding such a transport map has diverse applications in machine learning, such as generative modeling and image-to-image translation. In this paper, we introduce a scalable and simulation-free approach for solving the Entropic Unbalanced Optimal Transport (EUOT) problem. We derive the dynamical form of this EUOT problem, which is a generalization of the Schr\u00f6dinger bridges (SB) problem. Based on this, we derive dual formulation and optimality conditions of the EUOT problem from the stochastic optimal control interpretation. By leveraging these properties, we propose a simulation-free algorithm to solve EUOT, called Simulation-free EUOT (SF-EUOT). While existing SB models require expensive simulation costs during training and evaluation, our model achieves simulation-free training and one-step generation by utilizing the reciprocal property. Our model demonstrates significantly improved scalability in generative modeling and image-to-image translation tasks compared to previous SB methods.", "title_embedding_index": 14004, "title_abs_embedding_index": 14029}, {"title": "PiCO: Peer Review in LLMs based on Consistency Optimization", "link_suffix": "/forum?id=sfQ6XpApfS", "link": "https://openreview.net/forum?id=sfQ6XpApfS", "pdf_link": "https://openreview.net/pdf?id=sfQ6XpApfS", "keywords": "Large Language Model, Unsupervised Evaluation, Peer Review, Consistency Optimization", "abstract": "Existing large language models (LLMs) evaluation methods typically focus on testing the performance on some closed-environment and domain-specific benchmarks with human annotations. In this paper, we explore a novel unsupervised evaluation direction, utilizing peer-review mechanisms to measure LLMs automatically without any human feedback. In this setting, both open-source and closed-source LLMs lie in the same environment, capable of answering unlabeled questions and evaluating each other, where each LLM\u2019s response score is jointly determined by other anonymous ones.  During this process, we found that those answers that are more recognized by other ``reviewers'' (models) usually come from LLMs with stronger abilities, while these models can also evaluate others' answers more accurately.  We formalize it as a consistency assumption, i.e., the ability and score of the model usually have consistency.  We exploit this to optimize each model's confidence, thereby re-ranking the LLMs to be closer to human rankings. We perform experiments on multiple datasets with standard rank-based metrics, validating the effectiveness of the proposed approach.", "title_embedding_index": 14005, "title_abs_embedding_index": 14030}, {"title": "Advancing Neural Network Performance through Emergence-Promoting Initialization Scheme", "link_suffix": "/forum?id=DLhjxxXYwH", "link": "https://openreview.net/forum?id=DLhjxxXYwH", "pdf_link": "https://openreview.net/pdf?id=DLhjxxXYwH", "keywords": "Emergence, Initialization, cascade effect", "abstract": "We introduce a novel yet straightforward neural network initialization scheme that modifies conventional methods like Xavier and Kaiming initialization. Inspired by the concept of emergence and leveraging the emergence measures proposed by Li (2023), our method adjusts the layer-wise weight scaling factors to achieve higher emergence values. This enhancement is easy to implement, requiring no additional optimization steps for initialization compared to GradInit. We evaluate our approach across various architectures, including MLP and convolutional architectures for image recognition, and transformers for machine translation. We demonstrate substantial improvements in both model accuracy and training speed, with and without batch normalization. The simplicity, theoretical innovation, and demonstrable empirical advantages of our method make it a potent enhancement to neural network initialization practices. These results suggest a promising direction for leveraging emergence to improve neural network training methodologies.", "title_embedding_index": 14006, "title_abs_embedding_index": 14031}, {"title": "Boosting Perturbed Gradient Ascent for Last-Iterate Convergence in Games", "link_suffix": "/forum?id=Jrt9iWalFy", "link": "https://openreview.net/forum?id=Jrt9iWalFy", "pdf_link": "https://openreview.net/pdf?id=Jrt9iWalFy", "keywords": "Last-Iterate Convergence, Learning in Games, Noisy Feedback", "abstract": "This paper presents a payoff perturbation technique, introducing a strong convexity to players' payoff functions in games. This technique is specifically designed for first-order methods to achieve last-iterate convergence in games where the gradient of the payoff functions is monotone in the strategy profile space, potentially containing additive noise. Although perturbation is known to facilitate the convergence of learning algorithms, the magnitude of perturbation requires careful adjustment to ensure last-iterate convergence. Previous studies have proposed a scheme in which the magnitude is determined by the distance from a periodically re-initialized anchoring or reference strategy. Building upon this, we propose Gradient Ascent with Boosting Payoff Perturbation, which incorporates a novel perturbation into the underlying payoff function, maintaining the periodically re-initializing anchoring strategy scheme. This innovation empowers us to provide faster last-iterate convergence rates against the existing payoff perturbed algorithms, even in the presence of additive noise.", "title_embedding_index": 14007, "title_abs_embedding_index": 14032}, {"title": "BlockDance: Reuse Structurally Similar Spatio-Temporal Features to Accelerate Diffusion Transformers", "link_suffix": "/forum?id=yJAk0n0NyU", "link": "https://openreview.net/forum?id=yJAk0n0NyU", "pdf_link": "https://openreview.net/pdf?id=yJAk0n0NyU", "keywords": "Diffusion Models, Efficient Image and Video Generation", "abstract": "Diffusion models have demonstrated impressive generation capabilities, particularly with recent advancements leveraging transformer architectures to improve both visual and artistic quality. However, Diffusion Transformers (DiTs) continue to encounter challenges related to low inference speed, primarily due to the iterative denoising process.\nTo address this issue, we propose BlockDance, a training-free approach that explores feature similarities at adjacent time steps to accelerate DiTs.\nUnlike previous feature-reuse methods that lack tailored reuse strategies for features at different scales, BlockDance prioritizes the identification of the most structurally similar features, referred to as Structurally Similar Spatio-Temporal (STSS) features. These features are primarily located within the structure-focused blocks of the transformer during the later stages of denoising.\nBlockDance caches and reuses these highly similar features to mitigate redundant computation, thereby accelerating DiTs while maximizing consistency with the generated results of the original model.\nFurthermore, considering the diversity of generated content and the varying distributions of redundant features, we introduce BlockDance-Ada, a lightweight decision-making network tailored for instance-specific acceleration.\nBlockDance-Ada dynamically allocates resources and provides superior content quality.\nBoth BlockDance and BlockDance-Ada have demonstrated effectiveness across diverse generation tasks and models, achieving an acceleration ranging from 25% to 50% while preserving generation quality.", "title_embedding_index": 14008, "title_abs_embedding_index": 14033}, {"title": "Retrieval Augmented Diffusion Model for Structure-informed Antibody Design and Optimization", "link_suffix": "/forum?id=a6U41REOa5", "link": "https://openreview.net/forum?id=a6U41REOa5", "pdf_link": "https://openreview.net/pdf?id=a6U41REOa5", "keywords": "Generative model, Retrieval augmented geneartion, Protein design", "abstract": "Antibodies are essential proteins responsible for immune responses in organisms, capable of specifically recognizing antigen molecules of pathogens. Recent advances in generative models have significantly enhanced rational antibody design. However, existing methods mainly create antibodies from scratch without template constraints, leading to model optimization challenges and unnatural sequences. To address these issues, we propose a retrieval-augmented diffusion framework, termed RADAb, for efficient antibody design. Our method leverages a set of structural homologous motifs that align with query structural constraints to guide the generative model in inversely optimizing antibodies according to desired design criteria. Specifically, we introduce a structure-informed retrieval mechanism that integrates these exemplar motifs with the input backbone through a novel dual-branch denoising module, utilizing both structural and evolutionary information. Additionally, we develop a conditional diffusion model that iteratively refines the optimization process by incorporating both global context and local evolutionary conditions. Our approach is agnostic to the choice of generative models. Empirical experiments demonstrate that our method achieves state-of-the-art performance in multiple antibody inverse folding and optimization tasks, offering a new perspective on biomolecular generative models.", "title_embedding_index": 14009, "title_abs_embedding_index": 14034}, {"title": "Estimating Committor Functions via Deep Adaptive Sampling on Rare Transition Paths", "link_suffix": "/forum?id=rEEjYlzXUD", "link": "https://openreview.net/forum?id=rEEjYlzXUD", "pdf_link": "https://openreview.net/pdf?id=rEEjYlzXUD", "keywords": "committor function, deep adaptive sampling, rare event, transition path", "abstract": "The committor functions are central to investigating rare but important events in molecular simulations. It is known that computing the committor function suffers from the curse of dimensionality. Recently, using neural networks to estimate the committor function has gained attention due to its potential for high-dimensional problems. Training neural networks to approximate the committor function needs to sample transition data from straightforward simulations of rare events, which is very inefficient. The scarcity of transition data makes it challenging to approximate the committor function. To address this problem, we propose an efficient framework to generate data points in the transition state region that helps train neural networks to approximate the committor function. We design a Deep Adaptive Sampling method for TRansition paths (DASTR), where deep generative models are employed to generate samples to capture the information of transitions effectively. In particular, we treat a non-negative function in terms of the integrand in the loss functional as an unnormalized probability density function and approximate it with the deep generative model. The new samples from the deep generative model are located in the region of the transition and fewer samples are located in the other region, which provides effective samples for approximating the committor function and significantly improves the accuracy. We demonstrate the effectiveness of the proposed method with both simulations and realistic examples.", "title_embedding_index": 14010, "title_abs_embedding_index": 14035}, {"title": "FinTruthQA: A Benchmark Dataset for Evaluating the Quality of Financial Information Disclosure", "link_suffix": "/forum?id=o2Gg2tSKBn", "link": "https://openreview.net/forum?id=o2Gg2tSKBn", "pdf_link": "https://openreview.net/pdf?id=o2Gg2tSKBn", "keywords": "Natural Language Processing (NLP), Financial Disclosure, Information Quality Assessment, Financial Q&A", "abstract": "Accurate and transparent financial information disclosure is crucial in the fields of accounting and finance, ensuring market efficiency and investor confidence. Among many information disclosure platforms, the Chinese stock exchanges' investor interactive platform provides a novel and interactive way for listed firms to disclose information of interest to investors through an online question-and-answer (Q&A) format. However, it is common for listed firms to respond to questions with limited or no substantive information, and automatically evaluating the quality of financial information disclosure on large amounts of Q&A pairs is challenging. This paper builds a benchmark FinTruthQA, that can evaluate advanced natural language processing (NLP) techniques for the automatic quality assessment of information disclosure in financial Q&A data. FinTruthQA comprises 6,000 real-world financial Q&A entries and each Q&A was manually annotated based on four conceptual dimensions of accounting:question identification,question relevance,answer readability, andanswer relevance. We benchmarked various NLP techniques on FinTruthQA, including statistical machine learning models, pre-trained language model and their fine-tuned versions, as well as large language models (LLMs).  Experiments showed that existing NLP models have strong predictive ability forquestion identificationandquestion relevancetasks, but are suboptimal foranswer readabilityandanswer relevancetasks. By establishing this benchmark, we provide a robust foundation for the automatic evaluation of information disclosure, significantly enhancing the transparency and quality of financial reporting. FinTruthQA can be used by auditors, regulators, and financial analysts for real-time monitoring and data-driven decision-making, as well as by researchers for advanced studies in accounting and finance, ultimately fostering greater trust and efficiency in the financial markets.", "title_embedding_index": 14011, "title_abs_embedding_index": 14036}, {"title": "Learning to Permute with Discrete Diffusion", "link_suffix": "/forum?id=EO8xpnW7aX", "link": "https://openreview.net/forum?id=EO8xpnW7aX", "pdf_link": "https://openreview.net/pdf?id=EO8xpnW7aX", "keywords": "Finite Symmetric Groups, Discrete Diffusion, Permutations, Riffle Shuffles, Plackett-Luce Distribution, Sorting, Jigsaw Puzzle", "abstract": "The group of permutations $S_n$, also known as the finite symmetric groups, are essential in fields such as combinatorics, physics, and chemistry. However, learning a probability distribution over $S_n$ poses significant challenges due to its intractable size and discrete nature. In this paper, we introduceSymmetricDiffusers, a novel discrete diffusion model that simplifies the task of learning a complicated distribution over $S_n$ by decomposing it into learning simpler transitions of the reverse diffusion using deep neural networks. We identify the riffle shuffle as an effective forward transition and provide empirical guidelines for selecting the diffusion length based on the theory of random walks on finite groups. Additionally, we propose a generalized Plackett-Luce (PL) distribution for the reverse transition, which is provably more expressive than the PL distribution. We further introduce a theoretically grounded \"denoising schedule\" to improve sampling and learning efficiency. Extensive experiments show that our model achieves state-of-the-art or comparable performances on solving tasks including sorting 4-digit MNIST images, jigsaw puzzles, and traveling salesman problems.", "title_embedding_index": 14012, "title_abs_embedding_index": 14037}, {"title": "Inv-PnCO: Invariant Predict-and-Combinatorial Optimization under Distribution Shifts", "link_suffix": "/forum?id=AFAmM5dsFu", "link": "https://openreview.net/forum?id=AFAmM5dsFu", "pdf_link": "https://openreview.net/pdf?id=AFAmM5dsFu", "keywords": "Combinatorial Optimization, Predict-and-optimize, Generalization", "abstract": "Machine learning has been well introduced to solve combinatorial optimization (CO) problems over the decade, while most works only consider the deterministic setting. Yet in real-world applications, decisions have often to be made in uncertain environments, which is typically reflected by the stochasticity of the coefficients of the problem at hand, considered as a special case of the more general and emerging \"predict-and-optimize\" (PnO) paradigm in the sense that the prediction and optimization are jointly learned and performed. In this paper, we consider the problem of learning to solve CO under the above uncertain setting and formulate it as \"predict-and-combinatorial optimization\" (PnCO), particularly in a challenging yet practical out-of-distribution (OOD) setting, where there is a distribution shift between training and testing CO instances. We propose the Invariant Predict-and-Combinatorial Optimization (Inv-PnCO) framework to alleviate this challenge. Inv-PnCO derives a learning objective that reduces the distance of distribution of solutions with the true distribution and uses a regularization term to learn invariant decision-oriented factors that are stable under various environments, thereby enhancing the generalizability of predictions and subsequent optimizations. We also provide a theoretical analysis of how the proposed loss reduces OOD error. The empirical evaluation across three distinct tasks on knapsack, visual shortest path planning, and traveling salesman problem covering array, image, and graph inputs underscores the efficacy of Inv-PnCO to enhance the generalizability, both for predict-then-optimize and predict-and-optimize approaches.", "title_embedding_index": 14013, "title_abs_embedding_index": 14038}, {"title": "Self-Alignment Optimization for Language Models", "link_suffix": "/forum?id=QWMgTMRUnB", "link": "https://openreview.net/forum?id=QWMgTMRUnB", "pdf_link": "https://openreview.net/pdf?id=QWMgTMRUnB", "keywords": "Self-Alignment, Annotation-free, Large Language Model", "abstract": "Traditional reinforcement learning from human feedback (RLHF) relies heavily on costly and time-consuming human-annotated datasets. Even Reinforcement Learning from AI Feedback (RLAIF), which trains a reward model using AI-generated preference data before refining the language model through reinforcement learning, remains expensive. These methods often necessitate either specialized reward model designs or larger models (e.g., GPT-4) for external labeling. In this paper, we introduce a dataset-free and annotation-free framework called Self-Alignment Optimization (SAO), which addresses the aforementioned issue by aligning the model with its own prompts and feedback as preferences. SAO begins with a chat-based model that engages in persona role-play to generate diverse prompts and responses, which are then self-evaluated and used for preference optimization.\nExtensive experiments with two strong LLMs on several benchmarks demonstrate the effectiveness of SAO. Specifically, on AlpacaEval 2.0, Gemma-2-9B-it-SAO achieves a Length-Controlled Win Rate (LC)  of 69.2% and win rate (WR) of 66.0%, surpassing the baseline model by 18.1% and 27.9%. Llama-3-Instruct-8B-SAO reaches 33.3% LC and 39.0%  WR, with performance improvements of 10.4% and 16.4%, respectively. On the MT-Bench benchmark, Gemma-2-9B-it-SAO and Llama-3-8B-Instruct-SAO score 7.41 and 6.76, compared to their pre-SAO scores of 7.09 and 6.70. The Arena-Hard benchmark shows even greater gains from SAO, with Gemma-2-9B-it's WR increasing from 52.6% to 70.1% and Llama-3-Instruct-8B's WR rising from 40.3% to 56.4%. In addition, our further experiments demonstrate that models fine-tuned with SAO exhibit similar or even superior performance on downstream NLP tasks compared to baseline models, rather than those trained with external labeled datasets, which enhance alignment ability but may compromise some general capabilities. We anticipate that this work will provide new insights for future research on self-improvement in LLMs.", "title_embedding_index": 14014, "title_abs_embedding_index": 14039}, {"title": "Outcome-Driven Action Flexibility for Robust Offline Reinforcement Learning", "link_suffix": "/forum?id=UoYxPYMUWd", "link": "https://openreview.net/forum?id=UoYxPYMUWd", "pdf_link": "https://openreview.net/pdf?id=UoYxPYMUWd", "keywords": "Offline reinforcement learning; Robust reinforcement learning;Outcome-driven action flexibility;", "abstract": "We address the challenge of offline reinforcement learning using realistic data, specifically non-expert data collected through sub-optimal behavior policies. A primary concern is that the learned policy must be conservative enough to manage \\textit{distribution shift} while maintaining sufficient flexibility for generalization. To tackle this issue, we introduce a novel method called Outcome-Driven Action Flexibility (ODAF), which seeks to reduce reliance on the empirical action distribution of the behavior policy.\nSpecifically, we develop a new reward mechanism that evaluates whether the subsequent states, following the current policy, meet specified performance requirements (e.g., safety\u2014remaining within the state support area), rather than solely depending on the characteristics of the actions taken (e.g., whether the action imitates the behavior policy).\nBesides theoretical justification, we provide empirical evidence on widely used D4RL benchmarks, demonstrating that our ODAF method, implemented using uncertainty quantification techniques, effectively tolerates unseen transitions for improved \"trajectory stitching,\" while enhancing the agent's ability to learn from realistic non-expert data.", "title_embedding_index": 14015, "title_abs_embedding_index": 14040}, {"title": "Uncertainty Quantification with Generative-Semantic Entropy Estimation for Large Language Models", "link_suffix": "/forum?id=LDmJfJlo83", "link": "https://openreview.net/forum?id=LDmJfJlo83", "pdf_link": "https://openreview.net/pdf?id=LDmJfJlo83", "keywords": "Large Language Models, Uncertainty Quantification, Explainable AI, Trustworthy AI", "abstract": "In recent years,  powerful foundation models, including Large Language Models (LLMs) and Large Multi-Modal Models (LMMs) have ushered in a new epoch of multi-faceted, intelligent conversational agents. Despite their significant early successes and widespread use, foundation models nevertheless currently suffer from several critical challenges, including their lack of transparency and predilection for \"hallucinations.\"  To this end, we introduce Generative-Semantic Entropy Estimation (GSEE), a model-agnostic algorithm that efficiently estimates the generative uncertainty associated with foundation models, while requiring no additional auxiliary model inference steps. In principle, for any foundation model input data, e.g., a text prompt, image, text + image, etc., GSEE numerically estimates the uncertainty encapsulated in the internal, semantic manifold of the LLM generated responses to the input data. In this way, high uncertainty is indicative of hallucinations and low generative confidence. Through experiments, we demonstrate the superior performance of GSEE for uncertainty quantification (UQ) amongst state-of-the-art methods across a variety of models, datasets, and problem settings, including: unbounded language prompting, constrained language prompting, high/low generative stochasticity, acute semantic diversity prompting, and as a barometer for hallucination/predictive accuracy.", "title_embedding_index": 14016, "title_abs_embedding_index": 14041}, {"title": "DyDiff: Long-Horizon Rollout via Dynamics Diffusion for Offline Reinforcement Learning", "link_suffix": "/forum?id=ayUh0A6LIJ", "link": "https://openreview.net/forum?id=ayUh0A6LIJ", "pdf_link": "https://openreview.net/pdf?id=ayUh0A6LIJ", "keywords": "reinforcement learning, diffusion model, dynamics model", "abstract": "With the great success of diffusion models (DMs) in generating realistic synthetic vision data, many researchers have investigated their potential in decision-making and control. Most of these works utilized DMs to sample directly from the trajectory space, where DMs can be viewed as a combination of dynamics models and policies. In this work, we explore how to decouple DMs\u2019 ability as dynamics models in fully offline settings, allowing the learning policy to roll out trajectories. As DMs learn the data distribution from the dataset, their intrinsic policy is actually the behavior policy induced from the dataset, which results in a mismatch between the behavior policy and the learning policy. We propose Dynamics Diffusion, short as DyDiff, which can inject information from the learning policy to DMs iteratively. DyDiff ensures long-horizon rollout accuracy while maintaining policy consistency and can be easily deployed on model-free algorithms. We provide theoretical analysis to show the advantage of DMs on long-horizon rollout over models and demonstrate the effectiveness of DyDiff in the context of offline reinforcement learning, where the rollout dataset is provided but no online environment for interaction. Our code is athttps://anonymous.4open.science/r/DyDiff.", "title_embedding_index": 14017, "title_abs_embedding_index": 14042}, {"title": "Improving Neural Optimal Transport via Displacement Interpolation", "link_suffix": "/forum?id=CfZPzH7ftt", "link": "https://openreview.net/forum?id=CfZPzH7ftt", "pdf_link": "https://openreview.net/pdf?id=CfZPzH7ftt", "keywords": "Optimal Transport, Displacement Interpolation, Image-to-image Translation", "abstract": "Optimal Transport (OT) theory investigates the cost-minimizing transport map that moves a source distribution to a target distribution. Recently, several approaches have emerged for learning the optimal transport map for a given cost function using neural networks. We refer to these approaches as the OT Map. OT Map provides a powerful tool for diverse machine learning tasks, such as generative modeling and unpaired image-to-image translation. However, existing methods that utilize max-min optimization often experience training instability and sensitivity to hyperparameters. In this paper, we propose a novel method to improve stability and achieve a better approximation of the OT Map by exploiting displacement interpolation, dubbed Displacement Interpolation Optimal Transport Model (DIOTM). We derive the dual formulation of displacement interpolation at specific time $t$ and prove how these dual problems are related across time. This result allows us to utilize the entire trajectory of displacement interpolation in learning the OT Map. Our method improves the training stability and achieves superior results in estimating optimal transport maps. We demonstrate that DIOTM outperforms existing OT-based models on image-to-image translation tasks.", "title_embedding_index": 14018, "title_abs_embedding_index": 14043}, {"title": "MMEval: Evaluating Video Generation Models for Motion Quality", "link_suffix": "/forum?id=Vli7PVO60W", "link": "https://openreview.net/forum?id=Vli7PVO60W", "pdf_link": "https://openreview.net/pdf?id=Vli7PVO60W", "keywords": "image-to-video, evaluation", "abstract": "Recent advancements in video generation, especially with diffusion models, have led to new challenges in evaluating the generated outputs, highlighting the need for well-curated evaluation metrics and benchmarks. While prior work has focused on assessing text-to-video models for overall video quality, such as temporal coherence and prompt consistency, they overlook a crucial aspect: motion modeling abilities of generative models. To address this gap, we propose a structured approach to evaluate image-to-video generation models, with a focus on their motion modeling abilities. For example, we assess how accurately models generate motions like \"circular movement for a rotating ferris wheel\" or \"oscillatory motion for a pendulum\". We categorize videos  into linear, circular, and oscillatory motion-types and formulate metrics to capture key motion properties for each category. Our benchmark, MMEval, along with the code and image-prompt-video sets, will be publicly released.", "title_embedding_index": 14019, "title_abs_embedding_index": 14044}, {"title": "Towards Aligned Data Forgetting via Twin Machine Unlearning", "link_suffix": "/forum?id=liqUhMECuY", "link": "https://openreview.net/forum?id=liqUhMECuY", "pdf_link": "https://openreview.net/pdf?id=liqUhMECuY", "keywords": "machine unlearning", "abstract": "Modern privacy regulations have spurred the evolu-\ntion of machine unlearning, a technique enabling a trained model\nto efficiently forget specific training data. In prior unlearning\nmethods, the concept of \u201cdata forgetting\u201d is often interpreted\nand implemented as achieving zero classification accuracy on\nsuch data. Nevertheless, the authentic aim of machine unlearn-\ning is to achieve alignment between the unlearned model and\nthe gold model (i.e., the model derived from re-training from\nscratch without the data to be forgotten). Here, \u201calignment\u201d\nsignifies the encouragement for both models to achieve identical\nclassification accuracy. Owing to its generalization ability, the\ngold model can correctly classify a portion of the forgotten data,\nresulting in a non-zero classification accuracy. To better align\nthe unlearned model with the gold model, we propose a Twin\nMachine Unlearning (TMU) approach, where a twin unlearning\nproblem is defined corresponding to the original unlearning\nproblem. Consequently, the generalization-label predictor trained\non the twin problem can be transferred to the original problem,\nfacilitating aligned data forgetting. Additionally, we introduce a\nnoise-perturbed fine-tuning scheme to balance the trade-off be-\ntween retaining the model\u2019s generalization ability and enhancing\nits resilience to Membership Inference Attacks. Comprehensive\nempirical experiments illustrate that our approach significantly\nenhances the alignment between the unlearned model and the\ngold model. Meanwhile, our method allows data forgetting\nwithout compromising the model\u2019s accuracy.", "title_embedding_index": 14020, "title_abs_embedding_index": 14045}, {"title": "Train Small, Infer Large: Memory-Efficient LoRA Training for Large Language Models", "link_suffix": "/forum?id=s7DkcgpRxL", "link": "https://openreview.net/forum?id=s7DkcgpRxL", "pdf_link": "https://openreview.net/pdf?id=s7DkcgpRxL", "keywords": "Large Language Model, Memory-Efficient LoRA", "abstract": "Large Language Models (LLMs) have significantly advanced natural language processing with exceptional task generalization capabilities. Low-Rank Adaption (LoRA) offers a cost-effective fine-tuning solution, freezing the original model parameters and training only lightweight, low-rank adapter matrices. However, the memory footprint of LoRA is largely dominated by the original model parameters. To mitigate this, we propose LoRAM, a memory-efficient LoRA training scheme founded on the intuition that many neurons in over-parameterized LLMs have low training utility but are essential for inference. LoRAM presents a unique twist: it trains on a pruned model to obtain pruned low-rank matrices, which are then recovered and utilized with the original model for inference. Additionally, minimal-cost continual pre-training, performed by the model publishers in advance, aligns the knowledge discrepancy between pruned and original models. Our extensive experiments demonstrate the efficacy of LoRAM across various pruning strategies and downstream tasks. For a model with 70 billion parameters, LoRAM enables training on a GPU with only 20G HBM, replacing an A100-80G GPU for LoRA training and 15 GPUs for full fine-tuning. Specifically, QLoRAM implemented by structured pruning combined with 4-bit quantization, for LLaMA-3.1-70B (LLaMA-2-70B), reduces the parameter storage cost that dominates the memory usage in low-rank matrix training by 7.07\u00d7 (8.26\u00d7), while achieving dominant performance gains over both the original LLaMA-3.1-70B (LLaMA-2-70B) and LoRA-trained LLaMA-3.1-8B (LLaMA-2-13B).", "title_embedding_index": 14021, "title_abs_embedding_index": 14046}, {"title": "Revisit, Extend, and Enhance Hessian-Free Influence Functions", "link_suffix": "/forum?id=WT2bL7sCM1", "link": "https://openreview.net/forum?id=WT2bL7sCM1", "pdf_link": "https://openreview.net/pdf?id=WT2bL7sCM1", "keywords": "Influence Function, Hessian-Free, Data Valuation", "abstract": "Influence functions serve as crucial tools for assessing sample influence. By employing the first-order Taylor extension, sample influence can be estimated without the need for expensive model retraining. However, applying influence functions directly to deep models presents challenges, primarily due to the non-convex nature of the loss function and the large size of model parameters. This difficulty not only makes computing the inverse of the Hessian matrix costly but also renders it non-existent in some cases. Various approaches, including matrix decomposition, have been explored to expedite and approximate the inversion of the Hessian matrix, with the aim of making influence functions applicable to deep models. In this paper, we revisit a specific, albeit naive, yet effective approximation method known as TracIn, and simplify it further, introducing the name Inner Product (IP). This method substitutes the inverse of the Hessian matrix with an identity matrix. We offer deeper insights into why this straightforward approximation method is effective. Furthermore, we extend its applications beyond measuring model utility to include considerations of fairness and robustness. Finally, we enhance IP through an ensemble strategy. To validate its effectiveness, we conduct experiments on synthetic data and extensive evaluations on noisy label detection, sample selection for large language model fine-tuning, and defense against adversarial attacks.", "title_embedding_index": 14022, "title_abs_embedding_index": 14047}, {"title": "MME-FINANCE: A Multimodal Finance Benchmark for Expert-level Understanding and Reasoning", "link_suffix": "/forum?id=EFzBhrEp8Y", "link": "https://openreview.net/forum?id=EFzBhrEp8Y", "pdf_link": "https://openreview.net/pdf?id=EFzBhrEp8Y", "keywords": "Multimodal; Benchmark", "abstract": "The remarkable capability of existing Multimodal Large Language Models~(MLLMs) to understand general natural images have been extensively demonstrated in plentiful benchmarks. Nevertheless, the potential of MLLMs in finance domain remains to be fully explored. Financial images exhibit a wide range of variations, encompass intricate details, and demand professional expertise for proper interpretation, thereby posing a significant challenge for MLLMs in terms of their fine-grained perception and complex reasoning capabilities. To bridge this gap, we introduce MME-FINANCE, a novel benchmark designed specifically to assess MLLMs' performance in open-ended financial Visual Question Answering (VQA). Our benchmark consists of over 1,000 VQA pairs spanning a wide range of complex financial scenarios.  We devise multi-tiered financial tasks tailored to the specific characteristics of the financial domain, aiming to comprehensively evaluate the perception, reasoning, and cognition capabilities of MLLMs. \nFurthermore, we employ a multimodal evaluation approach that incorporates visual data to score the model predictions, thereby aligning more closely with human judgment. Extensive experimental evaluations of 18 mainstream MLLMs reveal their limitations in financial tasks and provide insights to inspire further research.", "title_embedding_index": 14023, "title_abs_embedding_index": 14048}, {"title": "Uncovering Gaps in How Humans and LLMs Interpret Subjective Language", "link_suffix": "/forum?id=gye2U9uNXx", "link": "https://openreview.net/forum?id=gye2U9uNXx", "pdf_link": "https://openreview.net/pdf?id=gye2U9uNXx", "keywords": "safety, alignment, constitutional ai, language model failures, misalignment, automated evaluation, automated red-teaming", "abstract": "Humans often rely on subjective natural language to direct language models (LLMs); for example, users might instruct the LLM to write anenthusiasticblogpost, while developers might train models to behelpfulandharmlessusing LLM-based edits. The LLM'soperational semanticsof such subjective phrases---how it adjusts its behavior when each phrase is included in the prompt---thus dictates how aligned it is with human intent. In this work, we uncover instances ofmisalignmentbetween LLMs' actual operational semantics and what humans expect. Our method, TED (thesaurus error detector), first constructs a thesaurus that captures whether two phrases have similar operational semantics according to the LLM. It then elicits failures by unearthing disagreements between this thesaurus and a reference semantic thesaurus. TED routinely produces surprising instances of misalignment; for example, Mistral 7B Instruct produces moreharassingoutputs when it edits text to bewitty, and Llama 3 8B Instruct producesdishonestarticles when instructed to make the articlesenthusiastic. Our results demonstrate that we can uncover unexpected LLM behavior by characterizing relationships between abstract concepts, rather than supervising individual outputs directly.", "title_embedding_index": 14024, "title_abs_embedding_index": 14049}]