[{"title": "AgentHarm: Benchmarking Robustness of LLM Agents on Harmful Tasks", "link_suffix": "/forum?id=AC5n7xHuR1", "link": "https://openreview.net/forum?id=AC5n7xHuR1", "pdf_link": "https://openreview.net/pdf?id=AC5n7xHuR1", "keywords": "Robustness, jailbreaking, adversarial attacks, LLM agents, AI safety", "abstract": "The robustness of LLMs to jailbreak attacks, where users design prompts to circumvent safety measures and misuse model capabilities, has been studied primarily for LLMs acting as simple chatbots. Meanwhile, LLM agents---which use external tools and can execute multi-stage tasks---may pose a greater risk if misused, but their robustness remains underexplored. To facilitate research on LLM agent misuse, we propose a new benchmark called AgentHarm. The benchmark includes a diverse set of 110 explicitly malicious agent tasks (440 with augmentations), covering 11 harm categories including fraud, cybercrime, and harassment. In addition to measuring whether models refuse harmful agentic requests, scoring well on AgentHarm requires jailbroken agents to maintain their capabilities following an attack to complete a multi-step task. We evaluate a range of leading LLMs, and find (1) leading LLMs are surprisingly complaint with malicious agent requests without jailbreaking, (2) simple universal jailbreak strings can be adapted to effectively jailbreak agents, and (3) these jailbreaks enable coherent and malicious multi-step agent behavior and retain model capabilities. We publicly release AgentHarm to enable simple and reliable evaluation of attacks and defenses for LLM-based agents.", "title_embedding_index": 10150, "title_abs_embedding_index": 10175}, {"title": "A Reasoning-Based Approach to Cryptic Crossword Clue Solving", "link_suffix": "/forum?id=Bo5eKnJPML", "link": "https://openreview.net/forum?id=Bo5eKnJPML", "pdf_link": "https://openreview.net/pdf?id=Bo5eKnJPML", "keywords": "NLP, Cryptic Crosswords, Reasoning, Proof/Verification", "abstract": "Cryptic crossword clues are challenging language tasks, for which new test sets are released on a regular basis by more than a dozen newspapers outside the USA.  Each cryptic clue contains both the definition of the answer to be placed in the crossword grid (in common with regular crosswords), and 'wordplay' thatprovesthat the answer is correct (i.e. a solver can be confident that an answer is correct without needing crossing words to confirm it).  This work describes a system that combines local LLMs that (i) guess answers directly, and (ii) propose wordplay explanations, with (iii) a verifier system that operates on codified reasoning steps output by an LLM. Overall, this system establishes a new state-of-the-art performance on the challenging Cryptonite dataset of clues from The Times and The Telegraph newspapers in the UK.", "title_embedding_index": 10151, "title_abs_embedding_index": 10176}, {"title": "CoINR: Compressed Implicit Neural Representations", "link_suffix": "/forum?id=ZWi6RpT4mJ", "link": "https://openreview.net/forum?id=ZWi6RpT4mJ", "pdf_link": "https://openreview.net/pdf?id=ZWi6RpT4mJ", "keywords": "Implicit Neural Representations, Signal Compression, Model Compression, Compressed Sensing", "abstract": "Implicit Neural Representations (INRs) are increasingly recognized as a versatile data modality for representing discretized signals, offering benefits such as infinite query resolution and reduced storage requirements. Existing signal compression approaches for INRs typically employ one of two strategies: 1. direct quantization with entropy coding of the trained INR; 2. deriving a latent code on top of the INR through a learnable transformation. Thus, their performance is heavily dependent on the quantization and entropy coding schemes employed. In this paper, we introduce CoINR, an innovative compression algorithm that leverages the patterns in the vector spaces formed by weights of INRs. We compress these vector spaces using a high-dimensional sparse code within a dictionary. Further analysis reveals that the atoms of the dictionary used to generate the sparse code do not need to be learned or transmitted to successfully recover the INR weights. We demonstrate that the proposed approach can be integrated with any existing INR-based signal compression technique. Our results indicate that CoINR achieves substantial reductions in storage requirements for INRs across various configurations, outperforming conventional INR-based compression baselines. Furthermore, CoINR maintains high-quality decoding across diverse data modalities, including images, occupancy fields, and Neural Radiance Fields.", "title_embedding_index": 10152, "title_abs_embedding_index": 10177}, {"title": "Approximation algorithms for combinatorial optimization with predictions", "link_suffix": "/forum?id=AEFVa6VMu1", "link": "https://openreview.net/forum?id=AEFVa6VMu1", "pdf_link": "https://openreview.net/pdf?id=AEFVa6VMu1", "keywords": "Approximation Algorithm, Predictions, ML-augmented, Combinatorial Optimization", "abstract": "We initiate a systematic study of utilizing predictions to improve over approximation guarantees of classic algorithms, without increasing the running time. We propose a generic method for a wide class of optimization problems that ask to select a feasible subset of input items of minimal (or maximal) total weight. This gives simple (near-)linear-time algorithms for, e.g., Vertex Cover, Steiner Tree, Minimum Weight Perfect Matching, Knapsack, and Maximum Clique. Our algorithms produce an optimal solution when provided with perfect predictions and their approximation ratio smoothly degrades with increasing prediction error. With small enough prediction error we achieve approximation guarantees that are beyond the reach without predictions in given time bounds, as exemplified by the NP-hardness and APX-hardness of many of the above problems. Although we show our approach to be optimal for this class of problems as a whole, there is a potential for exploiting specific structural properties of individual problems to obtain improved bounds; we demonstrate this on the Steiner Tree problem. We conclude with an empirical evaluation of our approach.", "title_embedding_index": 10153, "title_abs_embedding_index": 10178}, {"title": "Learn from Interactions: General-Sum Interactive Inverse Reinforcement Learning", "link_suffix": "/forum?id=OuYCW4ACDl", "link": "https://openreview.net/forum?id=OuYCW4ACDl", "pdf_link": "https://openreview.net/pdf?id=OuYCW4ACDl", "keywords": "Inverse Reinforcement Learning", "abstract": "This paper studies the problem that a learner aims to learn the reward function of the expert from the interaction with the expert and how to interact with the expert. We formulate the problem as a stochastic bi-level optimization problem and develop a double-loop algorithm \"general-sum interactive inverse reinforcement learning\" (GSIIRL). In the GSIIRL, the learner first learns the reward function of the expert in the inner loop and then learns how to interact with the expert in the outer loop. We theoretically prove the convergence of our algorithm and validate our algorithm through simulations.", "title_embedding_index": 10154, "title_abs_embedding_index": 10179}, {"title": "PlicoTabTransformer: Folding Tabular Embeddings Into M Vectors", "link_suffix": "/forum?id=ioOgrS0UKx", "link": "https://openreview.net/forum?id=ioOgrS0UKx", "pdf_link": "https://openreview.net/pdf?id=ioOgrS0UKx", "keywords": "Tabular data, Transformer, Self-attention, Positional embeddings, Contrastive loss, Classification", "abstract": "Tabular data represents the most prevalent and extensively utilized form of structured data in various domains. Traditionally dominated by tree-based algorithms, researchers are actively exploring the application of deep neural networks on tabular data. Notably, the TabTransformer (Huang et al., 2020) and FT-transformer (Gorishniy et al., 2021) showed that feeding column embeddings of the tabular\nfeatures into a transformer could learn a representation of the columns and how the embeddings interact with one another. This paper introduces PlicoTabTransformer, an enhancement of the previous methods, which is designed to learn multiple representations of the column embeddings. By incorporating a transformer with multiple learnable position embeddings and a contrastive learning loss, our\nmethod learns multiple distinct and orthogonal representations (denoted as plicovectors) of the column embeddings. We evaluated the PlicoTabTransformer with the pytorch-frame benchmark. Our experimental demonstrated that the PlicoTabTransformer is overall top ranked algorithm and achieves state of the art performance in several datasets compared to other deep learning method closing the gap\nwith tree based algorithms. Our method provides an added advantage to visualise redundancies and a potential dimensionality reduction technique.", "title_embedding_index": 10155, "title_abs_embedding_index": 10180}, {"title": "Extending Myerson's Optimal Auctions to Correlated Bidders via Neural Network Interpolation", "link_suffix": "/forum?id=WkSP7DfwVW", "link": "https://openreview.net/forum?id=WkSP7DfwVW", "pdf_link": "https://openreview.net/pdf?id=WkSP7DfwVW", "keywords": "mechanism design, optimal auction, revenue maximization, virtual valuation", "abstract": "We aim to design revenue-maximizing single-item auctions that are deterministic, strategy-proof and ex post individually rational.  Myerson's seminal work on optimal auction design solved this problem for independent bidders. Myerson introduced the novel concept of virtual valuation and showed that revenue maximization is equivalent to virtual valuation maximization. Coincidentally, by greedily allocating the item to the bidder with the highest (ironed) virtual valuation, the resulting allocation is guaranteed to be monotone -- a necessary and sufficient condition for strategy-proofness.For correlated bidders, Myerson's greedy allocation no longer guarantees monotonicity/strategy-proofness. We propose a simple yet empirically effective approach for designing near-optimal auctions for correlated bidders.  We train a neural network to interpolate the greedy allocation, while enforcing that the interpolation must be verifiably monotone.Empirically, our method consistently achieves near-optimal revenue across a wide range of distributions, including adversarially generated cases. Compared to existing baselines, our approach shows substantial improvement, often reducing the gap to the (unattainable) greedy upper bound by an order of magnitude.Furthermore, we demonstrate the generality of our approach by extending it to multi-unit auctions with unit demand, where we achieve similarly strong performance. Additionally, our verification techniques can be integrated into the RegretNet framework to design fully strategy-proof auctions.", "title_embedding_index": 10156, "title_abs_embedding_index": 10181}, {"title": "Region-Aware Generalized Face Anti-Spoofing via Chebyshev Convolutional Graph Networks", "link_suffix": "/forum?id=eFGIWUqHQm", "link": "https://openreview.net/forum?id=eFGIWUqHQm", "pdf_link": "https://openreview.net/pdf?id=eFGIWUqHQm", "keywords": "GNN, face spoofing, CNN", "abstract": "Face Anti-Spoofing (FAS) is critical for safeguarding face recognition systems from spoofing attacks. However, current methods based on Convolutional Neural Networks (CNNs) and Vision Transformers face limitations in modeling the diverse, region-specific attack behaviors, leading to reduced generalization. This challenge arises due to two main factors: (1) attacks manifest differently across facial regions due to variations in color, texture, and material properties; and (2) the large data space hinders effective generalization.To address these issues, we propose a novel approach utilizing Chebyshev Convolutional Graph Neural Networks (ChebConv GNNs) to model spatial information within a graph-based structure. ChebConv is particularly efficient in processing visual data from graphs derived from images. Our method processes regions around facial landmarks through the initial layers of a DenseNet to extract rich, local node features for each region. By assigning a node to each facial region, we construct a unified graph structure where the nodes correspond to the same regions across all faces. This enables the network to model local features and inter-region relationships effectively, reducing the data space and enhancing generalization.To further improve generalization across unseen domains, we integrate a Domain-Adversarial Graph Network. Additionally, we introduce an auxiliary self-supervised task to encourage the learning of region-specific texture features. Experimental results demonstrate that our method significantly outperforms existing approaches in terms of both accuracy and generalization.", "title_embedding_index": 10157, "title_abs_embedding_index": 10182}, {"title": "Revisiting Critical Learning Periods in Deep Neural Networks", "link_suffix": "/forum?id=zUtl4kJa0C", "link": "https://openreview.net/forum?id=zUtl4kJa0C", "pdf_link": "https://openreview.net/pdf?id=zUtl4kJa0C", "keywords": "DNN, Critical Learning Period", "abstract": "Deep neural networks (DNNs) exhibit critical learning periods (CLPs) during early training phases, when exposure to defective data can permanently impair model performance. The prevalent understanding of such periods, primarily based on the interpretation of Fisher Information (FI), attributes CLPs to the memorization phase. However, our theoretical and empirical study exhibits that such explanations of CLPs are inaccurate because of the misunderstanding of the relationship between FI and model memorization. As such, we revisit the CLPs in DNNs from the information theory and optimization perspectives, gaining a better and more accurate understanding of CLPs. \nWe visualize model memorization dynamics and observe that CLPs extend beyond the memorization phase. Additionally, we introduce the concept of the effective gradient, a novel metric able to quantify the actual influence of each training epoch on the optimization trajectory. Our empirical and theoretical analyses reveal that the norm of effective gradients generally diminishes over training epochs and eventually converges to zero, highlighting the disproportionate larger impact of initial training on final model outcomes. Besides, this insight also clarifies the mechanism behind permanent performance degradation due to defective initial training: the model becomes trapped in the suboptimal region of parameter space. Our work offers novel and in-depth understandings of CLPs and sheds light on enhancing model performance and robustness through such periods.", "title_embedding_index": 10158, "title_abs_embedding_index": 10183}, {"title": "Robust Model Evaluation over Large-scale Federated Networks", "link_suffix": "/forum?id=eOE2g28eX9", "link": "https://openreview.net/forum?id=eOE2g28eX9", "pdf_link": "https://openreview.net/pdf?id=eOE2g28eX9", "keywords": "Distributionally Robust Optimization (DRO), Generalization Bound, Federated Model Evaluation, Glivenko-Cantelli Theorem and DKW Bound", "abstract": "In this paper, we address the challenge of certifying the performance of a machine learning model on an unseen target network. We consider a source network \u201cA\u201d of $K$ clients, each with private data from unique and heterogeneous distributions, assumed to be independent samples from a broader meta-distribution $ \\mu $. Our goal is to provide certified guarantees for the model\u2019s performance on a different, unseen target network \u201cB,\u201d governed by another meta-distribution $ \\mu' $, assuming the deviation between $\\mu$ and $\\mu'$ is bounded by either the {\\it Wasserstein} distance or an $f$-{\\it divergence}. We derive theoretical guarantees for the model\u2019s empirical average loss and provide uniform bounds on the risk CDF, where the latter correspond to novel and adversarially robust versions of the Glivenko-Cantelli theorem and the Dvoretzky-Kiefer-Wolfowitz (DKW) inequality. Our bounds are computable in polynomial time with a polynomial number of queries to the $K$ clients, preserving client privacy by querying only the model\u2019s (potentially adversarial) loss on private data. We also establish non-asymptotic generalization bounds that consistently converge to zero as both $K$ and the minimum client sample size grow. Extensive empirical evaluations validate the robustness and practicality of our bounds across real-world tasks.", "title_embedding_index": 10159, "title_abs_embedding_index": 10184}, {"title": "ComputAgeBench: Epigenetic Aging Clocks Benchmark", "link_suffix": "/forum?id=0ApkwFlCxq", "link": "https://openreview.net/forum?id=0ApkwFlCxq", "pdf_link": "https://openreview.net/pdf?id=0ApkwFlCxq", "keywords": "biological age, epigenetic aging clocks, DNA methylation, aging biomarkers, longevity", "abstract": "The success of clinical trials of longevity drugs relies heavily on identifying integrative health and aging biomarkers, such as biological age. Epigenetic aging clocks predict the biological age of an individual using their DNA methylation profiles, commonly retrieved from blood samples. However, there is no standardized methodology to validate and compare epigenetic clock models as yet. We propose ComputAgeBench, a unifying framework that comprises such a methodology and a dataset for comprehensive benchmarking of different clinically relevant aging clocks. Our methodology exploits the core idea that reliable aging clocks must be able to distinguish between healthy individuals and those with aging-accelerating conditions. Specifically, we collected and harmonized 66 public datasets of blood DNA methylation, covering 19 such conditions across different ages and tested 13 published clock models. We believe our work will bring the fields of aging biology and machine learning closer together for the research on reliable biomarkers of health and aging.", "title_embedding_index": 10160, "title_abs_embedding_index": 10185}, {"title": "TPP-LLM: Modeling Temporal Point Processes by Efficiently Fine-Tuning Large Language Models", "link_suffix": "/forum?id=RofgmKmk5n", "link": "https://openreview.net/forum?id=RofgmKmk5n", "pdf_link": "https://openreview.net/pdf?id=RofgmKmk5n", "keywords": "temporal point processes, large language models, event prediction, parameter-efficient fine-tuning, low-rank adaptation", "abstract": "Temporal point processes (TPPs) are widely used to model the timing and occurrence of events in domains such as social networks, transportation systems, and e-commerce. In this paper, we introduce TPP-LLM, a novel framework that integrates large language models (LLMs) with TPPs to capture both the semantic and temporal aspects of event sequences. Unlike traditional methods that rely on categorical event type representations, TPP-LLM directly utilizes the textual descriptions of event types, enabling the model to capture rich semantic information embedded in the text. While LLMs excel at understanding event semantics, they are less adept at capturing temporal patterns. To address this, TPP-LLM incorporates temporal embeddings and employs parameter-efficient fine-tuning (PEFT) methods to effectively learn temporal dynamics without extensive retraining. This approach improves both predictive accuracy and computational efficiency. Experimental results across diverse real-world datasets demonstrate that TPP-LLM outperforms state-of-the-art baselines in sequence modeling and event prediction, highlighting the benefits of combining LLMs with TPPs.", "title_embedding_index": 10161, "title_abs_embedding_index": 10186}, {"title": "GTR: Improving Large 3D Reconstruction Models through Geometry and Texture Refinement", "link_suffix": "/forum?id=Oxpkn0YLG1", "link": "https://openreview.net/forum?id=Oxpkn0YLG1", "pdf_link": "https://openreview.net/pdf?id=Oxpkn0YLG1", "keywords": "3D Reconstruction, Feed-forward", "abstract": "We propose a novel approach for 3D mesh reconstruction from multi-view images. We improve upon the large reconstruction model LRM that use a transformer-based triplane generator and a Neural Radiance Field (NeRF) model trained on multi-view images. We introduce three key components to significantly enhance the 3D reconstruction quality. First of all, we examine the original LRM architecture and find several shortcomings. Subsequently, we introduce respective modifications to the LRM architecture, which lead to improved multi-view image representation and more computationally efficient training. Second, in order to improve geometry reconstruction and enable supervision at full image resolution, we extract meshes from the NeRF in a differentiable manner and fine-tune the NeRF model through mesh rendering. These modifications allow us to achieve state-of-the-art performance on both 2D and 3D evaluation metrics on Google Scanned Objects (GSO) dataset and OmniObject3D dataset. Finally, we introduce a lightweight per-instance texture refinement procedure to better reconstruct complex textures, such as text and portraits on assets. To address this, we introduce a lightweight per-instance texture refinement procedure. This procedure fine-tunes the triplane representation and the NeRF's color estimation model on the mesh surface using the input multi-view images in just 4 seconds. This refinement achieves faithful reconstruction of complex textures. Additionally, our approach enables various downstream applications, including text/image-to-3D generation.", "title_embedding_index": 10162, "title_abs_embedding_index": 10187}, {"title": "Spectral Graph Coarsening Using Inner Product Preservation and the Grassmann Manifold", "link_suffix": "/forum?id=6bKQVm6EOr", "link": "https://openreview.net/forum?id=6bKQVm6EOr", "pdf_link": "https://openreview.net/pdf?id=6bKQVm6EOr", "keywords": "Graph coarsening, Graph signal processing, Grassmann manifold, Node classification", "abstract": "In this work, we propose a new functorial graph coarsening approach that preserves inner products between node features. \nExisting graph coarsening methods often overlook the mutual relationships between node features, focusing primarily on the graph structure.\nBy treating node features as functions on the graph and preserving their inner products, our method ensures that the coarsened graph retains both structural and feature relationships, facilitating substantial benefits for downstream tasks. \nTo this end, we present the Inner Product Error (IPE) that quantifies how well inner products between node features are preserved. By leveraging the underlying geometry of the problem on the Grassmann manifold, we formulate an optimization objective that minimizes the IPE, even for unseen smooth functions. We show that minimizing the IPE also promotes improvements in other standard coarsening metrics. We demonstrate the effectiveness of our method through visual examples that highlight its clustering ability. Additionally, empirical results on benchmarks for graph coarsening and node classification show superior performance compared to state-of-the-art methods.", "title_embedding_index": 10163, "title_abs_embedding_index": 10188}, {"title": "Language Model-Driven Data Pruning Enables Efficient Active Learning", "link_suffix": "/forum?id=jBatISjqSn", "link": "https://openreview.net/forum?id=jBatISjqSn", "pdf_link": "https://openreview.net/pdf?id=jBatISjqSn", "keywords": "Active Learning, Data Pruning, Language Model", "abstract": "Active learning (AL) optimizes data labeling efficiency by selecting the most informative instances for annotation. A key component in this procedure is an acquisition function that guides the selection process and identifies the suitable instances for labeling from the unlabeled pool. However, these acquisition methods suffer from high computational costs with large unlabeled data pools, posing a roadblock to their applicability on large datasets. To address this challenge and bridge this gap, we introduce a novel plug-and-play unlabeled data pruning strategy, ActivePrune, which leverages language models to prune the unlabeled pool. ActivePrune implements a two-stage pruning process: an initial fast evaluation using perplexity scores from an n-gram language model, followed by a high-quality selection using metrics for data quality computed through a quantized LLM. Additionally, to enhance the diversity in the unlabeled pool, we propose a novel perplexity reweighting method that systematically brings forward underrepresented instances for selection in subsequent labeling iterations. Experiments on translation, sentiment analysis, topic classification, and summarization tasks on four diverse datasets and four active learning strategies demonstrate that ActivePrune outperforms existing data pruning methods. Finally, we compare the selection quality $\\leftrightarrow$ efficiency tradeoff of the data pruning methods and demonstrate that ActivePrune is computationally more efficient than other LLM score-based pruning methods, and provides up to 74% reduction in the end-to-end time required for active learning.", "title_embedding_index": 10164, "title_abs_embedding_index": 10189}, {"title": "Relevance-Based Embeddings for Efficient Relevance Retrieval", "link_suffix": "/forum?id=aDG34Bhbs1", "link": "https://openreview.net/forum?id=aDG34Bhbs1", "pdf_link": "https://openreview.net/pdf?id=aDG34Bhbs1", "keywords": "Information search, Relevance search, Nearest neighbor search, Relevance-based embeddings, Recommendation systems", "abstract": "In many machine learning applications, the most relevant items for a particular query should be efficiently extracted. The relevance function is usually an expensive neural similarity model making the exhaustive search infeasible. A typical solution to this problem is to train another model that separately embeds queries and items to a vector space, where similarity is defined via the dot product or cosine similarity. This allows one to search the most relevant objects through fast approximate nearest neighbors search at the cost of some reduction in quality. To compensate for this reduction, the found candidates are re-ranked by the expensive similarity model. In this paper, we investigate an alternative approach that utilizes the relevances of the expensive model to make relevance-based embeddings (RBE). The idea is to describe each query (item) by its relevance for a set of support items (queries) and use these new representations to obtain query (item) embeddings. We theoretically prove that relevance-based embeddings are powerful enough to approximate any complex similarity model (under mild conditions). An important ingredient of RBE is the choice of support items. We investigate several strategies and demonstrate that significant improvements can be obtained compared to random choice. Our experiments on diverse datasets illustrate the power of relevance-based embeddings.", "title_embedding_index": 10165, "title_abs_embedding_index": 10190}, {"title": "InstantPortrait: One-Step Portrait Editing via Diffusion Multi-Objective Distillation", "link_suffix": "/forum?id=ZkFMe3OPfw", "link": "https://openreview.net/forum?id=ZkFMe3OPfw", "pdf_link": "https://openreview.net/pdf?id=ZkFMe3OPfw", "keywords": "Portrait Editing, Diffusion Multi-Objective Distillation, On Step Inference, Identity Preservation, Text Fidelity", "abstract": "Real-time instruction-based portrait image editing is crucial in various applications, including filters, augmented reality, and video communications, etc. However, real-time portrait editing presents three significant challenges: identity preservation, fidelity to editing instructions, and fast model inference. Given that these aspects often present a trade-off, concurrently addressing them poses an even greater challenge. While diffusion-based image editing methods have shown promising capabilities in personalized image editing in recent years, they lack a dedicated focus on portrait editing and thus suffer from the aforementioned problems as well. To address the gap, this paper introduces an Instant-Portrait Network (IPNet), the first one-step diffusion-based model for portrait editing. We train the network in two stages. We first employ an annealing identity loss to train an Identity Enhancement Network (IDE-Net), to ensure robust identity preservation. We then train the IPNet using a novel diffusion Multi-Objective Distillation approach that integrates adversarial loss, identity distillation loss, and a novel Facial-Style Enhancing loss. The Diffusion Multi-Objective Distillation approach efficiently reduces inference steps, ensures identity consistency, and enhances the precision of instruction-based editing. Extensive comparison with prior models demonstrates IPNet as a superior model in terms of identity preservation, text fidelity, and inference speed.", "title_embedding_index": 10166, "title_abs_embedding_index": 10191}, {"title": "GPUDrive: Data-driven, multi-agent driving simulation at 1 million FPS", "link_suffix": "/forum?id=ERv8ptegFi", "link": "https://openreview.net/forum?id=ERv8ptegFi", "pdf_link": "https://openreview.net/pdf?id=ERv8ptegFi", "keywords": "Simulation, benchmark, multi-agent reinforcement learning, autonomous vehicles, planning", "abstract": "Multi-agent learning algorithms have been successful at generating superhuman planning in various games but have had limited impact on the design of deployed multi-agent planners. A key bottleneck in applying these techniques to multi-agent planning is that they require billions of steps of experience. To enable the study of multi-agent planning at scale, we present GPUDrive, a GPU-accelerated, multi-agent simulator built on top of the Madrona Game Engine that can generate over a million simulation steps per second. Observation, reward, and dynamics functions are written directly in C++, allowing users to define complex, heterogeneous agent behaviors that are lowered to high-performance CUDA. We show that using GPUDrive we can effectively train reinforcement learning agents over many scenes in the Waymo Open Motion Dataset, yielding highly effective goal-reaching agents in minutes for individual scenes and enabling agents to navigate thousands of scenarios within hours. The code base with pre-trained agents is available at \\url{redacted-url} and demonstration videos at \\url{https://sites.google.com/view/gpudrive/}.", "title_embedding_index": 10167, "title_abs_embedding_index": 10192}, {"title": "Align Your Intents: Offline Imitation Learning via Optimal Transport", "link_suffix": "/forum?id=9TL99KnTv5", "link": "https://openreview.net/forum?id=9TL99KnTv5", "pdf_link": "https://openreview.net/pdf?id=9TL99KnTv5", "keywords": "Optimal Transport, Reinforcement Learning, Offline RL, Intention learning", "abstract": "Offline reinforcement learning (RL) addresses the problem of sequential decision-making by learning optimal policy through pre-collected data, without interacting with the environment. As yet, it has remained somewhat impractical, because one rarely knows the reward explicitly and it is hard to distill it retrospectively. Here, we show that an imitating agent can still learn the desired behavior merely from observing the expert, despite the absence of explicit rewards or action labels. In our method, AILOT (Aligned Imitation Learning via Optimal Transport), we involve special representation of states in a form of intents that incorporate pairwise spatial distances within the data. Given such representations, we define intrinsic reward function via optimal transport distance between the expert's and the agent's trajectories. We report that AILOT outperforms state-of-the art offline imitation learning algorithms on D4RL benchmarks and improves the performance of other offline RL algorithms by dense reward relabelling in the sparse-reward tasks.", "title_embedding_index": 10168, "title_abs_embedding_index": 10193}, {"title": "Hierarchical Uncertainty Estimation for Learning-based Registration in Neuroimaging", "link_suffix": "/forum?id=w8LMtFY97b", "link": "https://openreview.net/forum?id=w8LMtFY97b", "pdf_link": "https://openreview.net/pdf?id=w8LMtFY97b", "keywords": "Image registration, uncertainty estimation, medical image analysis", "abstract": "Over recent years, deep learning based image registration has achieved impressive accuracy in many domains, including medical imaging and, specifically, human neuroimaging with magnetic resonance imaging (MRI). However, the uncertainty estimation associated with these methods has been largely limited to the application of generic techniques (e.g., Monte Carlo dropout) that do not exploit the peculiarities of the problem domain, particularly spatial modeling. Here, we propose a principled way to propagate uncertainties (epistemic or aleatoric) estimated at the level of spatial location by these methods, to the level of global transformation models, and further to downstream tasks. Specifically, we justify the choice of a Gaussian distribution for the local uncertainty modeling, and then propose a framework where uncertainties spread across hierarchical levels, depending on the choice of transformation model. Experiments on publicly available data sets show that Monte Carlo dropout correlates very poorly with the reference registration error, whereas our uncertainty estimates correlate much better. % with the reference registration error. Crucially, the results also show that uncertainty-aware fitting of transformations improves the registration accuracy of brain MRI scans. Finally, we illustrate how sampling from the posterior distribution of the transformations can be used to propagate uncertainties to downstream neuroimaging tasks.", "title_embedding_index": 10169, "title_abs_embedding_index": 10194}, {"title": "Training Language Models to Win Debates with Self-Play Improves Judge Accuracy", "link_suffix": "/forum?id=gAEEjGv5Oa", "link": "https://openreview.net/forum?id=gAEEjGv5Oa", "pdf_link": "https://openreview.net/pdf?id=gAEEjGv5Oa", "keywords": "AI Safety, Scalable Oversight, Reinforcement Learning, Debate", "abstract": "We test the robustness of debate as a method of scalable oversight by training models to debate with data generated via self-play. In a long-context reading comprehension task, we find that language model based evaluators answer questions more accurately when judging models optimized to win debates. By contrast, we find no such relationship for consultancy models trained to persuade a judge without an opposing debater present. In quantitative and qualitative comparisons between our debate models and novel consultancy baselines, we find evidence that debate training encourages stronger and more informative arguments, showing promise that it can help provide high-quality supervision for tasks that are difficult to directly evaluate.", "title_embedding_index": 10170, "title_abs_embedding_index": 10195}, {"title": "Fast Few-Shot Graph Flow Prediction", "link_suffix": "/forum?id=rrn6XXB3u0", "link": "https://openreview.net/forum?id=rrn6XXB3u0", "pdf_link": "https://openreview.net/pdf?id=rrn6XXB3u0", "keywords": "graph flow, flow prediction, graph neural network, few-shot learning, traffic prediction, neural network", "abstract": "Accurate prediction of traffic flow is crucial for optimizing transportation networks, mitigating congestion, and improving urban planning. However, existing approaches like graph neural networks (GNNs) and traffic simulations face challenges in predicting flow for unseen road networks without historical data. Without abundant training data, GNNs often generalize poorly to new graphs, while simulations can be computationally infeasible for large-scale networks. This paper tackles the problem of few-shot traffic flow prediction in unseen road networks. We propose a novel traffic simulation algorithm that efficiently predicts flow based on node and edge attributes. Through theoretical analysis, we demonstrate our approach closely approximates true flow with asymptotically optimal runtime complexity. Experiments on real-world road networks show our simulation algorithm outperforms GNNs for predicting traffic in unseen cities after training on only three cities. While motivated by traffic prediction in road networks, we expect our contributions to have broader applicability to general graph flow prediction problems across domains.", "title_embedding_index": 10171, "title_abs_embedding_index": 10196}, {"title": "When LLMs Play the Telephone Game: Cumulative Changes and Attractors in Iterated Cultural Transmissions", "link_suffix": "/forum?id=fN8yLc3eA7", "link": "https://openreview.net/forum?id=fN8yLc3eA7", "pdf_link": "https://openreview.net/pdf?id=fN8yLc3eA7", "keywords": "Large Language Models, Multi-Turn Behaviour, Cultural Evolution, Attractors, Transmission Chain", "abstract": "As large language models (LLMs) start interacting with each other and generating an increasing amount of text online, it becomes crucial to better understand how information is transformed as it passes from one LLM to the next. While significant research has examined individual LLM behaviors, existing studies have largely overlooked the collective behaviors and information distortions arising from iterated LLM interactions. Small biases, negligible at the single output level, risk being amplified in iterated interactions, potentially leading the content to evolve towards attractor states. In a series oftelephone game experiments, we apply a transmission chain design borrowed from the human cultural evolution literature: LLM agents iteratively receive, produce, and transmit texts from the previous to the next agent in the chain. By tracking the evolution of texttoxicity,positivity,difficulty, andlengthacross transmission chains, we uncover the existence of biases and attractors, and study their dependence on the initial text, the instructions, language model, and model size. For instance, we find that more open-ended instructions lead to stronger attraction effects compared to more constrained tasks. We also find that different text properties display different sensitivity to attraction effects, withtoxicityleading to stronger attractors thanlength. These findings highlight the importance of accounting for multi-step transmission dynamics and represent a first step towards a more comprehensive understanding of LLM cultural dynamics.", "title_embedding_index": 10172, "title_abs_embedding_index": 10197}, {"title": "Fairness-Aware Graph Learning: A Benchmark", "link_suffix": "/forum?id=M4RhGr2lAy", "link": "https://openreview.net/forum?id=M4RhGr2lAy", "pdf_link": "https://openreview.net/pdf?id=M4RhGr2lAy", "keywords": "Graph Learning Algorithms, Algorithmic Fairness, Performance Benchmark", "abstract": "Fairness-aware graph learning has gained increasing attention in recent years. Nevertheless, there lacks a comprehensive benchmark to evaluate and compare different fairness-aware graph learning methods, which blocks practitioners from choosing appropriate ones for broader real-world applications. In this paper, we present an extensive benchmark on ten representative fairness-aware graph learning methods. Specifically, we design a systematic evaluation protocol and conduct experiments on seven real-world datasets to evaluate these methods from multiple perspectives, including group fairness, individual fairness, the balance between different fairness criteria, and computational efficiency. Our in-depth analysis reveals key insights into the strengths and limitations of existing methods. Additionally, we provide practical guidance for applying fairness-aware graph learning methods in applications. To the best of our knowledge, this work serves as an initial step towards comprehensively understanding representative fairness-aware graph learning methods to facilitate future advancements in this area.", "title_embedding_index": 10173, "title_abs_embedding_index": 10198}, {"title": "End-to-End Learning under Endogenous Uncertainty", "link_suffix": "/forum?id=JWGJblAs7V", "link": "https://openreview.net/forum?id=JWGJblAs7V", "pdf_link": "https://openreview.net/pdf?id=JWGJblAs7V", "keywords": "End-to-end learning, contextual stochastic optimization", "abstract": "How can we effectively learn to make decisions when there are no ground-truth counterfactual observations? We propose an end-to-end learning approach to the contextual stochastic optimization problem under decision-dependent uncertainty. We propose both exact methods and efficient sampling-based methods to implement our approach. We also introduce a new class of two-stage stochastic optimization problems to the end-to-end learning framework. Here, the first stage is an information-gathering problem to decide which random variable to \"\"poll'' and gain information about before making a second-stage decision based off of it. We provide theoretical analysis showing that (1) optimally minimizing our proposed objective produces optimal decisions and (2) generalization bounds between in-sample and out-of-sample cost.\nWe  computationally test the proposed approach on multi-item assortment problems where demand is affected by cross-item complementary and supplementary effects. \nOur results show a performance improvement of over 20% compared to traditional methods.\nWe also introduce an experiment for the information-gathering problem on a real-world electricity generation problem. We show our method proposes decisions with more than 7% lower cost than other decision-making methods.", "title_embedding_index": 10174, "title_abs_embedding_index": 10199}]