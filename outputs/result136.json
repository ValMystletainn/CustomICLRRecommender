[{"title": "CheapNet: Cross-attention on Hierarchical representations for Efficient protein-ligand binding Affinity Prediction", "link_suffix": "/forum?id=A1HhtITVEi", "link": "https://openreview.net/forum?id=A1HhtITVEi", "pdf_link": "https://openreview.net/pdf?id=A1HhtITVEi", "keywords": "Protein-Ligand Binding Affinity, Hierarchical Representation Learning, Cross-Attention Mechanism, Drug Discovery", "abstract": "Accurately predicting protein-ligand binding affinity is a critical challenge in drug discovery, crucial for understanding drug efficacy. While existing models typically rely on atom-level interactions, they often fail to capture the complex, higher-order interactions, resulting in noise and computational inefficiency. Transitioning to modeling these interactions at the cluster level is challenging because it is difficult to determine which atoms form meaningful clusters that drive the protein-ligand interactions. To address this, we propose CheapNet, a novel interaction-based model that integrates atom-level representations with hierarchical cluster-level interactions through a cross-attention mechanism. By employing differentiable pooling of atom-level embeddings, CheapNet efficiently captures essential higher-order molecular representations crucial for accurate binding predictions. Extensive evaluations demonstrate that CheapNet not only achieves state-of-the-art performance across multiple binding affinity prediction tasks but also maintains prediction accuracy with reasonable computational efficiency.", "title_embedding_index": 6750, "title_abs_embedding_index": 6775}, {"title": "Multi-scale Minimal Sufficient Representation Learning for Domain Generalization in Sleep Staging", "link_suffix": "/forum?id=Ww599CnVnU", "link": "https://openreview.net/forum?id=Ww599CnVnU", "pdf_link": "https://openreview.net/pdf?id=Ww599CnVnU", "keywords": "Domain generalization, Multi-scale, Contrastive learning, Information theory, Sleep staging", "abstract": "Deep learning-based automatic sleep staging demonstrates strong performance as a promising solution for diagnosing sleep disorders. However, deep learning models often struggle to generalize on unseen subjects due to variability in physiological signals, resulting in degraded performance in out-of-distribution scenarios. To address this issue, domain generalization approaches have recently been studied actively to ensure generalized performance on unseen domains during the training. Among those techniques, contrastive learning has proven its validity in learning domain-invariant features by aligning samples of the same class across different domains. Despite its potential, many existing methods are insufficient for extracting truly domain-invariant representations, as they do not explicitly reduce domain-relevant information embedded in the features. In this paper, we argue that addressing superfluous information is a key to bridging the domain gap. Furthermore, existing methods often neglect the multi-scale nature of sleep signals, potentially missing important temporal and spectral characteristics. To address these limitations, we propose a novel Multi-Scale Minimal Sufficient representation learning (MSMS) framework, which effectively reduces domain-relevant information while preserving essential temporal and spectral features for sleep stage classification. We evaluate our method on publicly available sleep staging benchmark datasets, SleepEDF-20 and MASS. Experimental results demonstrate that our approach consistently outperforms state-of-the-art methods.", "title_embedding_index": 6751, "title_abs_embedding_index": 6776}, {"title": "Benchmarking Machine Learning Methods for Stock Prediction", "link_suffix": "/forum?id=bsXxNkhvm6", "link": "https://openreview.net/forum?id=bsXxNkhvm6", "pdf_link": "https://openreview.net/pdf?id=bsXxNkhvm6", "keywords": "Stock forecast, Benchmark, Evaluation, AI4Finance", "abstract": "Machine learning has been widely applied to stock movement prediction. However, research in this field is often hindered by the lack of high-quality benchmark datasets and comprehensive evaluation methods. To address these challenges, we introduce \\textit{BenchStock}, a benchmark that includes standardized datasets from the two largest stock markets (the U.S. and China) along with an evaluation method designed to facilitate a thorough examination of machine learning stock prediction methods. This benchmark covers a range of models, from traditional machine learning techniques to the latest deep learning approaches. Using BenchStock, we conducted large-scale experiments predicting individual stock returns over three decades in both markets to assess both short-term and long-term performance. To evaluate the impact of these predictions in actual market conditions, we constructed a portfolio based on the predictions and used a backtesting program to simulate its performance. The experiments revealed several key findings that have not been reported: 1) Most methods outperformed the S&P 500 in the U.S. market but experienced significant losses in the Chinese market. 2) Prediction accuracy of a method was not correlated with its portfolio return.  3) Advanced deep learning methods did not outperform traditional approaches. 4) The performance of the models was highly dependent on the testing period. These findings highlight the complexity of stock prediction and call for more in-depth machine learning research in this field.", "title_embedding_index": 6752, "title_abs_embedding_index": 6777}, {"title": "CaLMol: Disentangled Causal Graph LLM for Molecular Relational Learning", "link_suffix": "/forum?id=0Ra0E43kK0", "link": "https://openreview.net/forum?id=0Ra0E43kK0", "pdf_link": "https://openreview.net/pdf?id=0Ra0E43kK0", "keywords": "Molecular Relational Learning, Large language Model, Graph Neural Network, Causal Learning", "abstract": "Molecular Relational Learning (MRL), focused on understanding interactions between molecular pairs, is essential for drug design by utilizing both structural properties and textual knowledge, such as expert documents. However, most existing MRL methods assume static molecular distributions, meaning the distributions remain consistent across training and testing stages. This assumption may lead to the exploitation of variant correlations between structures and texts regarding interactions, thereby failing in the ubiquitous scenarios involving new drug predictions. To bridge this gap, we investigate zero-shot MRL by leveraging invariant relationships between molecular texts and structures w.r.t interactions for new molecules, which is largely unexplored in the literature and is highly non-trivial with following challenges: 1) How to disentangle molecular structure components between each pair to intrinsically determine interactions and address potential structural distribution shift issues for new drugs? 2) How to align molecular structures with semantic textual information to achieve invariant molecular relation predictions for new drugs? To tackle these challenges, we propose a novel Causally Disentangled Invariant Graph Large Language Model (LLM) for Molecular Relational Learning (CaLMol), capable of exploiting invariant molecular relationships to predict interactions for new drugs. Specifically, we propose Causal Molecule Substructure Disentanglement to capture the invariant well-recognized substructure pair for a specific molecule interaction. Then, we propose Molecule Structure and Property aware LLM Alignment to use molecule (with invariant substructure)-textual property pair to align structure information to semantic information, and use them together to guide the interaction prediction. On this basis, LLM can also provide further explanations.\nExtensive experiments on qualitative and quantitative tasks including 8 datasets demonstrate that our proposed CaLMol achieves advanced performance on predicting molecule interactions involving new molecules.", "title_embedding_index": 6753, "title_abs_embedding_index": 6778}, {"title": "A Riemannian Framework for Learning Reduced-order Lagrangian Dynamics", "link_suffix": "/forum?id=RoN6M3i7gJ", "link": "https://openreview.net/forum?id=RoN6M3i7gJ", "pdf_link": "https://openreview.net/pdf?id=RoN6M3i7gJ", "keywords": "physics-inspired networks, dynamics learning, model-order reduction, Riemannian geometry, deformable objects", "abstract": "By incorporating physical consistency as inductive bias, deep neural networks display increased generalization capabilities and data efficiency in learning nonlinear dynamic models. However, the complexity of these models generally increases with the system dimensionality, requiring larger datasets, more complex deep networks, and significant computational effort.\nWe propose a novel geometric network architecture to learn physically-consistent reduced-order dynamic parameters that accurately describe the original high-dimensional system behavior.\nThis is achieved by building on recent advances in model-order reduction and adopting a Riemannian perspective to jointly learn a structure-preserving latent space and the associated low-dimensional dynamics.\nOur approach enables accurate long-term predictions of the high-dimensional dynamics of rigid and deformable systems with increased data efficiency by inferring interpretable and physically plausible reduced Lagrangian models.", "title_embedding_index": 6754, "title_abs_embedding_index": 6779}, {"title": "A Closer Look at Backdoor Attacks on CLIP", "link_suffix": "/forum?id=Ud7I21wHnl", "link": "https://openreview.net/forum?id=Ud7I21wHnl", "pdf_link": "https://openreview.net/pdf?id=Ud7I21wHnl", "keywords": "Backdoor Attacks, CLIP, Representation Decomposing, Attention Heads", "abstract": "We present a comprehensive empirical study on how backdoor attacks affect CLIP by analyzing the representations of backdoor images. Specifically, based on the methodology of representation decomposing, image representations can be decomposed into a sum of representations across individual image patches, attention heads (AHs), and multi-layer perceptrons (MLPs) in different model layers. By\nexamining the effect of backdoor attacks on model components, we have the following empirical findings. (1) Different backdoor attacks would infect different model components, i.e., local patch-based backdoor attacks mainly affect AHs, while global noise-based backdoor attacks mainly affect MLPs. (2) Infected AHs are centered on the last layer, while infected MLPs are decentralized on several late layers. (3) Some AHs are not greatly infected by backdoor attacks, and even infected AHs could still maintain the original functionality. These observations motivate us to defend against backdoor attacks by detecting infected AHs, repairing their representations or filtering backdoor samples with too many infected AHs, in the inference stage. Experimental results validate our empirical findings and demonstrate the effectiveness of the defense methods", "title_embedding_index": 6755, "title_abs_embedding_index": 6780}, {"title": "Reassessing the Validity of Spurious Correlations Benchmarks", "link_suffix": "/forum?id=CU8CNDw6Vv", "link": "https://openreview.net/forum?id=CU8CNDw6Vv", "pdf_link": "https://openreview.net/pdf?id=CU8CNDw6Vv", "keywords": "benchmarking, evaluation, spurious correlations", "abstract": "Neural networks can fail when the data contains spurious correlations. To understand this phenomenon, researchers have proposed numerous spurious correlations benchmarks upon which to evaluate mitigation methods. However, we observe that these benchmarks exhibit substantial disagreement, with the best methods on one benchmark performing poorly on another. We explore this disagreement, and examine benchmark validity by defining three desiderata that a benchmark should satisfy in order to meaningfully evaluate methods. Our results have implications for both benchmarks and mitigations: we find that certain benchmarks are not meaningful measures of method performance, and that several methods are not sufficiently robust for widespread use. We present a simple recipe for practitioners to choose methods using themost similarbenchmark to their given problem.", "title_embedding_index": 6756, "title_abs_embedding_index": 6781}, {"title": "CARTS: Advancing Neural Theorem Proving with Diversified Tactic Calibration and Bias-Resistant Tree Search", "link_suffix": "/forum?id=VQwI055flA", "link": "https://openreview.net/forum?id=VQwI055flA", "pdf_link": "https://openreview.net/pdf?id=VQwI055flA", "keywords": "Neural Theorem Proving, Diversified Tactic Calibration, Bias-Resistant Tree Search, Monte Carlo Tree Search", "abstract": "Recent advancements in neural theorem proving integrate large language models with tree search algorithms like Monte Carlo Tree Search (MCTS), where the language model suggests tactics and the tree search finds the complete proof path. However, many tactics proposed by the language model converge to semantically or strategically similar, reducing diversity and increasing search costs by expanding redundant proof paths. This issue exacerbates as computation scales and more tactics are explored per state. Furthermore, the trained value function suffers from false negatives, label imbalance, and domain gaps due to biased data construction.  To address these challenges, we propose CARTS (diversified tactic CAlibration and bias-Resistant Tree Search), which balances tactic diversity and importance while calibrating model confidence. CARTS also introduce preference modeling and an adjustment term related to the ratio of valid tactics to improve the bias-resistance of the value function. Experimental results demonstrate that CARTS consistently outperforms previous methods achieving a pass@l rate of 49.6% on the miniF2F-test benchmark. Further analysis confirms that CARTS improves tactic diversity and leads to a more balanced tree search.", "title_embedding_index": 6757, "title_abs_embedding_index": 6782}, {"title": "Simple ReFlow: Improved Techniques for Fast Flow Models", "link_suffix": "/forum?id=fpvgSDKXGY", "link": "https://openreview.net/forum?id=fpvgSDKXGY", "pdf_link": "https://openreview.net/pdf?id=fpvgSDKXGY", "keywords": "flow matching, diffusion models, reflow, optimal transport, generative modeling", "abstract": "Diffusion and flow-matching models achieve remarkable generative performance but at the cost of many neural function evaluations (NFE), which slows inference and limits applicability to time-critical tasks. The ReFlow procedure can accelerate sampling by straightening generation trajectories. But it is an iterative procedure, typically requiring training on simulated data, and results in reduced sample quality. To mitigate sample deterioration, we examine the design space of ReFlow and highlight potential pitfalls in prior heuristic practices. We then propose seven improvements for training dynamics, learning and inference, which are verified with thorough ablation studies on CIFAR10 $32 \\times 32$, AFHQv2 $64 \\times 64$, and FFHQ $64 \\times 64$. Combining all our techniques, we achieve state-of-the-art FID scores (without / with guidance, resp.) for fast generation via neural ODEs: $2.23$ / $1.98$ on CIFAR10, $2.30$ / $1.91$ on AFHQv2, $2.84$ / $2.67$ on FFHQ, and $3.49$ / $1.74$ on ImageNet-64, all with merely $9$ NFEs.", "title_embedding_index": 6758, "title_abs_embedding_index": 6783}, {"title": "Repetitive Contrastive Learning Enhances Mamba's Selectivity in Time Series Prediction", "link_suffix": "/forum?id=tIURLNBTPx", "link": "https://openreview.net/forum?id=tIURLNBTPx", "pdf_link": "https://openreview.net/pdf?id=tIURLNBTPx", "keywords": "Mamba; Self-supervised learning; Time Series Prediction", "abstract": "The prediction of long sequences has always been a challenge in time series forecasting tasks. Due to Mamba's sequence selection capability, many Mamba-based models have been proposed, achieving state-of-the-art results in long sequence prediction problems. However, much research has focused on integrating mamba-ssm into specific model structures for better performance, while the core of mamba-ssm, its sequence selection capability, has not been deeply explored. We believe there is significant potential in Mamba's sequence selection capability and propose a Repetitive Contrastive Learning method to enhance it. Specifically, we use Repeating Sequence Augmentation to increase the sequences and introduce Gaussian noise, and enhance the mamba block's sequence selection capability through Inter-sequence and Intra-sequence contrast. We transfer the pre-trained parameters to various Mamba-based models for fine-tuning and compare the performance improvements. Experiments demonstrate that our method can universally enhance the performance of Mamba-based models without additional memory requirements, irrespective of model and parameter constraints.", "title_embedding_index": 6759, "title_abs_embedding_index": 6784}, {"title": "IDS-Agent: An LLM Agent for Explainable Intrusion Detection in IoT Networks", "link_suffix": "/forum?id=uuCcK4cmlH", "link": "https://openreview.net/forum?id=uuCcK4cmlH", "pdf_link": "https://openreview.net/pdf?id=uuCcK4cmlH", "keywords": "intrusion detection, LLM agent, internet of things, LLM", "abstract": "Emerging threats to IoT networks have accelerated the development of intrusion\ndetection systems (IDSs), characterized by a shift from traditional approaches\nbased on attack signatures or anomaly detection to approaches based on machine\nlearning (ML). However, current ML-based IDSs often lack result explanations\nand struggle to address zero-day attacks due to their fixed output label space. In\nthis paper, we propose IDS-Agent, the first IDS based on an AI agent powered\nby large language models (LLMs). For each input network traffic and a detection\nrequest from the user, IDS-Agent predicts whether the traffic is benign or being\nattacked, with an explanation of the prediction results. The workflow of IDS-Agent\ninvolves iterative reasoning by a core LLM over the observation and action gen-\neration informed by the reasoning and retrieved knowledge. The action space of\nIDS-Agent includes data extraction and preprocessing, classification, knowledge\nretrieval, and results aggregation \u2013 these actions will be executed using abundant\ntools, mostly specialized for IDS. Furthermore, the IDS-Agent is equipped with\na memory and knowledge base that retains information from current and pre-\nvious sessions, along with IDS-related documents, enhancing its reasoning and\naction generation capabilities. The system prompts of IDS-Agent can be easily\ncustomized to adjust detection sensitivity or identify previously unknown types\nof attacks. In our experiments, we demonstrate the strong detection capabilities\nof IDS-Agent compared with ML-based IDSs and an IDS based on LLM with\nprompt engineering. IDS-Agent outperforms these SOTA baselines on the ACI-IoT\nand CIC-IoT benchmarks, with 0.97 and 0.75 detection F1 scores, respectively.\nIDS-Agent also achieves a recall of 0.61 in detecting zero-day attacks.", "title_embedding_index": 6760, "title_abs_embedding_index": 6785}, {"title": "A Theoretical Perspective: When and How Self-consuming Training Loops Generalize", "link_suffix": "/forum?id=WttfQGwpES", "link": "https://openreview.net/forum?id=WttfQGwpES", "pdf_link": "https://openreview.net/pdf?id=WttfQGwpES", "keywords": "Generative Models, Synthetic Data, Transformer, Generalization Error, Learning Theory", "abstract": "High-quality data is essential for training large generative models, yet the vast reservoir of real data available online has become nearly depleted. Consequently, models increasingly generate their own data for further training, forming Self-consuming Training Loops (STLs). However, the empirical results have been strikingly inconsistent: some models degrade or even collapse, while others successfully avoid these failures, leaving a significant gap in theoretical understanding to explain this discrepancy. This paper introduces the intriguing notion ofrecursive stabilityand presents the first theoretical generalization analysis, revealing how both model architecture and the proportion between real and synthetic data influence the success of STLs. We further extend this analysis to transformers in in-context learning, showing that even a constant-sized proportion of real data ensures convergence, while also providing insights into optimal synthetic data sizing.", "title_embedding_index": 6761, "title_abs_embedding_index": 6786}, {"title": "Balancing Domain-Invariant and Domain-Specific Knowledge for Domain Generalization with Online Knowledge Distillation", "link_suffix": "/forum?id=6u4Tv9cW0E", "link": "https://openreview.net/forum?id=6u4Tv9cW0E", "pdf_link": "https://openreview.net/pdf?id=6u4Tv9cW0E", "keywords": "Transfer Learning, Domain Generalization, Knowledge Distillation", "abstract": "Deep learning models often experience performance degradation when the distribution of testing data differs from that of training data.\nDomain generalization addresses this problem by leveraging knowledge from multiple source domains to enhance model generalizability.\nRecent studies have shown that distilling knowledge from large pretrained models effectively improves a model's ability to generalize to unseen domains. However, current knowledge distillation-based domain generalization approaches overlook the importance of domain-specific knowledge and rely on a two-stage training process, which limits the effectiveness of knowledge transfer. To overcome these limitations, we propose the Balanced Online knowLedge Distillation (BOLD) framework for domain generalization. BOLD employs a multi-domain expert teacher model, with each expert specializing in specific source domains to preserve domain-specific knowledge. This approach enables the student to distil both domain-invariant and domain-specific knowledge from the teacher. Additionally, BOLD adopts an online knowledge distillation strategy where the teacher and students learn simultaneously, allowing the teacher to adapt based on the student's feedback, thereby enhancing knowledge transfer and improving the student's generalizability. Extensive experiments conducted with state-of-the-art baselines on seven domain generalization benchmarks demonstrate the effectiveness of the BOLD framework. We also provide a theoretical analysis that underscores the effectiveness of domain-specific knowledge and the online knowledge distillation strategy in domain generalization.", "title_embedding_index": 6762, "title_abs_embedding_index": 6787}, {"title": "Pareto Low-Rank Adapters: Efficient Multi-Task Learning with Preferences", "link_suffix": "/forum?id=icDoYdUhRa", "link": "https://openreview.net/forum?id=icDoYdUhRa", "pdf_link": "https://openreview.net/pdf?id=icDoYdUhRa", "keywords": "Pareto Front Learning, multi-task learning, low rank", "abstract": "Multi-task trade-offs in machine learning can be addressed via Pareto Front Learning (PFL) methods that parameterize the Pareto Front (PF) with a single model. PFL permits to select the desired operational point during inference, contrary to traditional Multi-Task Learning (MTL) that optimizes for a single trade-off decided prior to training. However, recent PFL methodologies suffer from limited scalability, slow convergence, and excessive memory requirements, while exhibiting inconsistent mappings from preference to objective space. We introduce PaLoRA, a novel parameter-efficient method that addresses these limitations in two ways. First, we augment any neural network architecture with task-specific low-rank adapters and continuously parameterize the Pareto Front in their convex hull. Our approach steers the original model and the adapters towards learning general and task-specific features, respectively. Second, we propose a deterministic sampling schedule of preference vectors that reinforces this division of labor, enabling faster convergence and strengthening the validity of the mapping from preference to objective space throughout training. Our experiments show that PaLoRA outperforms state-of-the-art MTL and PFL baselines across various datasets, scales to large networks, reducing the memory overhead $23.8-31.7$ times compared with competing PFL baselines in scene understanding benchmarks.", "title_embedding_index": 6763, "title_abs_embedding_index": 6788}, {"title": "Electrocardiogram Foundation Model Using Temporally Augmented Patient Contrastive Learning", "link_suffix": "/forum?id=7zJDTnogdG", "link": "https://openreview.net/forum?id=7zJDTnogdG", "pdf_link": "https://openreview.net/pdf?id=7zJDTnogdG", "keywords": "Contrastive learning, Sef-supervised pre-training, Electrocardiograms, Deep learning, Foundation model.", "abstract": "Electrocardiograms ($ECGs$) capture the electrical activity of the heart, offering rich diagnostic and prognostic insights. Traditionally, electrocardiograms are interpreted by human experts, but deep learning is now encroaching on this domain and combining human-like intelligence with machine precision for a deeper insight. Self-supervised pretraining is essential for maximizing the potential of scarce medical data. Applied to $ECGs$, patient-contrastive learning has shown promising results, by utilizing the natural variations in the cardiac signals. In this study, we introduce Temporally Augmented Patient Contrastive Learning of Representations ($TA\\text{-}PCLR$), a novel approach that incorporates temporal augmentations into a patient contrastive self-supervised foundation model. Trained on one of the largest diverse cohorts of more than six million unlabelled electrocardiograms from three different continents, we demonstrate the efficacy of our approach and show its value as a feature extraction tool for small and medium-sized labeled datasets. We also validate the performance on an open-source external cohort, surpassing similar pretraining approaches while outperforming an ensemble of fully supervised deep networks on some labels. Additionally, we conduct a detailed exploration of how pre-training dataset distribution impacts supervised task performance, marking the first investigation of its kind.", "title_embedding_index": 6764, "title_abs_embedding_index": 6789}, {"title": "Reasoning Elicitation in Language Models via Counterfactual Feedback", "link_suffix": "/forum?id=VVixJ9QavY", "link": "https://openreview.net/forum?id=VVixJ9QavY", "pdf_link": "https://openreview.net/pdf?id=VVixJ9QavY", "keywords": "language models, reasoning, fine-tuning, counterfactuals", "abstract": "Despite the increasing effectiveness of language models, their reasoning capabilities remain underdeveloped. In particular, causal reasoning through counterfactual question answering is lacking. This work aims to bridge this gap. We first derive novel metrics that balance accuracy in factual and counterfactual questions, capturing a more complete view of the reasoning abilities of language models than traditional factual-only based metrics. Second, we propose several fine-tuning approaches that aim to elicit better reasoning mechanisms, in the sense of the proposed metrics. Finally, we evaluate the performance of the fine-tuned language models in a variety of realistic scenarios. In particular, we investigate to what extent our fine-tuning approaches systemically achieve better generalization with respect to the base models in several problems that require, among others, inductive and deductive reasoning capabilities.", "title_embedding_index": 6765, "title_abs_embedding_index": 6790}, {"title": "Narcissus: Leveraging Early Training Dynamics for Unsupervised Anomaly Detection", "link_suffix": "/forum?id=PzkYyZH9Fx", "link": "https://openreview.net/forum?id=PzkYyZH9Fx", "pdf_link": "https://openreview.net/pdf?id=PzkYyZH9Fx", "keywords": "Anomaly Detection, Unsupervised Learning, Early Stopping", "abstract": "Anomaly detection is a critical learning task with many significant and diverse applications. Currently, semi-supervised methods provide the state-of-the-art accuracy performance but require labeled normal data for training. Unsupervised approaches, on the other hand, do not have this requirement but can only offer inferior anomaly detection performance. In this paper, we introduce NARCISSUS, a novel unsupervised anomaly detection method that achieves accuracy comparable to semi-supervised approaches. Our key insight is that a learning model when training with a mix of normal and sparse anomalous data converges first on normal data. Leveraging this insight, NARCISSUS employs a tailored early stopping scheme, eliminating the need for pseudo labels and costly label generation interactions. It also offers systematic solutions to minimize the influence of model uncertainty, ensuring robust detection. NARCISSUS is model-agnostic and can therefore make use of even a semi-supervised anomaly detection model underneath, thereby turning it into an unsupervised one. Comprehensive evaluations using time series, image and graph datasets show that NARCISSUS provides similar or better detection performance compared to best-performing semi-supervised methods while not requiring labeled data.", "title_embedding_index": 6766, "title_abs_embedding_index": 6791}, {"title": "Benchmarking Positional Encodings for GNNs and Graph Transformers", "link_suffix": "/forum?id=WOyjgWu92E", "link": "https://openreview.net/forum?id=WOyjgWu92E", "pdf_link": "https://openreview.net/pdf?id=WOyjgWu92E", "keywords": "positional encodings, graph neural networks, graph transformers, benchmarking", "abstract": "Recent advances in Graph Neural Networks (GNNs) and Graph Transformers (GTs) have been driven by innovations in architectures and Positional Encodings (PEs), which are critical for augmenting node features and capturing graph topology. PEs are essential for GTs, where topological information would otherwise be lost without message-passing. However, PEs are often tested alongside novel architectures, making it difficult to isolate their effect on established models. To address this, we present a comprehensive benchmark of PEs in a unified framework that includes both message-passing GNNs and GTs. We also establish theoretical connections between MPNNs and GTs and introduce a sparsified GRIT attention mechanism to examine the influence of global connectivity. Our findings demonstrate that previously untested combinations of GNN architectures and PEs can outperform existing methods, offering a more comprehensive picture of the state-of-the-art. To support future research and experimentation in our framework, we make the code publicly available.", "title_embedding_index": 6767, "title_abs_embedding_index": 6792}, {"title": "CryoGEN: Cryogenic Electron Tomography Reconstruction via Generative Energy Nets", "link_suffix": "/forum?id=uOb7rij7sR", "link": "https://openreview.net/forum?id=uOb7rij7sR", "pdf_link": "https://openreview.net/pdf?id=uOb7rij7sR", "keywords": "CryoET, Cryogenic Electron Tomography, Generative Model, Energy Model", "abstract": "Cryogenic electron tomography (Cryo-ET) is a powerful method for visualizing cellular structures in their native state, but its effectiveness is limited by anisotropic resolution caused by the missing-wedge problem, complicating the interpretation of tomograms. IsoNet, a deep learning method, addresses these challenges by iteratively reconstructing missing-wedge information and improving the signal-to-noise ratio of tomograms. However, IsoNet relies on recursively updating the predictions, which can lead to training instability and model collapse. In this study, we present CryoGEN, an improved energy-based method that effectively mitigates resolution anisotropy. Our approach eliminates the need for recursive subtomogram generation, offering a more stable and consistent methodology. Applying CryoGEN to various datasets\u2014including immature HIV particles and neuronal synapses\u2014demonstrates its capability to enhance structural interpretability. Moreover, CryoGEN holds significant potential for improving the functional interpretation of cellular tomograms in future high-resolution (Cryo-ET) studies, thereby providing substantial value and advancing progress in biological research.", "title_embedding_index": 6768, "title_abs_embedding_index": 6793}, {"title": "CDQuant: Accurate Post-training Weight Quantization of LLMs using Greedy Coordinate Descent", "link_suffix": "/forum?id=BehxaBcSML", "link": "https://openreview.net/forum?id=BehxaBcSML", "pdf_link": "https://openreview.net/pdf?id=BehxaBcSML", "keywords": "quantization, large pre-trained models, post-training, coordinate descent", "abstract": "Large language models (LLMs) have recently demonstrated remarkable performance across diverse language tasks. But their deployment is often constrained by their substantial computational and storage requirements. Quantization has emerged as a key technique for addressing this challenge, enabling the compression of large models with minimal impact on performance. The recent GPTQ algorithm, a post-training quantization (PTQ) method, has proven highly effective for compressing LLMs, sparking a wave of research that leverages GPTQ as a core component. Recognizing the pivotal role of GPTQ in the PTQ landscape, we introduce CDQuant, a simple and scalable alternative to GPTQ with improved performance. CDQuant uses greedy coordinate descent to minimize the layer-wise reconstruction loss to achieve high-quality quantized weights. Our algorithm is easy to implement and scales efficiently to models with hundreds of billions of parameters. We perform extensive evaluation on Gemma, and PaLM2 model families, and demonstrate that CDQuant consistently outperforms GPTQ  in 2-4 bit weight quantization. Moreover, CDQuant improves the performance of state-of-the-art PTQ techniques such as QuIP and FrameQuant when used as a replacement for their GPTQ component, resulting in further gains in quality.", "title_embedding_index": 6769, "title_abs_embedding_index": 6794}, {"title": "GuardAgent: Safeguard LLM Agent by a Guard Agent via Knowledge-Enabled Reasoning", "link_suffix": "/forum?id=YixNDE12wm", "link": "https://openreview.net/forum?id=YixNDE12wm", "pdf_link": "https://openreview.net/pdf?id=YixNDE12wm", "keywords": "LLM agent, guardrail, safety, LLM", "abstract": "The rapid advancement of large language models (LLMs) has catalyzed the deployment of LLM-powered agents across numerous applications, raising new concerns regarding their safety and trustworthiness. In addition, existing methods for enhancing the safety of LLMs are not directly transferable to LLM-powered agents due to their diverse objectives and output modalities. In this paper, we propose\nGuardAgent, the first LLM agent as a guardrail to protect other LLM agents.\nSpecifically, GuardAgent oversees a target LLM agent by checking whether its\ninputs/outputs satisfy a set of given guard requests, e.g., safety rules or privacy\npolicies defined by the users. The pipeline of GuardAgent consists of two steps: 1) create a task plan by analyzing the provided guard requests, and 2) generate\nguardrail code based on the task plan and execute the code by calling APIs or\nusing external engines. In both steps, an LLM is utilized as the core reasoning\ncomponent, supplemented by in-context demonstrations retrieved from a memory\nmodule storing information from previous sessions. Such knowledge-enabled reasoning of GuardAgent allows it to understand various textual guard requests and\naccurately \u201ctranslate\u201d them into executable code that provides reliable guardrails.\nFurthermore, GuardAgent is equipped with an extendable toolbox containing\nrelevant APIs and functions, and requires no additional LLM training, underscoring\nits flexibility and low operational overhead. In addition to GuardAgent, we\npropose two novel benchmarks: an EICU-AC benchmark for assessing privacy-\nrelated access control for healthcare agents and a Mind2Web-SC benchmark for\nassessing safety regulations for web agents. When using Llama3-70B/Llama3.1-\n70B/GPT-4 as the core LLM, GuardAgent achieves 98.4%/98.4%/98.7% and\n83.5%/84.5%/90.0% guarding accuracy on these two benchmarks in moderating\ninvalid inputs and outputs of two types of agents, respectively. We also show the\nability of GuardAgent to define necessary functions that are absent from the\ntoolbox, which further highlights the flexibility of GuardAgent in adaption to\nnew LLM agents and guard requirements.", "title_embedding_index": 6770, "title_abs_embedding_index": 6795}, {"title": "Improving Consistency Models with Generator-Induced Flows", "link_suffix": "/forum?id=onrNYdciJQ", "link": "https://openreview.net/forum?id=onrNYdciJQ", "pdf_link": "https://openreview.net/pdf?id=onrNYdciJQ", "keywords": "generative models, diffusion models, flow models, consistency models", "abstract": "Consistency models imitate the multi-step sampling of score-based diffusion in a single forward pass of a neural network.\nThey can be learned in two ways: consistency distillation and consistency training. The former relies on the true velocity field of the corresponding differential equation, approximated by a pre-trained neural network.\nIn contrast, the latter uses a single-sample Monte Carlo estimate of this velocity field. The related estimation error induces a discrepancy between consistency distillation and training that, we show, still holds in the continuous-time limit. To alleviate this issue, we propose a novel flow that transports noisy data towards their corresponding outputs derived from the currently trained model - as a proxy of the true flow. Our empirical findings demonstrate that this approach mitigates the previously identified discrepancy.\nFurthermore, we present theoretical and empirical evidence indicating that our generator-induced flow surpasses dedicated optimal transport-based consistency models in effectively reducing the noise-data transport cost.\nConsequently, our method not only accelerates consistency training convergence but also enhances its overall performance.", "title_embedding_index": 6771, "title_abs_embedding_index": 6796}, {"title": "Data Unlearning in Diffusion Models", "link_suffix": "/forum?id=SuHScQv5gP", "link": "https://openreview.net/forum?id=SuHScQv5gP", "pdf_link": "https://openreview.net/pdf?id=SuHScQv5gP", "keywords": "diffusion models, selective forgetting, unlearning, data deletion, generative models, memorization", "abstract": "Recent work has shown that diffusion models memorize and reproduce training data examples. At the same time, large copyright lawsuits and legislation such as GDPR have highlighted the need for erasing datapoints from diffusion models. However, retraining from scratch is often too expensive. This motivates the setting of data unlearning, i.e., the study of efficient techniques for unlearning specific datapoints from the training set. Existing concept unlearning techniques require an anchor prompt/class/distribution to guide unlearning, which is not available in the data unlearning setting. General-purpose machine unlearning techniques were found to be either unstable or failed to unlearn data. We therefore propose a family of new loss functions called Subtracted Importance Sampled Scores (SISS) that utilize importance sampling and are the first method to unlearn data with theoretical guarantees. SISS is constructed as a weighted combination between simpler objectives that are responsible for preserving model quality and unlearning the targeted datapoints. When evaluated on CelebA-HQ and MNIST, SISS achieved Pareto optimality along the quality and unlearning strength dimensions. On Stable Diffusion, SISS successfully mitigated memorization on nearly 90% of the prompts we tested. We release our code online.", "title_embedding_index": 6772, "title_abs_embedding_index": 6797}, {"title": "Measuring And Improving Engagement of Text-to-Image Generation Models", "link_suffix": "/forum?id=TmCcNuo03f", "link": "https://openreview.net/forum?id=TmCcNuo03f", "pdf_link": "https://openreview.net/pdf?id=TmCcNuo03f", "keywords": "image generation models, text to image models, engagement, stable diffusion, dalle", "abstract": "Recent advances in text-to-image generation have achieved impressive aesthetic quality, making these models usable for both personal and commercial purposes. However, in the fields of marketing and advertising, images are often created to be more engaging, as reflected in user behaviors such as increasing clicks, likes, and purchases, in addition to being aesthetically pleasing. Further, we find that existing image generation metrics like aesthetics, CLIPScore, PickScore, ImageReward, etc. fail to capture viewer engagement. To this end, we introduce the challenge of optimizing the image generation process for improved viewer engagement. In order to study image engagement and utility in real-world marketing scenarios, we collect EngagingImageNet, the first large-scale dataset of images, along with associated user engagement metrics. To address the lack of reliable metrics for assessing image utility, we use the EngagingImageNet dataset to train EngageNet, an engagement-aware Vision Language Model (VLM) that predicts viewer engagement of images by leveraging contextual information about the tweet content, enterprise details, and posting time. We then explore methods to enhance the engagement of text-to-image models, making initial strides in this direction. These include conditioning image generation on improved prompts, supervised fine-tuning of stable diffusion on high-performing images, and reinforcement learning to align stable diffusion with EngageNet-based reward signals, all of which lead to the generation of images with higher viewer engagement. Finally, we propose the Engagement Arena, to benchmark text-to-image models based on their ability to generate engaging images, using EngageNet as the evaluator, thereby encouraging the research community to measure further advances in the engagement of text-to-image modeling. These contributions provide a new pathway for advancing utility-driven image generation, with significant implications for the commercial application of image generation.", "title_embedding_index": 6773, "title_abs_embedding_index": 6798}, {"title": "Convergence of Score-Based Discrete Diffusion Models: A Discrete-Time Analysis", "link_suffix": "/forum?id=pq1WUegkza", "link": "https://openreview.net/forum?id=pq1WUegkza", "pdf_link": "https://openreview.net/pdf?id=pq1WUegkza", "keywords": "Discrete diffusion model, Convergence analysis, Time-discretization, Discrete score function", "abstract": "Diffusion models have achieved great success in generating high-dimensional samples across various applications. While the theoretical guarantees for continuous-state diffusion models have been extensively studied, the convergence analysis of the discrete-state counterparts remains under-explored. In this paper, we study the theoretical aspects of score-based discrete diffusion models under the Continuous Time Markov Chain (CTMC) framework. We introduce a discrete-time sampling algorithm in the general state space $[S]^d$ that utilizes score estimators at predefined time points. We derive convergence bounds for the Kullback-Leibler (KL) divergence and total variation (TV) distance between the generated sample distribution and the data distribution, considering both scenarios with and without early stopping under specific assumptions. Notably, our KL divergence bounds are nearly linear in dimension $d$, aligning with state-of-the-art results for diffusion models. Our convergence analysis employs a Girsanov-based method and establishes key properties of the discrete score function, which are essential for characterizing the discrete-time sampling process.", "title_embedding_index": 6774, "title_abs_embedding_index": 6799}]