[
    {
        "title": "An Intelligent Agentic System for Complex Image Restoration Problems",
        "link_suffix": "/forum?id=3RLxccFPHz",
        "link": "https://openreview.net/forum?id=3RLxccFPHz",
        "pdf_link": "https://openreview.net/pdf?id=3RLxccFPHz",
        "keywords": "image resotration, low-level vision, agent, large language model, vision language model",
        "abstract": "Real-world image restoration (IR) is inherently complex and often requires combining multiple specialized models to address diverse degradations. Inspired by human problem-solving, we propose AgenticIR, an agentic system that mimics the human approach to image processing by following five key stages: Perception, Scheduling, Execution, Reflection, and Rescheduling. AgenticIR leverages large language models (LLMs) and vision-language models (VLMs) that interact via text generation to dynamically operate a toolbox of IR models. We fine-tune VLMs for image quality analysis and employ LLMs for reasoning, guiding the system step by step. To compensate for LLMs' lack of specific IR knowledge and experience, we introduce a self-exploration method, allowing the LLM to observe and summarize restoration results into referenceable documents. Experiments demonstrate AgenticIR's potential in handling complex IR tasks, representing a promising path toward achieving general intelligence in visual processing."
    },
    {
        "title": "Large Language Model Confidence Estimation via Black-Box Access",
        "link_suffix": "/forum?id=lJcSDsGgYH",
        "link": "https://openreview.net/forum?id=lJcSDsGgYH",
        "pdf_link": "https://openreview.net/pdf?id=lJcSDsGgYH",
        "keywords": "Large language model, confidence estimation",
        "abstract": "Estimating uncertainty or confidence in the responses of a model can be significant in evaluating trust not only in the responses, but also in the model as a whole. In this paper, we explore the problem of estimating confidence for responses of large language models (LLMs) with simply black-box or query access to them. We propose a simple and extensible framework where, we engineer novel features and train a (interpretable) model (viz. logistic regression) on these features to estimate the confidence. We empirically demonstrate that our simple framework is effective in estimating confidence of Flan-ul2, Llama-13b and Mistral-7b on four benchmark Q&A tasks as well as of Pegasus-large and BART-large on two benchmark summarization tasks with it surpassing baselines by even over $10%$ (on AUROC) in some cases. Additionally, our interpretable approach provides insight into features that are predictive of confidence, leading to the interesting and useful discovery that our confidence models built for one LLM generalize zero-shot across others on a given dataset."
    },
    {
        "title": "Enhancing Mathematical Reasoning in Language Models Through Focused Differentiation Training",
        "link_suffix": "/forum?id=vuuYbA1vB2",
        "link": "https://openreview.net/forum?id=vuuYbA1vB2",
        "pdf_link": "https://openreview.net/pdf?id=vuuYbA1vB2",
        "keywords": "large language model, alignment",
        "abstract": "Enhancing the mathematical capabilities of large language models (LLMs) is crucial for applications requiring precise and rigorous mathematical reasoning. Current models, even when trained with methods like Direct Preference Optimization (DPO), often struggle to effectively differentiate between correct and erroneous mathematical responses, especially when errors occur in multi-step solutions. Traditional approaches focusing on token or logit-level analysis fail to capture the nuanced semantic differences in mathematical reasoning. To address this challenge, we propose leveraging the rich semantic information embedded in the hidden state space of LLMs. Our novel approach, Focused Differentiation Training (FDT), fine-tunes the model by emphasizing the differences between the hidden states of correct and incorrect responses, rather than their common features. Unlike other methods that detect errors at the token or logits level and often rely on human input or more powerful models, our approach enhances mathematical reasoning capabilities using only the model's inherent abilities. This methodology promotes a more accurate alignment with mathematical correctness, thereby improving the model's ability to evaluate and generate precise mathematical responses. Experimental results demonstrate that our algorithm substantially outperforms traditional alignment methods in mathematical tasks, offering a robust solution for enhancing the mathematical reasoning capabilities of language models."
    },
    {
        "title": "Revisiting the Othello World Model Hypothesis",
        "link_suffix": "/forum?id=1OkVexYLct",
        "link": "https://openreview.net/forum?id=1OkVexYLct",
        "pdf_link": "https://openreview.net/pdf?id=1OkVexYLct",
        "keywords": "Othello gaming modeling, feature alignment, LLM",
        "abstract": "\\citet{li2023emergent} used the Othello board game as a test case for the ability of GPT-2 to induce world models, and were followed up by \\citet{nanda-etal-2023-emergent}. We briefly discuss the original experiments, expanding them to include more language models with more comprehensive probing. Specifically, we analyze sequences of Othello board states and train the model to predict the next move based on previous moves. We evaluate six language models (GPT-2, T5, Bart, Flan-T5, Mistral, and LLaMA-2) on the Othello task and conclude that these models not only learn to play Othello, but also induce the Othello board layout. We find that all models achieve up to 99% accuracy in unsupervised grounding and exhibit high similarity in the board features they learned. This provides considerably stronger evidence for the Othello World Model Hypothesis than previous works."
    },
    {
        "title": "Open-Set Graph Anomaly Detection via Normal Structure Regularisation",
        "link_suffix": "/forum?id=kSvoX0xdlO",
        "link": "https://openreview.net/forum?id=kSvoX0xdlO",
        "pdf_link": "https://openreview.net/pdf?id=kSvoX0xdlO",
        "keywords": "Graph Anomaly Detection, Outlier Detection, Social Network Analysis, Graph Mining",
        "abstract": "This paper considers an important Graph Anomaly Detection (GAD) task, namely open-set GAD, which aims to train a detection model using a small number of normal and anomaly nodes (referred to asseen anomalies) to detect both seen anomalies andunseen anomalies(i.e., anomalies that cannot be illustrated the training anomalies). Those labelled training data provide crucial prior knowledge about abnormalities for GAD models, enabling substantially reduced detection errors. However, current supervised GAD methods tend to over-emphasise fitting the seen anomalies, leading to many errors of detecting the unseen anomalies as normal nodes. Further, existing open-set AD models were introduced to handle Euclidean data, failing to effectively capture discriminative features from graph structure and node attributes for GAD. In this work, we propose a novel open-set GAD approach, namely $\\underline{n}ormal$ $\\underline{s}tructure$ $\\underline{reg}ularisation$ (NSReg), to achieve generalised detection ability to unseen anomalies, while maintaining its effectiveness on detecting seen anomalies. The key idea in NSReg is to introduce a regularisation term that enforces the learning of compact, semantically-rich representations of normal nodes based on their structural relations to other nodes. When being optimised with supervised anomaly detection losses, the regularisation term helps incorporate strong normality into the modelling, and thus, it effectively avoids over-fitting the seen anomalies and learns a better normality decision boundary, largely reducing the false negatives of detecting unseen anomalies as normal. Extensive empirical results on seven real-world datasets show that NSReg significantly outperforms state-of-the-art competing methods by at least 14% AUC-ROC on the unseen anomaly classes and by 10% AUC-ROC on all anomaly classes."
    },
    {
        "title": "Adversarial Self Flow Matching: Few-steps Image Generation with Straight Flows",
        "link_suffix": "/forum?id=MVltEnKJaO",
        "link": "https://openreview.net/forum?id=MVltEnKJaO",
        "pdf_link": "https://openreview.net/pdf?id=MVltEnKJaO",
        "keywords": "generative models, flow matching, adversarial training",
        "abstract": "Flow Matching provides a method to train Ordinary Differential Equation (ODE)-based generative models and facilitates various probability path designs between initial and target distributions. Among these designs, straight flows are particularly interesting for reducing sampling steps. While some works have successfully straightened flows and achieved image generation in a few steps, they often suffer from cumulative errors or provide only piecewise or minibatch-level straightness. We propose Adversarial Self Flow Matching (ASFM), which can straighten flows and align the generated data distribution with the real data distribution. ASFM consists of two complementary components. Online Self Training straightens flows by constructing a conditional vector field using paired data, enabling one-step image generation during training. Adversarial Training aligns the one-step generated data with real data, thereby reducing cumulative errors when straightening flows. Experiments demonstrate that ASFM can build straight flows across the entire time span between two complete distributions and achieve highly competitive results across multiple datasets among Flow Matching-based methods. For instance, ASFM achieves 8.15 and 14.9 FID scores with NFE=6 on CelebA-HQ (256) and AFHQ-Cat (256), respectively."
    },
    {
        "title": "QERA: an Analytical Framework for Quantization Error Reconstruction",
        "link_suffix": "/forum?id=LB5cKhgOTu",
        "link": "https://openreview.net/forum?id=LB5cKhgOTu",
        "pdf_link": "https://openreview.net/pdf?id=LB5cKhgOTu",
        "keywords": "parameter-efficient fine-tuning, PEFT, LoRA, QLoRA, quantization, post-training quantization, PTQ, low-rank approximation",
        "abstract": "The growing number of parameters and computational demands of large language models (LLMs) present significant challenges for their efficient deployment.\nRecently, there is an increasing interest in quantizing weights to extremely low precision while offsetting the resulting error with low-rank, high-precision error reconstruction terms.\nThe combination of quantization and low-rank approximation is now popular in both adapter-based, parameter-efficient fine-tuning methods such as LoftQ and low-precision inference techniques including ZeroQuant-V2.\nUsually, the low-rank terms are calculated via the singular value decomposition (SVD) of the weight quantization error,\nminimizing the Frobenius and spectral norms of the weight approximation error.\nRecent methods like LQ-LoRA and LQER introduced hand-crafted heuristics to minimize errors in layer outputs (activations) rather than weights, resulting improved quantization results.\nHowever, these heuristic methods lack an analytical solution to guide the design of quantization error reconstruction terms.\nIn this paper, we revisit this problem and formulate an analytical framework, named Quantization Error Reconstruction Analysis (QERA),\nand offer a closed-form solution to the problem.\nWe show QERA benefits both existing low-precision fine-tuning and inference methods --\nQERA achieves a fine-tuned accuracy gain of $\\Delta_{\\text{acc}}$ = 6.05% of 2-bit RoBERTa-base on GLUE compared to LoftQ;\nand obtains $\\Delta_{\\text{acc}}$ = 2.97% higher post-training quantization accuracy of 4-bit Llama-3.1-70B on average than ZeroQuant-V2 and $\\Delta_{\\text{ppl}}$ = $-$ 0.28 lower perplexity on WikiText2 than LQER."
    },
    {
        "title": "Contextualized Messages Boost Graph Representations",
        "link_suffix": "/forum?id=9RcofuNF5p",
        "link": "https://openreview.net/forum?id=9RcofuNF5p",
        "pdf_link": "https://openreview.net/pdf?id=9RcofuNF5p",
        "keywords": "deep learning, graph neural network, representational capability, soft-isomorphic relational graph convolution network",
        "abstract": "Graph neural networks (GNNs) have gained significant attention in recent years for their ability to process data that may be represented as graphs. This has prompted several studies to explore their representational capability based on the graph isomorphism task. These works inherently assume a countable node feature representation, potentially limiting their applicability. Interestingly, only a few study GNNs with uncountable node feature representation. In the paper, a novel perspective on the representational capability of GNNs is investigated across all levels\u2014node-level, neighborhood-level, and graph-level\u2014when the space of node feature representation is uncountable. More specifically, the strict injective and metric requirements aresoftlyrelaxed by employing apseudometricdistance on the space of input to create asoft-injectivefunction such that distinct inputs may producesimilaroutputs if and only if thepseudometricdeems the inputs to be sufficientlysimilaron some representation. As a consequence, a simple and computationally efficientsoft-isomorphicrelational graph convolution network (SIR-GCN) that emphasizes the contextualized transformation of neighborhood feature representations viaanisotropicanddynamicmessage functions is proposed. A mathematical discussion on the relationship between SIR-GCN and widely used GNNs is then laid out to put the contribution into context, establishing SIR-GCN as a generalization of classical GNN methodologies. Experiments on synthetic and benchmark datasets then demonstrate the relative superiority of SIR-GCN, outperforming comparable models in node and graph property prediction tasks."
    },
    {
        "title": "Advancing Prompt-Based Methods for Replay-Independent General Continual Learning",
        "link_suffix": "/forum?id=V6uxd8MEqw",
        "link": "https://openreview.net/forum?id=V6uxd8MEqw",
        "pdf_link": "https://openreview.net/pdf?id=V6uxd8MEqw",
        "keywords": "Continual Learning, General Continual Learning, Catastrophic Forgetting, Prompt Tuning, Sharpness-Aware Minimization",
        "abstract": "General continual learning (GCL) is a broad concept to describe real-world continual learning (CL) problems, which are often characterized by online data streams without distinct transitions between tasks, i.e., blurry task boundaries. These requirements result in poor initial performance, limited generalizability, and severe catastrophic forgetting, heavily impacting the effectiveness of mainstream GCL models trained from scratch. While the use of a frozen pretrained backbone with appropriate prompt tuning can partially address these challenges, such prompt-based methods remain sub-optimal for CL of remaining tunable parameters on the fly. In this regard, we propose an innovative approach named MISA(Mask and Initial Session Adaption) to advance prompt-based methods in GCL. It includes a forgetting-aware initial session adaption that employs pretraining data to initialize prompt parameters and improve generalizability, as well as a non-parametric logit mask of the output layers to mitigate catastrophic forgetting. Empirical results demonstrate substantial performance gains of our approach compared to recent competitors, especially without a replay buffer (e.g., up to 18.39%, 22.06%, and 11.96% performance lead on CIFAR-100, Tiny-ImageNet, and ImageNet-R, respectively). Moreover, our approach features the plug-in nature for prompt-based methods, independence of replay, ease of implementation, and avoidance of CL-relevant hyperparameters, serving as a strong baseline for GCL research."
    },
    {
        "title": "Is Your Model Really A Good Math Reasoner? Evaluating Mathematical Reasoning with Checklist",
        "link_suffix": "/forum?id=nDvgHIBRxQ",
        "link": "https://openreview.net/forum?id=nDvgHIBRxQ",
        "pdf_link": "https://openreview.net/pdf?id=nDvgHIBRxQ",
        "keywords": "Mathmatical Reasoning, Geometry reasoning, Evaluation, (Multi-Modal) LLMs",
        "abstract": "Exceptional mathematical reasoning ability is one of the key features that demonstrate the power of large language models (LLMs). How to comprehensively define and evaluate the mathematical abilities of LLMs, and even reflect the user experience in real-world scenarios, has emerged as a critical issue. Current benchmarks predominantly concentrate on problem-solving capabilities, presenting a substantial risk of model overfitting and fails to accurately measure the genuine mathematical reasoning abilities. In this paper, we argue that if a model really understands a problem, it should be robustly and readily applied across a diverse array of tasks. To this end, we introduce MathCheck, a well-designed checklist for testing task generalization and reasoning robustness, as well as an automatic tool to generate checklists efficiently. MathCheck includes multiple mathematical reasoning tasks and robustness tests to facilitate a comprehensive evaluation of both mathematical reasoning ability and behavior testing. Utilizing MathCheck, we develop MathCheck-GSM and MathCheck-GEO to assess mathematical textual reasoning and multi-modal reasoning capabilities, respectively, serving as upgraded versions of benchmarks including GSM8k, GeoQA, UniGeo, and Geometry3K. We adopt MathCheck-GSM and MathCheck-GEO to evaluate over 26 LLMs and 17 multi-modal LLMs, assessing their comprehensive mathematical reasoning abilities. Our results demonstrate that while frontier LLMs like GPT-4o continue to excel in various abilities on the checklist, many other model families exhibit a significant decline. Further experiments indicate that, compared to traditional math benchmarks, MathCheck better reflects true mathematical abilities and represents mathematical intelligence more linearly, thereby supporting our design. Using MathCheck, we can also efficiently conduct informative behavior analysis to deeply investigate models. Finally, we show that our proposed checklist paradigm can easily extend to other reasoning tasks for their comprehensive evaluation."
    },
    {
        "title": "Large Language Model Predicting the Performance of Small Organic Molecule Corrosion Inhibitors",
        "link_suffix": "/forum?id=J6dEAiPPe0",
        "link": "https://openreview.net/forum?id=J6dEAiPPe0",
        "pdf_link": "https://openreview.net/pdf?id=J6dEAiPPe0",
        "keywords": "AI, LLM, GPT-4o, Machine Learning, Material Science, Corrosion, Magnesium, Regression, Prediction, Foundation Model",
        "abstract": "Large language models (LLMs) like GPT-4o have shown promise in solving everyday tasks and addressing basic scientific challenges by utilizing extensive pre-trained knowledge. In this work, we explore their potential to predict the efficiency of various organic compounds for the inhibition of corrosion of the magnesium alloy ZE41, a material crucial for many industrial applications. Traditional approaches, such as basic neural networks, rely on non-contextual data, often requiring large datasets and significant effort per sample to achieve accurate predictions. They struggle particularly with small datasets, limiting their effectiveness in discovering new corrosion inhibitors. LLMs can contextualize and interpret limited data points by drawing on their vast knowledge, including chemical properties of molecules and their influence on corrosion processes in other materials like iron. By prompting the model with a small dataset, LLMs can provide meaningful predictions without the need for extensive training. Our study demonstrates that LLMs can predict corrosion inhibition outcomes, and reduce the amount of data needed."
    },
    {
        "title": "CSP: An Efficient Baseline for Learning on Large-Scale Structured Data",
        "link_suffix": "/forum?id=XA9A8mkFqa",
        "link": "https://openreview.net/forum?id=XA9A8mkFqa",
        "pdf_link": "https://openreview.net/pdf?id=XA9A8mkFqa",
        "keywords": "Hypergraph representation learning, Hypergraph convolution, Label propagation, Model complexity, Naive Bayes",
        "abstract": "Last decade has seen the emergence of numerous methods for learning on graphs, particularly Graph Neural Networks (GNNs). These methods, however, are often not directly applicable to more complex structures like bipartite graphs (equivalent to hypergraphs), which represent interactions among two entity types (e.g., a user liking a movie). This paper proposes Convolutional Signal Propagation (CSP), a non-parametric simple and scalable method that natively operates on bipartite graphs (hypergraphs) and can be implemented with just a few lines of code. After defining CSP, we demonstrate its relationship with well-established methods like label propagation, Naive Bayes, and Hypergraph Convolutional Networks. We evaluate CSP against several reference methods on real-world datasets from multiple domains, focusing on retrieval and classification tasks. Our results show that CSP offers competitive performance while maintaining low computational complexity, making it an ideal first choice as a baseline for hypergraph node classification and retrieval. Moreover, despite operating on hypergraphs, CSP achieves good results in tasks typically not associated with hypergraphs, such as natural language processing."
    },
    {
        "title": "CliBench: A Multifaceted and Multigranular Evaluation of Large Language Models for Clinical Decision Making",
        "link_suffix": "/forum?id=E3LDsbUSRZ",
        "link": "https://openreview.net/forum?id=E3LDsbUSRZ",
        "pdf_link": "https://openreview.net/pdf?id=E3LDsbUSRZ",
        "keywords": "Clinical Decisions, Large Language Model, Benchmark",
        "abstract": "The integration of Artificial Intelligence (AI), especially Large Language Models (LLMs), into the clinical diagnosis process offers significant potential to improve the efficiency and accessibility of medical care. While LLMs have shown some promise in the medical domain, their application in clinical diagnosis remains underexplored, especially in real-world clinical practice, where highly sophisticated, patient-specific decisions need to be made. Current evaluations of LLMs in this field are often narrow in scope, focusing on specific diseases or specialties and employing simplified diagnostic tasks. To bridge this gap, we introduce CliBench, a novel benchmark developed from the MIMIC IV dataset, offering a comprehensive and realistic assessment of LLMs' capabilities in clinical diagnosis. This benchmark not only covers diagnosis from a diverse range of medical cases across various specialties but also incorporates tasks of clinical significance: treatment procedure identification, lab test ordering and medication prescriptions. Supported by structured output ontologies, CliBench enables a precise and multi-granular evaluation, offering an in-depth understanding of LLM's capability on diverse clinical tasks of desired granularity. We conduct a zero-shot evaluation of leading LLMs to assess their proficiency in clinical decision-making. Our preliminary results shed light on the potential and limitations of current LLMs in clinical settings, providing valuable insights for future advancements in LLM-powered healthcare."
    },
    {
        "title": "The Best of Both Worlds: Bridging Quality and Diversity in Data Selection with Bipartite Graph",
        "link_suffix": "/forum?id=V9oT5Jmxpu",
        "link": "https://openreview.net/forum?id=V9oT5Jmxpu",
        "pdf_link": "https://openreview.net/pdf?id=V9oT5Jmxpu",
        "keywords": "large language models, supervised fine-tuning, data selection, efficiency",
        "abstract": "The performance of large language models (LLMs) in natural language processing (NLP) tasks is significantly influenced by the quality and diversity of data used for supervised fine-tuning (SFT). Current data selection methods often focus solely on quality or diversity, leading to underperforming models due to suboptimal training data. In this paper, we introduce GraphFilter, a novel method that represents the dataset as a bipartite graph, linking sentences to their constituent n-grams. This representation effectively captures the relationships between sentences and linguistic patterns, facilitating the selection of sentences that enhance n-gram diversity. To balance quality and diversity during selection, we propose a priority function that combines the quality metric with the diversity metric in a multiplicative manner. GraphFilter iteratively selects high-priority sentences, updates the bipartite graph by removing covered n-grams, and re-calculates priorities to reflect the evolving data landscape. We conduct extensive experiments using three model backbones across six widely used benchmarks. The results demonstrate that GraphFilter outperforms all nine baseline approaches, achieving superior model performance and computational efficiency. Our analyses validate the effectiveness of our design choices, examine the subsets selected by GraphFilter and other methods, highlight the importance of instruction diversity, and explore the role of quality and diversity in relation to subset sizes. GraphFilter establishes a new foundation for effective data selection strategies, encouraging further research in data selection for LLMs."
    },
    {
        "title": "Deep-ComAIR: A Framework for Predicting TCR-pMHC Binding through Complex Structural Analysis",
        "link_suffix": "/forum?id=vuBhwseAKn",
        "link": "https://openreview.net/forum?id=vuBhwseAKn",
        "pdf_link": "https://openreview.net/pdf?id=vuBhwseAKn",
        "keywords": "AI for science, adaptive immunity, TCR-pMHC binding, multimodal integration",
        "abstract": "The binding process between T cell receptor (TCR) and the peptide-major histocompatibility complex (pMHC) is a fundamental mechanism in adaptive immunity. Current research on binding prediction primarily emphasizes the sequence and structural features of critical regions within these molecules, often neglecting the intricate structural changes that occur at the binding process, which can lead to biased representations. To address this gap, we propose a novel framework, titled \u201cDeep-ComAIR,\u201d which effectively models the binding process by focusing on the complex structure of TCR-pMHC rather than individual components. This model enhances prediction accuracy by integrating features from three modalities: sequence, structural, and gene. Our approach achieves state-of-the-art results evidenced by an area under the receiver operating characteristic curve (AUROC) of 0.983 in binding reactivity prediction and a Pearson correlation coefficient of 0.833 in binding affinity prediction. These results highlight the framework's potential to deepen our understanding of TCR-pMHC interactions at the structural level and facilitate advancements in immunotherapy and vaccine design."
    },
    {
        "title": "RecurFormer: Not All Transformer Heads Need Self-Attention",
        "link_suffix": "/forum?id=FyJaV0TVF2",
        "link": "https://openreview.net/forum?id=FyJaV0TVF2",
        "pdf_link": "https://openreview.net/pdf?id=FyJaV0TVF2",
        "keywords": "Transformer, Large Language Models, Recurrent Neural Networks",
        "abstract": "Transformer-based large language models (LLMs) excel in modeling complex language patterns but face significant computational costs during inference, especially with long inputs due to the attention mechanism's memory overhead. We observe that certain attention heads exhibit a distribution where the attention weights concentrate on tokens near the query token, termed as recency aware, which focuses on local and short-range dependencies. Leveraging this insight, we propose RecurFormer, a novel architecture that replaces these attention heads with linear recurrent neural networks (RNNs), specifically the Mamba architecture. This replacement reduces the cache size without evicting tokens, thus maintaining generation quality. RecurFormer retains the ability to model long-range dependencies through the remaining attention heads and allows for reusing pre-trained Transformer-based LLMs weights with continual training. Experiments demonstrate that RecurFormer matches the original model's performance while significantly enhancing inference efficiency. Our approach provides a practical solution to the computational challenges of Transformer-based LLMs inference, making it highly attractive for tasks involving long inputs."
    },
    {
        "title": "Towards Robustness of Person Search against Corruptions",
        "link_suffix": "/forum?id=gY2IHLUJhk",
        "link": "https://openreview.net/forum?id=gY2IHLUJhk",
        "pdf_link": "https://openreview.net/pdf?id=gY2IHLUJhk",
        "keywords": "Corruption, Person Search, Robustness",
        "abstract": "Person search aims to simultaneously detect and re-identify a query person within an entire scene, involving detection and re-identification as a multi-task problem.\nWhile existing studies have made significant progress in achieving superior performance on clean datasets, the challenge of robustness under various corruptions remains largely unexplored.\nTo address this gap, we propose two benchmarks, CUHK-SYSU-C and PRW-C, designed to assess the robustness of person search models across diverse corruption scenarios.\nPrevious researches on corruption have been conducted independently for single tasks such as re-identification and detection.\nHowever, recent advancements in person search adopt an end-to-end multi-task learning framework that processes the entire scene as input, unlike the combination of single tasks. \nThis raises the question of whether independent achievements can ensure corruption robustness for person search.\nOur findings reveal that merely combining independent, robust detection and re-identification models is not sufficient for achieving robust person search. \nWe further investigate the vulnerability of the detection and representation stages to corruption and explore its impact on both foreground and background areas.\nBased on these insights, we propose a foreground-aware augmentation and regularization method to enhance the robustness of person search models.\nSupported by our comprehensive robustness analysis and evaluation framework our benchmarks provide, our proposed technique substantially improves the robustness of existing person search models.\nCode will be made publicly available."
    },
    {
        "title": "Sample Efficient Robust Offline Self-Play for Model-based Reinforcement Learning",
        "link_suffix": "/forum?id=3lXZjsir0e",
        "link": "https://openreview.net/forum?id=3lXZjsir0e",
        "pdf_link": "https://openreview.net/pdf?id=3lXZjsir0e",
        "keywords": "robust Markov games, self-play, distribution shift, model uncertainty, reinforcement learning",
        "abstract": "Multi-agent reinforcement learning (MARL), as a thriving field, explores how multiple agents independently make decisions in a shared dynamic environment. Due to environmental uncertainties and fluctuations, policies in MARL must remain robust to tackle the sim-to-real gap. Although robust RL has been extensively explored in single-agent settings, it has seldom received attention in self-play, where strategic interactions heighten uncertainties. We focus on robust two-player zero-sum Markov games (TZMGs) in offline RL, specifically on tabular robust TZMGs (RTZMGs) with a given uncertainty set. To address sample scarcity, we introduce a model-based algorithm (RTZ-VI-LCB) for RTZMGs, which integrates robust value iteration considering uncertainty level, applying a data-driven penalty term to the robust value estimates. We establish the finite-sample complexity of RTZ-VI-LCB by accounting for distribution shifts in the historical dataset, without requiring for full state-action space coverage. To the best of our knowledge, we provide the upper bound in RTZMGs, which first achieves optimal sample complexity on the dependency of action spaces. Our algorithm is capable of learning under partial coverage and environmental uncertainty. An information-theoretic lower bound is developed to show that learning RTZMGs is at least as difficult as standard TZMGs when the uncertainty level is sufficiently small. This result confirms the tightness of our upper bound, which is near-optimal for the big uncertainty level, except for the horizon length."
    },
    {
        "title": "PTAD: Prototype-Oriented Tabular Anomaly Detection via Mask Modeling",
        "link_suffix": "/forum?id=Vi6p2TeujL",
        "link": "https://openreview.net/forum?id=Vi6p2TeujL",
        "pdf_link": "https://openreview.net/pdf?id=Vi6p2TeujL",
        "keywords": "Tabular anomaly detection, Prototype learning, Mask modeling",
        "abstract": "Tabular anomaly detection, which aims at identifying deviant samples, has been crucial in a variety of real-world applications, such as medical disease identification, financial fraud detection, intrusion monitoring, etc. Although recent deep learning-based methods have achieved competitive performances, these methods suffer from representation entanglement and the lack of global correlation modeling, which leads to the 'abnormal leakage' issue and hinders anomaly detection performance. To tackle the problem, we incorporate mask modeling and prototype learning into tabular anomaly detection. The core idea is to design learnable masks by disentangled representation learning within a projection space and extracting nominal dependencies as explicit global prototypes. Specifically, the overall model involves two parts: (i) During encoding, we perform mask modeling in both the data space and projection space with orthogonal basis vectors for masking out the suspicious abnormal locations; (ii) During decoding, we decode multiple masked representations in parallel for reconstruction and learn association prototypes to extract nominal characteristic correlations. Our proposal derives from a distribution-matching perspective, where both projection space learning and association prototype learning are formulated as optimal transport problems, and the calibration distances are utilized to refine the anomaly scores. By conducting both quantitative and qualitative experiments on 20 tabular benchmarks, our model surpasses other competitors and possesses good interpretability."
    },
    {
        "title": "Exploring Temporal Semantic for Incomplete Clustering",
        "link_suffix": "/forum?id=4vm6Nn2DW9",
        "link": "https://openreview.net/forum?id=4vm6Nn2DW9",
        "pdf_link": "https://openreview.net/pdf?id=4vm6Nn2DW9",
        "keywords": "Temporal semantic, incomplete clustering, human motion segmentation",
        "abstract": "Clustering data with incomplete features has garnered considerable scholarly attention; however, the specific challenge of clustering sequential data with missing attributes remains largely under-explored. Conventional heuristic methods generally address this issue by first imputing the missing features, thereby making the clustering results heavily reliant on the quality of imputation. In this paper, we introduce a novel clustering framework, termed ETC-IC, which directly clusters incomplete data with rigorous theoretical guarantees, whilst concurrently leveraging temporal semantic consistency to enhance clustering performance. Empirical evaluations demonstrate that the proposed model consistently surpasses current state-of-the-art methods in clustering human motion data."
    },
    {
        "title": "Identifying and Analyzing Task-Encoding Tokens in Large Language Models",
        "link_suffix": "/forum?id=KMRCb2VCLq",
        "link": "https://openreview.net/forum?id=KMRCb2VCLq",
        "pdf_link": "https://openreview.net/pdf?id=KMRCb2VCLq",
        "keywords": "Task-encoding tokens, In-context learning, Large language models, Interpretability and analysis",
        "abstract": "In-context learning (ICL) has emerged as an effective solution for few-shot learning with large language models (LLMs). Previous research suggests that LLMs perform ICL by analogizing from the provided demonstrations, similar to how humans learn new tasks. However, how LLMs leverage demonstrations to specify a task and learn a corresponding computational function through ICL remains underexplored. Drawing from the way humans learn from content-label mappings in demonstrations, we categorize the tokens in an ICL prompt into content, stopword, and template tokens, with the latter two typically ignored by humans due to their uninformative nature. Our goal is to identify the type of tokens whose representations highly and directly influence LLM's performance, a property we refer to astask-encoding. By ablating representations from the attention of the test example, we find that the representations of informative content tokens have less influence on performance, while template and stopword tokens are more prone to be task-encoding tokens, which contrasts with the human attention to informative words. We further give evidence about the function of task-encoding tokens by showing that their representations aggregate information from the content tokens. Moreover, we demonstrate experimentally that lexical meaning, repetition, and structural cues are the main distinguishing characteristics of these tokens. Our work sheds light on how LLMs learn to perform tasks from demonstrations and deepens our understanding of the roles different types of tokens play in LLMs."
    },
    {
        "title": "Mixture of Experts Guided by Gaussian Splatters Matters: A new Approach to Weakly-Supervised Video Anomaly Detection",
        "link_suffix": "/forum?id=rrdNQZRHEm",
        "link": "https://openreview.net/forum?id=rrdNQZRHEm",
        "pdf_link": "https://openreview.net/pdf?id=rrdNQZRHEm",
        "keywords": "Video Anomaly Detection, Weakly-supervised Learning",
        "abstract": "Video Anomaly Detection (VAD) has proved to be a challenging task due to the in-\nherent variability of anomalous events and the scarcity of data available. Under the\ncommon Weakly-Supervised VAD (WSVAD) paradigm, only a video-level label\nis available during training, while the predictions are carried out at the frame-level.\nDespite decent progress on simple anomalous events (such as explosions), more\ncomplex real-world anomalies (such as shoplifting) remain challenging. There\nare two main reasons for this: (I) current state-of-the-art models do not address\nthe diversity between anomalies during training and process diverse categories\nof anomalies with a shared model, thereby ignoring the category-specific key at-\ntributes; and (II) the lack of precise temporal information (i.e., weak-supervision)\nlimits the ability to learn how to capture complex abnormal attributes that can\nblend with normal events, effectively allowing to use only the most abnormal snip-\npets of an anomaly. We hypothesize that these issues can be addressed by sharing\nthe task between multiple expert models that would increase the possibility of cor-\nrectly encoding the singular characteristics of different anomalies. Furthermore,\nmultiple Gaussian kernels can guide the experts towards a more comprehensive\nand complete representation of anomalous events, ensuring that each expert pre-\ncisely distinguishes between normal and abnormal events at the frame-level. To\nthis end, we introduce Gaussian Splatting-guided Mixture of Experts (GS-MoE),\na novel approach that leverages a set of experts trained with a temporal Gaussian\nsplatting loss on specific classes of anomalous events and integrates their predic-\ntions via a mixture of expert models to capture complex relationships between\ndifferent anomalous patterns. The introduction of temporal Gaussian splatting\nloss allows the model to leverage temporal consistency in weakly-labeled data,\nenabling more robust identification of subtle anomalies over time. The novel loss\nfunction, designed to enhance weak supervision, further improves model perfor-\nmance by guiding expert networks to focus on segments of data with a higher like-\nlihood of containing anomalies. Experimental results on the UCF-Crime and XD-\nViolence datasets demonstrate that our framework achieves SOTA performance,\nscoring 91.58% AUC on UCF-Crime."
    },
    {
        "title": "Enhancing Descriptive Image Quality Assessment with a Large-scale Multi-modal Dataset",
        "link_suffix": "/forum?id=kWGHZuW5yJ",
        "link": "https://openreview.net/forum?id=kWGHZuW5yJ",
        "pdf_link": "https://openreview.net/pdf?id=kWGHZuW5yJ",
        "keywords": "image quality assessment, vision language model",
        "abstract": "With the rapid advancement of Vision Language Models (VLMs), VLM-based Image Quality Assessment (IQA) seeks to describe image quality linguistically to align with human expression and capture the multifaceted nature of IQA tasks. However, current methods are still far from practical usage. First, prior works focus narrowly on specific sub-tasks or settings, which do not align with diverse real-world applications. Second, their performance is sub-optimal due to limitations in dataset coverage, scale, and quality. To overcome these challenges, we introduce Enhanced Descriptive image Quality Assessment (EDQA). Our method includes a multi-functional IQA task paradigm that encompasses both assessment and comparison tasks, brief and detailed responses, full-reference and non-reference scenarios. We introduce a ground-truth-informed dataset construction approach to enhance data quality, and scale up the dataset to 495K under the brief-detail joint framework. Consequently, we construct a comprehensive, large-scale, and high-quality dataset, named EDQA-495K. We also retain image resolution during training to better handle resolution-related quality issues, and estimate a confidence score that is helpful to filter out low-quality responses. Experimental results demonstrate that EDQA significantly outperforms traditional score-based methods, prior VLM-based IQA models, and proprietary GPT-4V in distortion identification, instant rating, and reasoning tasks. Our advantages are further confirmed by real-world applications including assessing the web-downloaded images and ranking model-processed images. Datasets and codes will be released publicly."
    },
    {
        "title": "Beyond Circuit Connections: A Non-Message Passing Graph Transformer Approach for Quantum Error Mitigation",
        "link_suffix": "/forum?id=XnVttczoAV",
        "link": "https://openreview.net/forum?id=XnVttczoAV",
        "pdf_link": "https://openreview.net/pdf?id=XnVttczoAV",
        "keywords": "Quantum Error Mitigation; Graph Transformer",
        "abstract": "Despite the progress in quantum computing, one major bottleneck against the practical utility is its susceptibility to noise, which frequently occurs in current quantum systems. Existing quantum error mitigation (QEM) methods either lack generality to noise and circuit types or fail to capture the global dependencies of entire systems in addition to circuit structure. In this work, we first propose a unique circuit-to-graph encoding scheme with qubit-wise noisy measurement aggregated. Then, we introduce GTraQEM, a non-message passing graph transformer designed to effectively mitigate errors in expected circuit measurement outcomes. GTraQEM are equipped with a quantum-specific positional encoding, a structure matrix as attention bias guiding nonlocal aggregation, and a virtual quantum-representative node to further grasp graph representations, which guarantees to model the long-range entanglement. Experimental evaluations demonstrate that GTraQEM outperforms state-of-the-art QEM methods on both random and structured quantum circuits across noise types and scales among diverse settings."
    },
    {
        "title": "Leveraging Object Detection for Diverse and Accurate Long-Horizon Events Forecasting",
        "link_suffix": "/forum?id=6UfYLQJ8pA",
        "link": "https://openreview.net/forum?id=6UfYLQJ8pA",
        "pdf_link": "https://openreview.net/pdf?id=6UfYLQJ8pA",
        "keywords": "Event Sequences, Marked Temporal Point Processes, Long Horizon Forecasting, Object Detection, Optimal Assignment",
        "abstract": "Long-horizon event forecasting is critical across various domains, including retail, finance, healthcare, and social networks. Traditional methods, such as Marked Temporal Point Processes (MTPP), often rely on autoregressive models to predict multiple future events. However, these models frequently suffer from issues like converging to constant or repetitive outputs, which limits their effectiveness and general applicability. To address these challenges, we introduce DeTPP (Detection-based Temporal Point Processes), a novel approach inspired by object detection techniques from computer vision. DeTPP employs a unique matching-based loss function that selectively prioritizes reliably predictable events, improving the accuracy and diversity of predictions during inference. Our method establishes a new state-of-the-art in long-horizon event forecasting, achieving up to a 77% relative improvement over existing MTPP and next-K methods. The proposed hybrid approach enhances the accuracy of next event prediction by up to 2.7% on a large transactional dataset. Notably, DeTPP is also among the fastest methods for inference."
    }
]