[{"title": "Everything, Everywhere, All at Once: Is Mechanistic Interpretability Identifiable?", "link_suffix": "/forum?id=5IWJBStfU7", "link": "https://openreview.net/forum?id=5IWJBStfU7", "pdf_link": "https://openreview.net/pdf?id=5IWJBStfU7", "keywords": "AI interpretability, mechanistic interpretability, causal consistency, explanatory algorithms, circuits", "abstract": "As AI systems are increasingly deployed in high-stakes real-world applications, ensuring their interpretability has become critical. Mechanistic Interpretability (MI) is a promising approach that aims to reverse-engineer neural networks to extract simple, human-understandable algorithms embedded in the neural structure that explain the model\u2019s behavior. \nIn this work, we investigate a fundamental concern with concrete formalizations of MI: do current criteria guarantee the identifiability of the explanation? We borrow the concept of identifiability from statistics to express the intuition that an explanation should be unique, meaning that the criteria for selecting explanations should not allow for multiple, incompatible solutions.We identify two broad strategies to produce MI explanations: (i) \"where-then-what\", which first identifies a subset of the network (a circuit) that replicates the model's behavior before deriving its interpretation, and (ii) \"what-then-where\", which begins with candidate explanatory algorithms and searches in the activation subspaces of the neural model where the candidate algorithm may be implemented, relying on notions of causal alignment between the states of the candidate algorithm and the neural network. We systematically test the identifiability of both strategies using simple tasks (learning Boolean functions) and multi-layer perceptrons that are small enough to allow for the complete enumeration of candidate explanations. Our experiments reveal that current criteria suffer from identifiability issues at every stage: multiple circuits can replicate model behavior, multiple interpretations can exist for a circuit, several algorithms can be causally aligned with the neural network, and each algorithm can be aligned to multiple, different, subspaces of the neural network.These findings suggest that current criteria are too permissive and need refinement to ensure identifiability. \nWe discuss the generalization of our results to larger models and potential fixes based on stricter criteria. Our work aims to contribute constructively to the ongoing effort to develop rigorous formalizations of MI's assumptions.", "title_embedding_index": 5600, "title_abs_embedding_index": 5625}, {"title": "On a Hidden Property in Computational Imaging", "link_suffix": "/forum?id=TSrhLq5hSA", "link": "https://openreview.net/forum?id=TSrhLq5hSA", "pdf_link": "https://openreview.net/pdf?id=TSrhLq5hSA", "keywords": "Computational Imaging, Latent Space Representation, Inverse Problems", "abstract": "Computational imaging plays a vital role in various scientific and medical applications, such as Full Waveform Inversion (FWI), Computed Tomography (CT), and Electromagnetic (EM) inversion. These methods address inverse problems by reconstructing physical properties (e.g., the acoustic velocity map in FWI) from measurement data (e.g., seismic waveform data in FWI), where both modalities are governed by complex mathematical equations. In this paper, we empirically demonstrate that despite their differing governing equations, three inverse problems\u2014FWI, CT, and EM inversion\u2014share a hidden property within their latent spaces. Specifically, using FWI as an example, we show that both modalities (the velocity map and seismic waveform data) follow the same set of one-way wave equations in the latent space, yet have distinct initial conditions that are linearly correlated. This suggests that after projection into the latent embedding space, the two modalities correspond to different solutions of the same equation, connected through their initial conditions. Our experiments confirm that this hidden property is consistent across all three imaging problems, providing a novel perspective for understanding these computational imaging tasks.", "title_embedding_index": 5601, "title_abs_embedding_index": 5626}, {"title": "Unsupervised Multi-Agent Diversity With Wasserstein Distance", "link_suffix": "/forum?id=1Euu8FPr3d", "link": "https://openreview.net/forum?id=1Euu8FPr3d", "pdf_link": "https://openreview.net/pdf?id=1Euu8FPr3d", "keywords": "Multi-Agent Reinforcement Learning, Multi-Agent diversity, Cooperation, Wasserstein Distance", "abstract": "In cooperative Multi-Agent Reinforcement Learning (MARL), agents sharing policy network parameters are observed to learn similar behaviors, which impedes efficient exploration and easily results in the local optimum of cooperative policies. In order to encourage multi-agent diversity, many recent efforts have contributed to distinguishing different trajectories by maximizing the mutual information objective, given agent identities. Despite their successes, these mutual information-based methods do not necessarily promote exploration. To encourage multi-agent diversity and sufficient exploration, we propose a novel Wasserstein Multi-Agent Diversity (WMAD) exploration method that maximizes the Wasserstein distance between the trajectory distributions of different agents in a latent representation space. Since the Wasserstein distance is defined over two distributions, we further extend it to learn diverse policies for multiple agents. We empirically evaluate our method in various challenging multi-agent tasks and demonstrate its superior performance and sufficient exploration compared to existing state-of-the-art methods.", "title_embedding_index": 5602, "title_abs_embedding_index": 5627}, {"title": "Entropy Reveals What You Know: An Entropy-Guided Method for Enhancing the Reliability of Large Language Models", "link_suffix": "/forum?id=Z8Mfy0iK4n", "link": "https://openreview.net/forum?id=Z8Mfy0iK4n", "pdf_link": "https://openreview.net/pdf?id=Z8Mfy0iK4n", "keywords": "Reliability, Large language Model", "abstract": "While large language models (LLMs) encode vast amounts of knowledge within their parameters for some mainstream entities, factual inconsistencies and untruthfulness in LLMs often lead to unreliable responses and cause significant risks in practical applications.\nThis paper aims to improve model reliability by enhancing consistency in answers to known facts and encouraging refusal to answer for uncertain questions.\nSpecifically, we introduce \\textbf{SREF}, an entropy-guided approach designed to enhance the reliability of language models by incorporating \\textbf{S}elf-\\textbf{REF}erences, models' understanding of rephrasing questions, with inputs.\nWe analyze and reveal the effectiveness of SREF in enhancing model reliability from the perspectives of entropy and KL divergence.\nExtensive experiments on 12 LLMs demonstrate that outputs generated with SREF yield more reliable results, including an average improvement of 16.01% over the baselines and a 15.10% average improvement in consistency, while also adapting to identify and acknowledge uncertain facts.", "title_embedding_index": 5603, "title_abs_embedding_index": 5628}, {"title": "Linguini: A benchmark for language-agnostic linguistic reasoning", "link_suffix": "/forum?id=QiyQJqpcYe", "link": "https://openreview.net/forum?id=QiyQJqpcYe", "pdf_link": "https://openreview.net/pdf?id=QiyQJqpcYe", "keywords": "linguistic-reasoning", "abstract": "We propose a new benchmark to measure a language model\u2019s linguistic reasoning skills without relying on pre-existing language-specific knowledge. The test covers 894 questions grouped in 160 problems across 75 (mostly) extremely low-resource languages, extracted from the International Linguistic Olympiad corpus. To attain high accuracy on this benchmark, models don\u2019t need previous knowledge of the tested language, since all the information required to solve the linguistic puzzle is provided within the context. We find that, while all analyzed models rank below 25% accuracy, there is a significant gap between open and closed models, with the best-performing proprietary model at 24.05% and the best-performing open model at 8.84%.", "title_embedding_index": 5604, "title_abs_embedding_index": 5629}, {"title": "Mutual Information Preserving Neural Network Pruning", "link_suffix": "/forum?id=2IhkyiF3to", "link": "https://openreview.net/forum?id=2IhkyiF3to", "pdf_link": "https://openreview.net/pdf?id=2IhkyiF3to", "keywords": "structured pruning, model compression, mutual information", "abstract": "Model pruning is attracting increasing interest because of its positive implications in terms of resource consumption and costs. A variety of methods have been developed in the past years. In particular, structured pruning techniques discern the importance of nodes in neural networks (NNs) and filters in convolutional neural networks (CNNs). Global versions of these rank all nodes in a network and select the top-$k$, offering an advantage over local methods that rank nodes only within individual layers. By evaluating all nodes simultaneously, global techniques provide greater control over the network architecture, which improves performance. However, the ranking and selecting process carried out during global pruning can have several major drawbacks. First, the ranking is not updated in real time based on the pruning already performed, making it unable to account for inter-node interactions. Second, it is not uncommon for whole layers to be removed from a model, which leads to untrainable networks. Lastly, global pruning methods do not offer any guarantees regarding re-training. In order to address these issues, we introduce Mutual Information Preserving Pruning (MIPP). The fundamental principle of our method is to select nodes such that the mutual information (MI) between the activations of adjacent layers is maintained. We evaluate MIPP on an array of vision models and datasets, including a pre-trained ResNet50 on ImageNet, where we demonstrate MIPP\u2019s ability to outperform state-of-the-art methods. The implementation of MIPP will be made available upon publication.", "title_embedding_index": 5605, "title_abs_embedding_index": 5630}, {"title": "GRADE: Quantifying Sample Diversity in Text-to-Image Models", "link_suffix": "/forum?id=JddNOaw66n", "link": "https://openreview.net/forum?id=JddNOaw66n", "pdf_link": "https://openreview.net/pdf?id=JddNOaw66n", "keywords": "sample diversity, text-to-image, diffusion, evaluation", "abstract": "Text-to-image (T2I) models are remarkable at generating realistic images based on textual descriptions. However, textual prompts are inherentlyunderspecified: they do not specify all possible attributes of the required image. This raises key questions: do T2I models generate diverse outputs on typical underspecified prompts? How can we automatically measure diversity? We proposeGRADE:GranularAttributeDiversityEvaluation, an automatic method for quantifying sample diversity. GRADE leverages the world knowledge embedded in large language models and visual question-answering systems to identify relevant concept-specific axes of diversity (e.g., ''shape'' and ''color'' for the concept ''cookie''). It then estimates attribute distributions and quantifies diversity using (normalized) entropy. GRADE achieves over 90% human agreement while exhibiting weak correlation to commonly used diversity metrics. We use GRADE to measure the overall diversity of 12 T2I models using 400 concept-attribute pairs, revealing that even the most diverse models display limited variation. Further, we find these models often exhibitdefault behaviors, a situation where the model consistently generates concepts with the same attributes (e.g., 98% of the cookies are round). Finally, we demonstrate that a key reason for low diversity is due to underspecified captions in training data.", "title_embedding_index": 5606, "title_abs_embedding_index": 5631}, {"title": "Language Models Need Inductive Biases to Count Inductively", "link_suffix": "/forum?id=s3IBHTTDYl", "link": "https://openreview.net/forum?id=s3IBHTTDYl", "pdf_link": "https://openreview.net/pdf?id=s3IBHTTDYl", "keywords": "Language Model Architecture, Expressivity, Length Generalization", "abstract": "Counting is a fundamental example of generalization, whether viewed through the mathematical lens of Peano's axioms defining the natural numbers or the cognitive science literature for children learning to count. The argument holds for both cases that learning to count means learning to count infinitely. While few papers have tried to distill transformer \"reasoning\" to the simplest case of counting, investigating length generalization does occur throughout the literature. In the \"train short, test long\" paradigm of NLP, length refers to the training sentence length. In formal language recognition, length refers to the input sequence length, or the maximum stack size induced by a pushdown automata. In general problem solving, length refers to the number of hops in a deductive reasoning chain or the recursion depth. For all cases, counting is central to task success. And crucially, generalizing counting inductively is central to success on OOD instances. This work provides extensive empirical results on training language models to count. We experiment with architectures ranging from RNNs, Transformers, State-Space Models and RWKV. We present carefully-designed task formats, auxiliary tasks and positional embeddings to avoid limitations in generalization with OOD-position and OOD-vocabulary. We find that while traditional RNNs trivially achieve inductive counting, Transformers have to rely on positional embeddings to count out-of-domain. As counting is the basis for many arguments concerning the expressivity of Transformers, our finding calls for the community to reexamine the application scope of primitive functions defined in formal characterizations. Finally, modern RNNs also largely underperform traditional RNNs in generalizing counting inductively. We discuss how design choices that enable parallelized training of modern RNNs cause them to lose merits of a recurrent nature.", "title_embedding_index": 5607, "title_abs_embedding_index": 5632}, {"title": "Plan-RAG: Planning-guided Retrieval Augmented Generation", "link_suffix": "/forum?id=cUuOKnjVQJ", "link": "https://openreview.net/forum?id=cUuOKnjVQJ", "pdf_link": "https://openreview.net/pdf?id=cUuOKnjVQJ", "keywords": "Language Models, Retrieval Augmented Generation, LLM, RAG", "abstract": "We introduce Planning-guided Retrieval Augmented Generation (Plan-RAG), a novel framework that augments the retrieve-then-reason paradigm of existing RAG frameworks to plan-then-retrieve. Plan-RAG formulates a reasoning plan as a directed acyclic graph (DAG), decomposing queries into interrelated atomic sub-queries. Answer generation follows the DAG structure, allowing significant gains in efficiency through parallelized retrieval and generation. While state-of-the-art RAG solutions require extensive data generation and fine-tuning of language models (LMs), Plan-RAG incorporates frozen LMs as plug-and-play experts to generate high-quality answers. Compared to existing RAG solutions, Plan-RAG demonstrates significant improvements in reducing hallucinations and bolstering attribution due to its structured sub-query decomposition. Plan-RAG offers a new perspective on integrating external knowledge in LMs while ensuring attribution by design, contributing towards more reliable and interpretable LM-based systems.", "title_embedding_index": 5608, "title_abs_embedding_index": 5633}, {"title": "Automated Red Teaming with GOAT: the Generative Offensive Agent Tester", "link_suffix": "/forum?id=Ly0SQh7Urv", "link": "https://openreview.net/forum?id=Ly0SQh7Urv", "pdf_link": "https://openreview.net/pdf?id=Ly0SQh7Urv", "keywords": "red teaming, adversarial machine learning, adversarial examples, attacks on language models", "abstract": "Red teaming assesses how large language models (LLMs) can produce content that violates norms, policies, and rules set during their safety training. However, most existing automated methods in the literature are not representative of the way humans tend to interact with AI models. Common users of AI models may not have advanced knowledge of adversarial machine learning methods or access to model internals, and they do not spend a lot of time crafting a single highly effective adversarial prompt. Instead, they are likely to make use of techniques commonly shared online and exploit the multi-turn conversational nature of LLMs. While manual testing addresses this gap, it is an inefficient and often expensive process. To address these limitations, we introduce the Generative Offensive Agent Tester (GOAT), an automated agentic red teaming system that simulates plain language adversarial conversations while leveraging multiple adversarial prompting techniques to identify vulnerabilities in LLMs. We instantiate GOAT with 7 red teaming attacks by prompting a general-purpose model in a way that encourages reasoning through the choices of methods available, the current target model\u2019s response, and the next steps. Our approach is designed to be extensible and efficient, allowing human testers to focus on exploring new areas of risk while automation covers the scaled adversarial stress-testing of known risk territory. We present the design and evaluation of GOAT, demonstrating its effectiveness in identifying vulnerabilities in state-of-the-art LLMs, with an ASR@10 of 97% against Llama 3.1 and 88% against GPT-4 on the JailbreakBench dataset", "title_embedding_index": 5609, "title_abs_embedding_index": 5634}, {"title": "Gaussian Mixture Vector Quantization with Aggregated  Categorical Posterior", "link_suffix": "/forum?id=mLxxv5gts0", "link": "https://openreview.net/forum?id=mLxxv5gts0", "pdf_link": "https://openreview.net/pdf?id=mLxxv5gts0", "keywords": "Mixtures of Gaussians, Vector Quantization, Variational Auto-Encoders, Representation Learning", "abstract": "The vector quantization is a widely used method to map continuous representation to discrete space and has important application in tokenization for generative mode, bottlenecking information and many other tasks in machine learning. \nVector Quantized Variational Autoencoder (VQ-VAE) is a type of variational autoencoder using discrete embedding as latent. We generalize the technique further, enriching the probabilistic framework with a Gaussian mixture as the underlying generative model. This framework leverages a codebook of latent means and adaptive variances to capture complex data distributions. This principled framework avoids various heuristics and strong assumptions that are needed with the VQ-VAE to address training instability and to improve codebook utilization.  This approach integrates the benefits of both discrete and continuous representations within a variational Bayesian framework. Furthermore, by introducing the \\textit{Aggregated Categorical Posterior Evidence Lower Bound} (ALBO), we offer a principled alternative optimization objective that aligns variational distributions with the generative model. Our experiments demonstrate that GM-VQ improves codebook utilization and reduces information loss without relying on handcrafted heuristics.", "title_embedding_index": 5610, "title_abs_embedding_index": 5635}, {"title": "Quantifying AI Psychology: A Psychometric Benchmark for Large Language Models", "link_suffix": "/forum?id=31UkFGMy8t", "link": "https://openreview.net/forum?id=31UkFGMy8t", "pdf_link": "https://openreview.net/pdf?id=31UkFGMy8t", "keywords": "Large language model, evaluation, psychometrics, psychology", "abstract": "Large Language Models (LLMs) have demonstrated exceptional capabilities in solving various tasks, progressively evolving into general-purpose assistants. The increasing integration of LLMs into society has sparked interest in whether they exhibit psychological patterns, and whether these patterns remain consistent across different contexts---questions that could deepen the understanding of their behaviors. Inspired by psychometrics, this paper presents a framework for investigating psychology in LLMs, including psychological dimension identification, assessment dataset design, and assessment with results validation. Following this framework, we introduce a comprehensive psychometric benchmark for LLMs that covers five psychological dimensions: personality, values, emotion, theory of mind, and motivation. This benchmark includes 13 datasets featuring diverse scenarios and item types. Our findings suggest that LLMs display a broad spectrum of psychological patterns. We also uncover significant discrepancies between LLMs' self-reported traits and their response patterns in real-world scenarios, revealing complexities in their behaviors. This paper offers a thorough psychometric assessment of LLMs, providing insights into reliable evaluation and potential applications in AI and social sciences. Our dataset and code can be accessed via this \\href{https://anonymous.4open.science/r/LLM-Psychometrics-Benchmark-2A19}{link}.", "title_embedding_index": 5611, "title_abs_embedding_index": 5636}, {"title": "General Scene Adaptation for Vision-and-Language Navigation", "link_suffix": "/forum?id=2oKkQTyfz7", "link": "https://openreview.net/forum?id=2oKkQTyfz7", "pdf_link": "https://openreview.net/pdf?id=2oKkQTyfz7", "keywords": "vision-and-language navigation; scene adaptation; multi-modal learning", "abstract": "Vision-and-Language Navigation (VLN) tasks mainly evaluate agents based on one-time execution of individual instructions across multiple environments, aiming to develop agents capable of functioning in any environment in a zero-shot manner. However, real-world navigation robots often operate in persistent environments with relatively consistent physical layouts, visual observations, and language styles from instructors. Such a gap in the task setting presents an opportunity to improve VLN agents by incorporating continuous adaptation to specific environments. To better reflect these real-world conditions, we introduce GSA-VLN (General Scene Adaptation for VLN), a novel task requiring agents to execute navigation instructions within a specific scene and simultaneously adapt to it for improved performance over time. To evaluate the proposed task, one has to address two challenges in existing VLN datasets: the lack of out-of-distribution (OOD) data, and the limited number and style diversity of instructions for each scene. Therefore, we propose a new dataset, GSA-R2R, which significantly expands the diversity and quantity of environments and instructions for the Room-to-Room (R2R) dataset to evaluate agent adaptability in both ID and OOD contexts. Furthermore, we design a three-stage instruction orchestration pipeline that leverages large language models (LLMs) to refine speaker-generated instructions and apply role-playing techniques to rephrase instructions into different speaking styles. This is motivated by the observation that each individual user often has consistent signatures or preferences in their instructions,  taking the use case of home robotic assistants as an example. We conducted extensive experiments on GSA-R2R to thoroughly evaluate our dataset and benchmark various methods, revealing key factors enabling agents to adapt to specific environments. Based on our findings, we propose a novel method, Graph-Retained DUET (GR-DUET), which incorporates memory-based navigation graphs with an environment-specific training strategy, achieving state-of-the-art results on all GSA-R2R splits.", "title_embedding_index": 5612, "title_abs_embedding_index": 5637}, {"title": "Cross-Entropy Is All You Need To Invert the Data Generating Process", "link_suffix": "/forum?id=hrqNOxpItr", "link": "https://openreview.net/forum?id=hrqNOxpItr", "pdf_link": "https://openreview.net/pdf?id=hrqNOxpItr", "keywords": "supervised learning, representation learning, identifiability, linear representation hypothesis", "abstract": "Supervised learning has become a cornerstone of modern machine learning, yet a comprehensive theory explaining its effectiveness remains elusive. Empirical phenomena, such as neural analogy-making and the linear representation hypothesis, suggest that supervised models can learn interpretable factors of variation in a linear fashion. Recent advances in self-supervised learning, particularly nonlinear Independent Component Analysis, have shown that these methods can recover latent structures by inverting the data generating process. We extend these identifiability results to parametric instance discrimination, \nthen show how insights transfer to the ubiquitous setting of supervised learning with cross-entropy minimization. We prove that even in standard classification tasks, models learn representations of ground-truth factors of variation up to a linear transformation. We corroborate our theoretical contribution with a series of empirical studies. First, using simulated data matching our theoretical assumptions, we demonstrate successful disentanglement of latent factors. Second, we show that on DisLib, a widely-used disentanglement benchmark, simple classification tasks recover latent structures up to linear transformations. Finally, we reveal that models trained on ImageNet encode representations that permit linear decoding of proxy factors of variation.\nTogether, our theoretical findings and experiments offer a compelling explanation for recent observations of linear representations, such as superposition in neural networks. This work takes a significant step toward a cohesive theory that accounts for the unreasonable effectiveness of supervised deep learning.", "title_embedding_index": 5613, "title_abs_embedding_index": 5638}, {"title": "On-Device Transfer Learning based on Mixed Precision Partitioning", "link_suffix": "/forum?id=eqKHuxIpp5", "link": "https://openreview.net/forum?id=eqKHuxIpp5", "pdf_link": "https://openreview.net/pdf?id=eqKHuxIpp5", "keywords": "Machine Learning, Transfer Learning, On-device training", "abstract": "The application of machine learning is becoming more widespread, with a growing number of use cases. The development of centralized data training and the exponential growth of data generation raise significant privacy and security concerns. On-device training offers a solution by enhancing privacy and reducing the need for communication between the cloud and the device. Furthermore, on-device transfer learning (TL) can leverage the knowledge gained from pre-trained models, hence, accelerating the training process. However, backpropagation, especially in embedded systems, requires more memory than running inference, which becomes a challenge for devices with limited resources. This paper aims to improve the efficiency and performance of on-device TL. We propose an open source mixed-precision partitioning framework that identifies optimal partitioning layers for retraining, combining quantized and bfloat16 layers to enhance performance and energy efficiency. Our approach is validated through experiments on ResNet-18 and SqueezeNetV1.1 models using Flowers-102, STL-10, and OxfordIIITPet datasets. The partitioned mixed-precision model is able to transfer the knowledge from the pre-trained model to new datasets without losing accuracy compared to the baseline bfloat16 model. These results illustrate the potential for resource-constrained devices to perform TL locally.", "title_embedding_index": 5614, "title_abs_embedding_index": 5639}, {"title": "Off-Policy Maximum Entropy RL with Visitation Measures", "link_suffix": "/forum?id=L34BvDTwls", "link": "https://openreview.net/forum?id=L34BvDTwls", "pdf_link": "https://openreview.net/pdf?id=L34BvDTwls", "keywords": "reinforcement learning, maximum entropy RL, exploration", "abstract": "We introduce a new maximum entropy reinforcement learning framework based on the distribution of states and actions visited by a policy. More precisely, an intrinsic reward function is added to the reward function of the Markov decision process that shall be controlled. For each state and action, this intrinsic reward is the relative entropy of the discounted distribution of states and actions (or features from these states and actions) during the next time steps. We prove that this distribution is the fixed point of a contractive operator. Furthermore, the problem of maximizing the expected discounted sum of these intrinsic rewards is proven to be an approximation of the minimization of an upper bound on the suboptimality gap of the state-action value function of the policy. We finally describe how existing algorithms can integrate these intrinsic rewards to enhance exploration and introduce a practical algorithm for learning this fixed point off-policy, using state-action transitions, relying on N-step bootstrapping of the operator. Empirically, this maximum entropy reinforcement learning framework provides exploration policies with good coverage of the state-action space, and high-performing control policies, which both can be computed off-policy.", "title_embedding_index": 5615, "title_abs_embedding_index": 5640}, {"title": "SpiritSight Agent: Advanced GUI Agent with One Look", "link_suffix": "/forum?id=jY2ow7jRdZ", "link": "https://openreview.net/forum?id=jY2ow7jRdZ", "pdf_link": "https://openreview.net/pdf?id=jY2ow7jRdZ", "keywords": "GUI Agent, VLLM, decision-making", "abstract": "Graphical User Interface (GUI) Agents show amazing abilities in assisting human-computer interaction, automating human user's navigation on digital devices. An ideal GUI Agent is expected to achieve high accuracy, low latency, and generality across various GUI platforms. Recent visual-based approaches show promises, taking the advantages of advanced Vision Language Models (VLMs). Although they generally meet the requirements of generality and low latency, these visual-based GUI Agents often fall short in terms of localization accuracy. To address this issue, we propose $\\textbf{SpiritSight}$, a visual-based generalist end-to-end GUI agent with outstanding grounding abilities. First, we create a multi-level, large-scale, high-quality GUI training dataset with scalable methods and train SpiritSight using curriculum learning, empowering it with robust GUI understanding and localization capabilities. Second, we introduce the $\\textbf{Universal Block Parsing (UBP)}$ method, which frames the localization task as a multi-image QA problem, further enhancing SpiritSight's ability to ground GUI objects. With the above-mentioned efforts, SpiritSight constantly outperforms previous SOTA methods across numerous major automated GUI navigation benchmarks. Notably, SpiritSight-8B achieves a 46.1% step Success Rate(SR) on the Mind2Web benchmark without any candidates element input, $\\textbf{more than doubling}$ the performance of SeeClick (20.9%) with a comparable model scale. SpiritSight also outperforms other visual-language-based methods in various GUI platforms, demonstrating its superior capability and compatibility in GUI Agent tasks. The models and the code will be made available upon publications.", "title_embedding_index": 5616, "title_abs_embedding_index": 5641}, {"title": "Cohesion: Coherence-Based Diffusion for Long-Range Dynamics Forecasting", "link_suffix": "/forum?id=5bDBahNmmH", "link": "https://openreview.net/forum?id=5bDBahNmmH", "pdf_link": "https://openreview.net/pdf?id=5bDBahNmmH", "keywords": "PDE, diffusion, dynamics, emulator", "abstract": "We recast existing works on probabilistic dynamics forecasting through a unified framework connecting turbulence and diffusion principles: Cohesion. Specifically, we relate the coherent part of nonlinear dynamics as a conditioning prior in a denoising process, which can be efficiently estimated using reduced-order models. This fast generation of long prior sequences allows us to reframe forecasting as trajectory planning, a common task in RL. This reformulation is beneficial because we can perform a single conditional denoising pass for an entire sequence, rather than autoregressively over long lead time, gaining orders-of-magnitude speedups with little performance loss. Nonetheless, Cohesion supports flexibility through temporal composition that allows iterations to be performed over smaller subsequences, with autoregressive being a special case. To ensure temporal consistency within and between subsequences, we incorporate a model-free, small receptive window via temporal convolution that leverages large NFEs during denoising. Finally, we perform our guidance in a classifier-free manner to handle a broad range of conditioning scenarios for zero-shot forecasts. Our experiments demonstrate that Cohesion outperforms state-of-the-art probabilistic emulators for chaotic systems over long lead time, including in Kolmogorov Flow and Shallow Water Equation. Its low spectral divergence highlights Cohesion's ability to resolve multi-scale physical structures, even in partially-observed cases, and are thus essential for long-range, high-fidelity, physically-realistic emulation.", "title_embedding_index": 5617, "title_abs_embedding_index": 5642}, {"title": "Bridging Context Gaps: Leveraging Coreference Resolution for Long Contextual Understanding", "link_suffix": "/forum?id=cPozlf9OaF", "link": "https://openreview.net/forum?id=cPozlf9OaF", "pdf_link": "https://openreview.net/pdf?id=cPozlf9OaF", "keywords": "Coreference Resolution, Long Contextual Understanding, Information Extraction", "abstract": "Large language models (LLMs) have shown remarkable capabilities in natural language processing; however, they still face difficulties when tasked with understanding lengthy contexts and executing effective question answering. These challenges often arise due to the complexity and ambiguity present in longer texts. To enhance the performance of LLMs in such scenarios, we introduce the Long Question Coreference Adaptation (LQCA) method. This innovative framework focuses on coreference resolution tailored to long contexts, allowing the model to identify and manage references effectively. The LQCA method encompasses four key steps: resolving coreferences within sub-documents, computing the distances between mentions, defining a representative mention for coreference, and answering questions through mention replacement. By processing information systematically, the framework provides easier-to-handle partitions for LLMs, promoting better understanding. Experimental evaluations on a range of LLMs and datasets have yielded positive results, with a notable improvements on OpenAI-o1-mini and GPT-4o models, highlighting the effectiveness of leveraging coreference resolution to bridge context gaps in question answering.", "title_embedding_index": 5618, "title_abs_embedding_index": 5643}, {"title": "Addressing domain shift with diffusion-based adaptation for real image dehazing", "link_suffix": "/forum?id=f4aMqhYG7z", "link": "https://openreview.net/forum?id=f4aMqhYG7z", "pdf_link": "https://openreview.net/pdf?id=f4aMqhYG7z", "keywords": "diffusion-based adaptation, domain gap, real image dehazing", "abstract": "Conventional supervised single-image dehazing methods, which are trained with substantial synthetic hazy-clean image pairs, have achieved promising performance. However, they often fail to tackle out-of-distribution hazy images, due to the domain shift between source and target scenarios (e.g., between indoor and outdoor, between synthetic and real). In this work, we observe the opportunity for improving such dehazing models' generalization ability without modifying the architectures or weights of conventional models by adopting the diffusion model to transfer the distribution of input images from target domain to source domain. Specifically, we train a denoising diffusion probabilistic model (DDPM) with source hazy images to capture prior probability distribution of the source domain. Then, during the test-time the obtained DDPM can adapt target hazy inputs to source domain in the reverse process from the perspective of conditional generation. The adapted inputs are fed into a certain state-of-the-art (SOTA) dehazing model pre-trained on source domain to predict the haze-free outputs. Note that, the whole proposed pipeline, termed \\textbf{Diff}usion-based \\textbf{AD}aptation (DiffAD), is model-agnostic and plug-and-play. Besides, to enhance the efficiency in real image dehazing, we further employ the predicted haze-free outputs as the pseudo labels to fine-tune the underlying model. Extensive experimental results demonstrate that our DiffAD is effective, achieving superior performance against SOTA dehazing methods in domain-shift scenarios.", "title_embedding_index": 5619, "title_abs_embedding_index": 5644}, {"title": "Scalable Mechanistic Neural Networks", "link_suffix": "/forum?id=Oazgf8A24z", "link": "https://openreview.net/forum?id=Oazgf8A24z", "pdf_link": "https://openreview.net/pdf?id=Oazgf8A24z", "keywords": "Scientific Machine Learning, Ordinary Differential Equations, Time Series, Dynamical Systems", "abstract": "We propose Scalable Mechanistic Neural Network (S-MNN), an enhanced neural network framework designed for scientific machine learning applications involving long temporal sequences. By reformulating the original Mechanistic Neural Network (MNN) (Pervez et al., 2024), we reduce the computational time and space complexities from cubic and quadratic with respect to the sequence length, respectively, to linear. This significant improvement enables efficient modeling of long-term dynamics without sacrificing accuracy or interpretability. Extensive experiments demonstrate that S-MNN matches the original MNN in precision while substantially reducing computational resources. Consequently, S-MNN can drop-in replace the original MNN in applications, providing a practical and efficient tool for integrating mechanistic bottlenecks into neural network models of complex dynamical systems.", "title_embedding_index": 5620, "title_abs_embedding_index": 5645}, {"title": "SeLoRA: Self-Expanding Low-Rank Adaptation of Latent Diffusion Model for Medical Image Synthesis", "link_suffix": "/forum?id=T6QqcRiypb", "link": "https://openreview.net/forum?id=T6QqcRiypb", "pdf_link": "https://openreview.net/pdf?id=T6QqcRiypb", "keywords": "Text-to-Image Synthesis, Low-Rank Adaptation, Medical Imaging, Parameter Efficient Finetuning", "abstract": "The persistent challenge of medical image synthesis posed by the scarcity of annotated data and the need to synthesize \"missing modalities\" for multi-modal analysis, underscored the imperative development of effective synthesis methods. Recently, the combination of Low-Rank Adaptation (LoRA) with latent diffusion models (LDMs) has emerged as a viable approach for efficiently adapting pre-trained large language models, in the medical field. However, the direct application of LoRA assumes uniform ranking across all linear layers, overlooking the significance of different weight matrices, and leading to sub-optimal outcomes. Prior works on LoRA prioritize the reduction of trainable parameters, and there exists an opportunity to further tailor this adaptation process to the intricate demands of medical image synthesis. In response, we present SeLoRA, a Self-Expanding Low-Rank Adaptation Module, that dynamically expands its ranking across layers during training, strategically placing additional ranks on crucial layers, to allow the model to elevate synthesis quality where it matters most. The proposed method not only enables LDMs to fine-tune on medical data efficiently but also empowers the model to achieve improved image quality with minimal ranking. The code of our SeLoRA method is publicly available onhttps://anonymous.4open.science/r/SeLoRA-980D.", "title_embedding_index": 5621, "title_abs_embedding_index": 5646}, {"title": "Equally Critical: Samples, Targets, and Their Mappings in Datasets", "link_suffix": "/forum?id=FM21yYBhuE", "link": "https://openreview.net/forum?id=FM21yYBhuE", "pdf_link": "https://openreview.net/pdf?id=FM21yYBhuE", "keywords": "Data-efficient Learning", "abstract": "Neural scaling laws highlight the trade-off between test error reduction and increased resources in machine learning, revealing diminishing returns as data volume, model size, and computational power increase.\nThis inefficiency poses sustainability challenges, as marginal performance gains necessitate exponential resource consumption.\nRecent works have investigated these laws from a data-efficient standpoint, primarily concentrating on sample optimization, while largely neglecting the influence of target.\nIn this study, we first demonstrate that, given an equivalent training budget, employing soft targets on a 10% subset can outperform the use of one-hot targets on the full dataset. Building on this observation, we review existing paradigms in the sample-target relationship, categorizing them into distinct sample-to-target mapping strategies.\nSubsequently, we propose a unified loss framework to assess their impact on training efficiency. Finally, we conduct a comprehensive analysis of how variations in target and sample types, quantities, and qualities influence training efficiency across three training strategies, providing six key insights to enhance training efficacy.", "title_embedding_index": 5622, "title_abs_embedding_index": 5647}, {"title": "Linear Partial Gromov-Wasserstein Embedding", "link_suffix": "/forum?id=BA1eG7vCNb", "link": "https://openreview.net/forum?id=BA1eG7vCNb", "pdf_link": "https://openreview.net/pdf?id=BA1eG7vCNb", "keywords": "Optimal transport, Gromov-Wasserstein distance, Unbalanced optimal transport", "abstract": "The Gromov\u2013Wasserstein (GW) problem, a variant of the classical optimal transport (OT) problem, has attracted growing interest in the machine learning and data science communities due to its ability to quantify similarity between measures in different metric spaces. However, like the classical OT problem, GW imposes an equal mass constraint between measures, which restricts its application in many machine learning tasks. To address this limitation, the partial Gromov-Wasserstein (PGW) problem has been introduced, which relaxes the equal mass constraint, enabling the comparison of general positive Radon measures. Despite this, both GW and PGW face significant computational challenges due to their non-convex nature. To overcome these challenges, we propose the linear partial Gromov-Wasserstein (LPGW) embedding, a linearized embedding technique for the PGW problem. For $K$ different metric measure spaces, the pairwise computation of the PGW distance requires solving the PGW problem $\\mathcal{O}(K^2)$ times. In contrast, the proposed linearization technique reduces this to $\\mathcal{O}(K)$ times. Similar to the linearization technique for the classical OT problem, we prove that LPGW defines a valid metric for metric measure spaces. Finally, we demonstrate the effectiveness of LPGW in practical applications such as shape retrieval and learning with transport-based embeddings, showing that LPGW preserves the advantages of PGW in partial matching while significantly enhancing computational efficiency.", "title_embedding_index": 5623, "title_abs_embedding_index": 5648}, {"title": "Whale-X: Learning Scalable Embodied World Models with Enhanced Generalizability", "link_suffix": "/forum?id=SH3DTbytqF", "link": "https://openreview.net/forum?id=SH3DTbytqF", "pdf_link": "https://openreview.net/pdf?id=SH3DTbytqF", "keywords": "world model, sequential decision-making, embodied control", "abstract": "World models play a crucial role in decision-making within embodied environments, enabling cost-free explorations that would otherwise be expensive in the real world. However, to support faithful imagination in out-of-distribution (OOD) regions, world models must possess significant generalizability, which poses substantial challenges for previous scalable approaches. This paper addresses two primary sources of the world model generalization error: the \\emph{policy distribution shift} caused by the divergence between test and data-collection policies, and the \\emph{compounding error} arising from long-horizon autoregressive rollout. To tackle these issues, we introduce the \\emph{policy-conditioning} and the \\emph{retracing-rollout} techniques, respectively. Incorporating these two techniques, we present Whale, a scalable spatial-temporal transformer-based world model with enhanced generalizability. We first demonstrate the effectiveness of the two techniques, showcasing their consistent superiority over previous baselines in both trajectory generation quality and value estimation accuracy. Furthermore, we propose Whale-X, a 414M parameter world model trained on 970K trajectories from Open X-Embodiment datasets. We show that Whale-X exhibits promising scalability and strong generalizability in real-world manipulation scenarios using minimal demonstrations.", "title_embedding_index": 5624, "title_abs_embedding_index": 5649}]