[{"title": "No Factor Left Behind: Towards arbitrary amount of factors in the medical cohort analysis", "link_suffix": "/forum?id=Bx5kcMkb8l", "link": "https://openreview.net/forum?id=Bx5kcMkb8l", "pdf_link": "https://openreview.net/pdf?id=Bx5kcMkb8l", "keywords": "Medical cohort analysis, Risk assessment, generalization, prompt engineering, open source model", "abstract": "Driven by the goal of data-driven analysis on the large-scale cohort, a large language model(LLM) has solidified itself as a critical focus of artificial intelligence medical research today. However, such efforts have coalesced around a small group of evidence, leaving behind the vast majority of factors collected in the cohort investigation. What does it take to break the more than 70 factors while ensuring responsible, high-quality prediction, all while keeping medical considerations in mind? In No Factor Left Behind, we first took on this challenge by numerical interpretable evidence contextualizing the need for Premature rupture of membranes (PROM) risk assessment through exploratory interviews with domain experts. Then, we created datasets and models aimed at narrowing the performance gap between low and high-frequency factors. More specifically, we developed a model based on factor-value pairs trained on data obtained with robust and effective data mining techniques tailored for low-frequency factors. We propose multiple architectural and training improvements to counteract overfitting while training on 70 factors. Critically, we interpreted the risk of PROM over 7000 cohort participants' directions using numerical interpretable evidence with precise values of factors combined with human evaluation covering all factors in the dataset to assess medical safety. Our model achieves a performance of 79% accuracy (78 factors) and 96% accuracy(40 factors) with risk assessment at the screening level, laying the novel insight for realizing a general medical cohort analysis method in the era of LLMs.", "title_embedding_index": 13050, "title_abs_embedding_index": 13075}, {"title": "FuzzyCLIP: Clustering-Driven Stacked Prompt in Zero-Shot Anomaly Detection", "link_suffix": "/forum?id=pgVMJdhgPI", "link": "https://openreview.net/forum?id=pgVMJdhgPI", "pdf_link": "https://openreview.net/pdf?id=pgVMJdhgPI", "keywords": "Anomaly detection, Zero-shot anomaly detection, CLIP, Industrial defect inspection", "abstract": "How to enhance the alignment of text and image features in CLIP model is a key challenge in zero-shot industrial anomaly detection tasks. Recent studies mostly rely on precise category prompts for pre-training, but this approach is prone to overfitting, which limits the generalization ability of the mode. To address this issue, we propose the concept of fuzzy prompts and introduce Clustering-Driven Stacked Prompts (CSP) along with the Ensemble Feature Alignment (EFA) module to improve the alignment between text and image features. This design significantly outperforms other methods in terms of training speed, stability, and final convergence results, showing remarkable efficiency in enhancing anomaly detection segmentation performance. What is even more surprising is that fuzzy stacked prompts exhibit strong generalization in classification tasks, enabling them to adapt to various anomaly classification tasks without any additional operations. Therefore, we further propose the Regulating Prompt Learning (RPL) module, which leverages the strong generalization ability of fuzzy stacked prompts to regularize prompt learning, thereby improving performance in anomaly detection classification tasks. We conducted extensive experiments on seven industrial anomaly detection datasets, which demonstrate that our method achieves state-of-the-art performance in zero-shot anomaly detection and segmentation tasks.", "title_embedding_index": 13051, "title_abs_embedding_index": 13076}, {"title": "A Bounding Box is Worth One Token: Interleaving Layout and Text in a Large Language Model for Document Understanding", "link_suffix": "/forum?id=lFijzkTUNB", "link": "https://openreview.net/forum?id=lFijzkTUNB", "pdf_link": "https://openreview.net/pdf?id=lFijzkTUNB", "keywords": "LLM, DocAI, Visually Rich Document Understanding, KIE", "abstract": "Recently, many studies have demonstrated that exclusively incorporating OCR-derived text and spatial layouts with large language models (LLMs) can be highly effective for document understanding tasks. However, existing methods that integrate spatial layouts with text have limitations, such as producing overly long text sequences or failing to fully leverage the autoregressive traits of LLMs. In this work, we introduce Interleaving Layout and Text in a Large Language Model (LayTextLLM)} for document understanding. In particular, LayTextLLM projects each bounding box to a single embedding and interleaves it with text, efficiently avoiding long sequence issues while leveraging autoregressive traits of LLMs. LayTextLLM not only streamlines the interaction of layout and textual data but also shows enhanced performance in Key Information Extraction (KIE) and Visual Question Answering (VQA). Comprehensive benchmark evaluations reveal significant improvements, with a 27.2% increase on KIE tasks and 12.0% on VQA tasks compared to previous state-of-the-art document understanding MLLMs, as well as a 15.1% improvement over other SOTA OCR-based LLMs on KIE tasks.", "title_embedding_index": 13052, "title_abs_embedding_index": 13077}, {"title": "LLMScan: Causal Scan for LLM Misbehavior Detection", "link_suffix": "/forum?id=xdGsiYNfje", "link": "https://openreview.net/forum?id=xdGsiYNfje", "pdf_link": "https://openreview.net/pdf?id=xdGsiYNfje", "keywords": "Large Language Model, LLM Safety, LLM Misbehavior Detection, Causality Analysis, Model Scan", "abstract": "Despite the success of Large Language Models (LLMs) across various fields, their potential to generate untruthful, biased and harmful responses poses significant risks, particularly in critical applications. This highlights the urgent need for systematic methods to detect and prevent such misbehavior. While existing approaches target specific issues such as harmful responses, this work introduces LLMScan, an innovative LLM monitoring technique based on causality analysis, offering a comprehensive solution. LLMScan systematically monitors the inner workings of an LLM through the lens of causal inference, operating on the premise that the LLM's `brain' behaves differently when misbehaving. By analyzing the causal contributions of the LLM's input tokens and transformer layers, LLMScan effectively detects misbehavior. Extensive experiments across various tasks and models reveal clear distinctions in the causal distributions between normal behavior and misbehavior, enabling the development of accurate, lightweight detectors for a variety of misbehavior detection tasks.", "title_embedding_index": 13053, "title_abs_embedding_index": 13078}, {"title": "ROPO: Robust Preference Optimization for Large Language Models", "link_suffix": "/forum?id=0nxocR2qx4", "link": "https://openreview.net/forum?id=0nxocR2qx4", "pdf_link": "https://openreview.net/pdf?id=0nxocR2qx4", "keywords": "preference optimization, large language models, noise tolerance", "abstract": "Preference alignment is pivotal for empowering large language models (LLMs) to generate helpful and harmless responses. However, the performance of preference alignment is highly sensitive to the prevalent noise in the preference data. Recent efforts for this problem either marginally alleviate the impact of noise without the ability to actually reduce its presence, or rely on costly teacher LLMs prone to reward misgeneralization. To address these challenges, we propose theRObustPreferenceOptimization (ROPO) framework, a novel iterative alignment approach that integratesnoise-toleranceandfiltering of noisy sampleswithout the aid of external models. Specifically, ROPO first formulates the training process with adaptive noise reduction as an optimization problem, which can be efficiently solved in an iterative paradigm. Then, to enhance this iterative solving process with noise-tolerance and noise-identification capabilities, we derive a robust loss that suppresses the gradients from samples with high uncertainty. We demonstrate both empirically and theoretically that the derived loss is key to the noise-tolerance and effective filtering of noisy samples. Furthermore, inspired by our derived loss, we propose a robustness-guided rejection sampling technique to compensate for the potential important information in discarded queries. Experiments on three widely-used datasets of dialogue and post-summarization demonstrate that ROPO significantly outperforms existing preference alignment methods in the practical noise setting and under artificial random symmetric noise, with its advantage increasing as the noise rate increases.", "title_embedding_index": 13054, "title_abs_embedding_index": 13079}, {"title": "RiTTA: Modeling Event Relations in Text-to-Audio Generation", "link_suffix": "/forum?id=fQbIZY9a3G", "link": "https://openreview.net/forum?id=fQbIZY9a3G", "pdf_link": "https://openreview.net/pdf?id=fQbIZY9a3G", "keywords": "Text-to-Audio Generation, Audio Events Relation, Benchmark, Dataset Corpus", "abstract": "Despite significant advancements in Text-to-Audio (TTA) generation models achieving high-fidelity audio with fine-grained context understanding, they struggle to model the relations between audio events described in the input text. However, previous TTA methods have not systematically explored audio event relation modeling, nor have they proposed frameworks to enhance this capability. In this work, we systematically study audio event relation modeling in TTA generation models. We first establish a benchmark for this task by: (1) proposing a comprehensive relation corpus covering all potential relations in real-world scenarios; (2) introducing a new audio event corpus encompassing commonly heard sounds; and (3) proposing new evaluation metrics to assess audio event relation modeling from various perspectives. Furthermore, we propose a finetuning framework to enhance existing TTA models' ability to model audio events relation.", "title_embedding_index": 13055, "title_abs_embedding_index": 13080}, {"title": "GeVLM: 3D Object Grounding with Geometry-enhanced Vision Language Model", "link_suffix": "/forum?id=7nWKBRQuLT", "link": "https://openreview.net/forum?id=7nWKBRQuLT", "pdf_link": "https://openreview.net/pdf?id=7nWKBRQuLT", "keywords": "3D object grounding, visual large language model, viewpoint consistency", "abstract": "Understanding 3D scenes with point cloud data in tasks such as object referencing, question-answering, and captioning poses significant challenges to vision language models (VLMs), due to the complexity of integrating both linguistic and spatial information. While existing methods have mapped point cloud features into LLM space to enable 3D scene comprehension, they often overlook viewpoint information and the relative spatial distance between objects, this can lead to confusion in interpreting spatial descriptions and grounding objects. This paper presents a geometry-enhanced vision LM (GeVLM) to address these challenges. Specifically, we propose viewpoint-consistent position encoding (VCPE) and distance-aware cross-entropy (DACE) loss, which enhance the model's ability to interpret relative spatial relationships agnostic to camera viewpoint and incorporate distance information in the label space. We additionally introduce the DetailedScanRefer dataset, which provides identifiers and spatial annotation for each object mentioned in the referencing description to further emphasize spatial relationships. GeVLM demonstrates significant improvements over the Chat-3D v2 baseline, particularly with 4.0% and 2.7% absolute increase inAcc@0.25andAcc@0.50respectively on the ScanRefer benchmark.", "title_embedding_index": 13056, "title_abs_embedding_index": 13081}, {"title": "Selective Preference Optimization via Token-Level Reward Function Estimation", "link_suffix": "/forum?id=Bvqsas4TYX", "link": "https://openreview.net/forum?id=Bvqsas4TYX", "pdf_link": "https://openreview.net/pdf?id=Bvqsas4TYX", "keywords": "large language models, preference optimization, alignment", "abstract": "Recent advancements in large language model alignment leverage token-level supervisions to perform fine-grained preference optimization. However, existing token-level alignment methods either optimize on all available tokens, which can be noisy and inefficient, or perform selective training with complex and expensive key token selection strategies.\nIn this work, we propose Selective Preference Optimization (SePO), a novel selective alignment strategy that centers on efficient key token selection without requiring strong, fine-grained supervision signals. We theoretically prove the feasibility of Direct Preference Optimization (DPO) as token-level reward function estimators, which applies to any existing alignment datasets and enables cost-efficient token selection with small-scale model sizes and training data. We then train an oracle model with DPO on the target data and utilize the estimated reward function to score all tokens within the target dataset, where only the key tokens are selected to supervise the target policy model with a contrastive objective function. Extensive experiments on three public evaluation benchmarks show that SePO significantly outperforms competitive baseline methods by only optimizing on 30% key tokens. We also explore SePO as a new paradigm for weak-to-strong generalization, showing\nthat weak oracle models effectively supervise strong policy models with up to 16.8$\\times$ more parameters. SePO also selects useful supervision signals from out-of-distribution data, alleviating the over-optimization problem.", "title_embedding_index": 13057, "title_abs_embedding_index": 13082}, {"title": "Unified Multi-Task Learning & Model Fusion for Efficient Language Model Guardrailing", "link_suffix": "/forum?id=sOdeh2WJL2", "link": "https://openreview.net/forum?id=sOdeh2WJL2", "pdf_link": "https://openreview.net/pdf?id=sOdeh2WJL2", "keywords": "safety guardrailing, language model, synthetic data generation, multi-task learning, model fusion", "abstract": "The trend towards large language models (LLMs) for guardrailing against undesired behaviors is increasing and has shown promise for censoring user inputs. However, high inference speed, memory consumption, hosting expenses and generative non-structured outputs can make their use prohibitive.In this work, we show that task-specific data generation can lead to fine-tuned classifiers that significantly outperform current state of the art (SoTA) while being orders of magnitude smaller. Secondly, we show that using a single model, \\texttt{MultiTaskGuard}, that is pretrained on a large synthetically generated dataset with unique task instructions further improves generalization. Thirdly, our most performant models, \\texttt{UniGuard}, are found using our proposed search-based model merging approach that finds an optimal set of parameters to combine single-policy models and multi-policy guardrail modelsOn 7 public datasets and 4 new guardrail benchmarks we created, our efficient guardrail classifiers improve over the best performing SoTA publicly available LLMs and 3$^{\\text{rd}}$ party guardrail APIs in detecting unsafe and safe behaviors by an average \\textbf{29.92} (\\text{Aegis-LlamaGuard}) and \\textbf{21.62} (\\texttt{gpt-4o}) F1 respectively. Lastly, our guardrail synthetic data generation process leads to models that outperform training on real data using our custom defined policies that describe the guardrailing task.", "title_embedding_index": 13058, "title_abs_embedding_index": 13083}, {"title": "On the Power of Federated Learning for Online Sparse Linear Regression with Decentralized Data", "link_suffix": "/forum?id=QzxuHbWAD6", "link": "https://openreview.net/forum?id=QzxuHbWAD6", "pdf_link": "https://openreview.net/pdf?id=QzxuHbWAD6", "keywords": "Federated learning, online learning", "abstract": "In this paper, we study the necessity of federated learning (FL) for online linear regression with decentralized data. Previous work proved that FL is unnecessary for minimizing regret in full information setting, while we prove that it can be necessary if only limited attributes of each instance are observed. We call this problem online sparse linear regression with decentralized data (OSLR-DecD). We propose a federated algorithm for OSLR-DecD, and prove a lower bound on the regret of any noncooperative algorithm. In the case of $d=o(M)$, the upper bound on the regret of our algorithm is smaller than the lower bound, demonstrating the necessity of FL, in which $M$ is the number of clients and $d$ is the dimension of data. When $M=1$, we give the first lower bound on the regret and improve previous upper bounds. We invent three new techniques including an any-time federated online mirror descent with negative entropy regularization, a paradigm for client-server collaboration with privacy protection, and a reduction from online sparse linear regression to prediction with limited advice for establishing the lower bound on the regret, some of which might be of independent interest.", "title_embedding_index": 13059, "title_abs_embedding_index": 13084}, {"title": "Seeking Flat Minima with Mean Teacher on Semi- and Weakly-Supervised Domain Generalization for Object Detection", "link_suffix": "/forum?id=boz4LIvv3f", "link": "https://openreview.net/forum?id=boz4LIvv3f", "pdf_link": "https://openreview.net/pdf?id=boz4LIvv3f", "keywords": "object detection, domain generalization, semi-supervised learning, weakly-supervised learning", "abstract": "Object detectors do not work well when domains largely differ between training and testing data. To overcome this domain gap in object detection without requiring expensive annotations, we consider two problem settings: semi-supervised domain generalizable object detection (SS-DGOD) and weakly-supervised DGOD (WS-DGOD). In contrast to the conventional domain generalization for object detection that requires labeled data from multiple domains, SS-DGOD and WS-DGOD require labeled data only from one domain and unlabeled or weakly-labeled data from multiple domains for training. In this paper, we show that object detectors can be effectively trained on the two settings with the same Mean Teacher learning framework, where a student network is trained with pseudo-labels output from a teacher on the unlabeled or weakly-labeled data. We provide novel interpretations of why the Mean Teacher learning framework works well on the two settings in terms of the relationships between the generalization gap and flat minima in parameter space. On the basis of the interpretations, we also show that incorporating a simple regularization method into the Mean Teacher learning framework leads to flatter minima. The experimental results demonstrate that the regularization leads to flatter minima and boosts the performance of the detectors trained with the Mean Teacher learning framework on the two settings.", "title_embedding_index": 13060, "title_abs_embedding_index": 13085}, {"title": "Decoupled Offline to Online finetuning via Dynamics Model", "link_suffix": "/forum?id=Fxsd66d2wB", "link": "https://openreview.net/forum?id=Fxsd66d2wB", "pdf_link": "https://openreview.net/pdf?id=Fxsd66d2wB", "keywords": "Offline to Online Finetuning, Model-based RL, Decoupled Framework", "abstract": "Constrained by the sub-optimal dataset in offline reinforcement learning (RL), the offline trained agent should be online finetuned before deployment. Due to the conservative offline algorithms and unbalanced state distribution in offline dataset, offline to online finetuning faces severe distribution shift. This shift will disturb the policy improvement during online interaction, even a performance drop. A natural yet unexplored idea is whether policy improvement can be decoupled from distribution shift. In this work, we propose a decoupled offline to online finetuning framework using the dynamics model from model-based methods. During online interaction, only dynamics model is finetuned to overcome the distribution shift. Then the policy is finetuned in offline manner with finetuned dynamics and without further interaction. As a result, online stage only needs to deal with a simpler supervised dynamics learning, rather than the complex policy improvement with the interference from distribution shift. When finetuning the policy, we adopt the offline approach, which ensures the conservatism of the algorithm and fundamentally avoids the sudden performance crashes. We conduct extensive evaluation on the classical datasets of offline RL, demonstrating the effective elimination of distribution shift, stable and superior policy finetuning performance, and exceptional interaction efficiency within our decouple offline to online finetuning framework.", "title_embedding_index": 13061, "title_abs_embedding_index": 13086}, {"title": "Enhancing Graph Of Thought: Enhancing Prompts with LLM Rationales and Dynamic Temperature Control", "link_suffix": "/forum?id=l32IrJtpOP", "link": "https://openreview.net/forum?id=l32IrJtpOP", "pdf_link": "https://openreview.net/pdf?id=l32IrJtpOP", "keywords": "LLM, Enhancing Graph Of Thought, Complex Reasoning", "abstract": "We introduce Enhancing Graph Of Thoughts (EGOT), a method designed to enhance the performance of large language models (LLMs) on complex reasoning tasks. EGOT automates the process of generating accurate responses using given data and a base prompt. The process consists of several steps: It obtain an initial response from the answering node using the base prompt. Evaluation node evaluates the response and generates reasoning for it, utilizing the score's probabilities to enhance evaluation accuracy. The reasoning from both the answering node and the evaluation node is aggregated to identify the problem in the response. This aggregated reasoning is incorporated into the base prompt to obtain an enhanced response. These steps are organized in a graph architecture, where the final leaf nodes are merged to produce a final response. As the graph descends, the temperature is lowered using Cosine Annealing and scoring, to explore diverse responses with earlier nodes and to focus on precise responses with later nodes. The minimum temperature in Cosine Annealing is adjusted based on scoring, ensuring that nodes with low scores continue to explore diverse responses, while those with high scores confirm accurate responses. In sorting 256 elements using GPT-4o mini, EGOT performs 88.31% accuracy, respectively, GoT (Graph Of Thought) performance has 84.37%. In the frozen lake problem using GPT-4o, EGOT averages 0.55 jumps or falls into the hole, while TOT (Tree Of Thoughts) averages 0.89.", "title_embedding_index": 13062, "title_abs_embedding_index": 13087}, {"title": "Language Reconstruction with Brain Predictive Coding from fMRI Data", "link_suffix": "/forum?id=2hKDQ20zDa", "link": "https://openreview.net/forum?id=2hKDQ20zDa", "pdf_link": "https://openreview.net/pdf?id=2hKDQ20zDa", "keywords": "fMRI-to-text decoding, predictive coding theory", "abstract": "Many recent studies have shown that the perception of speech can be decoded from brain signals and subsequently reconstructed as continuous language. However, there is a lack of neurological basis for how the semantic information embedded within brain signals can be used more effectively to guide language reconstruction. Predictive coding theory suggests the human brain naturally engages in continuously predicting future words that span multiple timescales. This implies that the decoding of brain signals could potentially be associated with a predictable future. To explore the predictive coding theory within the context of language reconstruction, this paper proposes PredFT (FMRI-to-Text decoding with Predictive coding). PredFT consists of a main decoding network and a side network. The side network obtains brain predictive coding representation from related brain regions of interest (ROIs) with a self-attention module. This representation is then fused into the main decoding network for continuous language decoding. Experiments are conducted on two popular naturalistic language comprehension fMRI datasets. Results show that PredFT achieves current state-of-the-art decoding performance on several evaluation metrics. Additional observations on the selection of ROIs, along with the length and distance parameters in predictive coding further guide the adoption of predictive coding theory for language reconstruction.", "title_embedding_index": 13063, "title_abs_embedding_index": 13088}, {"title": "SplineGS: Learning Smooth Trajectories in Gaussian Splatting for Dynamic Scene Reconstruction", "link_suffix": "/forum?id=tMG6btjBfd", "link": "https://openreview.net/forum?id=tMG6btjBfd", "pdf_link": "https://openreview.net/pdf?id=tMG6btjBfd", "keywords": "4D Gaussian splatting, Dynamic scene reconstruction, Novel-view synthesis for dynamic scenes", "abstract": "Reconstructing complex scenes with deforming objects for novel view synthesis is a challenging task. Recent works have addressed this with 3D Gaussian Splatting, which effectively reconstructs static scenes with high quality in short training time, by adding specialized modules for the deformations of Gaussian blobs. However, designing an effective deformation module that incorporates appropriate spatiotemporal inductive biases still remains unresolved. To address this issue, we propose SplineGS in this paper, which utilizes non-uniform rational B-splines (NURBS), an extension of B-spline, to represent temporally smooth deformation. A set of representative trajectories are learned based on NURBS, and the individual trajectories of Gaussian blobs are represented as linear combinations of these trajectories for spatial smoothness. The weights of the combinations are trained based on a multi-resolution hash table and an MLP, with the positions of the Gaussian blobs as the keys. Thanks to this design, the proposed method does not need any regularizers for trajectories, which enables efficient training. Experiments demonstrate that the proposed method provides competitive performance over the existing methods with much shorter training time.", "title_embedding_index": 13064, "title_abs_embedding_index": 13089}, {"title": "Measuring the Impact of Equal Treatment as Blindness via Distributions of Explanations Disparity", "link_suffix": "/forum?id=ndU9EvrVBH", "link": "https://openreview.net/forum?id=ndU9EvrVBH", "pdf_link": "https://openreview.net/pdf?id=ndU9EvrVBH", "keywords": "Fairness, Audits, Explanations", "abstract": "Liberal political philosophy advocates for the policy of \\emph{equal treatment as blindness}, which seeks to achieve fairness by treating individuals without considering their protected characteristics directly. However, this policy has faced longstanding criticism for perpetuating existing inequalities. In machine learning, this policy can be translated into the concept of \\emph{fairness as unawareness}, and be measured using disparate treatment metrics such as Demographic Parity (a.k.a. Statistical Parity). Our analysis reveals that Demographic Parity does not faithfully measure whether individuals are being treated independently of the protected attribute by the model. We introduce the Explanation Disparity metric to measure fairness under \\emph{equal treatment as blindness} policy. Our metric evaluates the fairness of predictive models by analyzing the extent to which the protected attribute can be inferred from the distribution of explanation values, specifically using Shapley values. The proposed metric tests for statistical independence of the explanation distributions over populations with different protected characteristics. We show the theoretical properties of \"Explanation Disparity\" and devise an equal treatment inspector based on the AUC of a Classifier Two-Sample Test. We experiment with synthetic and natural data to demonstrate and compare the notion with related ones. We release \\texttt{explanationspace}, an open-source Python package with methods and tutorials", "title_embedding_index": 13065, "title_abs_embedding_index": 13090}, {"title": "Bundle Neural Network for message diffusion on graphs", "link_suffix": "/forum?id=scI9307PLG", "link": "https://openreview.net/forum?id=scI9307PLG", "pdf_link": "https://openreview.net/pdf?id=scI9307PLG", "keywords": "graph neural network, sheaf neural network, geometric deep learning, algebraic topology, vector bundles, expressivity", "abstract": "The dominant paradigm for learning on graphs is message passing. Despite being a strong inductive bias, the local message passing mechanism faces challenges such as over-smoothing, over-squashing, and limited expressivity. To address these issues, we introduce Bundle Neural Networks (BuNNs), a novel graph neural network architecture that operates viamessage diffusiononflat vector bundles\u2014 geometrically inspired structures that assign to each node a vector space and an orthogonal map. A BuNN layer evolves node features through a diffusion-type partial differential equation, where its discrete form acts as a special case of the recently introduced Sheaf Neural Network (SNN), effectively alleviating over-smoothing. The continuous nature of message diffusion enables BuNNs to operate at larger scales, reducing over-squashing. We establish the universality of BuNNs in approximating feature transformations on infinite families of graphs with injective positional encodings, marking the first positive uniform expressivity result of its kind. We support our claims with formal analysis and synthetic experiments. Empirically, BuNNs perform strongly on heterophilic and long-range tasks, which demonstrates their robustness on a diverse range of challenging real-world tasks.", "title_embedding_index": 13066, "title_abs_embedding_index": 13091}, {"title": "Learn out of the box: optimizing both diversity and performance in Offline Reinforcement Learning", "link_suffix": "/forum?id=rslH6DI73J", "link": "https://openreview.net/forum?id=rslH6DI73J", "pdf_link": "https://openreview.net/pdf?id=rslH6DI73J", "keywords": "Offline Reinforcement Learning, Diversity and Performance, Homogeneous Dataset", "abstract": "In offline reinforcement learning, most existing methods have focused primarily on optimizing performance, often neglecting the promotion of diverse behaviors. While some approaches generate diverse behaviors from well-constructed, heterogeneous datasets, their effectiveness is significantly reduced when applied to less diverse data. To address this, we introduce a novel intrinsic reward mechanism that encourages behavioral diversity, irrespective of the dataset's heterogeneity. By maximizing the mutual information between actions and policies under each state, our approach enables agents to learn a variety of behaviors, including those not explicitly represented in the data. Although performing out-of-distribution actions can lead to risky outcomes, we mitigate this risk by incorporating the ensemble-diversified actor-critic (EDAC) method to estimate Q-value uncertainty, preventing agents from adopting suboptimal behaviors. Through experiments using the D4RL benchmarks on MuJoCo tasks, we demonstrate that our method achieves behavioral diversity while maintaining performance across environments constructed from both heterogeneous and homogeneous datasets.", "title_embedding_index": 13067, "title_abs_embedding_index": 13092}, {"title": "Cheating Automatic LLM Benchmarks: Null Models Achieve High Win Rates", "link_suffix": "/forum?id=syThiTmWWm", "link": "https://openreview.net/forum?id=syThiTmWWm", "pdf_link": "https://openreview.net/pdf?id=syThiTmWWm", "keywords": "Large Language Models, Cheating, Automatic LLM Benchmarks", "abstract": "Automatic LLM benchmarks, such as AlpacaEval 2.0, Arena-Hard-Auto, and MT-Bench, have become popular for evaluating language models due to their cost-effectiveness and scalability compared to human evaluation. Achieving high win rates on these benchmarks can significantly boost the promotional impact of newly released language models. This promotional benefit may motivate tricks, such as manipulating model output length or style to game win rates, even though several mechanisms have been developed to control length and disentangle style to reduce gameability. Nonetheless, we show that even a\"null model\"that always outputs aconstantresponse (irrelevant to input instructions) can cheat automatic benchmarks and achieve top-ranked win rates: an $86.5\\%$ LC win rate on AlpacaEval 2.0; an $83.0$ score on Arena-Hard-Auto; and a $9.55$ score on MT-Bench. Moreover, the crafted cheating outputs aretransferablebecause we assume that the instructions of these benchmarks (e.g., $805$ samples of AlpacaEval 2.0) areprivateand cannot be accessed. While our experiments are primarily proof-of-concept, an adversary could use LLMs to generate more imperceptible cheating responses, unethically benefiting from high win rates and promotional impact. Our findings call for the development of anti-cheating mechanisms for reliable automatic benchmarks.", "title_embedding_index": 13068, "title_abs_embedding_index": 13093}, {"title": "LLMs Can Achieve High-quality Simultaneous Machine Translation as Efficiently as Offline", "link_suffix": "/forum?id=UqR2dFmfRB", "link": "https://openreview.net/forum?id=UqR2dFmfRB", "pdf_link": "https://openreview.net/pdf?id=UqR2dFmfRB", "keywords": "simultaneous machine translation, machine translation, Large Language Models, adaptive policy, supervised fine-tuning", "abstract": "When the complete source sentence is provided, Large Language Models (LLMs) perform excellently in offline machine translation even with a simple prompt\"Translate the following sentence from [src lang] into [tgt lang]:\". \nHowever, in many real scenarios, the source tokens arrive in a streaming manner and simultaneous machine translation (SiMT) is required, then theefficiencyandperformanceof decoder-only LLMs are significantly limited by their auto-regressive nature. \nTo enable LLMs to achieve high-quality SiMT as efficiently as offline translation, we propose a novel paradigm that includes constructing supervised fine-tuning (SFT) data for SiMT, along with new training and inference strategies. \nTo replicate the token input/output (I/O) stream in SiMT, the source and target tokens are rearranged into an interleaved sequence, separated by special tokens according to varying latency requirements. \nThis enables powerful LLMs to learn read and write operations adaptively, based on varying latency prompts, while still maintaining efficient auto-regressive decoding. \nExperimental results demonstrate that, even with limited SFT data, our approach achieves state-of-the-art performance across various simultaneous translation benchmarks and different evaluation metrics, and preserves the original capabilities of offline translation.\nMoreover, EAST generalizes well to document-level SiMT without requiring specific fine-tuning, even beyond the offline translation model.", "title_embedding_index": 13069, "title_abs_embedding_index": 13094}, {"title": "Enhancing lensless imaging via Explicit Learning of Model Mismatch", "link_suffix": "/forum?id=K8WaxpiSDw", "link": "https://openreview.net/forum?id=K8WaxpiSDw", "pdf_link": "https://openreview.net/pdf?id=K8WaxpiSDw", "keywords": "Lensless Imaging; Maximum a Posteriori;Model Mismatch Error;Image Reconstruction", "abstract": "Emerging lensless imaging techniques hold promise for miniaturized cameras, but their effectiveness is constrained by challenges like model mismatch from the point spread function (PSF), which undermines reconstruction methods dependent on accurate PSF modeling. To address this issue, we propose a joint Maximum a Posteriori (MAP) approach to simultaneously estimate model mismatch error (${\\rm M^{2}}$E) and reconstruct high-resolution images from lensless imaging measurements. Specifically, we propose an explicit latent space representation for ${\\rm M^{2}}$E to improve robustness against PSF inaccuracies. Additionally, we develop a multi-stage reconstruction network by unfolding the joint MAP estimator with a learned Laplacian Scale Mixture (LSM) prior and ${\\rm M^{2}}$E representation (${\\rm M^{2}}$ER) through end-to-end optimization. Extensive experiments show that our method surpasses current state-of-the-art methods.", "title_embedding_index": 13070, "title_abs_embedding_index": 13095}, {"title": "PersonaMath: Enhancing Math Reasoning through Persona-Driven Data Augmentation", "link_suffix": "/forum?id=VZzx0MPA85", "link": "https://openreview.net/forum?id=VZzx0MPA85", "pdf_link": "https://openreview.net/pdf?id=VZzx0MPA85", "keywords": "Large Language Model; Mathematical Reasoning", "abstract": "While closed-source Large Language Models (LLMs) demonstrate strong mathematical problem-solving abilities, open-source models continue to struggle with such tasks. To bridge this gap, we propose a data augmentation approach and introduce PersonaMathQA, a dataset derived from MATH and GSM8K, on which we train the PersonaMath models. Our approach consists of two stages: the first stage is learning from Persona Diversification, and the second stage is learning from Reflection. In the first stage, we regenerate detailed chain-of-thought (CoT) solutions as instructions using a closed-source LLM and introduce a novel persona-driven data augmentation technique to enhance the dataset's quantity and diversity. In the second stage, we incorporate reflection to fully leverage more challenging and valuable questions. Evaluation of our PersonaMath models on MATH and GSM8K reveals that the PersonaMath-7B model (based on LLaMA-2-7B) achieves an accuracy of 24.2% on MATH and 68.7% on GSM8K, surpassing all baseline methods and achieving state-of-the-art performance. Notably, our dataset contains only 70.3K data points\u2014merely 17.8% of MetaMathQA and 27% of MathInstruct\u2014yet our model outperforms these baselines, demonstrating the high quality and diversity of our dataset, which enables more efficient model training. We open-source the PersonaMathQA dataset, PersonaMath models, and our code for public usage.", "title_embedding_index": 13071, "title_abs_embedding_index": 13096}, {"title": "Semantic-aligned Query Synthesis for Active Learning", "link_suffix": "/forum?id=CZvbXXgjrn", "link": "https://openreview.net/forum?id=CZvbXXgjrn", "pdf_link": "https://openreview.net/pdf?id=CZvbXXgjrn", "keywords": "Active learning, Data synthesis, Machine learning, Influence function", "abstract": "Active learning (AL) reduces data annotation costs by querying labels from human annotators for the most informative unlabeled data points during model training. Existing AL methods generally assume the availability of a large amount of unlabeled samples for query selection. However, collecting raw data in practice can be expensive, even without considering the cost of labeling. Membership query synthesis circumvents the need for an unlabeled data pool by directly generating informative queries from the input space. Nevertheless, existing approaches often generate instances lacking semantic meaning, thereby increasing the difficulty of labeling. In this paper, we propose the Generative Membership Query Descriptor (GenMQD) method for AL to mitigate the risk of generating unrecognizable instances. The key idea is to generate textual descriptions of the desired data, instead of the data samples themselves. Then a pre-trained multi-modal alignment model (e.g., CLIP) can be leveraged to transform these features into natural language texts for data gathering purposes. Extensive experiments on image classification benchmark datasets against query synthesis state-of-the-art methods demonstrate that, on average, GenMQD can improve model accuracy by 2.43% when gathering and labeling 500 examples. A large-scale user study verifies that human oracles prefer GenMQD generated queries over generated image-based queries.", "title_embedding_index": 13072, "title_abs_embedding_index": 13097}, {"title": "On the Resilience of Multi-Agent Systems with Malicious Agents", "link_suffix": "/forum?id=Bp2axGAs18", "link": "https://openreview.net/forum?id=Bp2axGAs18", "pdf_link": "https://openreview.net/pdf?id=Bp2axGAs18", "keywords": "Multi-Agent Systems, Large Language Models, Resilience", "abstract": "Multi-agent systems, powered by large language models, have shown great abilities across various tasks due to the collaboration of expert agents, each focusing on a specific domain. However, when agents are deployed separately, there is a risk that malicious users may introduce malicious agents who generate incorrect or irrelevant results that are too stealthy to be identified by other non-specialized agents. Therefore, this paper investigates two essential questions: (1) What is the resilience of various multi-agent system structures (e.g., A$\\rightarrow$B$\\rightarrow$C, A$\\leftrightarrow$B$\\leftrightarrow$C) under malicious agents, on different downstream tasks?\n(2) How can we increase system resilience to defend against malicious agents? To simulate malicious agents, we devise two methods, AutoTransform and AutoInject, to transform any agent into a malicious one while preserving its functional integrity. We run comprehensive experiments on four downstream multi-agent systems tasks, namely code generation, math problems, translation, and text evaluation. Results suggest that the \"hierarchical\" multi-agent structure, i.e., A$\\rightarrow$(B$\\leftrightarrow$C), exhibits superior resilience with the lowest performance drop of $23.6%$, compared to $46.4%$ and $49.8%$ of other two structures. Additionally, we show the promise of improving multi-agent system resilience by demonstrating that two defense methods, introducing a mechanism for each agent to challenge others' outputs, or an additional agent to review and correct messages, can enhance system resilience. Our code and data are available in the supplementary materials and will be made publicly available upon publication.", "title_embedding_index": 13073, "title_abs_embedding_index": 13098}, {"title": "Addressing Representation Collapse in Vector Quantized Models with One Linear Layer", "link_suffix": "/forum?id=SqUiGfJ1So", "link": "https://openreview.net/forum?id=SqUiGfJ1So", "pdf_link": "https://openreview.net/pdf?id=SqUiGfJ1So", "keywords": "vector quantization, representation collapse", "abstract": "Vector Quantization (VQ) is a widely used method for converting continuous representations into discrete codes, which has become fundamental in unsupervised representation learning and latent generative models. However, VQ models are often hindered by the problem of representation collapse in the latent space, which leads to low codebook utilization and limits the scalability of the codebook for large-scale training. Existing methods designed to mitigate representation collapse typically reduce the dimensionality of latent space at the expense of model capacity, which do not fully resolve the core issue. In this study, we conduct a theoretical analysis of representation collapse in VQ models and identify its primary cause as the disjoint optimization of the codebook, where only a small subset of code vectors are updated through gradient descent. To address this issue, we propose \\textbf{SimVQ}, a novel method which reparameterizes the code vectors through a linear transformation layer based on a learnable latent basis. This transformation optimizes the \\textit{entire linear space} spanned by the codebook, rather than merely updating \\textit{the code vector} selected by the nearest-neighbor search in vanilla VQ models. Although it is commonly understood that the multiplication of two linear matrices is equivalent to applying a single linear layer, our approach works surprisingly well in resolving the collapse issue in VQ models with just one linear layer. We validate the efficacy of SimVQ through extensive experiments across various modalities, including image and audio data with different model architectures. The results show that SimVQ not only effectively addresses the problem of representation collapse but also proves highly adaptable and easy to implement, suggesting its broad applicability in diverse machine learning contexts.", "title_embedding_index": 13074, "title_abs_embedding_index": 13099}]