[
    {
        "title": "MeshAnything: Artist-Created Mesh Generation with Autoregressive Transformers",
        "link_suffix": "/forum?id=KGZAs8VcOM",
        "link": "https://openreview.net/forum?id=KGZAs8VcOM",
        "pdf_link": "https://openreview.net/pdf?id=KGZAs8VcOM",
        "keywords": "3D Generation, Transformers, Sequence Learning",
        "abstract": "Recently, 3D assets created via reconstruction and generation have matched the quality of manually crafted assets, highlighting their potential for replacement. However, this potential is largely unrealized because these assets always need to be converted to meshes for 3D industry applications, and the meshes produced by current mesh extraction methods are significantly inferior to Artist-Created Meshes (AMs), i.e., meshes created by human artists.Specifically, current mesh extraction methods rely on dense faces and ignore geometric features, leading to inefficiencies, complicated post-processing, and lower representation quality.\nTo address these issues, we introduce MeshAnything, a model that treats mesh extraction as a generation problem, producing AMs aligned with specified shapes.By converting 3D assets in any 3D representation into AMs, MeshAnything can be integrated with various 3D asset production methods, thereby enhancing their application across the 3D industry.\nThe architecture of MeshAnything comprises a VQ-VAE and a shape-conditioned decoder-only transformer. We first learn a mesh vocabulary using the VQ-VAE, then train the shape-conditioned decoder-only transformer on this vocabulary for shape-conditioned autoregressive mesh generation. Our extensive experiments show that our method generates AMs with hundreds of times fewer faces, significantly improving storage, rendering, and simulation efficiencies, while achieving precision comparable to previous methods."
    },
    {
        "title": "AnyPrefer: An Automatic Framework for Preference Data Synthesis",
        "link_suffix": "/forum?id=WpZyPk79Fu",
        "link": "https://openreview.net/forum?id=WpZyPk79Fu",
        "pdf_link": "https://openreview.net/pdf?id=WpZyPk79Fu",
        "keywords": "synthetic preference data generation; preference fine-tuning",
        "abstract": "High-quality preference data is essential for aligning foundation models with human values through preference learning. However, manual annotation of such data is often time-consuming and costly. Recent methods adopt a self-rewarding approach, where the target model generates and annotates its own preference data, but this can lead to inaccuracies due to the reward model sharing weights with the target model, amplifying inherent biases. To address these issues, we propose Anyprefer, a framework designed to synthesize high-quality preference data for the target model. Anyprefer frames the data synthesis process as a cooperative two-player Markov Game, where the target model and a judge model collaborate. Here, a series of external tools are introduced to assist the judge model in accurately rewarding the target model\u2019s responses, mitigating biases in the process. We also introduce a feedback mechanism to optimize prompts for both models, enhancing collaboration and improving data quality. The synthesized data is compiled into a new preference dataset, Anyprefer-V1, consisting of 58K high-quality preference pairs. Extensive experiments show that Anyprefer significantly improves model alignment across four applications, covering 21 datasets, achieving average improvements of 18.55 in five natural language generation datasets, 3.66 in nine vision-language understanding datasets, 30.05 in three medical image analysis datasets, and 14.50 in four visuo-motor control tasks."
    },
    {
        "title": "Towards False-claim-resistant Model Ownership Verification via Targeted Fingerprint",
        "link_suffix": "/forum?id=BGpCPmf1AO",
        "link": "https://openreview.net/forum?id=BGpCPmf1AO",
        "pdf_link": "https://openreview.net/pdf?id=BGpCPmf1AO",
        "keywords": "Model Fingerpinting, Ownership Verification, Model Copyright Protection, Trustworthy ML",
        "abstract": "The utilization of open-source pre-trained models has become a prevalent practice, but unauthorized reuse of pre-trained models may pose a threat to the intellectual property rights (IPR) of the model developers. Model fingerprinting, which does not necessitate modifying the model to verify whether a suspicious model is reused from the source model, stands as a promising approach to safeguarding the IPR. In this paper, we revisit existing model fingerprinting methods and demonstrate that they are vulnerable to false claim attacks where adversaries falsely assert ownership of any third-party model. We reveal that this vulnerability mostly stems from their untargeted nature, where they generally compare the outputs of given samples on different models instead of the similarities to specific references. Motivated by these findings, we propose a targeted fingerprinting paradigm ($i.e.$, FIT-Print) to counteract false claim attacks. Specifically, FIT-Print transforms the fingerprint into a targeted signature via optimization. Building on the principles of FIT-Print, we develop bit-wise and list-wise black-box model fingerprinting methods, $i.e.$, FIT-ModelDiff and FIT-LIME, which exploit the distance between model outputs and the feature attribution of specific samples as the fingerprint, respectively. Extensive experiments on benchmark models and datasets verify the effectiveness, conferrability, and resistance to false claim attacks of our FIT-Print."
    },
    {
        "title": "Segmentation-Enhanced Depth Estimation Using Camera Model Based Self-supervised Contrastive Learning",
        "link_suffix": "/forum?id=OOywAeccTZ",
        "link": "https://openreview.net/forum?id=OOywAeccTZ",
        "pdf_link": "https://openreview.net/pdf?id=OOywAeccTZ",
        "keywords": "Contrastive Learning; Depth Estimation",
        "abstract": "Depth estimation is a key topic in the field of computer vision. Self-supervised monocular depth estimation offers a powerful method to extract 3D scene information from a single camera image, allowing training on arbitrary image sequences without the need for depth labels. However, monocular unsupervised depth estimation still cannot address the issue of scale and often requires ground truth for calibration.\nIn the deep learning era, existing methods primarily rely on relationships between images to train unsupervised neural networks, often overlooking the fundamental information provided by the camera itself. In fact, the intrinsic and extrinsic parameters of the camera can be used to compute depth information for the ground and its related areas based on physical principles. This information can offer rich supervisory signals at no additional cost. Additionally, by assuming that objects like people, cars, and buildings share the same depth as the corresponding ground, the physical depth of the entire scene can be inferred, and gaps in the depth map can be filled.\nSince some areas may have depth estimation errors, to make full use of these regions, we introduce a contrastive learning self-supervised framework. This framework consists of two networks with the same structure: the Anchor network and the Target network. While calculating depth, the network also outputs semantic segmentation results to assist in computing the physics depth, which is then used as the label for the model. Semantic segmentation can identify dynamic objects, reducing photometric reprojection errors caused by moving objects. The predictions from the Anchor network are used as pseudo-labels for training the Target network. Reliability is determined by entropy, dividing the predicted depth into positive and negative samples to maximize the use of physics depth information."
    },
    {
        "title": "Personalized Representation from Personalized Generation",
        "link_suffix": "/forum?id=jw7P4MHLWw",
        "link": "https://openreview.net/forum?id=jw7P4MHLWw",
        "pdf_link": "https://openreview.net/pdf?id=jw7P4MHLWw",
        "keywords": "Synthetic data, personalization, diffusion models, data augmentation, representation learning",
        "abstract": "Modern vision models excel at general purpose downstream tasks. It is unclear, however, how they may be used for personalized vision tasks, which are both fine-grained and data-scarce. Recent work has successfully applied synthetic data to general-purpose representation learning, while advances in T2I diffusion models have enabled the generation of personalized images from just a few real examples. Here, we explore a potential connection between these ideas, and formalize the challenge of using personalized synthetic data to learn personalized representations, which encode knowledge about an object of interest and may be flexibly applied to any downstream task relating to the target object. We introduce an evaluation suite for this challenge, including reformulations of two existing datasets and a novel dataset explicitly constructed for this purpose, and propose a contrastive learning approach that makes creative use of image generators. We show that our method improves personalized representation learning for diverse downstream tasks, from recognition to segmentation, and analyze characteristics of image generation approaches that are key to this gain."
    },
    {
        "title": "Exploring Prosocial Irrationality for LLM Agents: A Social Cognition View",
        "link_suffix": "/forum?id=u8VOQVzduP",
        "link": "https://openreview.net/forum?id=u8VOQVzduP",
        "pdf_link": "https://openreview.net/pdf?id=u8VOQVzduP",
        "keywords": "LLM for Social Science, Prosocial irrationality of LLM Agents, Cognitive AI framework",
        "abstract": "Large language models (LLMs) have been shown to face hallucination issues due to the data they trained on often containing human bias; whether this is reflected in the decision-making process of LLM agents remains under-explored. As LLM Agents are increasingly employed in intricate social environments, a pressing and natural question emerges: Can we utilize LLM Agents' systematic hallucinations to mirror human cognitive biases, thus exhibiting irrational social intelligence? In this paper, we probe the irrational behavior among contemporary LLM agents by melding practical social science experiments with theoretical insights. Specifically, we propose CogMir, an open-ended Multi-LLM Agents framework that utilizes hallucination properties to assess and enhance LLM Agents\u2019 social intelligence through cognitive biases. Experimental results on CogMir subsets show that LLM Agents and humans exhibit high consistency in irrational and prosocial decision-making under uncertain conditions, underscoring the prosociality of LLM Agents as social entities and highlighting the significance of hallucination properties. Additionally, CogMir framework demonstrates its potential as a valuable platform for encouraging more research into the social intelligence of LLM Agents."
    },
    {
        "title": "Data Value Estimation on Private Gradients",
        "link_suffix": "/forum?id=mkXi7O0fun",
        "link": "https://openreview.net/forum?id=mkXi7O0fun",
        "pdf_link": "https://openreview.net/pdf?id=mkXi7O0fun",
        "keywords": "data valuation, estimation uncertainty, privacy-aware",
        "abstract": "For gradient-based machine learning (ML) methods commonly adopted in practice such as stochastic gradient descent, the de facto differential privacy (DP) technique is perturbing the gradients with random Gaussian noise. Data valuation attributes the ML performance to the training data and is widely used in privacy-aware applications that require enforcing DP such as data pricing, collaborative ML, and federated learning (FL). Can existing data valuation methods still be used when DP is enforced via gradient perturbations? We show that the answer is no with the default approach of injecting i.i.d. random noise to the gradients because the estimation uncertainty of the data value estimation paradoxically linearly scales with more estimation budget, producing estimates almost like random guesses. To address this issue, we propose to instead inject carefully correlated noise to provably remove the linear scaling of estimation uncertainty w.r.t. the budget. We also empirically demonstrate that our method gives better data value estimates on various ML tasks and is applicable to use cases including dataset valuation and FL."
    },
    {
        "title": "DeCo: Decoupling Token Compression from Semantic Abstraction in Multimodal Large Language Models",
        "link_suffix": "/forum?id=Rx5LhMMr0c",
        "link": "https://openreview.net/forum?id=Rx5LhMMr0c",
        "pdf_link": "https://openreview.net/pdf?id=Rx5LhMMr0c",
        "keywords": "Multimodal Large Language Model; Projector; Token Compression;",
        "abstract": "The visual projector, which bridges the vision and language modalities and facilitates cross-modal alignment, serves as a crucial component in Multimodal Large Language Models (MLLMs).\nHowever, measuring the effectiveness of projectors in vision-language alignment remains under-explored, with current evaluations relying primarily on the performance of MLLMs on downstream tasks.\nMotivated by this gap, this study conducts an in-depth examination of the projector module by analyzing the vision-language semantic flow within MLLMs. \nOur findings reveal that compressive projectors (e.g., QFormer) reduce the number of visual tokens by abstracting visual patches into a limited set of semantic concepts, such as objects or attributes, leading to a deficiency we term ``double abstraction'' in MLLMs. This phenomenon involves i) an initial visual semantic abstraction by the projector in the vision modality, which refers to pre-defined query tokens, and ii) a secondary extraction by the LLM  in the language modality based on text instructions.\nThe double abstraction is inefficient during training and leads to cumulative deficiencies in visual semantics. To address this issue, we propose the key insight of ''`\\textbf{De}couple Token \\textbf{Co}mpression from Semantic Abstraction \\textbf{(\\model)}'', where projectors compress visual tokens at the patch level non-semantically, while allowing the LLM to fully manage semantic understanding and abstraction.\nConsequently, we employ a simple compressor, i.e., 2D Adaptive Pooling, to downsample visual patches in a parameter-free manner. \nEmpirical evaluations demonstrate that 2D Adaptive Pooling outperforms traditional compressive projectors in both performance and efficiency, achieving gains of 0.9%, 7.1%, and 2.9% across the MLLM Benchmarks, Visual Localization, and Open-ended VQA tasks, respectively, while utilizing fewer trainable parameters and achieving faster convergence.\nFurthermore, it preserves vision spatial locality and exhibits robustness across various MLLM configurations, including different vision backbones, image resolutions, and LLMs."
    },
    {
        "title": "Equivariant Polynomial Functional Networks",
        "link_suffix": "/forum?id=qto91DryES",
        "link": "https://openreview.net/forum?id=qto91DryES",
        "pdf_link": "https://openreview.net/pdf?id=qto91DryES",
        "keywords": "neural functional network, equivariant model, polynomial layer, monomial matrix group",
        "abstract": "Neural Functional Networks (NFNs) have gained increasing interest due to their wide range of applications, including extracting information from implicit representations of data, editing network weights, and evaluating policies. A key design principle of NFNs is their adherence to the permutation and scaling symmetries inherent in the connectionist structure of the input neural networks.  Recent NFNs have been proposed with permutation and scaling equivariance based on either graph-based message-passing mechanisms or parameter-sharing mechanisms. However, graph-based equivariant NFNs suffer from high memory consumption and long running times. On the other hand, parameter-sharing-based NFNs built upon equivariant linear layers exhibit lower memory consumption and faster running time, yet their expressivity is limited due to the large size of the symmetric group of the input neural networks. The challenge of designing a permutation and scaling equivariant NFN that maintains low memory consumption and running time while preserving expressivity remains unresolved. In this paper, we propose a novel solution with the development of MAGEP-NFN (Monomial mAtrixGroupEquivariantPolynomialNFN). Our approach follows the parameter-sharing mechanism but differs from previous works by constructing a nonlinear equivariant layer represented as a polynomial in the input weights. This polynomial formulation enables us to incorporate additional relationships between weights from different input hidden layers, enhancing the model's expressivity while keeping memory consumption and running time low, thereby addressing the aforementioned challenge. We provide empirical evidence demonstrating that MAGEP-NFN achieves competitive performance and efficiency compared to existing baselines."
    },
    {
        "title": "Inference-Time Alignment of Diffusion Models with Direct Noise Optimization",
        "link_suffix": "/forum?id=x1uv2gdjKV",
        "link": "https://openreview.net/forum?id=x1uv2gdjKV",
        "pdf_link": "https://openreview.net/pdf?id=x1uv2gdjKV",
        "keywords": "Diffusion Models, Inference-Time Alignment, Optimization, RLHF",
        "abstract": "In this work, we focus on the alignment problem of diffusion models with a continuous reward function, which represents specific objectives for downstream tasks, such as increasing darkness or improving the aesthetics of images. The central goal of the alignment problem is to adjust the distribution learned by diffusion models such that the generated samples maximize the target reward function. We propose a novel alignment approach, named Direct Noise Optimization (DNO), that optimizes the injected noise during the sampling process of diffusion models. By design, DNO operates at inference-time, and thus is  tuning-free and  prompt-agnostic, with the alignment occurring in an online fashion during generation. We rigorously study the theoretical properties of DNO and also propose variants to deal with non-differentiable reward functions. Furthermore, we identify that naive implementation of DNO occasionally suffers from the  out-of-distribution reward hacking problem, where optimized samples have high rewards but are no longer in the support of the pretrained distribution. To remedy this issue, we leverage classical high-dimensional statistics theory to an effective probability regularization technique. We conduct extensive experiments on several important reward functions and demonstrate that the proposed DNO approach can achieve state-of-the-art reward scores within a reasonable time budget for generation."
    },
    {
        "title": "BlockLLM: Memory-Efficient Adaptation of LLMs by Selecting and Optimizing the Right Coordinate Blocks",
        "link_suffix": "/forum?id=L0pXYjtfE3",
        "link": "https://openreview.net/forum?id=L0pXYjtfE3",
        "pdf_link": "https://openreview.net/pdf?id=L0pXYjtfE3",
        "keywords": "Memory efficient training, LLM training",
        "abstract": "Training large language models (LLMs) for pretraining or adapting to new tasks and domains has become increasingly critical as their applications expand. However, as model and data sizes grow, the training process presents significant memory challenges, often requiring a prohibitive amount of GPU memory that may not be readily available. Existing methods such as low-rank adaptation (LoRA) add trainable low-rank matrix factorizations, altering the training dynamics and limiting the model's parameter search to a low-rank subspace. GaLore, a more recent method, employs Gradient Low-Rank Projection to reduce the memory footprint, in the full parameter training setting. However GaLore can only be applied to a subset of the LLM layers that satisfy the ``reversibility'' property, thus limiting their applicability. In response to these challenges, we introduce BlockLLM, an approach inspired by block coordinate descent. Our method carefully selects and updates a very small subset of the trainable parameters without altering any part of its architecture and training procedure. BlockLLM achieves state-of-the-art performance in both finetuning and pretraining tasks, while reducing the memory footprint of the underlying optimization process. Our experiments demonstrate that  BlockLLM achieves superior performance on finetuning both large and small models. On pretraining a Llama model on C$4$ dataset, BlockLLM is able to train with significantly less memory than the state-of-the-art, while still maintaining competitive performance."
    },
    {
        "title": "Exploiting Open-World Data for Adaptive Continual Learning",
        "link_suffix": "/forum?id=WhPLUfThB4",
        "link": "https://openreview.net/forum?id=WhPLUfThB4",
        "pdf_link": "https://openreview.net/pdf?id=WhPLUfThB4",
        "keywords": "Continual Learning, Semi-supervised Learning, Open-world",
        "abstract": "Continual learning (CL), which involves learning from sequential tasks without forgetting, is mainly explored in supervised learning settings where all data are labeled. However, high-quality labeled data may not be readily available at a large scale due to high labeling costs, making the application of existing CL methods in real-world scenarios challenging. In this paper, we study a more practical facet of CL: open-world continual learning, where the training data comes from the open-world dataset and is partially labeled and non-i.i.d. Building on the insight that task shifts in CL can be viewed as distribution transitions from known classes to novel classes, we propose OpenACL, a method that explicitly leverages novel classes in unlabeled data to enhance continual learning.  Specifically, OpenACL considers novel classes within open-world data as potential classes for upcoming tasks and mines the underlying pattern from them to empower the model's adaptability to upcoming tasks. Furthermore, learning from extensive unlabeled data also helps to tackle the issue of catastrophic forgetting. Extensive experiments validate the effectiveness of OpenACL and show the benefit of learning from open-world data."
    },
    {
        "title": "On the Sequence Evaluation based on Stochastic Processes",
        "link_suffix": "/forum?id=A53m6yce21",
        "link": "https://openreview.net/forum?id=A53m6yce21",
        "pdf_link": "https://openreview.net/pdf?id=A53m6yce21",
        "keywords": "stochastic representation, stochastic process, Brownian bridge, text coherence, human-AI differentiation",
        "abstract": "Generative models have gained significant prominence in Natural Language Processing (NLP), especially in tackling the complex task of modeling and evaluating long text sequences. This task is crucial for advancing various downstream applications, such as text generation and machine translation. Recent methods that utilize stochastic processes to capture the intrinsic dynamics of sequences have shown superior performance in generative modeling. However, the accurate encoding of both temporal and structural dependencies from text datasets, as well as leveraging this encoded information for sequence evaluation, remains an open area of research. In this paper, we propose a novel approach to learn the stochastic dynamics of long text sequences, utilizing a negative log-likelihood-based encoder that outperforms contrastive learning methods. We also introduce a likelihood-based evaluation metric for long-text assessment, which measures sequence coherence and can be applied to downstream tasks such as Human-AI discrimination. Our encoder preserves sequence coherence effectively and performs robustly on out-of-domain datasets. Additionally, the proposed evaluation metric captures both temporal and structural information comprehensively. Theoretical analysis demonstrates the superiority of our metric in sequence evaluation, and experimental results highlight its flexibility and exceptional performance across a variety of tasks, showcasing its utility in diverse NLP applications."
    },
    {
        "title": "SVG: 3D Stereoscopic Video Generation via Denoising Frame Matrix",
        "link_suffix": "/forum?id=sx2jXZuhIx",
        "link": "https://openreview.net/forum?id=sx2jXZuhIx",
        "pdf_link": "https://openreview.net/pdf?id=sx2jXZuhIx",
        "keywords": "video generation, stereoscopic video, inpainting, diffusion model",
        "abstract": "Video generation models have demonstrated great capability of producing impressive monocular videos, however, the generation of 3D stereoscopic video remains under-explored. We propose a pose-free and training-free approach for generating 3D stereoscopic videos using an off-the-shelf monocular video generation model. Our method warps a generated monocular video into camera views on stereoscopic baseline using estimated video depth, and employs a novel frame matrix video inpainting framework. The framework leverages the video generation model to inpaint frames observed from different timestamps and views. This effective approach generates consistent and semantically coherent stereoscopic videos without scene optimization or model fine-tuning. Moreover, we develop a disocclusion boundary re-injection scheme that further improves the quality of video inpainting by alleviating the negative effects propagated from disoccluded areas in the latent space. We validate the efficacy of our proposed method by conducting experiments on videos from various generative models, including Sora [4], Lumiere [2], WALT [8], and Zeroscope [12]. The experiments demonstrate that our method has a significant improvement over previous methods. Code will be released."
    },
    {
        "title": "Convex Distillation: Efficient Compression of Deep Networks via Convex Optimization",
        "link_suffix": "/forum?id=XCugWIuHR8",
        "link": "https://openreview.net/forum?id=XCugWIuHR8",
        "pdf_link": "https://openreview.net/pdf?id=XCugWIuHR8",
        "keywords": "Convex Neural Networks, Convex/Non-Convex Optimization, Knowledge Distillation, Model Compression, Label-Free Training, Classification",
        "abstract": "Deploying large and complex deep neural networks on resource-constrained edge devices poses significant challenges due to their computational demands and the complexities of non-convex optimization. Traditional compression methods such as distillation and pruning often retain non-convexity that complicates fine-tuning in real-time on such devices. Moreover, these methods often necessitate extensive end-to-end network fine-tuning after compression to preserve model performance, which is not only time-consuming but also requires fully annotated datasets, thus potentially negating the benefits of efficient network compression. In this paper, we introduce a novel distillation technique that efficiently compresses the model via convex optimization -- eliminating intermediate non-convex activation functions and using only intermediate activations from the original model. Our approach enables distillation in a label-free data setting and achieves performance comparable to the original model without requiring any post-compression fine-tuning.  We demonstrate the effectiveness of our method for image classification models on multiple standard datasets, and further show that in the data limited regime, our method can outperform standard non-convex distillation approaches. Our method promises significant advantages for deploying high-efficiency, low-footprint models on edge devices, making it a practical choice for real-world applications. We show that convex neural networks, when provided with rich feature representations from a large pre-trained non-convex model, can achieve performance comparable to their non-convex counterparts, opening up avenues for future research at the intersection of convex optimization and deep learning."
    },
    {
        "title": "IBCL: Zero-shot Model Generation under Stability-Plasticity Trade-offs",
        "link_suffix": "/forum?id=8shi3NhgJp",
        "link": "https://openreview.net/forum?id=8shi3NhgJp",
        "pdf_link": "https://openreview.net/pdf?id=8shi3NhgJp",
        "keywords": "continual learning, Bayesian learning, imprecise probability",
        "abstract": "Algorithms that balance the stability-plasticity trade-off are well-studied in the continual learning literature. However, only a few of them focus on obtaining models for specified trade-off preferences. When solving the problem of continual learning under specific trade-offs (CLuST), state-of-the-art techniques leverage rehearsal-based learning, which requires retraining when a model corresponding to a new trade-off preference is requested. This is inefficient since there exist infinitely many different trade-offs, and a large number of models may be requested. As a response, we propose Imprecise Bayesian Continual Learning (IBCL), an algorithm that tackles CLuST efficiently. IBCL replaces retraining with constant-time convex combination. Given a new task, IBCL (1) updates the knowledge base in the form of a convex hull of model parameter distributions and (2) generates one Pareto-optimal model per given trade-off via convex combination without any additional training. That is, obtaining models corresponding to specified trade-offs via IBCL is zero-shot. Experiments whose baselines are current CLuST algorithms show that IBCL improves by at most 45% on average per task accuracy and by 43% on peak per task accuracy, while maintaining a near-zero to positive backward transfer. Moreover, its training overhead, measured by number of batch updates, remains constant at every task, regardless of the number of preferences requested. Details at: \\url{https://github.com/ibcl-anon/ibcl}."
    },
    {
        "title": "NeuralMark: Advancing White-Box Neural Network Watermarking",
        "link_suffix": "/forum?id=gjFgBfbP2C",
        "link": "https://openreview.net/forum?id=gjFgBfbP2C",
        "pdf_link": "https://openreview.net/pdf?id=gjFgBfbP2C",
        "keywords": "Neural network; White-box watermarking; Hash mapping; Watermark filtering; Average pooling",
        "abstract": "As valuable digital assets, deep neural networks require ownership protection, making neural network watermarking (NNW) a promising solution. In this paper, we propose aNeuralMarkmethod to advance white-box NNW, which can be seamlessly integrated into various network architectures. NeuralMark first establishes a hash mapping between the secret key and the watermark, enabling resistance to forging attacks. The watermark then functions as a filter to select model parameters for embedding, providing resilience against overwriting attacks. Furthermore, NeuralMark utilizes average pooling to defend against fine-tuning and pruning attacks. Theoretically, we analyze its security boundary. Empirically, we verify its effectiveness across 14 distinct Convolutional and Transformer architectures, covering five image classification tasks and one text generation task."
    },
    {
        "title": "Adaptive Data Optimization: Dynamic Sample Selection with Scaling Laws",
        "link_suffix": "/forum?id=aqok1UX7Z1",
        "link": "https://openreview.net/forum?id=aqok1UX7Z1",
        "pdf_link": "https://openreview.net/pdf?id=aqok1UX7Z1",
        "keywords": "Data selection, Pre-training, Curriculum learning, Language Models, Scaling laws",
        "abstract": "The composition of pretraining data is a key determinant of foundation models' performance, but there is no standard guideline for allocating a limited computational budget across different data sources. Most current approaches either rely on extensive experiments with smaller models or dynamic data adjustments that also require proxy models, both of which significantly increase the workflow complexity and computational overhead. In this paper, we introduce Adaptive Data Optimization (ADO), an algorithm that optimizes data distributions in an online fashion, concurrent with model training. Unlike existing techniques, ADO does not require external knowledge, proxy models, or modifications to the model update. Instead, ADO uses per-domain scaling laws to estimate the learning potential of each domain during training and adjusts the data mixture accordingly, making it more scalable and easier to integrate. Experiments demonstrate that ADO can achieve comparable or better performance than prior methods while maintaining computational efficiency across different computation scales, offering a practical solution for dynamically adjusting data distribution without sacrificing flexibility or increasing costs. Beyond its practical benefits, ADO also provides a new perspective on data collection strategies via scaling laws."
    },
    {
        "title": "A near linear query lower bound for submodular maximization",
        "link_suffix": "/forum?id=7Z5LtCQlV0",
        "link": "https://openreview.net/forum?id=7Z5LtCQlV0",
        "pdf_link": "https://openreview.net/pdf?id=7Z5LtCQlV0",
        "keywords": "Submodular maximization, sublinear algorithm, query complexity, communication complexity",
        "abstract": "We revisit the problem of selecting $k$-out-of-$n$ elements with the goal of optimizing an objective function, and ask whether it can be solved approximately with sublinear query complexity.For objective functions that are monotone submodular, [Li, Feldman, Kazemi, Karbasi, NeurIPS'22] gave an $\\Omega(n/k)$ query lower bound for approximating to within any constant factor. We strengthen their lower bound to a nearly tight  $\\tilde{\\Omega}(n)$. This lower bound holds even for estimating the value of the optimal subset.When the objective function is additive (i.e.~$f(S) = \\sum_{i \\in S} w_i$ for unknown $w_i$s), we prove that finding an approximately  optimal subset still requires near-linear query complexity, but we can estimate the  value of the optimal subset in $\\tilde{O}(n/k)$ time, and that this is tight up to polylog factors."
    },
    {
        "title": "Perturbation-Restrained Sequential Model Editing",
        "link_suffix": "/forum?id=bfI8cp8qmk",
        "link": "https://openreview.net/forum?id=bfI8cp8qmk",
        "pdf_link": "https://openreview.net/pdf?id=bfI8cp8qmk",
        "keywords": "Sequential Model Editing, Matrix Perturbation, General Abilities, Large Language Models",
        "abstract": "Model editing is an emerging field that focuses on updating the knowledge embedded within large language models (LLMs) without extensive retraining. However, current model editing methods significantly compromise the general abilities of LLMs as the number of edits increases, and this trade-off poses a substantial challenge to the continual learning of LLMs. In this paper, we first theoretically analyze that the factor affecting the general abilities in sequential model editing lies in the condition number of the edited matrix. The condition number of a matrix represents its numerical sensitivity, and therefore can be used to indicate the extent to which the original knowledge associations stored in LLMs are perturbed after editing. Subsequently, statistical findings demonstrate that the value of this factor becomes larger as the number of edits increases, thereby exacerbating the deterioration of general abilities. To this end, a framework termed Perturbation Restraint on Upper bouNd for Editing (PRUNE) is proposed, which applies the condition number restraints in sequential editing. These restraints can lower the upper bound on perturbation to edited models, thus preserving the general abilities. Systematically, we conduct experiments employing three popular editing methods on three LLMs across four representative downstream tasks. Evaluation results show that PRUNE can preserve considerable general abilities while maintaining the editing performance effectively in sequential model editing."
    },
    {
        "title": "Incorporating Visual Correspondence into Diffusion Model for Visual Try-On",
        "link_suffix": "/forum?id=XXzOzJRyOZ",
        "link": "https://openreview.net/forum?id=XXzOzJRyOZ",
        "pdf_link": "https://openreview.net/pdf?id=XXzOzJRyOZ",
        "keywords": "Virtual Try-On, Image Generation",
        "abstract": "Diffusion models have shown preliminary success in virtual try-on (VTON) task. The typical dual-branch architecture comprises two UNets for implicit garment deformation and synthesized image generation respectively, and has emerged as the recipe for VTON task. Nevertheless, the problem remains challenging to preserve the shape and every detail of the given garment due to the intrinsic stochasticity of diffusion model. To alleviate this issue, we novelly propose to explicitly capitalize on visual correspondence as the prior to tame diffusion process instead of simply feeding the whole garment into UNet as the appearance reference. Specifically, we interpret the fine-grained appearance and texture details as a set of structured semantic points, and match the semantic points rooted in garment to the ones over target person through local flow warping. Such 2D points are then augmented into 3D-aware cues with depth/normal map of target person. The correspondence mimics the way of putting clothing on human body and the 3D-aware cues act as semantic point matching to supervise diffusion model training. A point-focused diffusion loss is further devised to fully take the advantage of semantic point matching. Extensive experiments demonstrate strong garment detail preservation of our approach, evidenced by state-of-the-art VTON performances on both VITON-HD and DressCode datasets."
    },
    {
        "title": "The adaptive complexity of log-concave sampling",
        "link_suffix": "/forum?id=EeqlkPpaV8",
        "link": "https://openreview.net/forum?id=EeqlkPpaV8",
        "pdf_link": "https://openreview.net/pdf?id=EeqlkPpaV8",
        "keywords": "sampling, adaptive complexity, computational statistics",
        "abstract": "In large-data applications, such as the inference process of diffusion models, it is desirable to design sampling algorithms with a high degree of parallelization. In this work, we study the adaptive complexity of sampling, which is the minimal number of sequential rounds required to achieve sampling given polynomially many queries executed in parallel at each round. For unconstrained sampling, we examine distributions that are log-smooth or log-Lipschitz and log strongly or non-strongly concave. We show that an almost linear iteration algorithm cannot return a sample with a specific exponentially small accuracy under total variation distance. For box-constrained sampling, we show that an almost linear iteration algorithm cannot return a sample with sup-polynomially small accuracy under total variation distance for log-concave distributions. Our proof relies upon novel analysis with the characterization of the output for the hardness potentials based on the chain-like structure with random partition and classical smoothing techniques."
    },
    {
        "title": "Language-Guided Object-Centric World Models for Predictive Control",
        "link_suffix": "/forum?id=29p13QihRM",
        "link": "https://openreview.net/forum?id=29p13QihRM",
        "pdf_link": "https://openreview.net/pdf?id=29p13QihRM",
        "keywords": "Object-Centric Representation, World Model, Predictive Control",
        "abstract": "A world model is essential for an agent to predict the future and plan in domains such as autonomous driving and robotics. To achieve this, recent advancements have focused on video generation, which has gained significant attention due to the impressive success of diffusion models. However, these models require substantial computational resources. To address these challenges, we propose a world model leveraging object-centric representation space using slot attention, guided by language instructions. Our model perceives the current state as an object-centric representation and predicts future states in this representation space conditioned on natural language instructions. This approach results in a more compact and computationally efficient model compared to diffusion-based generative alternatives. Furthermore, it flexibly predicts future states based on language instructions, and offers a significant advantage in manipulation tasks where object recognition is crucial. In this paper, we demonstrate that our latent predictive world model surpasses generative world models in visuo-linguo-motor control tasks, achieving superior sample and computation efficiency. We also investigate the generalization performance of the proposed method and explore various strategies for predicting actions using object-centric representations."
    },
    {
        "title": "ROS: A GNN-based Relax-Optimize-and-Sample Framework for Max-k-Cut Problems",
        "link_suffix": "/forum?id=CpiJWKFdHN",
        "link": "https://openreview.net/forum?id=CpiJWKFdHN",
        "pdf_link": "https://openreview.net/pdf?id=CpiJWKFdHN",
        "keywords": "Max-k-Cut, Learning to Optimize, Graph Neural Networks, Pre-train and Fine-tune, Sampling",
        "abstract": "The Max-$k$-Cut problem is a fundamental combinatorial optimization challenge that generalizes the classic $\\mathcal{NP}$-complete Max-Cut problem. While relaxation techniques are commonly employed to tackle Max-$k$-Cut, they often lack guarantees of equivalence between the solutions of the original problem and its relaxation. To address this issue, we introduce the Relax-Optimize-and-Sample (ROS) framework. In particular, we begin by relaxing the discrete constraints to the continuous probability simplex form. Next, we pre-train and fine-tune a graph neural network model to efficiently optimize the relaxed problem. Subsequently, we propose a sampling-based construction algorithm to map the continuous solution back to a high-quality Max-$k$-Cut solution. By integrating geometric landscape analysis with statistical theory, we establish the consistency of function values between the continuous solution and its mapped counterpart. Extensive experimental results on random regular graphs and the Gset benchmark demonstrate that the proposed ROS framework effectively scales to large instances with up to $20,000$ nodes in just a few seconds, outperforming state-of-the-art algorithms. Furthermore, ROS exhibits strong generalization capabilities across both in-distribution and out-of-distribution instances, underscoring its effectiveness for large-scale optimization tasks."
    },
    {
        "title": "MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular Annotations for Medicine",
        "link_suffix": "/forum?id=IwgmgidYPS",
        "link": "https://openreview.net/forum?id=IwgmgidYPS",
        "pdf_link": "https://openreview.net/pdf?id=IwgmgidYPS",
        "keywords": "Medical Foundation Model, Multimodal Dataset, Vision-Language Pretraining.",
        "abstract": "This paper introduces MedTrinity-25M, a comprehensive, large-scale multimodal dataset for medicine, covering over 25 million images across 10 modalities, with multigranular annotations for more than 65 diseases. These multigranular annotations encompass both global information, such as modality and organ detection, and local information like ROI analysis, lesion texture, and region-wise correlations. Unlike the existing multimodal datasets, which are limited by the availability of image-text pairs, we have developed the first automated pipeline that scales up multimodal data by generating multigranular visual and textual annotations in the form of image-ROI-description triplets without the need for any paired text descriptions. Specifically, data from over 30 different sources have been collected, preprocessed, and grounded using domain-specific expert models to identify ROIs related to abnormal regions. We then build a comprehensive knowledge base and prompt multimodal large language models to perform retrieval-augmented generation with the identified ROIs as guidance, resulting in multigranular textual descriptions. Compared to existing datasets, MedTrinity-25M provides the most enriched annotations, supporting a comprehensive range of multimodal tasks such as captioning and report generation, as well as vision-centric tasks like classification and segmentation. We propose LLaVA-Tri by pretraining LLaVA on MedTrinity-25M, achieving state-of-the-art performance on VQA-RAD, SLAKE and PathVQA, surpassing representative SOTA multimodal large language models. Furthermore, MedTrinity-25M can also be utilized to support large-scale pre-training of multimodal medical AI models, contributing to the development of future foundation models in the medical domain. We will make our dataset available."
    }
]