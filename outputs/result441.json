[
    {
        "title": "Pixel-Space Post-Training of Latent-Diffusion Models",
        "link_suffix": "/forum?id=F7yPR6XhFR",
        "link": "https://openreview.net/forum?id=F7yPR6XhFR",
        "pdf_link": "https://openreview.net/pdf?id=F7yPR6XhFR",
        "keywords": "latent diffusion models, fine-tuning, pixel space, image generation",
        "abstract": "Latent diffusion models (LDMs) have made significant advancements in the field of image generation in recent years. One major advantage of LDMs is their ability to operate in a compressed latent space, allowing for more efficient training and deployment. However, despite these advantages, challenges with LDMs still remain. For example, it has been observed that LDMs often generate high-frequency details and complex compositions imperfectly. We hypothesize that one reason for these flaws is due to the fact that all pre- and post-training of LDMs are done in latent space, which is typically $8 \\times 8$ lower spatial-resolution than the output images. To address this issue, we propose adding pixel-space supervision in the post-training process to better preserve high-frequency details. Experimentally, we show that adding a pixel-space objective significantly improves both supervised quality fine-tuning and preference-based post-training by a large margin on a state-of-the-art DiT transformer and U-Net diffusion models in both visual quality and visual flaw metrics, while maintaining the same text alignment quality."
    },
    {
        "title": "FM-TS: Flow Matching for Time Series Generation",
        "link_suffix": "/forum?id=2whSvqwemU",
        "link": "https://openreview.net/forum?id=2whSvqwemU",
        "pdf_link": "https://openreview.net/pdf?id=2whSvqwemU",
        "keywords": "Time Series Generation, Flow Matching, Generative AI",
        "abstract": "Time series generation has emerged as an essential tool for analyzing temporal data across numerous fields. \nWhile diffusion models have recently gained significant attention in generating high-quality time series, they tend to be computationally demanding and reliant on complex stochastic processes. \nTo address these limitations, we introduce FM-TS, a rectified Flow Matching-based framework for Time Series generation, which simplifies the time series generation process by directly optimizing continuous trajectories. This approach avoids the need for iterative sampling or complex noise schedules typically required in diffusion-based models. \nFM-TS is more efficient in terms of training and inference.\nMoreover, FM-TS is highly adaptive, supporting both conditional and unconditional time series generation. \nNotably, through our novel inference design, the model trained in an unconditional setting can seamlessly generalize to conditional tasks without the need for retraining. Extensive benchmarking across both settings demonstrates that FM-TS consistently delivers superior performance compared to existing approaches while being more efficient in terms of training and inference. \nFor instance, in terms of discriminative score, FM-TS achieves $0.005$, $0.019$, $0.011$, $0.005$, $0.053$, and $0.106$ on the Sines, Stocks, ETTh, MuJoCo, Energy, and fMRI unconditional time series datasets, respectively, significantly outperforming the second-best method which achieves $0.006$, $0.067$, $0.061$, $0.008$, $0.122$, and $0.167$ on the same datasets.\nWe have achieved superior performance in solar forecasting and MuJoCo imputation tasks, significantly enhanced by our innovative $t$ power sampling method."
    },
    {
        "title": "Video-STaR: Self-Training Enables Video Instruction Tuning with Any Supervision",
        "link_suffix": "/forum?id=JYV2hrtFSv",
        "link": "https://openreview.net/forum?id=JYV2hrtFSv",
        "pdf_link": "https://openreview.net/pdf?id=JYV2hrtFSv",
        "keywords": "Video Understanding, Visual Instruction Tuning, Self-Training, Chain-of-thought reasoning",
        "abstract": "The performance and reasoning capabilities of Large Multi-modal Models (LMMs) is dependent on the size and quality of their training datasets. However, collecting datasets that support chain-of-thought instruction tuning is highly challenging. Existing video instruction tuning datasets are often derived by prompting large language models with video captions to generate question-answer pairs, which makes them predominantly descriptive rather than reasoning-focused. \nMeanwhile, many labeled video datasets with diverse labels and supervision exist -- however, we find that their integration into LMMs is non-trivial. \nHerein, we present $\\underline{\\text{Video}}$ $\\underline{\\text{S}}\\text{elf}$-$\\underline{\\text{T}}\\text{raining}$ $\\text{with}$ $\\underline{\\text{a}}\\text{ugmented}$ $\\underline{\\text{R}}\\text{easoning}$ (Video-STaR), the first self-training approach for video instruction tuning. \nVideo-STaR allows the utilization ofanylabeled video dataset for video instruction tuning.\nIn Video-STaR, an LMM cycles between instruction generation and finetuning, which we show (I) improves general video understanding and (II) adapts LMMs to novel downstream tasks with existing supervision. \nDuring instruction generation, an LMM is prompted to propose an answer.  The answers are then filtered only to those that contain the original video labels, and the LMM is then re-trained on the generated dataset. \nBy training exclusively on generated answers containing the correct video labels, Video-STaR leverages these existing labels as weak supervision for video instruction tuning.\nOur results demonstrate that Video-STaR-augmented LMMs achieve notable improvements in (I) general Video QA, where TempCompass performance improved by 6.1%,and(II) downstream tasks, with a 9.9% increase in Kinetics700-QA accuracy and a 4.0% improvement in action quality assessment on FineDiving, while also exhibiting better interpretability."
    },
    {
        "title": "A Simple Diffusion Transformer on Unified Video, 3D, and Game Field Generation",
        "link_suffix": "/forum?id=w6YS9A78fq",
        "link": "https://openreview.net/forum?id=w6YS9A78fq",
        "pdf_link": "https://openreview.net/pdf?id=w6YS9A78fq",
        "keywords": "Diffusion Probabilistic Fields, World Model",
        "abstract": "The probabilistic field models the distribution of continuous functions defined over metric spaces. While these models hold great potential for unifying data generation across various modalities, including images, videos, and 3D geometry, they still struggle with long-context generation beyond simple examples. This limitation can be attributed to their MLP architecture, which lacks sufficient inductive bias to capture global structures through uniform sampling.\nTo address this, we propose a new and simple model that incorporates a view-wise sampling algorithm to focus on local structure learning, along with autoregressive generation to preserve global geometry. It adapts cross-modality conditions, such as text prompts for text-to-video generation, camera poses for 3D view generation, and control actions for game generation.\nExperimental results across various modalities demonstrate the effectiveness of our model, with its 675M parameter size, and highlight its potential as a foundational framework for scalable, modality-unified visual content generation."
    },
    {
        "title": "Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations",
        "link_suffix": "/forum?id=94kQgWXojH",
        "link": "https://openreview.net/forum?id=94kQgWXojH",
        "pdf_link": "https://openreview.net/pdf?id=94kQgWXojH",
        "keywords": "Vision language models, hallucinations, logit lens, interpretability",
        "abstract": "We investigate the internal representations of vision-language models (VLMs) to address hallucinations, a persistent challenge despite advances in model size and training. We project VLMs\u2019 internal image representations to their language vocabulary and identify differences in token output probabilities between real and hallucinated objects. We additionally use these output probabilities to spatially localize real objects. Building on this approach, we introduce a knowledge erasure algorithm that removes hallucinations by linearly orthogonalizing image features with respect to hallucinated object features. We show that targeted edits to a model\u2019s latent representations can reduce hallucinations by up to 25.7% on the COCO2014 dataset while preserving performance. Our findings demonstrate how a deeper understanding of VLMs\u2019 latent representations can enhance reliability and enable novel capabilities, such as zero-shot segmentation."
    },
    {
        "title": "Robustness of Quantum Algorithms for Nonconvex Optimization",
        "link_suffix": "/forum?id=JyQYYjtO88",
        "link": "https://openreview.net/forum?id=JyQYYjtO88",
        "pdf_link": "https://openreview.net/pdf?id=JyQYYjtO88",
        "keywords": "Nonconvex optimization, Robustness, Quantum algorithms",
        "abstract": "In this paper, we systematically study quantum algorithms for finding an $\\epsilon$-approximate second-order stationary point ($\\epsilon$-SOSP) of a $d$-dimensional nonconvex function, a fundamental problem in nonconvex optimization, with noisy zeroth- or first-order oracles as inputs. We first prove that, up to noise of $O(\\epsilon^{10}/d^5)$, perturbed accelerated gradient descent equipped with quantum gradient estimation takes $O(\\log d/\\epsilon^{1.75})$ quantum queries to find an $\\epsilon$-SOSP. We then prove that standard perturbed gradient descent is robust to the noise of $O(\\epsilon^6/d^4)$ and $O(\\epsilon/d^{0.5+\\zeta})$ for any $\\zeta>0$ on the zeroth- and first-order oracles, respectively, which provides a quantum algorithm with poly-logarithmic query complexity. We then propose a stochastic gradient descent algorithm using quantum mean estimation on the Gaussian smoothing of noisy oracles, which is robust to $O(\\epsilon^{1.5}/d)$ and $O(\\epsilon/\\sqrt{d})$ noise on the zeroth- and first-order oracles, respectively. The quantum algorithm takes $O(d^{2.5}/\\epsilon^{3.5})$ and $O(d^2/\\epsilon^3)$ queries to the two oracles, giving a polynomial speedup over the classical counterparts. As a complement, we characterize the domains where quantum algorithms can find an $\\epsilon$-SOSP with poly-logarithmic, polynomial, or exponential number of queries in $d$, or the problem is information-theoretically unsolvable even with an infinite number of queries. In addition, we prove an $\\Omega(\\epsilon^{-12/7})$ lower bound on $\\epsilon$ for any randomized classical and quantum algorithm to find an $\\epsilon$-SOSP using either noisy zeroth- or first-order oracles."
    },
    {
        "title": "MMCOMPOSITION: Revisiting the Compositionality of Pre-trained Vision-Language Models",
        "link_suffix": "/forum?id=0YXckVo7Kw",
        "link": "https://openreview.net/forum?id=0YXckVo7Kw",
        "pdf_link": "https://openreview.net/pdf?id=0YXckVo7Kw",
        "keywords": "Vision-Language Models, Compositionality, Benchmark",
        "abstract": "The advent of large Vision-Language Models (VLMs) has significantly advanced multimodal understanding, enabling more sophisticated and accurate integration of visual and textual information across various tasks, including image and video captioning, visual question answering, and cross-modal retrieval. Despite VLMs' superior capabilities, researchers lack a comprehensive understanding of their compositionality -- the ability to understand and produce novel combinations of known visual and textual components. Prior benchmarks provide only a relatively rough compositionality evaluation from the perspectives of objects, relations, and attributes while neglecting deeper reasoning about object interactions, counting, and complex compositions. However, compositionality is a critical ability that facilitates coherent reasoning and understanding across modalities for VLMs.  To address this limitation, we propose MMCOMPOSITION, a novel human-annotated benchmark for comprehensively and accurately evaluating VLMs' compositionality. Our proposed benchmark serves as a complement to these earlier works. With MMCOMPOSITION, we can quantify and explore the compositionality of the mainstream VLMs. Surprisingly, we find GPT-4o's compositionality inferior to the best open-source model, and we analyze the underlying reasons. Our experimental analysis reveals the limitations of VLMs in fine-grained compositional perception and reasoning, and points to areas for improvement in VLM design and training."
    },
    {
        "title": "Fourier Head: Helping Large Language Models Learn Complex Probability Distributions",
        "link_suffix": "/forum?id=4hPwLg7zD3",
        "link": "https://openreview.net/forum?id=4hPwLg7zD3",
        "pdf_link": "https://openreview.net/pdf?id=4hPwLg7zD3",
        "keywords": "LLM, Fourier, smooth function, multi-class classification",
        "abstract": "As the quality of large language models has improved, there has been increased interest in using them to model non-linguistic tokens. For example, the Decision Transformer recasts agentic decision making as a sequence modeling problem, using a decoder-only LLM to model the distribution over the discrete action space for an Atari agent. However, when adapting LLMs to non-linguistic domains, it remains unclear if softmax over discrete bins captures the continuous structure of the tokens and the potentially complex distributions needed for high quality token generation. We introduce a neural network layer, constructed using Fourier series, which we can easily substitute for any linear layer if we want the outputs to have a more continuous structure. We perform extensive analysis on synthetic datasets, as well as on large-scale decision making and time series forecasting tasks. We also provide theoretical evidence that this layer can better learn signal from data while ignoring high-frequency noise. All of our results support the effectiveness of our proposed Fourier head in scenarios where the underlying data distribution has a natural continuous structure. For example, the Fourier head improves a Decision Transformer agent's returns by 46% on the Atari Seaquest game, and increases a state-of-the-art times series foundation model's forecasting performance by 3.5% across 20 benchmarks unseen during training."
    },
    {
        "title": "Locality Sensitive Avatars From Video",
        "link_suffix": "/forum?id=SVta2eQNt3",
        "link": "https://openreview.net/forum?id=SVta2eQNt3",
        "pdf_link": "https://openreview.net/pdf?id=SVta2eQNt3",
        "keywords": "3D Computer Vision, Neural Rendering, Avatar Modeling",
        "abstract": "We present locality-sensitive avatar, a neural radiance field (NeRF) based network to learn human motions from monocular videos. To this end, we estimate a canonical representation between different frames of a video with a non-linear mapping from observation to canonical space, which we decompose into a skeletal rigid motion and a non-rigid counterpart. Our key contribution is to retain fine-grained details by modeling the non-rigid part with a graph neural network (GNN) that keeps the pose information local to neighboring body parts.\nCompared to former canonical representation based methods which solely operate on the coordinate space of a whole shape, our locality-sensitive motion modeling can reproduce both realistic shape contours and vivid fine-grained details. We evaluate on ZJU-MoCap, ActorsHQ, SynWild, and various outdoor videos. The experiments reveal that with the locality sensitive deformation to canonical feature space, we are the first to achieve state-of-the-art results across novel view synthesis, novel pose animation and 3D shape reconstruction simultaneously. For reproducibility, the code will be available upon publication."
    },
    {
        "title": "MOTRv3: Release-Fetch Supervision for End-to-End Multi-Object Tracking",
        "link_suffix": "/forum?id=ezPbPoYFME",
        "link": "https://openreview.net/forum?id=ezPbPoYFME",
        "pdf_link": "https://openreview.net/pdf?id=ezPbPoYFME",
        "keywords": "End-to-End; Multiple Object Tracking; Transformer",
        "abstract": "Although end-to-end multi-object trackers like MOTR enjoy the merits of simplicity, they suffer from the conflict between detection and association, resulting in unsatisfactory convergence dynamics. While MOTRv2 partly addresses this problem, it demands an additional detector. In this work, we serve as the first to reveal this conflict arises from unfair label assignment between detect and track queries, where detect queries are responsible for recognizing newly appearing targets and track queries are to associate them in following frames. Based on this observation, we propose MOTRv3, which balances the label assignment using the proposed release-fetch supervision strategy. In this strategy, labels are first released for detection and gradually fetched back for association. Besides, another two strategies named pseudo label distillation and track group denoising are designed to further strengthen the supervision for detection and association. Without extra detector during inference, MOTRv3 achieves impressive performance across diverse benchmarks, showing scaling up capability."
    },
    {
        "title": "Unhackable Temporal Reward for Scalable Video MLLMs",
        "link_suffix": "/forum?id=Gf1uBeuUJW",
        "link": "https://openreview.net/forum?id=Gf1uBeuUJW",
        "pdf_link": "https://openreview.net/pdf?id=Gf1uBeuUJW",
        "keywords": "Video MLLMs; Temporal hacking; Temporal Perplexity",
        "abstract": "In the pursuit of superior video-processing MLLMs, we have encountered a perplexing paradox: the \u201canti-scaling law\u201d, where more data and larger models lead to worse performance. This study unmasks the culprit: \u201ctemporal hacking\u201d, a phenomenon where models shortcut by fixating on select frames, missing the full video narrative. In this work, we systematically establish a comprehensive theory of temporal hacking, defining it from a reinforcement learning perspective, introducing the Temporal Perplexity (TPL) score to assess this misalignment, and proposing the Unhackable Temporal Rewarding (UTR) framework to mitigate the temporal hacking. Both theoretically and empirically, TPL proves to be a reliable indicator of temporal modeling quality, correlating strongly with frame activation patterns. Extensive experiments reveal that UTR not only counters temporal hacking but significantly elevates video comprehension capabilities. This work not only advances video-AI systems but also illuminates the critical importance of aligning proxy rewards with true objectives in MLLM development."
    },
    {
        "title": "Instax3D: Creating 3D Portrait from a single-view image in Minutes",
        "link_suffix": "/forum?id=mFCLLUtm83",
        "link": "https://openreview.net/forum?id=mFCLLUtm83",
        "pdf_link": "https://openreview.net/pdf?id=mFCLLUtm83",
        "keywords": "3D portrait, single-view reconstruction",
        "abstract": "We study single-view 3D portrait creation, specifically producing a full-head 3D\nportrait from a single headshot. This problem faces two challenges: 1) the 2D\nimage-based personalization methods lack comprehensive 3D awareness due to\nthe scarcity of multi-view 2D images or 3D assets in the training data, and 2) the\nscore distillation sampling optimization methods usually take hours to produce a\nsingle 3D asset, making the process quite time-consuming. To overcome these\nlimitations, we propose Instax3D, a generative Gaussian Splatting model with a\nvideo diffusion prior for rapid 3D portrait creation. We formulate the 3D portrait\ncreation problem as a \u201cgeneration and construction\u201d process. Specifically, Instax3D\nfirst synthesizes a consecutive video sequence using a finetuned video diffusion\nmodel, capitalizing on inherent diversity and multi-view knowledge from\nthe massive video data. Subsequently, Instax3D reconstructs the 3D portrait with\na multi-view FLAME-based Gaussian splatting representation from the generated\nvideo frames, structurally guided by an expressive 3D parametric model. Notably,\ngiven a reference headshot image, Instax3D can generate a 3D portrait in just 10\nminutes and render it at 40 FPS. This represents a 10\u00d7 improvement over previous\nmainstream optimization-based methods, which can take between one to two\nhours."
    },
    {
        "title": "Understanding Long Videos with Multimodal Language Models",
        "link_suffix": "/forum?id=OxKi02I29I",
        "link": "https://openreview.net/forum?id=OxKi02I29I",
        "pdf_link": "https://openreview.net/pdf?id=OxKi02I29I",
        "keywords": "long-video, visual question answering, interpretability",
        "abstract": "Large Language Models (LLMs) have allowed recent LLM-based approaches to achieve excellent performance on long-video understanding benchmarks. We investigate how extensive world knowledge and strong reasoning skills of LLMs influence evaluations on standard long video benchmarks. Surprisingly, we discover that LLM-based models yield surprisingly good accuracy on long-video tasks with limited video information, sometimes even with no video specific information. \nBuilding on this, we inject video-specific object-centric information extracted from off-the-shelf pre-trained models into an LLM-based setup. We utilize natural language as a medium for fusing this information. Our resulting Multimodal Video Understanding (MVU) framework demonstrates state-of-the-art performance across long-video understanding benchmarks as well as on robotics domain tasks. Our code will be released publicly."
    },
    {
        "title": "The Scene Language: Representing Scenes with Programs, Words, and Embeddings",
        "link_suffix": "/forum?id=wWcNhS4g1U",
        "link": "https://openreview.net/forum?id=wWcNhS4g1U",
        "pdf_link": "https://openreview.net/pdf?id=wWcNhS4g1U",
        "keywords": "3D scene generation; visual programs",
        "abstract": "We introduce the Scene Language, a visual scene representation that concisely and precisely describes the structure, semantics, and identity of visual scenes. The Scene Language represents a scene with three key components: a program that specifies the hierarchical and relational structure of entities in the scene, words in natural language that summarize the semantic class of each entity, and embeddings that capture the visual identity of each entity. This representation can be inferred from pre-trained language models via a training-free inference technique, given text or image inputs. The resulting scene can be rendered into images using traditional, neural, or hybrid graphics renderers. Together, this forms a robust, fully automated system for high-quality 3D and 4D scene generation. Compared with existing representations like scene graphs, our proposed Scene Language generates complex scenes with higher fidelity, while explicitly modeling the scene structures to enable precise control and editing. Project page:https://sclg-page.github.io/"
    },
    {
        "title": "RODIN: Injecting 2D Foundational Features to 3D Vision Language Understanding",
        "link_suffix": "/forum?id=Pt3lfU1NqC",
        "link": "https://openreview.net/forum?id=Pt3lfU1NqC",
        "pdf_link": "https://openreview.net/pdf?id=Pt3lfU1NqC",
        "keywords": "3D vision-language understanding",
        "abstract": "We present RODIN (Referential ODIN), a novel model for 3D vision-language understanding that directly operates on posed RGB-D frames. Consuming posed RGB-D from sensors, such as those from an iPhone, simplifies and speeds up inference compared to existing models that train and test using pointclouds sampled from a reconstructed mesh provided by a dataset. We hypothesize that existing approaches consume pointclouds sampled from mesh instead of sensor RGB-D point clouds due to inaccurate camera poses in existing 3D grounding benchmarks, and show that using the \"sensor\" pointclouds indeed leads to a 5-10% drop in performance on 3D referential grounding, for these methods. Yet sensor noise is unavoidable in real-world settings. RODIN instead addresses this with a scalable, end-to-end architecture for various 3D vision-language tasks. Specifically, RODIN combines powerful pretrained 2D weights trained on internet-scale data, adapts them to a 2D-3D encoder using the recently proposed ODIN, and combines that backbone with a proposed 3D mask-language decoder based on the Mask2Former used in SAM. RODIN achieves state-of-the-art performance on multiple 3D vision-language benchmarks, including referential grounding (SR3D, NR3D, ScanRefer), open-vocabulary object detection (ScanNet200 and Matterport3D), and question-answering (ScanQA and SQA3D). It outperforms previous methods for 3D vision-language tasks, despite consuming only sensor inputs. Because of its combination of effectively leveraging 2D pretrained architectures and finetuning end-to-end on sensor data, RODIN provides a scalable solution for embodied 3D perception."
    },
    {
        "title": "Vec2Face: Scaling Face Dataset Generation with Loosely Constrained Vectors",
        "link_suffix": "/forum?id=RoN6NnHjn4",
        "link": "https://openreview.net/forum?id=RoN6NnHjn4",
        "pdf_link": "https://openreview.net/pdf?id=RoN6NnHjn4",
        "keywords": "Identity privacy, Synthetic face dataset generation, Face recognition, Image generation",
        "abstract": "This paper studies how to synthesize face images of non-existent persons, to create a dataset that allows effective training of face recognition (FR) models. Besides generating realistic face images, two other important goals are: 1) the ability to generate a large number of distinct identities (inter-class separation), and 2) a proper variation in appearance of the images for each identity (intra-class variation).\nHowever, existing works 1) are typically limited in how many well-separated identities can be generated and 2) either neglect or use an external model for attribute augmentation. We propose Vec2Face, a holistic model that uses only a sampled vector as input and can flexibly generate and control the identity of face images and their attributes. Composed of a feature masked autoencoder and an image decoder, Vec2Face is supervised by face image reconstruction and can be conveniently used in inference. Using vectors with low similarity among themselves as inputs, Vec2Face generates well-separated identities. Randomly perturbing an input identity vector within a small range allows Vec2Face to generate faces of the same identity with proper variation in face attributes. It is also possible to generate images with designated attributes by adjusting vector values with a gradient descent method. Vec2Face has efficiently synthesized as many as 300K identities, whereas 60K is the largest number of identities created in the previous works. As for performance, FR models trained with the generated HSFace datasets, from 10k to 300k identities, achieve state-of-the-art accuracy, from 92% to 93.52%, on five real-world test sets. For the first time, the FR model trained using our synthetic training set achieves higher accuracy than that trained using a same-scale training set of real face images (on the CALFW test set)."
    },
    {
        "title": "Sample what you can't compress",
        "link_suffix": "/forum?id=vK8C37eHXM",
        "link": "https://openreview.net/forum?id=vK8C37eHXM",
        "pdf_link": "https://openreview.net/pdf?id=vK8C37eHXM",
        "keywords": "autoencoders_diffusion+generative models",
        "abstract": "For learned image representations, basic autoencoders often produce blurry results. Reconstruction quality can be improved by incorporating additional penalties such as adversarial (GAN) and perceptual losses. Arguably, these approaches lack a principled interpretation. Concurrently, in generative settings diffusion has demonstrated a remarkable ability to create crisp, high quality results and has solid theoretical underpinnings (from variational inference to direct study as the Fisher Divergence). Our work combines autoencoder representation learning with diffusion and is, to our knowledge, the first to demonstrate the efficacy of jointly learning a continuous encoder and decoder under a diffusion-based loss. We demonstrate that this approach yields better reconstruction quality as compared to GAN-based autoencoders while being easier to tune. We also show that the resulting representation is easier to model with a latent diffusion model as compared to the representation obtained from a state-of-the-art GAN-based loss. Since our decoder is stochastic, it can generate details not encoded in the otherwise deterministic latent representation; we therefore name our approach \"Sample what you can't compress\", or SWYCC for short."
    },
    {
        "title": "SORSA: Singular Values and Orthonormal Regularized Singular Vectors Adaptation of Large Language Models",
        "link_suffix": "/forum?id=PDV3SmO6Iw",
        "link": "https://openreview.net/forum?id=PDV3SmO6Iw",
        "pdf_link": "https://openreview.net/pdf?id=PDV3SmO6Iw",
        "keywords": "Deep Learning, LLM, PEFT, LoRA, SVD",
        "abstract": "The rapid advancement in large language models (LLMs) comes with a significant increase in their parameter size, presenting challenges for adaptation and fine-tuning. Parameter-efficient fine-tuning (PEFT) methods are widely used to adapt LLMs for downstream tasks efficiently. In this paper, we propose Singular Values and Orthonormal Regularized Singular Vectors Adaptation, or SORSA, a novel PEFT method. We introduce a method to analyze the variation of the parameters by performing singular value decomposition (SVD) and discuss and analyze SORSA's superiority in minimizing the alteration in the SVD aspect. Each SORSA adapter consists of two main parts: trainable principal singular weights $W_p = U_p \\text{diag}(S_p) V^\\top_p$, and frozen residual weights $W_r = U_r \\text{diag}(S_r) V^\\top_r$. These parts are initialized by performing SVD on pre-trained weights. Moreover, we implement and analyze an orthonormal regularizer, which we prove could decrease the condition number of $W_p$ and allows the optimization to be more efficient. SORSA adapters could be merged during inference, thus eliminating any inference latency. After all, SORSA shows a faster convergence than PiSSA and LoRA in our experiments. On the GSM-8K benchmark, Llama 2 7B adapted using SORSA achieved 56.03% accuracy, surpassing LoRA (42.30%), Full FT (49.05%), and PiSSA (53.07%). On the MATH benchmark, SORSA achieved 10.36% accuracy, outperforming LoRA (5.50%), Full FT (7.22%), and PiSSA (7.44%). We conclude that SORSA offers a new perspective on parameter-efficient fine-tuning, demonstrating remarkable performance."
    },
    {
        "title": "Discrete Codebook World Models for Continuous Control",
        "link_suffix": "/forum?id=lfRYzd8ady",
        "link": "https://openreview.net/forum?id=lfRYzd8ady",
        "pdf_link": "https://openreview.net/pdf?id=lfRYzd8ady",
        "keywords": "reinforcement learning, world model, representation learning, self-supervised learning, model-based reinforcement learning",
        "abstract": "In reinforcement learning (RL), world models serve as internal simulators, enabling agents to predict environment dynamics and future outcomes in order to make informed decisions. While previous approaches leveraging discrete latent spaces, such as DreamerV3, have achieved strong performance in discrete action environments, they are typically outperformed in continuous control tasks by models with continuous latent spaces, like TD-MPC2. This paper explores the use of discrete latent spaces for continuous control with world models. Specifically, we demonstrate that quantized discrete codebook encodings are more effective representations for continuous control, compared to alternative encodings, such as one-hot and label-based encodings. Based on these insights, we introduce DCWM: Discrete Codebook World Model, a model-based RL method which surpasses recent state-of-the-art algorithms, including TD-MPC2 and DreamerV3, on continuous control benchmarks."
    },
    {
        "title": "Connecting Federated ADMM to Bayes",
        "link_suffix": "/forum?id=ipQrjRsl11",
        "link": "https://openreview.net/forum?id=ipQrjRsl11",
        "pdf_link": "https://openreview.net/pdf?id=ipQrjRsl11",
        "keywords": "federated learning, bayesian, variational inference, admm",
        "abstract": "We provide new connections between two distinct federated learning approaches based on (i) ADMM and (ii) Variational Bayes (VB), and propose new variants by combining their complementary strengths. Specifically, we show that the dual variables in ADMM naturally emerge through the \"site\" parameters used in VB with isotropic Gaussian covariances. Using this, we derive two versions of ADMM from VB that use flexible covariances and functional regularisation, respectively. Through numerical experiments, we validate the improvements obtained in performance. The work shows connection between two fields that are believed to be fundamentally different and combines them to improve federated learning."
    },
    {
        "title": "HVT: A Comprehensive Vision Framework for Learning in Non-Euclidean Space",
        "link_suffix": "/forum?id=b2FFWnwZxl",
        "link": "https://openreview.net/forum?id=b2FFWnwZxl",
        "pdf_link": "https://openreview.net/pdf?id=b2FFWnwZxl",
        "keywords": "hyperbolic, topology, vit, transformer, vision, manifold",
        "abstract": "Data representation in non-Euclidean spaces has proven effective for capturing hierarchical and complex relationships in real-world datasets. Hyperbolic spaces, in particular, provide efficient embeddings for hierarchical structures. This paper introduces the Hyperbolic Vision Transformer (HVT), a novel extension of the Vision Transformer (ViT) that integrates hyperbolic geometry. While traditional ViTs operate in Euclidean space, our method enhances the self-attention mechanism by leveraging hyperbolic distance and M\u00f6bius transformations. This enables more effective modeling of hierarchical and relational dependencies in image data. We present rigorous mathematical formulations, showing how hyperbolic geometry can be incorporated into attention layers, feed-forward networks, and optimization. We offer improved performance for image classification using the ImageNet dataset."
    },
    {
        "title": "Safeguard is a Double-edged Sword: Denial-of-service Attack on Large Language Models",
        "link_suffix": "/forum?id=B6Sdw56GQJ",
        "link": "https://openreview.net/forum?id=B6Sdw56GQJ",
        "pdf_link": "https://openreview.net/pdf?id=B6Sdw56GQJ",
        "keywords": "large language model, adversarial machine learning, denial of service",
        "abstract": "Safety is a paramount concern of large language models (LLMs) in their open deployment. To this end, safeguard methods aim to enforce the ethical and responsible use of LLMs through safety alignment or guardrail mechanisms. However, we found that the malicious attackers could exploit false positives of safeguards, i.e., fooling the safeguard model to block safe content mistakenly, leading to a new denial-of-service (DoS) attack affecting LLM users. Specifically, through software or phishing attacks on user client software, attackers insert a short, seemingly innocuous adversarial prompt into user prompt templates in configuration files. This prompt triggers safeguard rejections of nearly all user requests from the client while remaining hidden in the user interface and non-trivial to detect. By designing an optimization process that utilizes gradient and attention information, our attack can automatically generate seemingly safe adversarial prompts, approximately only 30 characters long, that universally block over 97% of user requests on Llama Guard 3. The attack presents a new dimension of evaluating LLM safeguards focusing on false positives, different from the classic jailbreak."
    },
    {
        "title": "Learning 4D Embodied World Models",
        "link_suffix": "/forum?id=mnwlhvmKMN",
        "link": "https://openreview.net/forum?id=mnwlhvmKMN",
        "pdf_link": "https://openreview.net/pdf?id=mnwlhvmKMN",
        "keywords": "Embodied AI, World Model",
        "abstract": "In this paper, we present a 4D embodied world model, which takes in an image observation and language instruction as input and predicts a 4D dynamic mesh predicting how the scene will change as the embodied agent performs actions based on the given instructions. In contrast to previously learned world models which typically generate 2D videos, our 4D model provides detailed 3D information on precise configurations and shape of objects in a scene over time.\nThis allows us to effectively learn accurate inverse dynamic models for an embodied agent to execute a policy for interacting with the environment.\nTo construct a dataset to train such 4D world models,  we first annotate large-scale existing video robotics dataset using pretrained depth and normal prediction models to construct 3D consistent 4D models of each video. To efficiently learn generative models on this 4D data, we propose to train a video generative model on this annotated dataset, which jointly predicts RGB-DN (RGB, Depth, and Normal) for each video. We then present an algorithm to directly convert generated RGB, Depth and Normal images into high-quality dynamic 4D mesh models of the world. We illustrate how this enables us to predict high-quality meshes consistent across both time and space from embodied scenarios, render novel views for embodied scenes, as well as construct policies that substantially outperform those from prior 2D  and 3D models of the world. Our code, model, and dataset will be made publicly available."
    },
    {
        "title": "TokenFormer: Rethinking Transformer Scaling with Tokenized Model Parameters",
        "link_suffix": "/forum?id=oQ4igHyh3N",
        "link": "https://openreview.net/forum?id=oQ4igHyh3N",
        "pdf_link": "https://openreview.net/pdf?id=oQ4igHyh3N",
        "keywords": "Fully Attention-based Neural Network, Large Language Model, Model Scaling, Tokenized Model Parameters",
        "abstract": "Transformers have become the predominant architecture in foundation models due to their excellent performance across various domains. However, the substantial cost of scaling these models remains a significant concern. This problem arises primarily from their dependence on a fixed number of parameters within linear projections. When architectural modifications (e.g., channel dimensions) are introduced, the entire model typically requires retraining from scratch. As model sizes continue growing, this strategy results in increasingly high computational costs and becomes unsustainable. To overcome this problem, we introduce Tokenformer, a natively scalable architecture that leverages the attention mechanism not only for computations among input tokens but also for interactions between tokens and model parameters, thereby enhancing architectural flexibility. By treating model parameters as tokens, we replace all the linear projections in Transformers with our token-parameter attention layer, where input tokens act as queries and model parameters as keys and values. This reformulation allows for progressive and efficient scaling without necessitating retraining from scratch. Our model scales from 124M to 1.4B parameters by incrementally adding new key-value parameter pairs, achieving performance comparable to Transformers trained from scratch while greatly reducing training costs. Code will be available."
    },
    {
        "title": "VP-LLM: Text-Driven 3D Volume Completion with Large Language Models through Patchification",
        "link_suffix": "/forum?id=JmXu4fk5Mm",
        "link": "https://openreview.net/forum?id=JmXu4fk5Mm",
        "pdf_link": "https://openreview.net/pdf?id=JmXu4fk5Mm",
        "keywords": "Large Language Models, 3D completion, 3D generation",
        "abstract": "3D completion represents a critical task within the vision industries. Traditional diffusion-based methodologies have achieved commendable performance; however, they are hindered by several issues. Firstly, these methods primarily depend on models such as CLIP or BERT to encode textual information, thereby making them incapable of supporting detailed and complex instructions. Moreover, their model sizes usually increase rapidly when the scene is larger or the voxel resolution is higher, making it impossible to scale up. Witnessing the significant advancements in multi-modal understanding capabilities facilitated by recent developments in large language models (LLMs), we introduce Volume Patch LLM (VP-LLM), designed to executeuser-friendlyconditional 3D completion and denoising using a token-based single-forward pass approach. To integrate a 3D model into the textual domain of the LLM, the incomplete 3D model is initially divided into smaller patches\u2014a process we refer to as \"patchification\"\u2014in a way that each patch can be independently encoded, analogous to the tokenization configuration utilized by LLMs. These encoded patches are subsequently concatenated with the encoded text prompt sequence and inputted into an LLM, which is fine-tuned to capture the relationships between these patch tokens while embedding semantic meanings into the 3D object. Our findings indicate a robust ability of LLMs to interpret complex text instructions and comprehend 3D objects, surpassing the quality of results produced by state-of-the-art diffusion-based 3D completion models, especially when complex text prompts are given."
    }
]