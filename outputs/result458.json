[
    {
        "title": "Latent Space Chain-of-Embedding Enables Output-free LLM Self-Evaluation",
        "link_suffix": "/forum?id=jxo70B9fQo",
        "link": "https://openreview.net/forum?id=jxo70B9fQo",
        "pdf_link": "https://openreview.net/pdf?id=jxo70B9fQo",
        "keywords": "Large Language Models, Label-free Self-Evaluation, AI Reliability",
        "abstract": "LLM self-evaluation relies on the LLM's own ability to estimate response correctness, which can greatly improve its deployment reliability. \nIn this research track, we propose the Chain-of-Embedding (CoE) in the latent space to enable LLMs to perform output-free self-evaluation. CoE consists of all progressive hidden states produced during the inference time, which can be treated as the latent thinking path of LLMs. We find that when LLMs respond correctly and incorrectly, their CoE features differ, these discrepancies assist us in estimating LLM response correctness. Experiments in four diverse domains and seven LLMs fully demonstrate the effectiveness of our method. Meanwhile, its label-free design intent without any training and millisecond-level computational cost ensure real-time feedback in large-scale scenarios.\nMore importantly, we provide interesting insights into LLM response correctness from the perspective of hidden state changes inside LLMs."
    },
    {
        "title": "OML-AD: Online Machine Learning for Anomaly Detection in Time Series Data",
        "link_suffix": "/forum?id=xFvHcgj1fO",
        "link": "https://openreview.net/forum?id=xFvHcgj1fO",
        "pdf_link": "https://openreview.net/pdf?id=xFvHcgj1fO",
        "keywords": "Online Machine Learning, Anomaly Detection, Time Series, Concept Drift",
        "abstract": "Time series are ubiquitous and occur naturally in a variety of applications -- from data recorded by sensors in manufacturing processes, over financial data streams to climate data. Different tasks arise, such as regression, classification or segmentation of the time series. However, to reliably solve these challenges, it is important to filter out abnormal observations that deviate from the usual behavior of the time series. While many anomaly detection methods exist for independent data and stationary time series, these methods are not applicable to non-stationary time series. To allow for non-stationarity in the data, while simultaneously detecting anomalies, we propose OML-AD, a novel approach for anomaly detection (AD) based on online machine learning (OML). We provide an implementation of OML-AD within the Python library River and show that it outperforms state-of-the-art baseline methods in terms of accuracy and computational efficiency."
    },
    {
        "title": "Towards Realistic Example-based Modeling via 3D Gaussian Stitching",
        "link_suffix": "/forum?id=y10AP0BkID",
        "link": "https://openreview.net/forum?id=y10AP0BkID",
        "pdf_link": "https://openreview.net/pdf?id=y10AP0BkID",
        "keywords": "Gaussian splatting, Composition, Example-based Modeling",
        "abstract": "Using parts of existing models to rebuild new models, commonly termed as example-based modeling, is a classical methodology in the realm of computer graphics. Previous works mostly focus on shape composition, making them very hard to use for realistic composition of 3D objects captured from real-world scenes. This leads to combining multiple NeRFs into a single 3D scene to achieve seamless appearance blending. However, the current SeamlessNeRF method struggles to achieve interactive editing and harmonious stitching for real-world scenes due to its gradient-based strategy and grid-based representation.To this end, we present an example-based modeling method that combines multiple Gaussian fields in a point-based representation using sample-guided synthesis. Specifically, as for composition, we create a GUI to segment and transform multiple fields in real time, easily obtaining a semantically meaningful composition of models represented by 3D Gaussian Splatting (3DGS). For texture blending, due to the discrete and irregular nature of 3DGS, straightforwardly applying gradient propagation as SeamlssNeRF is not supported. Thus, a novel sampling-based cloning method is proposed to harmonize the blending while preserving the original rich texture and content. Our workflow consists of three steps: 1) real-time segmentation and transformation of a Gaussian model using a well-tailored GUI, 2) KNN analysis to identify boundary points in the intersecting area between the source and target models, and 3) two-phase optimization of the target model using sampling-based cloning and gradient constraints. Extensive experimental results validate that our approach significantly outperforms previous works in terms of realistic synthesis, demonstrating its practicality."
    },
    {
        "title": "Zero-Shot Subject-Driven Video Customization with Precise Motion Control",
        "link_suffix": "/forum?id=TX0OsLcaWf",
        "link": "https://openreview.net/forum?id=TX0OsLcaWf",
        "pdf_link": "https://openreview.net/pdf?id=TX0OsLcaWf",
        "keywords": "Video Generation, Customized Generation",
        "abstract": "Recent advances in customized video generation have enabled users to create videos tailored to both specific subjects and motion trajectories. However, existing methods often require complicated test-time fine-tuning and struggle with balancing subject learning and motion control, limiting their real-world applications. In this paper, we present $\\textbf{DreamCustomizer}$, a zero-shot video customization framework capable of generating videos with a specific subject and motion trajectory, guided by a single image and a bounding box sequence, respectively, and without the need for test-time fine-tuning. Specifically, we introduce reference attention, which leverages the model\u2019s inherent capabilities for subject learning, and devise a mask-guided motion module to achieve precise motion control by fully utilizing the robust motion signal of box masks derived from bounding boxes. While these two components achieve their intended functions, we empirically observe that motion control tends to dominate over subject learning. To address this, we propose two key designs: $\\textbf{1)}$ the masked reference attention, which integrates a blended latent mask modeling scheme into reference attention to enhance subject representations at the desired positions, and $\\textbf{2)}$ a reweighted diffusion loss, which differentiates the contributions of regions inside and outside the bounding boxes to ensure a balance between subject and motion control. Extensive experimental results on a newly curated dataset demonstrate that DreamCustomizer outperforms state-of-the-art methods in both subject customization and motion control. The dataset, code, and models will be made publicly available."
    },
    {
        "title": "FasterCache: Training-Free Video Diffusion Model Acceleration with High Quality",
        "link_suffix": "/forum?id=W49UjcpGxx",
        "link": "https://openreview.net/forum?id=W49UjcpGxx",
        "pdf_link": "https://openreview.net/pdf?id=W49UjcpGxx",
        "keywords": "Efficient Video Synthesis",
        "abstract": "In this paper, we present \\textbf{\\textit{FasterCache}}, a novel training-free strategy designed to accelerate the inference of video diffusion models with high-quality generation. By analyzing existing cache-based methods, we observe that \\textit{directly reusing adjacent-step features degrades video quality due to the loss of subtle variations}. We further perform a pioneering investigation of the acceleration potential of classifier-free guidance (CFG) and reveal significant redundancy between conditional and unconditional features within the same timestep. Capitalizing on these observations, we introduce FasterCache to substantially accelerate diffusion-based video generation. Our key contributions include a dynamic feature reuse strategy that preserves both feature distinction and temporal continuity, and CFG-Cache which optimizes the reuse of conditional and unconditional outputs to further enhance inference speed without compromising video quality. We empirically evaluate FasterCache on recent video diffusion models. Experimental results show that FasterCache can significantly accelerate video generation (\\eg 1.67$\\times$ speedup on Vchitect-2.0) while keeping video quality comparable to the baseline, and consistently outperform existing methods in both inference speed and video quality. \\textit{Our code will be made public upon publication.}"
    },
    {
        "title": "Correlation and Navigation in the Vocabulary Key Representation Space of Language Models",
        "link_suffix": "/forum?id=VipcVxaTnG",
        "link": "https://openreview.net/forum?id=VipcVxaTnG",
        "pdf_link": "https://openreview.net/pdf?id=VipcVxaTnG",
        "keywords": "Language Modeling, Next Token Prediction, Spurious Correlation, Generation Diversity",
        "abstract": "Language model (LM) decoding is based on the next-token prediction (NTP) probability distribution. For neural LMs (e.g., Transformer-based), NTP distribution is\nessentially a softmax-regularized dot product between an encoded input context\n(query) and fixed vocabulary representations (keys). In this paper, we study the\neffect of the key distribution on the NTP distribution, with a focus on whether\nthe similarity between keys will trigger spurious correlations in NTP. Through\nknowledge-probing tasks, we show that in the NTP distribution, the few top-ranked\ntokens are typically accurate. However, the middle-ranked prediction is highly biased\ntowards the tokens that are distributionally (not necessarily semantically) similar to\nthese top ones. For instance, if \u201cP\u201d is predicted as the top-1 token, \u201cA\u201d-\u201cZ\u201d will all\nbe ranked high in NTP, no matter whether they can lead to correct decoding results.\nThis hurts the sampling diversity and makes the sampling of correct, long-tail\nresults hopeless and noisy. We attempt to alleviate this issue via a novel in-context\nmethod that iteratively pushes the query representation away from explored regions.\nSpecifically, we include the explored decoding results in the context and prompt\nthe LM to generate something else, which encourages the LM to produce a query\nrepresentation that has small dot products with explored keys. Experiments on\nknowledge-probing tasks show that our method leads to efficient navigation away\nfrom explored keys to correct new keys. We further extend our method to open-ended and chain-of-thought (for reasoning) generation. Experiment results show\nthat ICN contributes to better generation diversity and improved self-consistency\nvoting performance. Finally, we discuss potential training issues caused by the\nfixed key space together with the challenges and possible ways to address them in\nfuture research."
    },
    {
        "title": "Reconstructive Visual Instruction Tuning",
        "link_suffix": "/forum?id=8q9NOMzRDg",
        "link": "https://openreview.net/forum?id=8q9NOMzRDg",
        "pdf_link": "https://openreview.net/pdf?id=8q9NOMzRDg",
        "keywords": "Large Multimodal Models, Multimodal Comprehension",
        "abstract": "This paper introduces reconstructive visual instruction tuning (ROSS), a family of Large Multimodal Models (LMMs) that exploit vision-centric supervision signals. In contrast to conventional visual instruction tuning approaches that exclusively supervise text outputs, ROSS prompts LMMs to supervise visual outputs via reconstructing input images. By doing so, it capitalizes on the inherent richness and detail present within input images themselves, which are often lost in pure text supervision. However, producing meaningful feedback from natural images is challenging due to the heavy spatial redundancy of visual signals. To address this issue, ROSS employs a denoising objective to reconstruct latent representations of input images, avoiding directly regressing exact raw RGB values. This intrinsic activation design inherently encourages LMMs to maintain image detail, thereby enhancing their fine-grained comprehension capabilities and reducing hallucinations. Empirically, ROSS consistently brings significant improvements across different visual encoders and language models. In comparison with extrinsic assistance state-of-the-art alternatives that aggregate multiple visual experts, ROSS delivers competitive performance with a single SigLIP visual encoder, demonstrating the efficacy of our vision-centric supervision tailored for visual outputs. The code will be made publicly available upon acceptance."
    },
    {
        "title": "Beyond 2D Representation: Learning 3D Scene Field for Robust Monocular Depth Estimation",
        "link_suffix": "/forum?id=gINO3tfVEP",
        "link": "https://openreview.net/forum?id=gINO3tfVEP",
        "pdf_link": "https://openreview.net/pdf?id=gINO3tfVEP",
        "keywords": "Monocular depth estimation, self-supervised, 3D scene field, 3D geometric",
        "abstract": "Monocular depth estimation has been extensively studied over the past few decades, yet achieving robust depth estimation in real-world scenes remains a challenge, particularly in the presence of reflections, shadow occlusions, and low-texture regions. Existing methods typically rely on extracting front-view 2D features for depth estimation, which often fail to capture those complex physical factors present in real-world scenes, leading to discontinuous, incomplete, or inconsistent depth maps. To address these issues, we turn to learning a more powerful 3D representation for robust monocular depth estimation, and propose a novel self-supervised monocular depth estimation framework based on the Three-dimensional Scene Field  representation, or TSF-Depth for short. Specifically, we build our TSF-Depth framework upon an encoder-decoder architecture. The encoder extracts scene features from the input 2D image, and subsequently reshapes it as a tri-plane feature field by incorporating scene prior encoding. This tri-plane feature field is designed to implicitly model the structure and appearance of the continuous 3D scene. We then estimate a high-quality depth map from the tri-plane feature field by simulating the camera imaging process. To do this, we construct a 2D feature map with 3D geometry by sampling from the tri-plane feature field using the coordinates of points where the line of sight intersects with the scene. The aggregated multi-view geometric features are subsequently fed into the decoder for depth estimation. Extensive experiments on KITTI and NYUv2 datasets show that TSF-Depth achieves state-of-the-art performance. We also validate the generalization capability of our model on Make3D and ScanNet datasets."
    },
    {
        "title": "Open-World Reinforcement Learning over Long Short-Term Imagination",
        "link_suffix": "/forum?id=vzItLaEoDa",
        "link": "https://openreview.net/forum?id=vzItLaEoDa",
        "pdf_link": "https://openreview.net/pdf?id=vzItLaEoDa",
        "keywords": "World models, reinforcement learning, visual control",
        "abstract": "Training visual reinforcement learning agents in a high-dimensional open world presents significant challenges. While various model-based methods have improved sample efficiency by learning interactive world models, these agents tend to be \u201cshort-sighted\u201d, as they are typically trained on short snippets of imagined experiences. We argue that the primary obstacle in open-world decision-making is improving the efficiency of off-policy exploration across an extensive state space. In this paper, we present LS-Imagine, which extends the imagination horizon within a limited number of state transition steps, enabling the agent to explore behaviors that potentially lead to promising long-term feedback. The foundation of our approach is to build a $\\textit{long short-term world model}$. To achieve this, we simulate goal-conditioned jumpy state transitions and compute corresponding affordance maps by zooming in on specific areas within single images. This facilitates the integration of direct long-term values into behavior learning. Our method demonstrates significant improvements over state-of-the-art techniques in MineDojo."
    },
    {
        "title": "BLEND: Behavior-guided Neural Population Dynamics Modeling via Privileged Knowledge Distillation",
        "link_suffix": "/forum?id=jE5ZbtMtcU",
        "link": "https://openreview.net/forum?id=jE5ZbtMtcU",
        "pdf_link": "https://openreview.net/pdf?id=jE5ZbtMtcU",
        "keywords": "Computational Neuroscience, Neural Dynamics Modeling, Behavior as Guidance, Privileged Knowledge Distillation",
        "abstract": "Modeling the nonlinear dynamics of neuronal populations represents a key pursuit in computational neuroscience. Recent research has increasingly focused on jointly modeling neural activity and behavior to unravel their interconnections. Despite significant efforts, these approaches often necessitate either intricate model designs or oversimplified assumptions. Given the frequent absence of perfectly paired neural-behavioral datasets in real-world scenarios when deploying these models, a critical yet understudied research question emerges: how to develop a model that performs well using only neural activity as input at inference, while benefiting from the insights gained from behavioral signals during training?To this end, we proposeBLEND, theBehavior-guided neuraLpopulation dynamics modElling framework via privileged kNowledgeDistillation. By considering behavior as privileged information, we train a teacher model that takes both behavior observations (privileged features) and neural activities (regular features) as inputs. A student model is then distilled using only neural activity. Unlike existing methods, our framework is model-agnostic and avoids making strong assumptions about the relationship between behavior and neural activity. This allows BLEND to enhance existing neural dynamics modeling architectures without developing specialized models from scratch. Extensive experiments across neural population activity modeling and transcriptomic neuron identity prediction tasks demonstrate strong capabilities of BLEND, reporting over 50% improvement in behavioral decoding and over 15% improvement in transcriptomic neuron identity prediction after behavior-guided distillation. Furthermore, we empirically explore various behavior-guided distillation strategies within the BLEND framework and present a comprehensive analysis of effectiveness and implications for model performance."
    },
    {
        "title": "INRscrecon: Enhancing 3D Spatial Transcriptomics Reconstruction through Implicit Neural Representations",
        "link_suffix": "/forum?id=wY5DE4Iuc8",
        "link": "https://openreview.net/forum?id=wY5DE4Iuc8",
        "pdf_link": "https://openreview.net/pdf?id=wY5DE4Iuc8",
        "keywords": "Spatial Transcriptomics reconstruction, Implicit Neural Representations, alignment",
        "abstract": "Single-cell spatial transcriptomics(scST) technology captures the coordinated of cells to unveil the intricate 3D cellular landscape of tissues. However, the accuracy of 3D tissue atlas is often compromised by experimental data gaps and distortions. Implicit Neural Representations (INRs), known for their ability to continuously encode signals, have shown promise in medical imaging domains such as CT scan reconstruction. We introduce INRscrecon, a novel framework that utilizes the continuity of INRs to precisely predict and correct missing and distorted data sections, thereby enhancing the clarity and accuracy of tissue 3D reconstructions. This approach effectively addresses misalignments and data inconsistencies inherent in traditional experimental setups, markedly refining the fidelity of 3D spatial transcriptomic reconstructions."
    },
    {
        "title": "COMBO: Compositional World Models for Embodied Multi-Agent Cooperation",
        "link_suffix": "/forum?id=YXRyYkb1im",
        "link": "https://openreview.net/forum?id=YXRyYkb1im",
        "pdf_link": "https://openreview.net/pdf?id=YXRyYkb1im",
        "keywords": "Embodied AI; Multi-agent Planning; Compositional World Model",
        "abstract": "In this paper, we investigate the problem of embodied multi-agent cooperation, where decentralized agents must cooperate given only egocentric views of the world. To effectively plan in this setting, in contrast to learning world dynamics in a single-agent scenario, we must simulate world dynamics conditioned on an arbitrary number of agents' actions given only partial egocentric visual observations of the world. To address this issue of partial observability, we first train generative models to estimate the overall world state given partial egocentric observations. To enable accurate simulation of multiple sets of actions on this world state, we then propose to learn a compositional world model for multi-agent cooperation by factorizing the naturally composable joint actions of multiple agents and compositionally generating the video conditioned on the world state. By leveraging this compositional world model, in combination with Vision Language Models to infer the actions of other agents, we can use a tree search procedure to integrate these modules and facilitate online cooperative planning. We evaluate our methods on three challenging benchmarks with 2-4 agents. The results show our compositional world model is effective and the framework enables the embodied agents to cooperate efficiently with different agents across various tasks and an arbitrary number of agents, showing the promising future of our proposed methods. More videos can be found at \\url{https://combo-iclr.github.io/COMBO/}."
    },
    {
        "title": "Image-level memorization detection via inversion-based inference perturbation",
        "link_suffix": "/forum?id=vwOq7twk7L",
        "link": "https://openreview.net/forum?id=vwOq7twk7L",
        "pdf_link": "https://openreview.net/pdf?id=vwOq7twk7L",
        "keywords": "Text-to-image diffusion model, data memorization detection, DDIM Inversion",
        "abstract": "Recent studies have discovered that widely used text-to-image diffusion models can replicate training samples during image generation, a phenomenon known as memorization. This raises significant concerns regarding data privacy and copyright infringement. Existing detection methods primarily focus on identifying memorized prompts, but a critical challenge remains: directly detecting whether a given image is memorized by the model without access to the original prompts. We refer to this challenge as image-level memorization detection, where current methods fall short. In this work, we first uncover two key characteristics of memorized images under perturbed inference: a notable similarity discrepancy and a large magnitude of text-conditional noise prediction. Building on these insights, we propose Inversion-based Inference Perturbation (IIP), a novel framework for image-level memorization detection. Our approach uses unconditional DDIM inversion to derive latent codes that contain core semantic information of original images and optimizes non-memorized prompt embeddings for effective perturbation. The resulting metrics show distinct characteristics of memorized images compared to non-memorized ones, offering a robust basis for detection. We construct a comprehensive setup for the image-level memorization detection task, carefully curating datasets to simulate realistic memorization scenarios. With this setup, we evaluate our IIP framework across three different memorization settings, demonstrating its state-of-the-art performance in identifying both training and generated memorized images, even in the presence of augmentation defenses."
    },
    {
        "title": "In-Context Learning for Full Bayesian Inference",
        "link_suffix": "/forum?id=a79bwlyUNp",
        "link": "https://openreview.net/forum?id=a79bwlyUNp",
        "pdf_link": "https://openreview.net/pdf?id=a79bwlyUNp",
        "keywords": "In-Context Learning, Prior-Fitted-Networks, Bayesian Inference",
        "abstract": "Transformers have emerged as the dominant architecture in the field of deep learning, with a broad range of applications and remarkable in-context learning (ICL) capabilities. While not yet fully understood, ICL has already proved to be an intriguing phenomenon, allowing transformers to learn in-context---without requiring further training. In this paper, we further advance the understanding of ICL by demonstrating that transformers can perform full Bayesian inference for commonly used statistical models in-context. More specifically, we introduce a general framework that builds on ideas from prior fitted networks and continuous normalizing flows and enables us to infer complex posterior distributions for models such as generalized linear models and latent factor models. Extensive experiments on real-world datasets demonstrate that our ICL approach yields posterior samples that are similar in quality to state-of-the-art MCMC or variational inference methods that do not operate in-context. The source code for this paper is available athttps://anonymous.4open.science/r/ICL_For_Full_Bayesian_Inference-3F53"
    },
    {
        "title": "EnzymeFlow: Generating Reaction-specific Enzyme Catalytic Pockets through Flow Matching and Co-Evolutionary Dynamics",
        "link_suffix": "/forum?id=HGz012J6TQ",
        "link": "https://openreview.net/forum?id=HGz012J6TQ",
        "pdf_link": "https://openreview.net/pdf?id=HGz012J6TQ",
        "keywords": "enzyme design; flow model; protein design; protein evolution",
        "abstract": "Enzyme design is a critical area in biotechnology, with applications ranging from drug development to synthetic biology. Traditional methods for enzyme function prediction or protein binding pocket design often fall short in capturing the dynamic and complex nature of enzyme-substrate interactions, particularly in catalytic processes. To address the challenges, we introduce EnzymeFlow, a generative model that employs flow matching with hierarchical pre-training and enzyme-reaction co-evolution to generate catalytic pockets for specific substrates and catalytic reactions. Additionally, we introduce a large-scale, experimentally validated dataset of enzyme-reaction pairs, specifically designed for the catalytic pocket generation task, comprising a total of $328,192$ pairs. By incorporating evolutionary dynamics and reaction-specific adaptations, EnzymeFlow becomes a powerful model for designing enzyme pockets, which is capable of catalyzing a wide range of biochemical reactions. Experiments on the new dataset demonstrate the model's effectiveness in designing high-quality, functional enzyme catalytic pockets, paving the way for advancements in enzyme engineering and synthetic biology. The EnzymeFlow code can be found athttps://anonymous.4open.science/r/EnzymeFlow-7420."
    },
    {
        "title": "SaRA: High-Efficient Diffusion Model Fine-tuning with Progressive Sparse Low-Rank Adaptation",
        "link_suffix": "/forum?id=wGVOxplEbf",
        "link": "https://openreview.net/forum?id=wGVOxplEbf",
        "pdf_link": "https://openreview.net/pdf?id=wGVOxplEbf",
        "keywords": "Diffusion Model, Fine-tuning",
        "abstract": "The development of diffusion models has led to significant progress in image and video generation tasks, with pre-trained models like the Stable Diffusion series playing a crucial role.\nHowever, a key challenge remains in downstream task applications: how to effectively and efficiently adapt pre-trained diffusion models to new tasks.\nInspired by model pruning which lightens large pre-trained models by removing unimportant parameters, we propose a novel model fine-tuning method to make full use of these ineffective parameters and enable the pre-trained model with new task-specified capabilities.\nIn this work, we first investigate the importance of parameters in pre-trained diffusion models and discover that parameters with the smallest absolute values do not contribute to the generation process due to training instabilities.\nBased on this observation, we propose a fine-tuning method termed SaRA that re-utilizes these temporarily ineffective parameters, equating to optimizing a sparse weight matrix to learn the task-specific knowledge.\nTo mitigate potential overfitting, we propose a nuclear-norm-based low-rank sparse training scheme for efficient fine-tuning.\nFurthermore, we design a new progressive parameter adjustment strategy to make full use of the finetuned parameters.\nFinally, we propose a novel unstructural backpropagation strategy, which significantly reduces memory costs during fine-tuning.\nOur method enhances the generative capabilities of pre-trained models in downstream applications and outperforms existing fine-tuning methods in maintaining model's generalization ability."
    },
    {
        "title": "Microcanonical Langevin Ensembles: Advancing the Sampling of Bayesian Neural Networks",
        "link_suffix": "/forum?id=QMtrW8Ej98",
        "link": "https://openreview.net/forum?id=QMtrW8Ej98",
        "pdf_link": "https://openreview.net/pdf?id=QMtrW8Ej98",
        "keywords": "Sampling, Bayesian Neural Networks",
        "abstract": "Despite recent advances, sampling-based inference for Bayesian Neural Networks (BNNs) remains a significant challenge in probabilistic deep learning. While sampling-based approaches do not require a variational distribution assumption, current state-of-the-art samplers still struggle to navigate the complex and highly multimodal posteriors of BNNs. As a consequence, sampling still requires considerably longer inference times than non-Bayesian methods even for small neural networks, despite recent advances in making software implementations more efficient. Besides the difficulty of finding high-probability regions, the time until samplers provide sufficient exploration of these areas remains unpredictable. To tackle these challenges, we introduce an ensembling approach that leverages strategies from optimization and a recently proposed sampler called Microcanonical Langevin Monte Carlo (MCLMC) for efficient, robust and predictable sampling performance. Compared to approaches based on the state-of-the-art No-U-Turn Sampler, our approach delivers substantial speedups up to an order of magnitude, while maintaining or improving predictive performance and uncertainty quantification across diverse tasks and data modalities. The suggested Microcanonical Langevin Ensembles and modifications to MCLMC additionally enhance the method's predictability in resource requirements, facilitating easier parallelization. All in all, the proposed method offers a promising direction for practical, scalable inference for BNNs."
    },
    {
        "title": "Posterior-Mean Rectified Flow: Towards Minimum MSE Photo-Realistic Image Restoration",
        "link_suffix": "/forum?id=hPOt3yUXii",
        "link": "https://openreview.net/forum?id=hPOt3yUXii",
        "pdf_link": "https://openreview.net/pdf?id=hPOt3yUXii",
        "keywords": "Image restoration, inverse problems, image processing, computer vision, machine learning, generative models, estimation",
        "abstract": "Photo-realistic image restoration algorithms are typically evaluated by distortion measures (e.g., PSNR, SSIM) and by perceptual quality measures (e.g., FID, NIQE), where the desire is to attain the lowest possible distortion without compromising on perceptual quality. To achieve this goal, current methods typically attempt to sample from the posterior distribution, or to optimize a weighted sum of a distortion loss (e.g., MSE) and a perceptual quality loss (e.g., GAN). Unlike previous works, this paper is concerned specifically with theoptimalestimator that minimizes the MSE under a constraint of perfect perceptual index, namely where the distribution of the reconstructed images is equal to that of the ground-truth ones. A recent theoretical result shows that such an estimator can be constructed by optimally transporting the posterior mean prediction (MMSE estimate) to the distribution of the ground-truth images. Inspired by this result, we introduce Posterior-Mean Rectified Flow (PMRF), a simple yet highly effective algorithm that approximates this optimal estimator. In particular, PMRF first predicts the posterior mean, and then transports the result to a high-quality image using a rectified flow model that approximates the desired optimal transport map. We investigate the theoretical utility of PMRF and demonstrate that it consistently outperforms previous methods on a variety of image restoration tasks."
    },
    {
        "title": "Deep Weight Factorization: Sparse Learning Through the Lens of Artificial Symmetries",
        "link_suffix": "/forum?id=vNdOHr7mn5",
        "link": "https://openreview.net/forum?id=vNdOHr7mn5",
        "pdf_link": "https://openreview.net/pdf?id=vNdOHr7mn5",
        "keywords": "Sparsity, Regularization, Neural Networks, Overparametrization",
        "abstract": "Sparse regularization techniques are well-established in machine learning, yet their application in neural networks remains challenging due to the non-differentiability of penalties like the $L_1$ norm, which is incompatible with stochastic gradient descent. A promising alternative is shallow weight factorization, where weights are decomposed into two factors, allowing for smooth optimization of $L_1$-penalized neural networks by adding differentiable $L_2$ regularization to the factors. \nIn this work, we introduce deep weight factorization, extending previous shallow approaches to more than two factors. We theoretically establish equivalence of our deep factorization with non-convex sparse regularization and analyze its impact on training dynamics and optimization. Due to the limitations posed by standard training practices, we propose a tailored initialization scheme and identify important learning rate requirements necessary for training factorized networks.\nWe demonstrate the effectiveness of our deep weight factorization through experiments on various architectures and datasets, consistently outperforming its shallow counterpart and widely used pruning methods."
    },
    {
        "title": "Multi-Agent Collaborative Data Selection for Efficient Language Model Pretraining",
        "link_suffix": "/forum?id=1fwZJzGdKj",
        "link": "https://openreview.net/forum?id=1fwZJzGdKj",
        "pdf_link": "https://openreview.net/pdf?id=1fwZJzGdKj",
        "keywords": "Language Model Pretraining; Data-efficient Training; Data Selection",
        "abstract": "Efficient data selection is crucial to accelerate the pretraining of large language models (LLMs). While various methods have been proposed to enhance data efficiency, limited research has addressed the inherent conflicts between these approaches to achieve optimal data selection for LLM pretraining. To tackle this problem, we propose a novel multi-agent collaborative data selection mechanism. In this framework, each data selection method serves as an independent agent, and an agent console is designed to dynamically integrate the information from all agents throughout the LLM training process. We conduct extensive empirical studies to evaluate our multi-agent framework. The experimental results demonstrate that our approach significantly improves data efficiency, accelerates convergence in LLM training, and achieves an average performance gain of 10.5% across multiple language model benchmarks compared to the state-of-the-art methods."
    },
    {
        "title": "Neural Tangent Kernel Analysis and Filtering for Robust Fourier Feature Embedding",
        "link_suffix": "/forum?id=NrLXQWwCMg",
        "link": "https://openreview.net/forum?id=NrLXQWwCMg",
        "pdf_link": "https://openreview.net/pdf?id=NrLXQWwCMg",
        "keywords": "Implicit Neural Representation, Spectral Bias, Positional Encoding, Fourier Features",
        "abstract": "Implicit Neural Representations (INRs) employ neural networks to represent continuous functions by mapping coordinates to the corresponding values of the target function, with applications e.g., inverse graphics. However, INRs face a challenge known as spectral bias when dealing with scenes containing varying frequencies. To overcome spectral bias, the most common approach is the Fourier features-based methods such as positional encoding. However, Fourier features-based methods will introduce noise to output, which degrades their performances when applied to downstream tasks. In response, this paper addresses this problem by first investigating the underlying causes through the lens of the Neural Tangent Kernel. Through theoretical analysis, we propose that using Fourier features embedding can be interpreted as fitting Fourier series expansion of the target function, from which we find that it is the insufficiency in the finitely sampled frequencies that causes the generation of noisy outputs. Leveraging these insights, we introduce bias-free MLPs as an adaptive linear filter to locally suppress unnecessary frequencies while amplifying essential ones by adjusting the coefficients at the coordinate level. Additionally, we propose a line-search-based algorithm to adjust the filter's learning rate dynamically, achieving Pareto efficiency between the adaptive linear filter module and the INRs. Extensive experiments demonstrate that our proposed method consistently improves the performance of INRs on typical tasks, including image regression, 3D shape regression, and inverse graphics."
    },
    {
        "title": "Token-by-Token Election: Improving Language Model Reasoning through Token-Level Multi-model Collaboration",
        "link_suffix": "/forum?id=QPZy2XMgzn",
        "link": "https://openreview.net/forum?id=QPZy2XMgzn",
        "pdf_link": "https://openreview.net/pdf?id=QPZy2XMgzn",
        "keywords": "LLM, multi-model collaboration",
        "abstract": "With the continuous development of large language models (LLMs), they have demonstrated amazing capabilities in many areas of natural language processing (NLP). However, due to their inherent limitations, the performance of a single model on many complex reasoning tasks has reached a bottleneck. A feasible solution is to introduce external feedback to further improve model performance, among which multi-model collaboration is a particularly promising approach. In this paper, we propose token-by-token election (TTE), a novel token-level multi-model collaboration strategy. Different from the common multi-model collaboration methods that operates at the overall answer level, TTE performs multi-model elections at the lowest token level. It selects the optimal token from the next token distributions given by multiple LLMs and then generates the answer autoregressively, allowing multiple LLMs to reach a consensus on each token. Inspired by human behavior, TTE consists of three election modes, including Cooperation, Competition, and Counting, all of which aim to sample the optimal token from multiple distributions. By strictly controlling the generation quality of each token, TTE can improve the quality of the overall answer and break through the performance bottleneck of a single LLM. Through extensive experiments on a variety of different types of reasoning benchmarks, we demonstrate the powerful performance of TTE, which further improves the performance compared to the current state-of-the-art single LLM and other multi-model collaborative methods. The code will be released on GitHub."
    },
    {
        "title": "MotionClone: Training-Free Motion Cloning for Controllable Video Generation",
        "link_suffix": "/forum?id=aY3L65HgHJ",
        "link": "https://openreview.net/forum?id=aY3L65HgHJ",
        "pdf_link": "https://openreview.net/pdf?id=aY3L65HgHJ",
        "keywords": "controllable video image, viusal representation, text-to-video generation",
        "abstract": "Motion-based controllable video generation offers the potential for creating captivating visual content. Existing methods typically necessitate model training to encode particular motion cues or incorporate fine-tuning to inject certain motion patterns, resulting in limited flexibility and generalization.\nIn this work, we propose MotionClone, a training-free framework that enables motion cloning from reference videos to versatile motion-controlled video generation, including text-to-video and image-to-video. Based on the observation that the dominant components in temporal-attention maps drive motion synthesis, while the rest mainly capture noisy or very subtle motions, MotionClone utilizes sparse temporal attention weights as motion representations for motion guidance, facilitating diverse motion transfer across varying scenarios. Meanwhile, MotionClone allows for the direct extraction of motion representation through a single denoising step, bypassing the cumbersome inversion processes and thus promoting both efficiency and flexibility. \nExtensive experiments demonstrate that MotionClone exhibits proficiency in both global camera motion and local object motion, with notable superiority in terms of motion fidelity, textual alignment, and temporal consistency."
    },
    {
        "title": "Learning View-invariant World Models for Visual Robotic Manipulation",
        "link_suffix": "/forum?id=vJwjWyt4Ed",
        "link": "https://openreview.net/forum?id=vJwjWyt4Ed",
        "pdf_link": "https://openreview.net/pdf?id=vJwjWyt4Ed",
        "keywords": "Robotic manipulation, reinforcement learning, world model",
        "abstract": "Robotic manipulation tasks often rely on visual inputs from cameras to perceive the environment. However, previous approaches still suffer from performance degradation when the camera\u2019s viewpoint changes during manipulation. In this paper, we propose ReViWo Representation learning for View-invariant World model), leveraging multi-view data to learn robust representations for control under viewpoint disturbance. ReViWo utilizes an autoencoder framework to reconstruct target images by an architecture that combines view-invariant representation (VIR) and view-dependent representation. To train ReViWo, we collect multi-view data in simulators with known view labels, meanwhile, ReViWo is simutaneously trained on Open X-Embodiment datasets without view labels. The VIR is then used to train a world model on pre-collected manipulation data and a policy through interaction with the world model. We evaluate the effectiveness of ReViWo in various viewpoint disturbance scenarios, including control under novel camera positions and frequent camera shaking, using the Meta-world and PandaGym robotics environments. The results demonstrate that ReViWo maintains robust performance under viewpoint disturbance, while baseline methods suffer from significant performance degradation. Furthermore, we show that the VIR captures task-relevant state information and remains stable for observations from novel viewpoints, validating the efficacy of the ReViWo approach."
    },
    {
        "title": "Recovery of Causal Graph Involving Latent Variables via Homologous Surrogates",
        "link_suffix": "/forum?id=fGhr39bqZa",
        "link": "https://openreview.net/forum?id=fGhr39bqZa",
        "pdf_link": "https://openreview.net/pdf?id=fGhr39bqZa",
        "keywords": "causal discovery, latent variables",
        "abstract": "Causal discovery with latent variables is an important and challenging problem. To identify latent variables and infer their causal relations, most existing works rely on the assumption that each latent variable has multiple pure children that must have no other parent except the latent variable itself. Considering that this assumption is potentially restrictive in practice and not strictly necessary in theory, by introducing the concept of homologous surrogate, this paper eliminates the need for pure child in the context of causal discovery with latent variables. The former fundamentally differs from the latter in the sense that a homologous surrogate is allowed to have other parents besides the latent variable it represents. We formulate two assumptions involving homologous surrogates and develop theoretical results under each assumption. Under the weaker assumption, our theoretical results imply that we can determine each variable's ancestors, that is, partially recover the causal graph. The stronger assumption further enables us to determine each variable's parents exactly, that is, fully recover the causal graph. Building on these theoretical results, we derive an algorithm that fully leverages the properties of homologous surrogates for causal graph recovery. Also, we validate its efficacy through experiments. Our work broadens the applicability of causal discovery."
    }
]