[
    {
        "title": "The Value of Sensory Information to a Robot",
        "link_suffix": "/forum?id=ikr5XomWHS",
        "link": "https://openreview.net/forum?id=ikr5XomWHS",
        "pdf_link": "https://openreview.net/pdf?id=ikr5XomWHS",
        "keywords": "robotics, limited sensing, perception, imitation learning, reinforcement learning, planning",
        "abstract": "A decision-making agent, such as a robot, must observe and react to any new task-relevant information that becomes available from its environment. We seek to study a fundamental scientific question: what value does sensory information hold to an agent at various moments in time during the execution of a task? Towards this, we empirically study agents of varying architectures, generated with varying policy synthesis approaches (imitation, RL, model-based control), on diverse robotics tasks. For each robotic agent, we characterize its regret in terms of performance degradation when state observations are withheld from it at various task states for varying lengths of time. We find that sensory information is surprisingly rarely task-critical in many commonly studied task setups. Task characteristics such as stochastic dynamics largely dictate the value of sensory information for a well-trained robot; policy architectures such as planning vs. reactive control generate more nuanced second-order effects. Further, sensing efficiency is curiously correlated with task proficiency: in particular, fully trained high-performing agents are more robust to sensor loss than novice agents early in their training. Overall, our findings characterize the tradeoffs between sensory information and task performance in practical sequential decision making tasks, and pave the way towards the design of more resource-efficient decision-making agents."
    },
    {
        "title": "Sensor-Invariant Tactile Representation",
        "link_suffix": "/forum?id=RnJY9WcpA3",
        "link": "https://openreview.net/forum?id=RnJY9WcpA3",
        "pdf_link": "https://openreview.net/pdf?id=RnJY9WcpA3",
        "keywords": "Tactile sensing, representation learning",
        "abstract": "High-resolution tactile sensors have become critical for embodied perception and robotic manipulation. \nHowever, a key challenge in the field is the lack of transferability between sensors due to design and manufacturing variations, which result in significant differences in tactile signals. \nThis limitation hinders the ability to transfer models or knowledge learned from one sensor to another. \nTo address this, we introduce a novel method for extracting Sensor-Invariant Tactile Representations (SITR), enabling zero-shot transfer across optical tactile sensors. \nOur approach utilizes a transformer-based architecture trained on a diverse dataset of simulated sensor designs, allowing it to generalize to new sensors in the real world with minimal calibration. \nExperimental results demonstrate the methodâ€™s effectiveness across various tactile sensing applications, facilitating data and model transferability for future advancements in the field."
    },
    {
        "title": "NutriBench: A Dataset for Evaluating Large Language Models in Nutrition Estimation from Meal Descriptions",
        "link_suffix": "/forum?id=6LtdZCyuZR",
        "link": "https://openreview.net/forum?id=6LtdZCyuZR",
        "pdf_link": "https://openreview.net/pdf?id=6LtdZCyuZR",
        "keywords": "Large Language Models, Nutrition Estimation, Dataset and Benchmark, AI for healthcare",
        "abstract": "Accurate nutrition estimation helps people make informed dietary choices and is essential in the prevention of serious health complications. We present NutriBench, the first publicly available natural language meal description nutrition benchmark. NutriBench consists of 11,857 meal descriptions generated from real-world global dietary intake data. The data is human-verified and annotated with macro-nutrient labels, including carbohydrates, proteins, fats, and calories. We conduct an extensive evaluation of Nutribench on the task of carbohydrate estimation, testing twelve leading Large Language Models (LLMs), including GPT-4o, Llama3.1, Qwen2, Gemma2, and OpenBioLLM models, using standard, Chain-of-Thought and Retrieval-Augmented Generation strategies. Additionally, we present a study involving professional nutritionists, finding that LLMs can provide more accurate and faster estimates. Finally, we perform a real-world risk assessment by simulating the effect of carbohydrate predictions on the blood glucose levels of individuals with type 1 diabetes. Our work highlights the opportunities and challenges of using LLMs for nutrition estimation, demonstrating their potential to aid professionals and laypersons and improve health outcomes. We will make our benchmark publicly available."
    },
    {
        "title": "GOttack: Universal Adversarial Attacks on Graph Neural Networks via Graph Orbits Learning",
        "link_suffix": "/forum?id=YbURbViE7l",
        "link": "https://openreview.net/forum?id=YbURbViE7l",
        "pdf_link": "https://openreview.net/pdf?id=YbURbViE7l",
        "keywords": "graphlet, orbit, adversarial machine learning, graph mining, graph convolutional networks, semi-supervised learning",
        "abstract": "Graph Neural Networks (GNNs) have demonstrated superior performance in node classification tasks across diverse applications. However, their vulnerability to adversarial attacks, where minor perturbations can mislead model predictions, poses significant challenges. This study introduces GOttack, a novel adversarial attack framework that exploits the topological structure of graphs to undermine the integrity of GNN predictions systematically.By defining a topology-aware method to manipulate graph orbits, our approach can generate adversarial modifications that are both subtle and effective, posing a severe test to the robustness of GNNs. We evaluate the efficacy of GOttack across multiple prominent GNN architectures using standard benchmark datasets. Our results show that GOttack outperforms existing state-of-the-art adversarial techniques and completes training in approximately 55% of the time required by the fastest competing model, achieving the highest average misclassification rate in 155 tasks. \nThis work not only sheds light on the susceptibility of GNNs to structured adversarial attacks but also shows that certain topological patterns may play a significant role in the underlying robustness of the GNNs."
    },
    {
        "title": "From Feature Visualization to Visual Circuits:  Effect of Model Perturbation",
        "link_suffix": "/forum?id=YomQ3llPD2",
        "link": "https://openreview.net/forum?id=YomQ3llPD2",
        "pdf_link": "https://openreview.net/pdf?id=YomQ3llPD2",
        "keywords": "feature visualization, visual circuits, robustness of interpretability, adversarial model manipulation",
        "abstract": "Understanding the inner workings of large-scale deep neural networks is challenging yet crucial in several high-stakes applications. Mechanistic interpretability is an emergent field that tackles this challenge, often by identifying human-understandable subgraphs in deep neural networks known as circuits. In vision-pretrained models, these subgraphs are typically interpreted by visualizing their node features through a popular technique called feature visualization. Recent works have analyzed the stability of different feature visualization types under the adversarial model manipulation framework. This paper addresses limitations in existing works by proposing a novel attack called ProxPulse that simultaneously manipulates two types of feature visualizations. Surprisingly, when analyzing these attacks within the context of visual circuits, we find that visual circuits exhibit some robustness to ProxPulse. Consequently, we introduce a new attack based on ProxPulse that reveals the manipulability of visual circuits, highlighting their lack of robustness. The effectiveness of these attacks is validated across a range of pre-trained models, from smaller architectures like AlexNet to medium-scale models like ResNet-50, and larger ones such as ResNet-152 and DenseNet-201 on the ImageNet dataset."
    },
    {
        "title": "Can Models Help us Create Better Models? Evaluating LLMs as Data Scientists",
        "link_suffix": "/forum?id=cNThpik3Jz",
        "link": "https://openreview.net/forum?id=cNThpik3Jz",
        "pdf_link": "https://openreview.net/pdf?id=cNThpik3Jz",
        "keywords": "llm, data science, benchmark, tabular data, feature engineering, dataset, kaggle, evaluation, code generation",
        "abstract": "We present a benchmark for large language models designed to tackle one of the most knowledge-intensive tasks in data science: writing feature engineering code, which requires domain knowledge in addition to a deep understanding of the underlying problem and data structure. The model is provided with a dataset description in a prompt and asked to generate code transforming it. The evaluation score is derived from the improvement achieved by an XGBoost model fitted on the modified dataset compared to the original data. By an extensive evaluation of state-of-the-art models and comparison to well-established benchmarks, we demonstrate that the FeatEng of our proposal can cheaply and efficiently asses the broad capabilities of LLMs, in contrast to the existing methods."
    },
    {
        "title": "Improving Adversarial Transferability in MLLMs via Dynamic Vision-Language Alignment Attack",
        "link_suffix": "/forum?id=YzFNJ571A7",
        "link": "https://openreview.net/forum?id=YzFNJ571A7",
        "pdf_link": "https://openreview.net/pdf?id=YzFNJ571A7",
        "keywords": "Adversarial Attack, Adversarial Transferability, Multimodal Large Language model, Multimodal Large Language Model Robustness",
        "abstract": "Multimodal Large Language Models (MLLMs), built upon LLMs, have recently gained attention for their capabilities in image recognition and understanding. However, while MLLMs are vulnerable to adversarial attacks, the transferability of these attacks across different models remains limited, especially under targeted attack setting. Existing methods primarily focus on vision-specific perturbations but struggle with the complex nature of vision-language modality alignment. In this work, we introduce the Dynamic Vision-Language Alignment (DynVLA) Attack, a novel approach that injects dynamic perturbations into the vision-language connector to enhance generalization across diverse vision-language alignment of different models. Our experimental results show that DynVLA significantly improves the transferability of adversarial examples across various MLLMs, including BLIP2, InstructBLIP, MiniGPT4, LLaVA, and closed-source models such as Gemini."
    },
    {
        "title": "Proto Successor Measure: Representing the space of all possible solutions of Reinforcement Learning",
        "link_suffix": "/forum?id=s9SVlWOcLt",
        "link": "https://openreview.net/forum?id=s9SVlWOcLt",
        "pdf_link": "https://openreview.net/pdf?id=s9SVlWOcLt",
        "keywords": "Zero-Shot Reinforcement Learning, Representation Learning, Unsupervised RL",
        "abstract": "Having explored an environment, intelligent agents should be able to transfer their knowledge to most downstream tasks within that environment. \nReferred to as ``zero-shot learning,\" this ability remains elusive for general-purpose reinforcement learning algorithms.  While recent works have attempted to produce zero-shot RL agents, they make assumptions about the nature of the tasks or the structure of the MDP. We present \\emph{Proto Successor Measure}: the basis set for all possible solutions of Reinforcement Learning in a dynamical system. We provably show that any possible policy can be represented using an affine combination of these policy independent basis functions. Given a reward function at test time, we simply need to find the right set of linear weights to combine these basis corresponding to the optimal policy.  We derive a practical algorithm to learn these basis functions using only interaction data from the environment and show that our approach can produce the optimal policy at test time for any given reward function without additional environmental interactions."
    },
    {
        "title": "Visual Representations in Humans and Machines: A Comparative Analysis of Artificial and Biological Neural Responses to Naturalistic Dynamic Visual Stimuli",
        "link_suffix": "/forum?id=M3y2msIfHZ",
        "link": "https://openreview.net/forum?id=M3y2msIfHZ",
        "pdf_link": "https://openreview.net/pdf?id=M3y2msIfHZ",
        "keywords": "self-supervised learning, visual representation, occipitotemporal cortex, human vision, masked autoencoders",
        "abstract": "Visual representations in the human brain are shaped by the pressure to support planning and interactions with the environment. Do visual representations in deep network models converge with visual representations in humans? Here, we investigate this question for a new class of effective self-supervised models: Masked Autoencoders (MAEs). We compare image MAEs and video MAEs to neural responses in humans as well as convolutional neural networks. The results reveal that representations learned by MAEs diverge from neural representations in humans and convolutional neural networks. Fine-tuning MAEs with a supervised task improves their correspondence with neural responses but is not sufficient to bridge the gap that separates them from supervised convolutional networks. Finally, video MAEs show closer correspondence to neural representations than image MAEs, revealing an important role of temporal information. However, convolutional networks based on optic flow show a closer correspondence to neural responses in humans than even video MAEs, indicating that while masked autoencoding yields visual representations that are effective at multiple downstream tasks, it is not sufficient to learn representations that converge with human vision."
    },
    {
        "title": "In-context Fine-tuning for Time-series Foundation Models",
        "link_suffix": "/forum?id=ryIHtXE9uG",
        "link": "https://openreview.net/forum?id=ryIHtXE9uG",
        "pdf_link": "https://openreview.net/pdf?id=ryIHtXE9uG",
        "keywords": "time-series, foundation models, zero-shot, few-shot, in-context",
        "abstract": "Motivated by the recent success of time-series foundation models for zero-shot forecasting, we present a methodology forin-context fine-tuningof a time-series foundation model. In particular, we design a pretrained foundation model that can be prompted (at inference time) with multiple time-series examples, in order to forecast a target time-series into the future. Our foundation model is specifically trained to utilize examples from multiple related time-series in its context window (in addition to the history of the target time-series) to help it adapt to the specific distribution of the target domain at inference time.  We show that such a foundation model that uses in-context examples  at inference time can obtain much better performance on popular forecasting benchmarks compared to supervised deep learning methods, statistical models as well as other time-series foundation models.  Interestingly, our in-context fine-tuning approach even rivals the performance of a foundation model that is explicitly fine-tuned on the target domain."
    },
    {
        "title": "JudgeRank: Leveraging Large Language Models for Reasoning-Intensive Reranking",
        "link_suffix": "/forum?id=tZiMLgsHMu",
        "link": "https://openreview.net/forum?id=tZiMLgsHMu",
        "pdf_link": "https://openreview.net/pdf?id=tZiMLgsHMu",
        "keywords": "reranking, retrieval, generation",
        "abstract": "Accurate document retrieval is crucial for the success of retrieval-augmented generation (RAG) applications, including open-domain question answering and code completion. While large language models (LLMs) have been employed as dense encoders or listwise rerankers in RAG systems, they often struggle with reasoning-intensive tasks because they lack nuanced analysis when judging document relevance. To address this limitation, we introduce JudgeRank, a novel agentic reranker that emulates human cognitive processes when assessing document relevance. Our approach consists of three key steps: (1) query analysis to identify the core problem, (2) document analysis to extract a query-aware summary, and (3) relevance judgment to provide a concise assessment of document relevance. We evaluate JudgeRank on the reasoning-intensive BRIGHT benchmark, demonstrating substantial performance improvements over first-stage retrieval methods and outperforming other popular reranking approaches. In addition, JudgeRank performs on par with fine-tuned state-of-the-art rerankers on the popular BEIR benchmark, validating its zero-shot generalization capability. Through comprehensive ablation studies, we demonstrate that JudgeRank's performance generalizes well across LLMs of various sizes while ensembling them yields even more accurate reranking than individual models."
    },
    {
        "title": "On the Hardness of Faithful Chain-of-Thought Reasoning in Large Language Models",
        "link_suffix": "/forum?id=1OyE9IK0kx",
        "link": "https://openreview.net/forum?id=1OyE9IK0kx",
        "pdf_link": "https://openreview.net/pdf?id=1OyE9IK0kx",
        "keywords": "Trustworthy Machine Learning, Explainability, Interpretability, Faithfulness, Large Language Models",
        "abstract": "As Large Language Models (LLMs) are being increasingly employed in critical domains such as healthcare, it is essential to make these models trustworthy. In this pursuit, Chain-of-Thought (CoT) prompting has emerged as a potential source of transparency in LLMs. While CoT reasoning is appealing to humans, prior studies have shown that these reasoning chains are not faithful i.e.; they do not accurately reflect the underlying LLM's behavior. Ensuring the faithfulness of LLM-generated CoT reasoning is crucial for decision-makers, who rely on them to determine if, when, and to what extent, trust the recommendations made by these models. While several works proposed strategies to enhance accuracy and truthfulness in LLMs, there has been a lack of exploration on the effectiveness of these common strategies to enhance the faithfulness of chain-of-thought (CoT) reasoning. Specifically, we explore the promise of in-context learning, fine-tuning, and activation editing to improve the faithfulness of the CoT reasoning. Our empirical analyses on benchmark tasks indicate that these strategies offer limited success in improving the faithfulness of the CoT reasoning, with only slight performance enhancements in controlled scenarios. Activation editing demonstrated minimal success, while fine-tuning and in-context learning achieved marginal improvements that failed to generalize across reasoning and truthful question-answering benchmarks. We subsequently analyse what makes faithful CoT reasoning challenging, and present findings to lay the groundwork for future research in trustworthy reasoning from LLMs.  In summary, our work underscores the inherent difficulty in eliciting faithful CoT reasoning from LLMs, suggesting that the current array of approaches may not be sufficient to address this challenge."
    },
    {
        "title": "NatureLM-audio: an Audio-Language Foundation Model for Bioacoustics",
        "link_suffix": "/forum?id=hJVdwBpWjt",
        "link": "https://openreview.net/forum?id=hJVdwBpWjt",
        "pdf_link": "https://openreview.net/pdf?id=hJVdwBpWjt",
        "keywords": "audio-language foundation models, multimodal large language models (llms), bioacoustics, animal vocalizations, zero-shot learning, in-context learning",
        "abstract": "Large language models (LLMs) prompted with text and audio represent the state of the art in various auditory tasks, including speech, music, and general audio, showing emergent abilities on unseen tasks. However, these capabilities have yet to be fully demonstrated in bioacoustics tasks, such as detecting animal vocalizations in large recordings, classifying rare and endangered species, and labeling context and behaviorâ€”tasks that are crucial for conservation, biodiversity monitoring, and the study of animal behavior. In this work, we present NatureLM-audio, the first audio-language foundation model specifically designed for bioacoustics. Our carefully curated training dataset comprises text-audio pairs spanning a diverse range of bioacoustics, speech, and music data, designed to address the challenges posed by limited annotated datasets in the field. We demonstrate successful transfer of learned representations to unseen taxa and tasks, and our model shows promising in-context learning capabilities and generalization. Importantly, we test NatureLM-audio on a novel benchmark (BEANS-Zero) and it sets the new state of the art (SotA) on several bioacoustics tasks, including zero-shot classification of unseen species. To advance bioacoustics research, we also open-source the code for generating training and benchmark data, as well as for training the model."
    },
    {
        "title": "Sparse components distinguish visual pathways & their alignment to neural networks",
        "link_suffix": "/forum?id=IqHeDe2lbl",
        "link": "https://openreview.net/forum?id=IqHeDe2lbl",
        "pdf_link": "https://openreview.net/pdf?id=IqHeDe2lbl",
        "keywords": "visual representations, alignment, sparse decomposition, neural pathways, brain and machine vision",
        "abstract": "The ventral, dorsal, and lateral streams in high-level human visual cortex are implicated in distinct functional processes. Yet, deep neural networks (DNNs) trained on a single task model the entire visual system surprisingly well, hinting at common computational principles across these pathways. To explore this inconsistency, we applied a novel sparse decomposition approach to identify the dominant components of visual representations within each stream. Consistent with traditional neuroscience research, we find a clear difference in component response profiles across the three visual streamsâ€”identifying components selective for faces, places, bodies, text, and food in the ventral stream; social interactions, implied motion, and hand actions in the lateral stream; and some less interpretable components in the dorsal stream. Building on this, we introduce Sparse Component Alignment (SCA), a new method for measuring representational alignment between brains and machines that better captures the latent neural tuning of these two visual systems. We find that standard visual DNNs are more aligned with ventral than either dorsal or lateral representations. SCA reveals these distinctions with greater resolution than conventional population-level geometry, offering a measure of representational alignment that is sensitive to a systemâ€™s underlying axes of neural tuning."
    },
    {
        "title": "Privacy-Preserving Personalized Federated Prompt Learning for Multimodal Large Language Models",
        "link_suffix": "/forum?id=Equ277PBN0",
        "link": "https://openreview.net/forum?id=Equ277PBN0",
        "pdf_link": "https://openreview.net/pdf?id=Equ277PBN0",
        "keywords": "Multimodal Large Language Model, Federated Prompt Learning, Personalization, Differential Privacy",
        "abstract": "Multimodal Large Language Models (LLMs) are pivotal in revolutionizing customer support and operations by integrating multiple modalities such as text, images, and audio. Federated Prompt Learning (FPL) is a recently proposed approach that combines pre-trained multimodal LLMs such as vision-language models with federated learning to create personalized, privacy-preserving AI systems. However, balancing the competing goals of personalization, generalization, and privacy remains a significant challenge. Over-personalization can lead to overfitting, reducing generalizability, while stringent privacy measures, such as differential privacy, can hinder both personalization and generalization. In this paper, we propose a Differentially Private Federated Prompt Learning (DP-FPL) approach to tackle this challenge by leveraging a low-rank adaptation scheme to capture generalization while maintaining a residual term that preserves expressiveness for personalization. To ensure privacy, we introduce a novel method where we apply local differential privacy to the two low-rank components of the local prompt, and global differential privacy to the global prompt. Our approach mitigates the impact of privacy noise on the model performance while balancing the tradeoff between personalization and generalization. Extensive experiments demonstrate the effectiveness of our approach over other benchmarks."
    },
    {
        "title": "MetaOptimize: A Framework for Optimizing Step Sizes and Other Meta-parameters",
        "link_suffix": "/forum?id=VRbypIkXrt",
        "link": "https://openreview.net/forum?id=VRbypIkXrt",
        "pdf_link": "https://openreview.net/pdf?id=VRbypIkXrt",
        "keywords": "Optimization, Automatic step-size optimization, Automatic hyperparameter optimization, Continual learning",
        "abstract": "We address the challenge of optimizing meta-parameters (i.e., hyperparameters) in machine learning algorithms, a critical factor influencing training efficiency and model performance. Moving away from the computationally expensive traditional meta-parameter search methods, we introduce MetaOptimize framework that dynamically adjusts meta-parameters, particularly step sizes (also known as learning rates), during training. More specifically, MetaOptimize can wrap around any first-order optimization algorithm, tuning step sizes on the fly to minimize a specific form of regret that accounts for long-term effect of step sizes on training, through a discounted sum of future losses.  We also introduce low complexity variants of MetaOptimize that, in  conjunction with its adaptability to multiple optimization algorithms, demonstrate performance competitive to those of best hand-crafted learning-rate schedules across various machine learning applications."
    },
    {
        "title": "Item Language Model",
        "link_suffix": "/forum?id=QFaj7InstQ",
        "link": "https://openreview.net/forum?id=QFaj7InstQ",
        "pdf_link": "https://openreview.net/pdf?id=QFaj7InstQ",
        "keywords": "large language models, interpretability methods, representation, recommender systems",
        "abstract": "Embeddings are extensively used in many domains to represent information about domain entities in a compressed manner. In recommendation systems, these embeddings are trained to extract meaningful information about an item/user from collaborative filtering data consisting users ratings or implicit feedback on items. These behavioral embeddings are usually not trained on data from language domain, but they encode very useful behavioral information which cannot be described using language. In contrast, in large language models (LLM) this collaborative data and behavioral entities(users/items) are not well represented as they are not textual and are specific to the recommendation system/product. Bridging this gap between behavioral understanding and language understanding can enable new item and language interleaved tasks. In our work we show how we can efficiently adapt rich behavioral embeddings as an additional behavioral input representation in pre-trained LLMs. To achieve this we adapt Querying Transformer technique with a new item contrastive loss and show improved item-text joint understanding in PALM2. Finally, we also demonstrate improved capabilities in recommendation domain over using the behavioral embeddings directly as input to PALM2."
    },
    {
        "title": "TOMVALLEY: EVALUATING THE THEORY OF MIND REASONING OF LLMS IN REALISTIC SOCIAL CONTEXT",
        "link_suffix": "/forum?id=9YhocG0o2l",
        "link": "https://openreview.net/forum?id=9YhocG0o2l",
        "pdf_link": "https://openreview.net/pdf?id=9YhocG0o2l",
        "keywords": "Theory of Mind, Benchmark, Social Reasoning, Large Language Models, Reasoning",
        "abstract": "As large language models (LLMs) are increasingly involved in human society, some studies try to evaluate LLMs' capability of theory of mind (ToM), which is about the understanding and reasoning of others' mental states and possible actions. However, these previous works simplify the ToM capability required in real social contexts during their evaluations. This can be reflected in three aspects: (1) most evaluations focus on astatic mental stateafter several social scenarios while ignoring the changes of mental states across different scenarios; (2) they mainly considerindependent mental states, however different kinds of mental states (beliefs, intentions, and emotions) and actions can influence one another in our real life; (3) there is anabsence of social settings and character profilesin their evaluation, even though humans can effortlessly obtain and utilize this information in ToM reasoning processes. This lack can underestimate the abilities of LLMs. This paper aims to evaluate LLMs' ToM capability in closer alignment with a realistic social context.\nCorrespondingly, we propose a new benchmark, namedToMValley, which alleviates the limitations mentioned above of previous works. Specifically, the benchmark is constructed using a framework that includes four steps: social background determination, mental state sketch, social scenario design, and rule-based question generation. Overall, there are 1100 social contexts and 78100 questions about characters' mental states. The quality of the benchmark is manually verified. Additionally, we evaluate ten popular LLMs onToMValley.  Experimental results suggest that LLMs' performances are significantly inferior to human levels by 11%. Subsequent investigation indicates that LLMs are ineffective at interpreting alterations in mental states across social scenarios. Furthermore, we observe that LLMs are incapable of addressing compositional questions that necessitate multi-hop reasoning within the social context."
    },
    {
        "title": "Optimizing 4D Gaussians for Dynamic Scene Video from Single Landscape Images",
        "link_suffix": "/forum?id=IcYDRzcccP",
        "link": "https://openreview.net/forum?id=IcYDRzcccP",
        "pdf_link": "https://openreview.net/pdf?id=IcYDRzcccP",
        "keywords": "Dynamic Scene Video, 4D Gaussian",
        "abstract": "To achieve realistic immersion in landscape images, fluids such as water and clouds need to move within the image while revealing new scenes from various camera perspectives. Recently, a field called dynamic scene video has emerged, which combines single image animation with 3D photography. These methods use pseudo 3D space, implicitly represented with Layered Depth Images (LDIs). LDIs separate a single image into depth-based layers, which enables elements like water and clouds to move within the image while revealing new scenes from different camera perspectives. However, as landscapes typically consist of continuous elements, including fluids, the representation of a 3D space  separates a landscape image into discrete layers, and it can lead to diminished depth perception and potential distortions depending on camera movement. Furthermore, due to its implicit modeling of 3D space, the output may be limited to videos in the 2D domain, potentially reducing their versatility. In this paper, we propose representing a complete 3D space for dynamic scene video by modeling explicit representations, specifically 4D Gaussians, from a single image. The framework is focused on optimizing 3D Gaussians by generating multi-view images from a single image and creating 3D motion to optimize 4D Gaussians. The most important part of proposed framework is consistent 3D motion estimation, which estimates common motion among multi-view images to bring the motion in 3D space closer to actual motions. As far as we know, this is the first attempt that considers animation while representing a complete 3D space from a single landscape image. Our model demonstrates the ability to provide realistic immersion in various landscape images through diverse experiments and metrics. Extensive experimental results arehttps://anonymous.4open.science/r/ICLR_3D_MOM-7B9E/README.md."
    },
    {
        "title": "TrackTheMind: program-guided adversarial data generation for theory of mind reasoning",
        "link_suffix": "/forum?id=246rHKUnnf",
        "link": "https://openreview.net/forum?id=246rHKUnnf",
        "pdf_link": "https://openreview.net/pdf?id=246rHKUnnf",
        "keywords": "theory of mind reasoning, adversarial data generation, program-guided data generation",
        "abstract": "Do large language models (LLMs) have theory of mind? A plethora of papers and benchmarks have been introduced to evaluate if current models have been able to develop this key ability of social intelligence. However, all rely on limited datasets with simple patterns that can potentially lead to problematic blind spots in evaluation and an overestimation of model capabilities. We introduce TrackTheMind, the first framework to allow large-scale generation of diverse and challenging theory of mind data for robust training and evaluation. Our approach leverages an A* search over a custom domain-specific language to produce complex story structures and novel, diverse, yet plausible scenarios to stress test the limits of LLMs. Our evaluation reveals that state-of-the-art LLMs, such as Llama-3.1-70B and GPT-4o, show accuracies as low as 5% on TrackTheMind-generated data, highlighting the need for more robust theory of mind evaluation. As our generations are a conceptual superset of prior work, fine-tuning on our data yields a 26-point accuracy improvement on the classic ToMi benchmark (Le et al., 2019). TrackTheMind also enables uncovering underlying skills and factors missing for models to show theory of mind, such as unreliable state tracking or data imbalances, which may contribute to models' poor performance on benchmarks."
    },
    {
        "title": "Contrastive Unlearning: A Contrastive Approach to Machine Unlearning",
        "link_suffix": "/forum?id=lgnAEBE1Xq",
        "link": "https://openreview.net/forum?id=lgnAEBE1Xq",
        "pdf_link": "https://openreview.net/pdf?id=lgnAEBE1Xq",
        "keywords": "Machine unlearning, Privacy, Contrastive learning",
        "abstract": "Machine unlearning aims to eliminate the influence of a subset of training samples (i.e., unlearning samples) from a trained model. Effectively and efficiently removing the unlearning samples without negatively impacting the overall model performance is challenging. Existing works mainly exploit input and output space and classification loss, which can result in ineffective unlearning or performance loss. In addition, they utilize on unlearning or remaining samples ineffectively, sacrificing either unlearning efficacy or efficiency. Our main insight is that direct optimization on the representation space utilizing both unlearning and remaining samples can effectively remove influence of unlearning samples while maintaining representations learned from remaining samples. We propose a contrastive unlearning framework, leveraging the concept of representation learning for more effective unlearning. It removes the influence of unlearning samples by contrasting their embeddings against the remaining samples' embeddings so that their embeddings are closer to the embeddings of unseen samples. Experiments on a variety of datasets and models on both class unlearning and sample unlearning showed that contrastive unlearning achieves the best unlearning effects and efficiency with the lowest performance loss compared with the state-of-the-art algorithms."
    },
    {
        "title": "Controllable Context Sensitivity and the Knob Behind It",
        "link_suffix": "/forum?id=Igm9bbkzHC",
        "link": "https://openreview.net/forum?id=Igm9bbkzHC",
        "pdf_link": "https://openreview.net/pdf?id=Igm9bbkzHC",
        "keywords": "analysis, interpretability, mechanistic interpretability, context vs prior knowledge, large language models",
        "abstract": "When making predictions, a language model must trade off how much it relies on its context vs. its prior knowledge. Choosing how sensitive the model is to its context is a fundamental functionality, as it enables them to excel at tasks like retrieval-augmented generation and question-answering.  In this paper, we search for a knob which controls this sensitivity, determining whether these models answer from the context or its prior knowledge. To guide this search, we design a task for controllable context sensitivity. In this task, we first feed the model a context (\"Paris is in England.\") and a question (\"Where is Paris?\"); we then instruct the model to either use its prior or contextual knowledge and evaluate whether it generates the correct answer for both intents (either England or France). When fine-tuned on this task, instruction-tuned versions of Llama-3.1, Mistral-v0.3, and Gemma-2 can solve it with high accuracy (85-95%). Analyzing these high-performing models, we narrow down which layers may be important to context sensitivity using a novel linear time algorithm. Then, in each model, we identify a 1-D subspace in a single layer that encodes whether the model follows context or prior knowledge. Interestingly, while we identify this subspace in a fine-tuned model, we find that setting its value serves as an effective knob in not only that model but also non-fine-tuned instruct and base models of that model family. Finally, we show a strong correlation between a model's performance and how distinctly it separates context-agreeing from context-ignoring answers in this subspace. These results suggest a single subspace facilitates how the model chooses between context and prior knowledge, hinting at a simple fundamental mechanism that controls this behavior."
    },
    {
        "title": "LoRC: Low-Rank Compression for LLMs KV Cache with a Progressive Compression Strategy",
        "link_suffix": "/forum?id=NI8AUSAc4i",
        "link": "https://openreview.net/forum?id=NI8AUSAc4i",
        "pdf_link": "https://openreview.net/pdf?id=NI8AUSAc4i",
        "keywords": "KV Cache Compression, Progressive Compression Strategy",
        "abstract": "The Key-Value (KV) cache is a crucial component in serving transformer-based autoregressive large language models (LLMs), enabling faster inference by storing previously computed KV vectors. However, its memory consumption scales linearly with sequence length and batch size, posing a significant bottleneck in LLM deployment. Existing approaches to mitigate this issue include: (1) efficient attention variants integrated in upcycling stages, which requires extensive parameter tuning thus unsuitable to pre-trained LLMs; (2) KV cache compression at test time, primarily through token eviction policies, which often overlook inter-layer dependencies and can be task-specific.This paper introduces an orthogonal approach to KV cache compression. We propose a low-rank approximation of  KV weight matrices, allowing for plug-in integration with existing transformer-based LLMs without model retraining. To effectively compress KV cache at the weight level, we adjust for layerwise sensitivity and introduce a progressive compression strategy, which is supported by our theoretical analysis on how compression errors accumulate in deep networks. Our method is designed to function without model tuning in upcycling stages or task-specific profiling in test stages. Extensive experiments with LLaMA models ranging from 8B to 70B parameters across various tasks show that our approach significantly reduces the GPU memory footprint while maintaining performance."
    },
    {
        "title": "SimpleStrat: Diversifying Language Model Generation with Stratification",
        "link_suffix": "/forum?id=yLYMFRZkdU",
        "link": "https://openreview.net/forum?id=yLYMFRZkdU",
        "pdf_link": "https://openreview.net/pdf?id=yLYMFRZkdU",
        "keywords": "Diverse Generation; Large Language Models Sampling; Stratified Sampling",
        "abstract": "Generating diverse responses from large language models (LLMs) is crucial for applications such as planning/search and synthetic data generation, where diversity provides distinct answers across generations.\nPrior approaches rely on increasing temperature to increase diversity. However, contrary to popular belief, we show not only does this approach produce lower quality individual generations as temperature increases, but it depends on model's next-token probabilities being similar to the true distribution of answers. We propose SimpleStrat, an alternative approach that uses the language model itself to partition the space into strata. At inference, a random stratum is selected and a sample drawn from within the strata.\nTo measure diversity, we introduce CoverageQA, a dataset of underspecified questions with multiple equally plausible answers, and assess diversity by measuring KL Divergence between the sampling distribution and uniform distribution over valid ground truth answers. As computing a posterior probability for proprietary models is infeasible, we measure recall on ground truth solutions.\nOur evaluation show using SimpleStrat achieves higher recall by 0.05 compared to GPT-4o and 0.36 average reduction in KL Divergence compared to Llama 3."
    },
    {
        "title": "VIPaint: Image Inpainting with Pre-Trained Diffusion Models via Variational Inference",
        "link_suffix": "/forum?id=dAavOuxZvo",
        "link": "https://openreview.net/forum?id=dAavOuxZvo",
        "pdf_link": "https://openreview.net/pdf?id=dAavOuxZvo",
        "keywords": "Diffusion Models, Variational Inference, Image Inpainting",
        "abstract": "Diffusion probabilistic models learn to remove noise that is artificially added to the data during training. Novel data, like images, may then be generated from Gaussian noise through a sequence of de-noising operations. While this Markov process implicitly defines a joint distribution over noise-free images, it is not simple to condition the generative process on masked observations of partial images. A number of heuristic sampling procedures have been proposed for solving inverse problems with diffusion priors, but these approaches do not directly approximate the true conditional distribution imposed by inference queries, and are often ineffective for high distortion levels and large image masks. Moreover, many of these baselines cannot be applied to latent diffusion models which use image encodings for efficiency. We instead develop a hierarchical variational inference algorithm that analytically marginalizes missing features, and uses a rigorous variational bound to optimize a non-Gaussian Markov approximation of the true diffusion posterior. Through extensive experiments with both pixel-based and latent diffusion models of images, we show that our VIPaint method significantly outperforms previous approaches in both the plausibility and diversity of imputations, and is easily generalized to other inverse problems like deblurring and superresolution."
    }
]