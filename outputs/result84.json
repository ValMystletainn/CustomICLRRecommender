[{"title": "Agent Q: Advanced Reasoning and Learning for Autonomous AI Agents", "link_suffix": "/forum?id=LuytzzohTa", "link": "https://openreview.net/forum?id=LuytzzohTa", "pdf_link": "https://openreview.net/pdf?id=LuytzzohTa", "keywords": "Web agent, RLHF, Tree Search, RL", "abstract": "Large Language Models (LLMs) have shown remarkable capabilities in natural language tasks requiring complex reasoning, yet their application in agentic, multi-step reasoning within interactive environments remains a difficult challenge. Traditional supervised pre-training on static datasets falls short in enabling autonomous agent capabilities needed to perform complex decision-making in dynamic settings like web navigation. Previous attempts to bridge this gap through supervised fine-tuning on curated expert demonstrations often suffer from compounding errors and limited exploration data, resulting in sub-optimal policy outcomes. To overcome these challenges, we propose a framework that combines guided Monte Carlo Tree Search (MCTS) search with a self-critique mechanism and iterative fine-tuning on agent interactions using an off-policy variant of the Direct Preference Optimization (DPO) algorithm. Our method allows LLM agents to learn effectively from both successful and unsuccessful trajectories, thereby improving their generalization in complex, multi-step reasoning tasks. We validate our approach in the WebShop environment, a simulated e-commerce platform\u2014where it consistently outperforms behavior cloning and reinforced fine-tuning baseline, and \\textbf{beats average human performance} when equipped with the capability to do online search. In real-world booking\nscenarios, our methodology boosts Llama-3 70B model's zero-shot performance from \\textbf{18.6% to 81.7%} success rate (a \\textbf{340% relative increase}) after a single day of data collection and further to \\textbf{95.4%} with online search. We believe this represents a substantial leap forward in the capabilities of autonomous agents, paving the way for more sophisticated and reliable decision-making in real-world settings.", "title_embedding_index": 4150, "title_abs_embedding_index": 4175}, {"title": "UNIQ: Offline Inverse  Q-learning for Avoiding Undesirable Demonstrations", "link_suffix": "/forum?id=jiGyD6Q2No", "link": "https://openreview.net/forum?id=jiGyD6Q2No", "pdf_link": "https://openreview.net/pdf?id=jiGyD6Q2No", "keywords": "offline imitation learning, safe imitation learning, undesirable demonstration, preference-based learning", "abstract": "We address the problem of offline learning a policy that avoids undesirable demonstrations. Unlike conventional offline imitation learning approaches that aim to imitate expert or near-optimal demonstrations, our setting involves avoiding undesirable behavior (specified using undesirable demonstrations). To tackle this problem, unlike standard imitation learning where the aim is to minimize the distance between learning policy and expert demonstrations, we formulate the learning task as maximizing a statistical distance, in the space of state-action stationary distributions, between the learning policy and the undesirable policy. This significantly different approach results in a novel training objective that necessitates a new algorithm to address it. Our algorithm, UNIQ, tackles these challenges by building on the inverse Q-learning framework, framing the learning problem as a cooperative (non-adversarial) task. We then demonstrate how to efficiently leverage unlabeled data for practical training. Our method is evaluated on standard benchmark environments, where it consistently outperforms state-of-the-art baselines.", "title_embedding_index": 4151, "title_abs_embedding_index": 4176}, {"title": "ShapeGaussian: Dynamic Gaussian Splatting for Monocular Videos with Non-Parametric Shape Regularization", "link_suffix": "/forum?id=d7Wb46YCpu", "link": "https://openreview.net/forum?id=d7Wb46YCpu", "pdf_link": "https://openreview.net/pdf?id=d7Wb46YCpu", "keywords": "scene reconstruction, Gaussian splatting, monocular video, shape prior, 2d keypoints", "abstract": "In this paper, we tackle the challenging and underconstrained problem of reconstructing dynamic objects from monocular videos using a new method we term ShapeGaussian. This approach incorporates shape priors to enhance reconstruction accuracy and chance of success with few multi-view clues. Our methodology employs a two-phase process. In the first stage, we establish a temporally consistent deformation model across frames based on depth maps derived from a pre-trained estimator. The second stage obtains high-quality photo-realistic reconstruction by optimizing 3D Gaussian jointly with non-parametric shape models. Through rendering this combined model into radiance fields, we achieve high-quality, photo-realistic reconstructions of dynamically deforming objects that maintain 3D consistency across novel views. Our results demonstrate that significant improvement over previous methods on human dynamics, particularly in scenarios with scarce multi-view cues, highlighting the persistent challenges and varied approaches in recent research aimed at this inherently complex task.", "title_embedding_index": 4152, "title_abs_embedding_index": 4177}, {"title": "Towards Zero-Shot Generalization in Offline Reinforcement Learning", "link_suffix": "/forum?id=EBT0oymkZb", "link": "https://openreview.net/forum?id=EBT0oymkZb", "pdf_link": "https://openreview.net/pdf?id=EBT0oymkZb", "keywords": "offline reinforcement learning, generalization", "abstract": "In this work, we study offline reinforcement learning (RL) with zero-shot generalization property (ZSG), where the agent has access to an offline dataset including experiences from different environments, and the goal of the agent is to train a policy over the training environments which performs well on test environments without further interaction. Existing work showed that classical offline RL fails to generalize to new, unseen environments. To address such an issue, we propose new offline RL frameworks with ZSG, based on empirical risk minimization or proximal policy optimization. We prove that our frameworks find the near-optimal policy with ZSG both theoretically and empirically, from general environments to specific settings such as linear Markov decision processes (MDPs). Our result serves as a first step in understanding the foundation of the generalization phenomenon in offline reinforcement learning.", "title_embedding_index": 4153, "title_abs_embedding_index": 4178}, {"title": "FlashSampling: Fast and Memory-Efficient Exact Sampling with Group-Gumbel-Max", "link_suffix": "/forum?id=V4Xs283LHH", "link": "https://openreview.net/forum?id=V4Xs283LHH", "pdf_link": "https://openreview.net/pdf?id=V4Xs283LHH", "keywords": "Fast sampling", "abstract": "Sampling operations in discrete space are widely used in different fields such as language models, reinforcement learning, VAE, GAN, and neural architecture search. Current sampling methods involve computing the softmax operation across the entire categories, leading to significant computational and memory requirements, particularly when dealing with large sampling categories. This paper presents a novel sampling approach known as FlashSampling, designed to alleviate the computational and communication overhead by circumventing the computation of the softmax operation. Our method maintains mathematical equivalence to conventional sampling strategies while demonstrating significantly enhanced speed and memory efficiency. This is achieved by partitioning the category into distinct groups for independent sampling and then leveraging the Gumble-Max trick to eliminate the need for softmax computation. We substantiate the correctness and efficacy of our method both through mathematical proofs and empirical validation. Extensive experimental outcomes illustrate marked enhancements in speed and memory utilization, with FlashSampling attaining up to 384% faster sampling times and 1822% reduced memory consumption.", "title_embedding_index": 4154, "title_abs_embedding_index": 4179}, {"title": "MedFuzz: Exploring the Robustness of Large Language Models in Medical Question Answering", "link_suffix": "/forum?id=00ezkB2iZf", "link": "https://openreview.net/forum?id=00ezkB2iZf", "pdf_link": "https://openreview.net/pdf?id=00ezkB2iZf", "keywords": "large language model, adversarial machine learning, automatic red teaming", "abstract": "Large language models (LLM) have achieved impressive performance on medical question-answering benchmarks. However, high benchmark accuracy does not imply robust performance in real-world clinical settings. Medical question-answering benchmarks rely on assumptions consistent with quantifying LLM performance but that may not hold in the open world of the clinic. Yet LLMs learn broad knowledge that could help the LLM perform in practical conditions regardless of unrealistic assumptions in celebrated benchmarks. We seek to quantify how robust LLM medical question-answering benchmark performance is to violations of unrealistic benchmark assumptions.  Specifically, we present an adversarial method that we call MedFuzz (for medical fuzzing).  MedFuzz attempts to modify benchmark questions in ways aimed at confounding the LLM. We demonstrate the approach by targeting unrealistic assumptions about patient characteristics presented in the MedQA benchmark. Successful \"attacks\" modify a benchmark item in ways that would be unlikely to fool a medical expert but nonetheless \"trick\" the LLM into changing from a correct to an incorrect answer. Further, we present a non-parametric test for calculating the statistic significance of a successful attack. We show how to use calculate \"MedFuzzed\" performance on a medical QA benchmark, as well to find individual cases of statistically significant successful attacks. The methods show promise at providing insights into the ability of an LLM to operate robustly in more realistic settings.", "title_embedding_index": 4155, "title_abs_embedding_index": 4180}, {"title": "CLIP-Enhance: Improving CLIP Zero-Shot Classification via von Mises-Fisher Clustering", "link_suffix": "/forum?id=KyeyEFPxJX", "link": "https://openreview.net/forum?id=KyeyEFPxJX", "pdf_link": "https://openreview.net/pdf?id=KyeyEFPxJX", "keywords": "zero-shot classification, multi-modal representation learning, knowledge distillation, CLIP", "abstract": "Contrastive language-image pre-training (CLIP) has revolutionized computer vision by integrating natural language understanding with image analysis, enabling zero-shot classification without prior training on specific classes. However, recent efforts to improve the performance of frozen CLIP models through prompt tuning and adapter mechanisms have introduced additional system complexity and training requirements, thus undermining CLIP's inherent efficiency in zero-shot knowledge transfer. In this paper, we propose to address two common challenges in zero-shot classification using CLIP: 1) the misalignment between textual and image embeddings, and 2) the long-tailed distribution of CLIP's training dataset. Our approach, CLIP-Enhance, is motivated by a re-interpretation of CLIP zero-shot classification as a clustering problem on a hypersphere using a von Mises-Fisher mixture model. Inspired by the DINO self-supervised learning framework, we optimize this mixture model to simultaneously improve the alignment of textual and image embeddings as well as represent data distribution disparities between training and evaluation datasets. Empirically, we show that jointly optimizing for both embedding alignment and concentration via self-supervised learning improves CLIP zero-shot classification significantly across multiple benchmark datasets. We also show empirically how CLIP-Enhance mitigates problems (1) and (2), as well as its robustness to limited data through a series of additional experiments.", "title_embedding_index": 4156, "title_abs_embedding_index": 4181}, {"title": "VISAGNN: Versatile Staleness-Aware Training for Efficient Large-Scale GNNs", "link_suffix": "/forum?id=wrVZ771SZQ", "link": "https://openreview.net/forum?id=wrVZ771SZQ", "pdf_link": "https://openreview.net/pdf?id=wrVZ771SZQ", "keywords": "Graph machine learning, Large scale GNNs, Staleness awareness", "abstract": "Graph Neural Networks (GNNs) have shown exceptional success in graph representation learning and a wide range of real-world applications. However, scaling deeper GNNs poses challenges due to the neighbor explosion problem when training on large-scale graphs. To mitigate this, a promising class of GNN training algorithms utilizes historical embeddings to reduce computation and memory costs while preserving the expressiveness of the model. These methods leverage historical embeddings for out-of-batch nodes, effectively approximating full-batch training without losing any neighbor information\u2014a limitation found in traditional sampling methods. However, the staleness of these historical embeddings often introduces significant bias, acting as a bottleneck that can adversely affect model performance. In this paper, we propose a novel VersatIle Staleness-Aware GNN, named VISAGNN, which dynamically and adaptively incorporates staleness criteria into the large-scale GNN training process. By embedding staleness into the message-passing mechanism, loss function, and historical embeddings during training, our approach enables the model to adaptively mitigate the negative effects of stale embeddings, thereby reducing estimation errors and enhancing downstream accuracy. Comprehensive experiments demonstrate the effectiveness of our method in overcoming the limitations of existing historical embedding techniques, highlighting its superior performance and efficiency on large-scale benchmarks, as well as significantly accelerated convergence. We will make the code publicly available upon acceptance of the work.", "title_embedding_index": 4157, "title_abs_embedding_index": 4182}, {"title": "Temporal Logic-Based Multi-Vehicle Backdoor Attacks against Offline RL Agents in End-to-end Autonomous Driving", "link_suffix": "/forum?id=em0gAL8fbK", "link": "https://openreview.net/forum?id=em0gAL8fbK", "pdf_link": "https://openreview.net/pdf?id=em0gAL8fbK", "keywords": "backdoor attack; autonomous driving safety", "abstract": "End-to-end autonomous driving (AD) systems integrate complex decision-making processes. \nAssessing the safety of these systems against potential security threats, including backdoor attacks, is a stepping stone for real-world deployment. \nHowever, traditional methods focus on static triggers, which do not adequately reflect the dynamic nature of these systems and could be impractical to deploy in the real world.\nTo address these limitations, we propose a novel backdoor attack against the end-to-end AD systems that leverage multi-vehicles' trajectories as triggers.\nWe employ different behavior models and their configurations to generate the trigger trajectories, which are then quantitatively evaluated using temporal logic specifications. \nThis evaluation guides the subsequent perturbations to the behavior model configurations.\nThrough an iterative process of regeneration and re-evaluation, we can refine and generate realistic and plausible trigger trajectories that involve multiple vehicles' complex interactions.\nFurthermore, we develop a negative training strategy by incorporating patch trajectories that share similarities with the triggers but are designated not to activate the backdoor. \nWe thus enhance the stealthiness of the attack, refining the system\u2019s responses to trigger scenarios. \nThrough extensive empirical studies using offline reinforcement learning (RL) driving agents with various trigger patterns and target action designs, we demonstrate the flexibility and effectiveness of our proposed attack, showing the under-exploration of existing end-to-end AD systems' vulnerabilities to such multi-vehicle-based backdoor attacks.\nWe also evaluate the attack against existing defenses and validate different design choices of our attack via a comprehensive ablation study.", "title_embedding_index": 4158, "title_abs_embedding_index": 4183}, {"title": "Disentangled representations of microscopy images", "link_suffix": "/forum?id=0iAZYF9hrl", "link": "https://openreview.net/forum?id=0iAZYF9hrl", "pdf_link": "https://openreview.net/pdf?id=0iAZYF9hrl", "keywords": "Microscopy images, Disentangled representations, Transfer learning, Interpretability", "abstract": "Microscopy image analysis is fundamental for different applications, from diagnosis to synthetic engineering and environmental monitoring. In the last few years, the number of available images has been constantly growing, thanks to technological advancements, pushing toward the development of automatic image analysis methods based on deep learning. Although deep neural networks have demonstrated great performance in this field, interpretability \u2014 an essential requirement for microscopy image analysis \u2014 remains an open challenge.This work proposes a Disentangled Representation Learning (DRL) methodology to enhance model interpretability for microscopy image classification. \nExploiting benchmark datasets coming from three different microscopic image domains, including plankton, yeast vacuoles, and human cells, we show how a DRL framework, based on transfer learning from synthetic features, can provide a good trade-off between accuracy and interpretability in this domain.", "title_embedding_index": 4159, "title_abs_embedding_index": 4184}, {"title": "TextEconomizer: Enhancing Lossy Text Compression with Denoising Autoencoder and Entropy Coding", "link_suffix": "/forum?id=DsMxVELk3K", "link": "https://openreview.net/forum?id=DsMxVELk3K", "pdf_link": "https://openreview.net/pdf?id=DsMxVELk3K", "keywords": "Text Compression, Denoising AutoEnccoder, Lossy Text, Entropy Coding, Latent Space", "abstract": "Lossy text compression reduces data size while preserving core meaning, making it ideal for summarization, automated analysis, and digital archives where exact fidelity is less critical. While extensively used in image compression, text compression techniques, such as integrating entropy coding with autoencoder latent representations in Seq2Seq text generation, have been underexplored. A key challenge is incorporating lossless entropy coding into denoising autoencoders to improve storage efficiency while maintaining high-quality outputs, even with noisy text. Prior studies have mainly focused on near-lossless token generation with little attention to space efficiency. In this paper, we present a denoising autoencoder with a rectified latent representation that compresses variable-sized inputs into a fixed-size latent space without prior knowledge of dataset dimensions. By leveraging entropy coding, our model achieves state-of-the-art compression ratios alongside competitive text quality, as measured by diverse metrics. Its parameter count is approximately 196 times smaller than comparable models. Additionally, it achieves a compression ratio of 67\u00d7 while maintaining high BLEU and ROUGE scores. This significantly outperforms existing transformer-based models in memory efficiency, marking a breakthrough in balancing lossless compression with optimal space optimization.", "title_embedding_index": 4160, "title_abs_embedding_index": 4185}, {"title": "Has the Deep Neural Network learned the Stochastic Process? An Evaluation Viewpoint", "link_suffix": "/forum?id=2U8owdruSQ", "link": "https://openreview.net/forum?id=2U8owdruSQ", "pdf_link": "https://openreview.net/pdf?id=2U8owdruSQ", "keywords": "evaluation, deep neural network, stochasticity, complex systems, forecasting", "abstract": "This paper presents the first systematic study of evaluating Deep Neural Networks (DNNs) designed to forecast the evolution of stochastic complex systems. We show that traditional evaluation methods, such as threshold-based classification metrics and error-based scoring rules, assess a DNN's ability to replicate the observed ground truth (Observed-GT) but fail to measure the DNN's learning of the underlying stochastic process. To address this gap, we propose a new property calledFidelity to Stochastic Process (F2SP), representing the DNN's ability to predict theStatistic-GT\u2014the ground truth of the stochastic process\u2014and introduce an evaluation metric that exclusively assesses fidelity to Statistic-GT. We formalize F2SP within a stochastic framework and establish criteria for validly measuring it. We demonstrate that the Expected Calibration Error (ECE) satisfies the necessary conditions for evaluating fidelity to Statistic-GT. Empirical experiments on synthetic datasets\u2014including wildfire, host-pathogen, and stock market models\u2014show that ECE exclusively measures F2SP. We further extend our study to real-world wildfire data, highlighting the limitations of conventional evaluation and discussing the practical utility of incorporating F2SP into model assessment. This work offers a new perspective on evaluating DNNs in stochastic complex systems by emphasizing the importance of capturing the underlying stochastic process.", "title_embedding_index": 4161, "title_abs_embedding_index": 4186}, {"title": "Anomaly Detection by Estimating Gradients of the Tabular Data Distribution", "link_suffix": "/forum?id=7QDIFrtAsB", "link": "https://openreview.net/forum?id=7QDIFrtAsB", "pdf_link": "https://openreview.net/pdf?id=7QDIFrtAsB", "keywords": "Anomaly detection, Tabular data, Noise Conditional Score-based Networks", "abstract": "Detecting anomalies in tabular data from various domains has become increasingly important in deep learning research. Simultaneously, the development of generative models has advanced, offering powerful mechanisms for detecting anomalies by modeling normal data. In this paper, we propose a novel method for anomaly detection using a noise conditional score network (NSCN). NSCNs, which can learn the gradients of log probability density functions over many noise-perturbed data distributions, are known for their diverse sampling even in low-density regions of the training data. This effect can also be utilized, and thus, the NSCN can be used directly as an anomaly indicator with an anomaly score derived from a simplified loss function. This effect will be analyzed in detail. Our method is trained on normal behavior data, enabling it to differentiate between normal and anomalous behaviors in test scenarios. To evaluate our approach extensively, we created the world's largest benchmark for anomaly detection in tabular data with 49 baseline methods consisting of the ADBench benchmark and several more datasets from the literature. Overall, our approach shows state-of-the-art performance across the benchmark.", "title_embedding_index": 4162, "title_abs_embedding_index": 4187}, {"title": "LICORICE: Label-Efficient Concept-Based Interpretable Reinforcement Learning", "link_suffix": "/forum?id=Mjn53GtMxi", "link": "https://openreview.net/forum?id=Mjn53GtMxi", "pdf_link": "https://openreview.net/pdf?id=Mjn53GtMxi", "keywords": "Reinforcement Learning, Explainable Reinforcement Learning, Concept Bottleneck Models, Concept-based Explainability, Interpretability, XRL", "abstract": "Recent advances in reinforcement learning (RL) have predominantly leveraged neural network-based policies for decision-making, yet these models often lack interpretability, posing challenges for stakeholder comprehension and trust. Concept bottleneck models offer an interpretable alternative by integrating human-understandable concepts into neural networks. However, a significant limitation in prior work is the assumption that annotations for these concepts are readily available during training, necessitating continuous real-time concept annotation. This reliance either places a significant burden on human annotators or incurs substantial costs in API queries and inference time when employing automated labeling methods, such as vision-language models (VLMs). To overcome this limitation, we introduce a novel training scheme that enables RL algorithms to efficiently learn a concept-based policy by only querying annotators to label a small set of data. Our algorithm, LICORICE, involves three main contributions: interleaving concept learning and RL training, using a concept ensembles to actively select informative data points for labeling, and decorrelating the concept data with a simple strategy. We show how LICORICE reduces human labeling efforts to 500 or fewer concept labels in three environments and 5000 in another complex environment at minimal or no cost to performance. We also explore the use of VLMs as automated concept annotators, finding them effective in some cases but challenging in others. This work significantly reduces the annotation burden for interpretable RL, making it more practical for real-world applications where transparency is crucial.", "title_embedding_index": 4163, "title_abs_embedding_index": 4188}, {"title": "Expanding the Web, Smaller Is Better: A Comprehensive Study in Post-training", "link_suffix": "/forum?id=EVa5OIYBoG", "link": "https://openreview.net/forum?id=EVa5OIYBoG", "pdf_link": "https://openreview.net/pdf?id=EVa5OIYBoG", "keywords": "Post-training, Continual Learning, Large Language Models", "abstract": "General-purpose large language models (GLLMs) like GPT-4 and LLaMA have demonstrated exceptional performance across a wide range of tasks. However, their performance often falls short in domain- or task-specific applications, where deeper, specialized knowledge is essential, while maintaining general knowledge remains crucial for handling broader, unseen tasks. Post-training has been widely applied to\nmake LLMs specialized, typically consisting of multiple stages, including DomainAdaptive Pre-Training (DAPT) and Supervised Fine-Tuning (SFT). In this work, we conduct a comprehensive study on three key aspects of post-training taking Finance as a target domain: (1) the distinct roles of DAPT and SFT in post-training, (2) strategies to mitigate knowledge forgetting across stages, and (3) evaluation methods that capture both general and domain-specific capabilities. \nOur results show that DAPT and SFT require distinct training objectives, joint training of DAPT and SFT is essential for maintaining stage knowledge and encouraging knowledge transfer across stages, and replay mechanisms are critical for preventing forgetting. Evaluation should encompass general, seen, and unseen tasks for a complete assessment. Based on these insights, we developed a Joint-and-Replay post-training recipe and built LLaMA3-8B-Fin, a smaller yet more powerful stateof-the-art financial LLM trained through post-training. Despite its smaller size, LLaMA3-8B-Fin surpasses larger models like GPT-4o and LLaMA3.1-70b on both seen and unseen financial tasks while retaining general knowledge, demonstrating that a well-structured post-training can \u201cexpand the web\u201d of capabilities in smaller LLMs, enabling them to outperform much larger models.", "title_embedding_index": 4164, "title_abs_embedding_index": 4189}, {"title": "ToEdit: How to Synthesize Text Data to Avoid Model Collapse?", "link_suffix": "/forum?id=mVCcWCjeEz", "link": "https://openreview.net/forum?id=mVCcWCjeEz", "pdf_link": "https://openreview.net/pdf?id=mVCcWCjeEz", "keywords": "synthetic data, model collapse", "abstract": "We explore model collapse caused by synthetic data, where AI models trained on such data experience a gradual decline in performance. \nOur initial analysis examines language model pretraining on mixed human and synthetic data, highlighting performance degradation. Further statistical analysis reveals distributional shifts and an over-concentration of n-gram features caused by synthetic data. Inspired by these insights, we propose token-level editing on human data, to obtain semi-synthetic data instead of fully using model outputs. As a proof of concept, we theoretically demonstrate that token-level editing can prevent model collapse, as the test error is constrained by a finite upper bound. We conducted extensive experiments on pretraining, continual pretraining, and supervised fine-tuning of language models. The results validate our theoretical proof that token-level editing improves data quality and enhances model performance.", "title_embedding_index": 4165, "title_abs_embedding_index": 4190}, {"title": "MAgICoRe: Multi-Agent, Iterative, Coarse-to-Fine Refinement for Reasoning", "link_suffix": "/forum?id=j9wBgcxa7N", "link": "https://openreview.net/forum?id=j9wBgcxa7N", "pdf_link": "https://openreview.net/pdf?id=j9wBgcxa7N", "keywords": "LLM Refinement, Reasoning, Multi-Agent", "abstract": "Large Language Models' (LLM) reasoning can be improved using test-time aggregation strategies, i.e., generating multiple samples for each problem and aggregating over them to find a better answer. While these improve performance, they often reach a saturation point beyond which additional samples provide no return. Refinement offers an alternative by using model-generated feedback to improve answer quality. However, refinement introduces three key challenges: (1) Excessive refinement: Uniformly refining all instances can cause over-correction and reduce overall performance. (2) Inability to localize and address errors: LLMs have a limited ability to self-correct and struggle to identify and correct their own mistakes in a targeted way. (3) Insufficient refinement: Deciding how many iterations of refinement are needed is non-trivial, and stopping too soon could leave errors unaddressed. To tackle these issues, we propose MAgICoRe, a framework for Multi-Agent Iteration for Coarse-to-fine Refinement. MAgICoRe aims to avoid excessive refinement by categorizing problems as easy or hard, solving easy problems with coarse-grained aggregation, and solving hard ones with fine-grained and iterative multi-agent refinement. To enable more granular error localization, we incorporate external step-wise reward model (RM) scores. To ensure effective refinement, we employ a multi-agent loop with three agents: the Solver, the Reviewer (which generates targeted feedback based on step-wise RM scores) and the Refiner (which incorporates feedback and generates new solutions). To ensure sufficient refinement, we re-evaluate updated solutions, iteratively initiating further rounds of multi-agent refinement. We evaluate MAgICoRe on Llama-3-8B and GPT-3.5 and show its effectiveness across five math reasoning datasets, with consistent gains for all datasets and models. Even one iteration of MAgICoRe beats Self-Consistency by 3.4%, Best-of-k by 3.2%, and Self-Refine by 4.0% while using less than 50% of the samples. Unlike iterative refinement with baseline methods, MAgICoRe continues to improve with more iterations. Finally, our ablations highlight the importance of MAgICoRe's use of RMs and multi-agent communication.", "title_embedding_index": 4166, "title_abs_embedding_index": 4191}, {"title": "MIND: Math Informed syNthetic Dialogues for Pretraining LLMs", "link_suffix": "/forum?id=TuOTSAiHDn", "link": "https://openreview.net/forum?id=TuOTSAiHDn", "pdf_link": "https://openreview.net/pdf?id=TuOTSAiHDn", "keywords": "pretraining, mathematical reasoning, synthetic dialogue, LLM, reasoning", "abstract": "The utility of synthetic data to enhance pretraining data quality and hence to improve downstream task accuracy has been widely explored in recent large language models (LLMs). Yet, these approaches fall inadequate in complex, multi-hop and mathematical reasoning tasks as the synthetic data typically fails to add complementary knowledge to the existing raw corpus. In this work, we propose a novel large-scale and diverse Math Informed syNthetic Dialogue (MIND) generation method that improves the mathematical reasoning ability of LLMs. Specifically, using MIND, we generate synthetic conversations based on OpenWebMath (OWM), resulting in a new math corpus, MIND-OWM. Our experiments with different conversational settings reveal that incorporating knowledge gaps between dialog participants is essential for generating high-quality math data. We further identify an effective way to format and integrate synthetic and raw data during pretraining to maximize the gain in mathematical reasoning, emphasizing the need to restructure raw data rather than use it as-is. Compared to pretraining just on raw data, a model pretrained on MIND-OWM shows significant boost in mathematical reasoning (GSM8K: +13.42%, MATH: +2.30%), including superior performance in specialized knowledge (MMLU: +4.55%, MMLU-STEM: +4.28%) and general purpose reasoning tasks (GENERAL REASONING: +2.51%).", "title_embedding_index": 4167, "title_abs_embedding_index": 4192}, {"title": "Sparse Matrix in Large Language Model Fine-tuning", "link_suffix": "/forum?id=GbgCRJedQ7", "link": "https://openreview.net/forum?id=GbgCRJedQ7", "pdf_link": "https://openreview.net/pdf?id=GbgCRJedQ7", "keywords": "Parameter-efficient Finetuning, Large Language Model, Large Language Model Systm", "abstract": "Various parameter-efficient fine-tuning (PEFT) methods, including LoRA and its variants, have gained popularity for reducing computational costs. However, there is often an accuracy gap between PEFT approaches and full fine-tuning (FT), and this discrepancy has not yet been systematically explored. In this work, we introduce a method for selecting sparse sub-matrices that aim to minimize the performance gap between PEFT vs. full fine-tuning (FT) while also reducing both fine-tuning computational cost and memory cost. Our Sparse Matrix Tuning (SMT) method begins by identifying the most significant sub-matrices in the gradient update, updating only these blocks during the fine-tuning process. In our experiments, we demonstrate that SMT consistently surpasses other PEFT baseline (e.g. LoRA and DoRA) in fine-tuning popular large language models such as LLaMA across a broad spectrum of tasks, while reducing the GPU memory footprint by 67% compared to FT. We also examine how the performance of LoRA and DoRA tends to plateau and decline as the number of trainable parameters increases, in contrast, our SMT method does not suffer from such issue.", "title_embedding_index": 4168, "title_abs_embedding_index": 4193}, {"title": "Perceptual Piercing: Human Visual Cue-Based Object Detection in Low Visibility Conditions", "link_suffix": "/forum?id=tccML2tDd4", "link": "https://openreview.net/forum?id=tccML2tDd4", "pdf_link": "https://openreview.net/pdf?id=tccML2tDd4", "keywords": "deep learning, computer vision, dehazing, bio-inspired networks, human visual perception", "abstract": "This study proposes a novel deep learning framework inspired by atmospheric scattering and human visual cortex mechanisms to enhance object detection under poor visibility scenarios such as fog, smoke, and haze. These conditions pose significant challenges for object recognition, impacting various sectors, including autonomous driving, aviation management, and security systems. The objective is to enhance the precision and reliability of detection systems under adverse environmental conditions. The research investigates the integration of human-like visual cues, particularly focusing on selective attention and environmental adaptability, to ascertain their impact on object detection's computational efficiency and accuracy. This paper proposes a multi-tiered strategy that integrates an initial quick detection process, followed by targeted region-specific dehazing, and concludes with an in-depth detection phase. The approach is validated using the Foggy Cityscapes, RESIDE-beta (OTS and RTTS) datasets and is anticipated to set new performance standards in detection accuracy while significantly optimizing computational efficiency. The findings offer a viable solution for enhancing object detection in poor visibility and contribute to the broader understanding of integrating human visual principles into deep learning algorithms for intricate visual recognition challenges.", "title_embedding_index": 4169, "title_abs_embedding_index": 4194}, {"title": "Attributing Model Behavior: The Predominant Influence of Dataset Complexity Over Hyperparameters in Classification", "link_suffix": "/forum?id=x8mr9zGkpr", "link": "https://openreview.net/forum?id=x8mr9zGkpr", "pdf_link": "https://openreview.net/pdf?id=x8mr9zGkpr", "keywords": "Model Behavior Attribution, Complexity Meta-features, Hyperparameters, Bias-Variance Decomposition", "abstract": "Understanding how different factors contribute to model performance is critical for advancing the attribution of model behavior. Previous research has independently explored the effects of hyperparameters and dataset complexity meta-features on classification performance. However, a key knowledge gap remains in understanding how these factors comparatively influence model behavior, particularly bias and variance. This study bridges this gap by investigating which factors, hyperparameters or complexity meta-features, exert a greater influence on the bias-variance in classification algorithms, specifically Random Forests (RF) and Support Vector Machines (SVM). Using 290 diverse OpenML tabular datasets and 304 hyperparameter configurations, we employ functional analysis of variance (fANOVA) to quantify the impact of these factors. Our findings reveal that dataset complexity meta-features exert a more significant influence on both bias and variance than hyperparameters for both RF and SVM models. To further substantiate our findings, we conducted an analysis based on the Manipulation Theory of Causation. This analysis demonstrates that optimizing dataset complexity can simultaneously reduce bias and variance, while hyperparameter tuning often leads to a bias-variance trade-off with limited impact on overall performance. To the best of our knowledge, this research is the first to directly compare the effects of hyperparameters and complexity meta-features on bias and variance in classification, contributing new insights into model behavior attribution.", "title_embedding_index": 4170, "title_abs_embedding_index": 4195}, {"title": "ExerCAKT: A Knowledge Tracing Model Based on GRU Capturing Contextual Features of Exercises", "link_suffix": "/forum?id=ftwMX4ORIS", "link": "https://openreview.net/forum?id=ftwMX4ORIS", "pdf_link": "https://openreview.net/pdf?id=ftwMX4ORIS", "keywords": "knowledge tracing, deep learning, recurrent neural networks, knowledge state modeling", "abstract": "Knowledge tracing aims to predict students' future performance based on their past interactions, helping online learning platforms and teachers assess learners' knowledge levels. This technology plays a critical role in achieving large-scale cognitive diagnosis. Recently, deep learning-based knowledge tracing models have demonstrated impressive results, with most research focusing on designing customized network architectures and novel optimization objectives. However, redundant parameters and overly complex loss functions can complicate model training and make it harder to maintain prediction accuracy. To further investigate the effectiveness of simple recurrent neural networks in this field, and to leverage their advantages in handling sequential exercise representation, this paper introduces a GRU-based knowledge tracing model named ExerCAKT (Exercise Context-Aware Knowledge Tracing). This model effectively captures contextual features of exercises and achieves robust knowledge state modeling through the use of a GRU-based knowledge state feature extractor and a GRU-based exercise feature extractor\u2014without relying on additional optimization objectives.The model's superior performance is validated through comparisons with baseline models, such as AKT and SIMPLEKT, on three public datasets in the knowledge tracing domain. Evaluations are conducted using AUC and ACC metrics at both the Knowledge Concept level and the question level. We validated that relying solely on simple recurrent neural networks, combined with appropriate representation methods, can still achieve excellent performance in this field. Our code will be available at xxx (Anonymous URL).", "title_embedding_index": 4171, "title_abs_embedding_index": 4196}, {"title": "Generative Reward Models", "link_suffix": "/forum?id=MwU2SGLKpS", "link": "https://openreview.net/forum?id=MwU2SGLKpS", "pdf_link": "https://openreview.net/pdf?id=MwU2SGLKpS", "keywords": "RLHF, reward model, LLM Judge, reasoning", "abstract": "Reinforcement Learning from Human Feedback (RLHF) has greatly improved the performance of modern Large Language Models (LLMs). The RLHF process is resource-intensive and technically challenging, generally requiring a large collection of human preference labels over model-generated outputs. Reinforcement Learning from AI Feedback (RLAIF) addresses this data collection challenge by leveraging synthetic preferences generated by an LLM. However, recent work has shown that synthetic preferences labels may not align well with human preference judgments.To address this, we propose a hybrid approach that unifies RLHF and RLAIF methodologies. We introduce GenRM, an iterative algorithm that trains an LLM on self-generated reasoning traces, leading to synthetic preference labels matching human preference judgments. Empirically, we show that zero-shot LLM-based judgments under-perform compared to Bradley-Terry reward models on in-distribution tasks (between 9-36%). In contrast, GenRM achieves in-distribution accuracy comparable to Bradley-Terry models, while significantly outperforming them on out-of-distribution tasks (between 10-45%). Moreover, GenRM surpasses the performance of using LLMs as judges on both in-distribution (by 9-31%) and out-of-distribution tasks (by 2- 6%). Our results show that combining the strengths of RLHF and RLAIF offers a promising approach for improving the quality of synthetic preference labels.", "title_embedding_index": 4172, "title_abs_embedding_index": 4197}, {"title": "Trace Reconstruction for DNA Data Storage using Language Models", "link_suffix": "/forum?id=rkfiJQMFcw", "link": "https://openreview.net/forum?id=rkfiJQMFcw", "pdf_link": "https://openreview.net/pdf?id=rkfiJQMFcw", "keywords": "DNA Data Storage, Trace Reconstruction, Language Models", "abstract": "DNA is a promising storage medium due to its high information density and\nlongevity. However, the storage process introduces errors, thus algorithms and\ncodes are required for reliable storage. A common important step in the recovery\nof the information from DNA is trace reconstruction. In the trace reconstruction\nproblem, the goal is to construct a sequence from noisy copies corrupted by deletion,\ninsertion, and substitution errors. In this paper, we propose to use language\nmodels trained with next-token prediction for trace reconstruction. A simple channel\nmodel for the DNA data storage pipeline allows for self-supervised pretraining\non large amounts of synthetic data. Additional finetuning on real data enables us\nto adapt to technology-dependent error statistics. The proposed method outperforms\nstate-of-the-art trace reconstruction algorithms for DNA data storage, often\nrecovering significantly more sequences.", "title_embedding_index": 4173, "title_abs_embedding_index": 4198}, {"title": "EFFECTIVE REGULARIZATION WITH RELATIVE-DISTANCE VARIANCES IN DEEP METRIC LEARNING", "link_suffix": "/forum?id=I4fi8dvIZS", "link": "https://openreview.net/forum?id=I4fi8dvIZS", "pdf_link": "https://openreview.net/pdf?id=I4fi8dvIZS", "keywords": "deep metric learning, image retrieval", "abstract": "This paper develops, for the first time, a novel method using relative-distance variance to regularize deep metric learning (DML), overcoming the drawbacks of existing pair-distance-based metrics, notably loss functions. Being a fundamental field in machine learning research, DML has been widely studied with the goal of learning a feature space where dissimilar data samples are further apart than similar ones. A typical approach of DML is to optimize the feature space by maximizing the relative distances between negative and positive pairs. Despite the rapid advancement, the pair-distance-based approach suffers from a few drawbacks that it heavily relies on the appropriate selection of margin to determine decision boundaries, and it depends on the effective selection of informative pairs, and resulting in low generalization across tasks. To address these issues, this paper explores the use of relative-distance variance and investigates its impact on DML through both empirical and theoretical studies. Based upon such investigation, we propose a novel Relative Distance Variance Constraint (RDVC) loss by regularizing the representation or embedding function learning. The proposed RDVC loss can seamlessly integrate with various pair-distance-based loss functions to ensure a robust and effective performance. Substantial experimental results have demonstrated the effectiveness of our proposed RDVC loss on both within-domain and cross-domain retrieval tasks. In particular, the RDVC loss is also shown useful in fine-grained zero-shot sketch-based image retrieval, a challenging task, revealing its general applicability to cross-domain and zero-shot learning.", "title_embedding_index": 4174, "title_abs_embedding_index": 4199}]