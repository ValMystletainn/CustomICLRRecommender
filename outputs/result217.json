[{"title": "Mixed Hierarchical Oracle and Multi-Agent Benchmark in Two-player Zero-sum Games", "link_suffix": "/forum?id=Xj6j48QIB3", "link": "https://openreview.net/forum?id=Xj6j48QIB3", "pdf_link": "https://openreview.net/pdf?id=Xj6j48QIB3", "keywords": "reinforcement learning, game theory, deep learning", "abstract": "Self-play methods have achieved remarkable success in two-player zero-sum games, attaining superhuman performance in many complex game domains. Parallelizing learners is a feasible approach to handling large-scale games. However, parallelizing learners often leads to suboptimal exploitation of computational resources, resulting in inefficiencies. In this study, we introduce the Mixed Hierarchical Oracle (MHO), designed to enhance computational efficiency and performance in large-scale two-player zero-sum games. MHO enables the parallelization of reinforcement learning tasks through a hierarchical pipeline that balances exploration and exploitation across oracle levels. It also avoids cold-start issues by using a \"model soup\" initialization strategy. Additionally, we present MiniStar, an open-source environment focused on small-scale combat scenarios, developed to facilitate research in self-play algorithms. Through extensive experiments on matrix games and the MiniStar environment, we demonstrate that MHO outperforms existing methods in terms of computational efficiency and performance.", "title_embedding_index": 10800, "title_abs_embedding_index": 10825}, {"title": "PFT: Enhancing Prompt Injection Robustness via Position-Enhanced Finetuning", "link_suffix": "/forum?id=l3bUmPn6u5", "link": "https://openreview.net/forum?id=l3bUmPn6u5", "pdf_link": "https://openreview.net/pdf?id=l3bUmPn6u5", "keywords": "Large Language Models (LLMs), Safety, Instruction Following, Adversarial Attacks, Position Encoding", "abstract": "Large Language Models (LLMs) are widely adopted in closed-domain applications, where differentiating between system instructions and user input is crucial to prevent unintended malicious actions. However, instruction-following LLMs often blindly follow instructions in user inputs, opening up the risk of prompt injection attacks. This paper investigates whether Supervised Fine-Tuning (SFT) can teach LLMs to strictly distinguish system instructions from user input. Our study reveals a key weakness: SFT-tuned models follow system instructions reliably only when the key instruction is placed immediately after the initial tokens. We find that the proximity of the key instruction to the initial tokens significantly influences the model's ability to execute the intended task, and consequently, its susceptibility to prompt injection attacks.To address this issue, we propose PFT, a novel position-enhanced fine-tuning approach that leverages position IDs to more effectively distinguish between system and user tokens. The experimental results demonstrate that PFT improves the robustness of SFT-tuned models against prompt injection attacks, even when the key instruction is placed arbitrarily in the system prompt, without compromising performance. Our work sheds light on the importance of prompt format in enhancing the security of LLMs and offers a practical solution to improve their robustness.", "title_embedding_index": 10801, "title_abs_embedding_index": 10826}, {"title": "Pareto-Optimal Learning from Preferences with Hidden Context", "link_suffix": "/forum?id=saJkPzTmZz", "link": "https://openreview.net/forum?id=saJkPzTmZz", "pdf_link": "https://openreview.net/pdf?id=saJkPzTmZz", "keywords": "Preference Learning, Lexicase Selection, Pareto-optimality, Hidden Context, Pluralistic Alignment", "abstract": "Ensuring AI models align with human values is essential for their safety and functionality. Reinforcement learning from human feedback (RLHF) leverages human preferences to achieve this alignment. However, when preferences are sourced from diverse populations, point estimates of reward can result in suboptimal performance or be unfair to specific groups. We propose Pareto Optimal Preference Learning (POPL), which enables pluralistic alignment by framing discrepant group preferences as objectives with potential trade-offs, aiming for policies that are Pareto-optimal on the preference dataset. POPL utilizes lexicase selection, an iterative process that selects diverse and Pareto-optimal solutions. Our theoretical and empirical evaluations demonstrate that POPL surpasses baseline methods in learning sets of reward functions and policies, effectively catering to distinct groups without access to group numbers or membership labels. We verify the performance of POPL on a stateless preference learning setting, a Minigrid RL domain, Metaworld robotics benchmarks, as well as large language model (LLM) fine-tuning. We illustrate that POPL can also serve as a foundation for techniques optimizing specific notions of group fairness, ensuring safe and equitable AI model alignment.", "title_embedding_index": 10802, "title_abs_embedding_index": 10827}, {"title": "DAG-SHAP: Feature Attribution in DAG based on Edge Intervention", "link_suffix": "/forum?id=ljZFM2mhbR", "link": "https://openreview.net/forum?id=ljZFM2mhbR", "pdf_link": "https://openreview.net/pdf?id=ljZFM2mhbR", "keywords": "feature attribution, Shapley value, causal intervention", "abstract": "Shapley value-based feature attribution methods face challenges in scenarios with complex feature interactions and causal relationships, even when a causal structure is provided. The assumption on the attribution objects of existing methods often deviates from practical scenarios as they cannot capture the exogenous influence of features through each edge in the causal graph, leading to unreasonable interpretations. To overcome these limitations, we propose a novel feature attribution method called DAG-SHAP, which is based on edge intervention. DAG-SHAP treats the exogenous contributions in each ongoing feature edge as an individual attribution object ensuring that both externality and exogenous contributions of features are appropriately captured. Additionally, we introduce an approximation method for efficiently computing DAG-SHAP. Extensive experiments on both synthetic and real datasets validate the effectiveness of DAG-SHAP. Our code can be found in the anonymous repository at \\url{https://anonymous.4open.science/r/dag-30F2}.", "title_embedding_index": 10803, "title_abs_embedding_index": 10828}, {"title": "CodeChain: An Open, Million-scale Dataset for Code Language Models at the Repository Level", "link_suffix": "/forum?id=RrWAtQNGAg", "link": "https://openreview.net/forum?id=RrWAtQNGAg", "pdf_link": "https://openreview.net/pdf?id=RrWAtQNGAg", "keywords": "Code Language Model, Data generation", "abstract": "Code large language models (LLMs) have shown remarkable advances in code understanding and generation tasks. Programming corpora serve as the foundation for various code LLMs. In reality, repositories consist of multiple files with numerous cross-file dependencies. Leveraging the dependency information can effectively enhance the code understanding and generation capabilities. However, existing works fail to utilize dependencies effectively. Consequently, there is a pressing need for an open dataset that specifically focuses on capturing and leveraging the cross-file dependencies.\nTo fill in this gap, we release Codechain, an augmentation of the code dataset at the repository level, provides a rich context for code LLMs to learn from. Specifically, to capture the cross-file dependencies, we first parse the code project into a topological graph where nodes represent files and edges denote dependencies. Then, we employ a novel random walk method to determine the code chain and concatenate the corresponding files. To utilize such corpus for supervised fine-tuning, we design Codechain to enable the model to thoroughly learn the code contents and its dependencies. Ultimately, we produce 562,587 code chains and 1,021,550 instruction samples. With Codechain, we train our model on multi-task learning objectives and evaluate on the public benchmarks. The experimental results demonstrate that model by learning the interconnected nature of codes significantly outperforms the previous methods, showcasing the effectiveness of Codechain in advancing the code understanding and generation", "title_embedding_index": 10804, "title_abs_embedding_index": 10829}, {"title": "BANGS: Game-theoretic Node Selection for Graph Self-Training", "link_suffix": "/forum?id=h51mpl8Tyx", "link": "https://openreview.net/forum?id=h51mpl8Tyx", "pdf_link": "https://openreview.net/pdf?id=h51mpl8Tyx", "keywords": "Graph Semi-supervised Learning, Graph Self-training, Game Theory Application", "abstract": "Graph self-training is a semi-supervised learning method that iteratively selects a set of unlabeled data to retrain the underlying graph neural network (GNN) model and improve its prediction performance. While selecting highly confident nodes has proven effective for self-training, this pseudo-labeling strategy ignores the combinatorial dependencies between nodes and suffers from a local view of the distribution.\nTo overcome these issues, we propose BANGS, a novel framework that unifies the labeling strategy with conditional mutual information as the objective of node selection. Our approach---grounded in game theory---selects nodes in a combinatorial fashion and provides theoretical guarantees for robustness under noisy objective. More specifically, unlike traditional methods that rank and select nodes independently, BANGS considers nodes as a collective set in the self-training process. Our method demonstrates superior performance and robustness across various datasets, base models, and hyperparameter settings, outperforming existing techniques. The codebase is available onhttps://anonymous.4open.science/r/BANGS-3EA4.", "title_embedding_index": 10805, "title_abs_embedding_index": 10830}, {"title": "Holistic Reasoning with Long-Context LMs: A Benchmark for Database Operations on Massive Textual Data", "link_suffix": "/forum?id=5LXcoDtNyq", "link": "https://openreview.net/forum?id=5LXcoDtNyq", "pdf_link": "https://openreview.net/pdf?id=5LXcoDtNyq", "keywords": "long-context, reasoning, LLM", "abstract": "The rapid increase in textual information means we need more efficient methods to sift through, organize, and understand it all. While retrieval-augmented generation (RAG) models excel in accessing information from large document collections, they struggle with complex tasks that require aggregation and reasoning over information spanning across multiple documents--what we call  \\textit{holistic reasoning}. Long-context language models (LCLMs) have great potential for managing large-scale documents, but their holistic reasoning capabilities remain unclear.  In this work, we introduce HoloBench, a novel framework that brings database reasoning operations into text-based contexts, making it easier to systematically evaluate how LCLMs handle holistic reasoning across large documents. Our approach adjusts key factors such as context length, information density, distribution of information, and query complexity to evaluate LCLMs comprehensively.Our experiments show that the amount of information in the context has a bigger influence on LCLM performance than the actual context length. Furthermore, the complexity of queries affects performance more than the amount of information, particularly for different types of queries. Interestingly, queries that involve finding maximum or minimum values are easier for LCLMs and are less affected by context length, even though they pose challenges for RAG systems. However, tasks requiring the aggregation of multiple pieces of information show a noticeable drop in accuracy as context length increases.  Additionally, we find that while grouping relevant information generally improves performance, the optimal positioning varies across models. Our findings surface both the advancements and the ongoing challenges in achieving a holistic understanding of long contexts. These can guide future developments in LCLMs and set the stage for creating more robust language models for real-world applications.", "title_embedding_index": 10806, "title_abs_embedding_index": 10831}, {"title": "Representation Alignment for Generation: Training Diffusion Transformers Is Easier Than You Think", "link_suffix": "/forum?id=DJSZGGZYVi", "link": "https://openreview.net/forum?id=DJSZGGZYVi", "pdf_link": "https://openreview.net/pdf?id=DJSZGGZYVi", "keywords": "Diffusion models, Representation learning", "abstract": "Recent studies have shown that the denoising process in (generative) diffusion models can induce meaningful (discriminative) representations inside the model, though the quality of these representations still lags behind those learned through recent self-supervised learning methods. We argue that one main bottleneck in training large-scale diffusion models for generation comes down to learning these good representations, and training can become significantly easier when the model is aided by strong external visual representations. We study this by introducing a straightforward regularization called REPresentation Alignment (REPA), which aligns the projections of noisy input hidden states in denoising networks with clean image representations obtained from external, pretrained visual encoders. The results are striking: our simple strategy yields significant improvements in both training efficiency and generation quality when applied to popular diffusion and flow-based transformers, such as DiTs and SiTs. For instance, our method can speed up SiT training by over 17.5$\\times$, matching the performance (without classifier-free guidance) of a SiT-XL model trained for 7M steps in less than 400K steps. Additionally, in terms of final generation quality, our approach achieves a superior FID score of 1.80 when using guidance.", "title_embedding_index": 10807, "title_abs_embedding_index": 10832}, {"title": "Statistical Significance of Clustering for High-Dimensional Count Data", "link_suffix": "/forum?id=tWgmOFfcQ1", "link": "https://openreview.net/forum?id=tWgmOFfcQ1", "pdf_link": "https://openreview.net/pdf?id=tWgmOFfcQ1", "keywords": "Unsupervised learning, Cluster index, Generalized PCA, Dimension reduction, High-dimension low-sample size data", "abstract": "Clustering is widely used in biomedical research for meaningful subgroup identification. However, most existing clustering algorithms do not account for the statistical uncertainty of the resulting clusters and consequently may generate spurious clusters due to natural sampling variation. To address this problem, the Statistical Significance of Clustering (SigClust) method was developed to evaluate significance of clusters in high-dimensional data. While SigClust has been successful in testing mixtures of continuous distributions, it is not specifically designed for discrete distributions, such as count data in genomics. Moreover, SigClust and its variations often suffer from reduced statistical power when applied to non-Gaussian high-dimensional data. To overcome these limitations, we propose SigClust-DEV, a method designed to evaluate the significance of clusters in count data. Through extensive simulations, we compare SigClust-DEV against other existing SigClust approaches across various count distributions and demonstrate its superior performance. Furthermore, we apply our method SigClust-DEV to Hydra single-cell RNA sequencing (scRNA) data and electronic health records (EHRs) of cancer patients to identify meaningful latent cell types and patient subgroups, respectively.", "title_embedding_index": 10808, "title_abs_embedding_index": 10833}, {"title": "Multi-Dimensional Conformal Prediction", "link_suffix": "/forum?id=loDppyW7e2", "link": "https://openreview.net/forum?id=loDppyW7e2", "pdf_link": "https://openreview.net/pdf?id=loDppyW7e2", "keywords": "classification, conformal prediction, uncertainty quantification", "abstract": "Conformal prediction has attracted significant attention as a distribution-free method for uncertainty quantification in black-box models, providing prediction sets with guaranteed coverage. However, its practical utility is often limited when these prediction sets become excessively large, reducing its overall effectiveness. In this paper, we introduce a novel approach to conformal prediction for classification problems, which leverages a multi-dimensional nonconformity score. By extending standard conformal prediction to higher dimensions, we achieve better separation between correct and incorrect labels. Utilizing this we can focus on regions with low concentrations of incorrect labels, leading to smaller, more informative prediction sets. To efficiently generate the multi-dimensional score, we employ a self-ensembling technique that trains multiple diverse classification heads on top of a backbone model. We demonstrate the advantage of our approach compared to baselines across different benchmarks.", "title_embedding_index": 10809, "title_abs_embedding_index": 10834}, {"title": "Neutral residues: revisiting adapters for model extension", "link_suffix": "/forum?id=8ZPLn3GCDb", "link": "https://openreview.net/forum?id=8ZPLn3GCDb", "pdf_link": "https://openreview.net/pdf?id=8ZPLn3GCDb", "keywords": "LLM, model extension", "abstract": "We address the problem of extending a pretrained large language model to a new domain that was not seen at training time, like adding a language for which the original model has seen no or little training data.  Popular solutions like fine-tuning or low-rank adaptation are successful at domain adaptation, but formally they do not add any extra capacity and degrade the performance in the original domain.Our paper analyzes this extension problem under three angles: data, architecture and training procedure, which are advantageously considered jointly. In particular, we improve adapters and make it possible to learn an entire new language while ensuring that the output of the neural network is  almost unchanged in the original domain. For this purpose, we modify the new residual blocks in a way that leads each new residual block to output near-zeros in the original domain.This solution of neutral residues, which borrows architectural components from mixture of experts, is effective: with only 20% extra learnable weights compared to an original model trained on English, we get results that are significantly better than concurrent approaches (fine-tuning, low-rank or vanilla adapters) in terms of the trade-off between learning a new language and not forgetting English.", "title_embedding_index": 10810, "title_abs_embedding_index": 10835}, {"title": "Mitigating Dialogue Hallucination for Large Vision Language Models via Adversarial Instruction Tuning", "link_suffix": "/forum?id=sw6Wpx2LGr", "link": "https://openreview.net/forum?id=sw6Wpx2LGr", "pdf_link": "https://openreview.net/pdf?id=sw6Wpx2LGr", "keywords": "Large Vision Language Model, Large Multi-modal Model, Hallucination, Adversarial Learning", "abstract": "Mitigating hallucinations of Large Vision Language Models (LVLMs) is crucial to enhance their reliability for general-purpose assistants. This paper shows that such hallucinations of LVLMs can be significantly exacerbated by preceding user-system dialogues. To precisely measure this, we first present an evaluation benchmark by extending popular multi-modal benchmark datasets with prepended hallucinatory dialogues powered by our novel Adversarial Question Generator (AQG), which can automatically generate image-related yet adversarial dialogues by adopting adversarial attacks on LVLMs. On our benchmark, the zero-shot performance of state-of-the-art LVLMs drops significantly for both the VQA and Captioning tasks. Next, we further reveal this hallucination is mainly due to the prediction bias toward preceding dialogues rather than visual content. To reduce this bias, we propose Adversarial Instruction Tuning (AIT) that robustly fine-tunes LVLMs against hallucinatory dialogues. Extensive experiments show our proposed approach successfully reduces dialogue hallucination while maintaining performance.", "title_embedding_index": 10811, "title_abs_embedding_index": 10836}, {"title": "Generating Likely Counterfactuals Using Sum-Product Networks", "link_suffix": "/forum?id=rGyi8NNqB0", "link": "https://openreview.net/forum?id=rGyi8NNqB0", "pdf_link": "https://openreview.net/pdf?id=rGyi8NNqB0", "keywords": "counterfactual explanations, mixed-integer optimization, sum-product networks", "abstract": "The need to explain decisions made by AI systems is driven by both recent regulation and user demand. The decisions are often explainable only post hoc. In counterfactual explanations, one may ask what constitutes the best counterfactual explanation. Clearly, multiple criteria must be taken into account, although \"distance from the sample\" is a key criterion. Recent methods that consider the plausibility of a counterfactual seem to sacrifice this original objective. Here, we present a system that provides high-likelihood explanations that are, at the same time, close and sparse. We show that the search for the most likely explanations satisfying many common desiderata for counterfactual explanations can be modeled using Mixed-Integer Optimization (MIO). We use a Sum-Product Network (SPN) to estimate the likelihood of a counterfactual. To achieve that, we propose an MIO formulation of an SPN, which can be of independent interest.", "title_embedding_index": 10812, "title_abs_embedding_index": 10837}, {"title": "Multi-Physics Operator Network for In-context learning (m-PhOeNIX)", "link_suffix": "/forum?id=ubUTIlAH0m", "link": "https://openreview.net/forum?id=ubUTIlAH0m", "pdf_link": "https://openreview.net/pdf?id=ubUTIlAH0m", "keywords": "Multi-physics operator learning, neural operator, catastrophic forgetting, continual learning, wavelet", "abstract": "We propose a multi-physics operator network for simultaneous and sequential learning of solution operators of multiple heterogeneous parametric partial differential equations. Existing neural operators are adept at learning the solution operator of only a single physical system, and adapting to new physical equations requires training a new surrogate model from scratch with physics-specific intensive hyperparameter tuning. The proposed multi-physics neural operator leverages the recent advancements in wavelet-based kernel integral-induced neural operator modeling and instantiates a memory-based ensembling strategy for projecting heterogeneous physical systems into a common shared feature space. The local channel-level ensembling is supported by context gates, which not only utilize the shared features to embed the features of multiple heterogeneous physical systems into the network parameters but also allow the multi-physics operator to learn new solution operators by transferring knowledge sequentially; this allows the proposed model to continually learn without forgetting. We illustrate the efficacy of our algorithm by simultaneously and sequentially learning six complex time-dependent solution operators of six physical systems. The inference results on the simultaneous and sequentially trained models depict the ability to infer previously seen physical systems without fine-tuning and catastrophic forgetting, indicating the characteristics of a foundation model. The framework also demonstrates the super-resolution property and generalization to out-of-distribution input conditions.", "title_embedding_index": 10813, "title_abs_embedding_index": 10838}, {"title": "Progressive Multi-scale Triplane Network for Text-to-3D Generation", "link_suffix": "/forum?id=nhAyhTxrXu", "link": "https://openreview.net/forum?id=nhAyhTxrXu", "pdf_link": "https://openreview.net/pdf?id=nhAyhTxrXu", "keywords": "Text-to-3D Generation, Diffusion Model", "abstract": "The challenge of text-to-3D generation lies in accurately and efficiently crafting 3D objects based on natural language descriptions, a capability that promises substantial reduction in manual design efforts and offers an intuitive interface for user interaction with digital environments. Despite recent advancements, effective recovery of fine-grained details and efficient optimization of high-resolution 3D outputs remain critical hurdles. Drawing inspiration from the efficacious paradigm of progressive learning, we present a novel Multi-scale Triplane Network (MTN) architecture coupled with a tailored progressive learning strategy. As the name implies, the Multi-scale Triplane Network consists of four triplanes transitioning from low to high resolution. This hierarchical structure allows the low-resolution triplane to serve as an initial shape for the high-resolution counterparts, easing the inherent complexity of the optimization process. Furthermore, we introduce the progressive learning scheme that systematically guides the network to shift its attention from prominent coarse-grained structures to intricate fine-grained patterns. This strategic progression ensures that the focus of the model evolves towards emulating the subtlest aspects of the described 3D object. Our experiment verifies that the proposed method performs favorably against contemporary methods. Even for the complex and nuanced textual descriptions, our method consistently excels, delivering robust and viable 3D shapes where other methods falter.", "title_embedding_index": 10814, "title_abs_embedding_index": 10839}, {"title": "Uncovering Overfitting in Large Language Model Editing", "link_suffix": "/forum?id=t8qcGXaepr", "link": "https://openreview.net/forum?id=t8qcGXaepr", "pdf_link": "https://openreview.net/pdf?id=t8qcGXaepr", "keywords": "Large language model, Knowledge editing, Editing overfit", "abstract": "Knowledge editing has been proposed as an effective method for updating and correcting the internal knowledge of Large Language Models (LLMs). However, existing editing methods often struggle with complex tasks, such as multi-hop reasoning. In this paper, we identify and investigate the phenomenon ofEditing Overfit, where edited models assign disproportionately high probabilities to the edit target, hindering the generalization of new knowledge in complex scenarios. We attribute this issue to the current editing paradigm, which places excessive emphasis on the direct correspondence between the input prompt and the edit target for each edit sample. To further explore this issue, we introduce a new benchmark, EVOKE (EValuation of Editing Overfit in Knowledge Editing), along with fine-grained evaluation metrics. Through comprehensive experiments and analysis, we demonstrate that Editing Overfit is prevalent in current editing methods and that common overfitting mitigation strategies are ineffective in knowledge editing. To overcome this, inspired by LLMs\u2019 knowledge recall mechanisms, we propose a new plug-and-play strategy called Learn to Inference (LTI), which introduce a Multi-stage Inference Constraint module to guide the edited models in recalling new knowledge similarly to how unedited LLMs leverage knowledge through in-context learning. Extensive experimental results across a wide range of tasks validate the effectiveness of LTI in mitigating Editing Overfit.", "title_embedding_index": 10815, "title_abs_embedding_index": 10840}, {"title": "Overcoming Lower-Level Constraints in Bilevel Optimization: A Novel Approach with Regularized Gap Functions", "link_suffix": "/forum?id=cyPMEXdqQ2", "link": "https://openreview.net/forum?id=cyPMEXdqQ2", "pdf_link": "https://openreview.net/pdf?id=cyPMEXdqQ2", "keywords": "bilevel optimization, constrained optimization, gap function, single-loop, Hessian-free, convergence analysis", "abstract": "Constrained bilevel optimization tackles nested structures present in constrained learning tasks like constrained meta-learning, adversarial learning, and distributed bilevel optimization. \nHowever, existing bilevel optimization methods mostly are typically restricted to specific constraint settings, such as linear lower-level constraints. \nIn this work, we overcome this limitation and develop a new single-loop, Hessian-free constrained bilevel algorithm capable of handling more general lower-level constraints. \nWe achieve this by employing a doubly regularized gap function tailored to the constrained lower-level problem, transforming constrained bilevel optimization into an equivalent single-level optimization problem with a single smooth constraint. \nWe rigorously establish the non-asymptotic convergence analysis of the proposed algorithm under the convexity of lower-level problem, avoiding the need for strong convexity assumptions on the lower-level objective or coupling convexity assumptions on lower-level constraints found in existing literature. \nAdditionally, the generality of our method allows for its extension to bilevel optimization with minimax lower-level problem. \nWe evaluate the effectiveness and efficiency of our algorithm on various synthetic problems, typical hyperparameter learning tasks, and generative adversarial network.", "title_embedding_index": 10816, "title_abs_embedding_index": 10841}, {"title": "Advantage-Guided Distillation for Preference Alignment in Small Language Models", "link_suffix": "/forum?id=xsx3Fpo3UD", "link": "https://openreview.net/forum?id=xsx3Fpo3UD", "pdf_link": "https://openreview.net/pdf?id=xsx3Fpo3UD", "keywords": "Preference Alignment; Large language model; Knowledge Distillation; Advantage Function", "abstract": "Alignment techniques such as RLHF enable LLMs to generate outputs that align with human preferences and play an essential role in their effectiveness. However, their impact often diminishes when applied to smaller language models, likely due to the limited capacity of these models. Instead of directly applying existing alignment techniques to smaller models, we propose to utilize a well-aligned teacher LLM to guide the alignment process for these models, thereby facilitating the transfer of the teacher's knowledge of human preferences to the student model. To achieve this, we first explore a straightforward approach, Dual-Constrained Knowledge Distillation (DCKD), that employs knowledge distillation with two KL-divergence constraints from the aligned teacher to the unaligned student. To further enhance the contrastive effect, we then propose Advantage-Guided Distillation for Preference Alignment (ADPA), which leverages an advantage function from the aligned teacher to deliver more nuanced, distribution-level reward signals for the student's alignment. Our experimental results demonstrate that these two approaches appreciably improve the alignment of smaller language models and narrow the performance gap with their larger counterparts.", "title_embedding_index": 10817, "title_abs_embedding_index": 10842}, {"title": "Pixel-Aware Accelerated Reverse Diffusion Modeling", "link_suffix": "/forum?id=W4djmqKZC6", "link": "https://openreview.net/forum?id=W4djmqKZC6", "pdf_link": "https://openreview.net/pdf?id=W4djmqKZC6", "keywords": "diffusion models, generative models, accelerated learning, deep learning", "abstract": "We propose in this paper an analytically new construct of a diffusion model whose drift and diffusion parameters yield a faster time-decaying Signal to Noise Ratio in the forward process. The proposed methodology significantly accelerates the forward diffusion process, reducing the required diffusion time steps from around 1000 seen in conventional models to 200-500 without compromising image quality in the reverse-time diffusion. In a departure from conventional models which typically use time-consuming multiple runs, we introduce a parallel data-driven model to generate a reverse-time diffusion trajectory in a single run of the model. The construct cleverly carries out the learning of the diffusion coefficients via an estimate of the structure of clean images. The resulting collective block-sequential generative model eliminates the need for MCMC-based sub-sampling correction for safeguarding and improving image quality, which further improve the acceleration of image generation. Collectively, these advancements yield a generative model that is at least 4 times faster than conventional approaches, while maintaining high fidelity and diversity in generated images, hence promising widespread applicability in rapid image synthesis tasks.", "title_embedding_index": 10818, "title_abs_embedding_index": 10843}, {"title": "Machine Learning Benchmark on Dynamic Functional Connectivity: Promise, Pitfalls, and Interpretations", "link_suffix": "/forum?id=GBpKUnM6gW", "link": "https://openreview.net/forum?id=GBpKUnM6gW", "pdf_link": "https://openreview.net/pdf?id=GBpKUnM6gW", "keywords": "Brain Dynamics, Functional Connectivity, Machine Learning, Task Recognition, Disease Diagnosis", "abstract": "An unprecedented amount of existing functional Magnetic Resonance Imaging (fMRI) data provides a new opportunity to understand the relationship between functional fluctuation and human cognition/behavior using a data-driven approach. To that end, tremendous efforts have been made in machine learning to predict cognitive states from evolving volumetric images of blood-oxygen-level-dependent (BOLD) signals. Due to the complex nature of brain function, however, the evaluation on learning performance and discoveries are not often consistent across current state-of-the-arts (SOTA). By capitalizing on large-scale existing neuroimaging data (39,784 data samples from seven databases), we seek to establish a well-founded empirical guideline for designing deep models for functional neuroimages by linking the methodology underpinning with knowledge from the neuroscience domain.  Specifically, we put the spotlight on (1) What is the current SOTA performance in cognitive task recognition and disease diagnosis using fMRI? (2) What are the limitations of current deep models? and (3) What is the general guideline for selecting the suitable machine learning backbone for new neuroimaging applications?\nWe have conducted a comprehensive evaluation and statistical analysis, in various settings, to answer the above outstanding questions. In addition, we explore a novel attention learning mechanism to provide meaningful spatial pattern of brain activation that is associated with various cognitive tasks and neurological conditions.", "title_embedding_index": 10819, "title_abs_embedding_index": 10844}, {"title": "Distributed Unlearning with Lossy Compression", "link_suffix": "/forum?id=lHIPPUjwp1", "link": "https://openreview.net/forum?id=lHIPPUjwp1", "pdf_link": "https://openreview.net/pdf?id=lHIPPUjwp1", "keywords": "Machine unlearning, distributed learning, lossy source coding", "abstract": "Machine unlearning enables to remove the contribution of a set of data points from a trained model. In a distributed setting, where a server orchestrates training using data available at a set of remote users, unlearning is essential to cope with the possible presence of malicious users. Existing distributed unlearning algorithms require the server to store all model updates observed in training, leading to immense storage overhead for preserving the ability to unlearn. In this work we study lossy compression schemes for facilitating distributed server-side unlearning with limited memory footprint. We identify suitable lossy compression mechanisms based on random lattice coding and sparsification. For a family of stochastic compression schemes encompassing probabilistic and subtractive dithered quantization, we derive an upper bound on the difference between the desired model that is trained from scratch and the model unlearned from lossy compressed stored updates. Our bound outperforms the state-of-the-art known bounds for non-compressed decentralized server-side unlearning, even when lossy compression is incorporated. We further provide a numerical study, shows that suited lossy compression can enable distributed unlearning with notably reduced memory footprint at the server while preserving the utility of the unlearned model.", "title_embedding_index": 10820, "title_abs_embedding_index": 10845}, {"title": "Unintentional Unalignment: Likelihood Displacement in Direct Preference Optimization", "link_suffix": "/forum?id=uaMSBJDnRv", "link": "https://openreview.net/forum?id=uaMSBJDnRv", "pdf_link": "https://openreview.net/pdf?id=uaMSBJDnRv", "keywords": "Direct Preference Optimization, DPO, Likelihood Displacement, Unalignment, Alignment, Language Models", "abstract": "Direct Preference Optimization (DPO), and its numerous variants, are increasingly used for aligning language models. Although they are designed to teach a model to generate preferred responses more frequently relative to dispreferred responses, prior work has observed that the likelihood of preferred responses often decreases during training. The current work sheds light on the causes and implications of this counter-intuitive phenomenon, which we termlikelihood displacement. We demonstrate that likelihood displacement can becatastrophic, shifting probability mass from preferred responses to semantically opposite ones. As a simple example, training a model to prefer $\\texttt{No}$ over $\\texttt{Never}$ can sharply increase the probability of $\\texttt{Yes}$. Moreover, when aligning the model to refuse unsafe prompts, we show that such displacement canunintentionally lead to unalignment, by shifting probability mass from preferred refusal responses to harmful responses (e.g., reducing the refusal rate of Llama-3-8B-Instruct from 74.4% to 33.4%). We theoretically characterize that likelihood displacement is driven by preferences that induce similar embeddings, as measured by acentered hidden embedding similarity (CHES)score. Empirically, the CHES score enables identifying which training samples contribute most to likelihood displacement in a given dataset. Filtering out these samples effectively mitigated unintentional unalignment in our experiments. More broadly, our results highlight the importance of curating data with sufficiently distinct preferences, for which we believe the CHES score may prove valuable.", "title_embedding_index": 10821, "title_abs_embedding_index": 10846}, {"title": "Newton Meets Marchenko-Pastur: Massively Parallel Second-Order Optimization with Hessian Sketching and Debiasing", "link_suffix": "/forum?id=Ty6TCjKNSF", "link": "https://openreview.net/forum?id=Ty6TCjKNSF", "pdf_link": "https://openreview.net/pdf?id=Ty6TCjKNSF", "keywords": "parallel and distributed optimization, Newton method, Marchenko-Pastur law, Hessian sketching", "abstract": "Motivated by recent advances in serverless cloud computing, in particular the ``function as a service'' (FaaS) model, \nwe consider the problem of minimizing a convex function in a massively parallel fashion, where communication between workers is limited.\nFocusing on the case of a twice-differentiable objective subject to an L2 penalty, we propose a scheme where the central node (server) effectively runs a Newton method, \noffloading its high per-iteration cost---stemming from the need to invert the Hessian---to the workers. \nIn our solution, workers produce independently coarse but low-bias estimates of the inverse Hessian, using an adaptive sketching scheme. The server then averages the descent directions produced by the workers, yielding a good approximation for the exact Newton step. The main component of our adaptive sketching scheme is a low-complexity procedure for selecting the sketching dimension, an issue that was left largely unaddressed in the existing literature on Hessian sketching for distributed optimization. Our solution is based on ideas from asymptotic random matrix theory, specifically the Marchenko-Pastur law. For Gaussian sketching matrices, we derive non asymptotic guarantees for our algorithm which are essentially dimension-free. Lastly, when the objective is self-concordant, we provide convergence guarantees for the approximate Newton's method with noisy Hessians, which may be of independent interest beyond the setting considered in this paper.", "title_embedding_index": 10822, "title_abs_embedding_index": 10847}, {"title": "SIMPL: Scalable and hassle-free optimisation of neural representations from behaviour", "link_suffix": "/forum?id=9kFaNwX6rv", "link": "https://openreview.net/forum?id=9kFaNwX6rv", "pdf_link": "https://openreview.net/pdf?id=9kFaNwX6rv", "keywords": "neuroscience; place cells; grid cells; representations; neural data; hippocampus;", "abstract": "High-dimensional neural activity in the brain is known to encode low-dimensional, time-evolving, behaviour-related variables. A fundamental goal of neural data analysis consists of identifying such variables and their mapping to neural activity. The canonical approach is to assume the latent variables are behaviour and visualize the subsequent tuning curves. However, significant mismatches between behaviour and the encoded variables may still exist --- the agent may be thinking of another location, or be uncertain of its own --- distorting the tuning curves and decreasing their interpretability. To address this issue a variety of methods have been proposed to learn this latent variable in an unsupervised manner; these techniques are typically expensive to train, come with many hyperparameters or scale poorly to large datasets complicating their adoption in practice. To solve these issues we propose SIMPL (Scalable Iterative Maximization of Population-coded Latents), an EM-style algorithm which iteratively optimizes latent variables and tuning curves. SIMPL is fast, scalable and exploits behaviour as an initial condition to further improve convergence and identifiability. We show SIMPL accurately recovers latent variables in biologically-inspired spatial and non-spatial tasks. When applied to a large rodent hippocampal dataset SIMPL efficiently finds a modified latent space with smaller, more numerous, and more uniformly-sized place fields than those based on behaviour, suggesting the brain may encode space with greater resolution than previously thought.", "title_embedding_index": 10823, "title_abs_embedding_index": 10848}, {"title": "Aligning Large Language Models With Preference Privacy", "link_suffix": "/forum?id=nHmaQf2wJC", "link": "https://openreview.net/forum?id=nHmaQf2wJC", "pdf_link": "https://openreview.net/pdf?id=nHmaQf2wJC", "keywords": "LLMs, Differential Privacy, Alignment", "abstract": "Alignment is a crucial part in the implementation pipeline of Large Language Models (LLMs) that utilizes human feedback to ensure that LLMs adhere to human values and societal norms. This introduces privacy threats associated with the identity and preferences of the labelers responsible for creating the human feedback data. Several recent works have explored using differential privacy (DP) as a notion to protect the privacy of human labeled data; primarily relying on DP-SGD based solutions, which privatize the gradients during fine-tuning and alignment. Human preferences, however are only associated with the labels of the (prompt, response) tuples; therefore DP-SGD based approaches can be superfluous, providing more privacy than necessary and can degrade model utility. In this work, we focus on the problem of aligning LLMs with preference level privacy, which only preserve the privacy of preferences provided by humans. We build and expand upon the concept of label DP for this problem, and present a series of increasingly sophisticated, yet practical privacy preserving mechanisms for alignment. Specifically, starting from a standard randomized response (RR) mechanism which randomly flips human preferences, and it's corresponding \\textit{unbiased} RR mechanism (which ensures an unbiased loss during alignment), we propose a new mechanism, PROPS (PROgressively Private Self-alignment). PROPS works in multiple stages as follows: in each stage, the privately trained and partially aligned model from the previous stage to act as a labeler for the training data for the next stage and combine it with RR which is repeated across multiple stages. Motivation for PROPS comes from the following critical observations: a) learning to label correct preferences might be an easier problem than generating responsible content; b) progressively combining RR with partially aligned models for labeling preferences significantly reduces the amount of necessary perturbation needed for privacy and also shows the potential of possibly reducing the number of human labeled preference samples. We present proof-of-concept experiments that demonstrate the feasibility and effectiveness of our proposed approach and show that preference privacy based alignment can still attain a comparable utility to their non-privately aligned counterparts.", "title_embedding_index": 10824, "title_abs_embedding_index": 10849}]