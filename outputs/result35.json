[
    {
        "title": "Process Supervision-Guided Policy Optimization for Code Generation",
        "link_suffix": "/forum?id=Cn5Z0MUPZT",
        "link": "https://openreview.net/forum?id=Cn5Z0MUPZT",
        "pdf_link": "https://openreview.net/pdf?id=Cn5Z0MUPZT",
        "keywords": "Process Reward Model, Code Generation, Large Language Model, Reinforcement Learning",
        "abstract": "Reinforcement learning (RL) with unit test feedback has enhanced large language models’ (LLMs) code generation, but relies on sparse rewards provided only after complete code evaluation, limiting learning efficiency and incremental improvements. When generated code fails all unit tests, no learning signal is received, hindering progress on complex tasks. To address this, we propose a Process Reward Model (PRM) that delivers dense, line-level feedback on code correctness during generation, mimicking human code refinement and providing immediate guidance. We explore various strategies for training PRMs and integrating them into the RL framework, finding that using PRMs both as dense rewards and for value function initialization significantly boosts performance. Our approach increases our in-house LLM’s pass rate from 28.2% to 29.8% on LiveCodeBench and from 31.8% to 35.8% on our internal benchmark. Our experimental results highlight the effectiveness of PRMs in enhancing RL-driven code generation, especially for long-horizon scenarios."
    },
    {
        "title": "Universally Optimal Watermarking Schemes for LLMs: from Theory to Practice",
        "link_suffix": "/forum?id=NQZImD0VGP",
        "link": "https://openreview.net/forum?id=NQZImD0VGP",
        "pdf_link": "https://openreview.net/pdf?id=NQZImD0VGP",
        "keywords": "Large language models, watermarking, hypothesis testing, distortion-free, detection, joint optimization",
        "abstract": "Large Language Models (LLMs) boosts human efficiency but also poses misuse risks, with watermarking serving as a reliable method to differentiate AI-generated content from human-created text. In this work, we propose a novel theoretical framework for watermarking LLMs. Particularly, we jointly optimize both the watermarking scheme and detector to maximize detection performance, while controlling the worst-case Type-I error and distortion in the watermarked text.  Within our framework, we characterize theuniversally minimum Type-II error, showing a fundamental trade-off between detection performance and distortion. More importantly, we identify the optimal type of detectors and watermarking schemes. Building upon our theoretical analysis, we introduce a practical, model-agnostic and computationally efficient token-level watermarking algorithm that invokes a surrogate model and the Gumbel-max trick. Empirical results on Llama-13B and Mistral-8$\\times$7B demonstrate the effectiveness of our method. Furthermore, we also explore how robustness can be integrated into our theoretical framework, which provides a foundation for designing future watermarking systems with improved resilience to adversarial attacks."
    },
    {
        "title": "Convergence Analysis of Adaptive Gradient Methods under Refined Smoothness and Noise Assumptions",
        "link_suffix": "/forum?id=UmMKbG2Ubr",
        "link": "https://openreview.net/forum?id=UmMKbG2Ubr",
        "pdf_link": "https://openreview.net/pdf?id=UmMKbG2Ubr",
        "keywords": "Adaptive gradient methods, stochastic nonconvex optimization, dimensional dependence",
        "abstract": "Adaptive gradient methods, such as AdaGrad, are among the most successful optimization algorithms for neural network training. While these methods are known to achieve better dimensional dependence than stochastic gradient descent (SGD) under favorable geometry for stochastic convex optimization, the theoretical justification for their success in stochastic non-convex optimization remains elusive. In fact, under standard assumptions of Lipschitz gradients and bounded noise variance, it is known that SGD is worst-case optimal (up to absolute constants) in terms of finding a near-stationary point measured by the $\\ell_2$-norm, making further improvements impossible. Motivated by this limitation, we introduce refined assumptions on the smoothness structure of the objective and the gradient noise variance, which better suit the coordinate-wise nature of adaptive gradient methods. Moreover, we adopt the $\\ell_1$-norm of the gradient as the stationarity measure, as opposed to the standard $\\ell_2$-norm, to align with the coordinate-wise analysis and obtain tighter convergence guarantees for AdaGrad. Under these new assumptions and the $\\ell_1$-norm stationarity measure, we establish anupper boundon the convergence rate of AdaGrad and a correspondinglower boundfor SGD. In particular, for certain configurations of problem parameters, we show that the iteration complexity of AdaGrad outperforms SGD by a factor of $d$. To the best of our knowledge, this is the first result to demonstrate a provable gain of adaptive gradient methods over SGD in a non-convex setting. We also present supporting lower bounds, including one specific to AdaGrad and one applicable to general deterministic first-order methods, showing that our upper bound for AdaGrad is tight and unimprovable up to a logarithmic factor under certain conditions."
    },
    {
        "title": "LeanQuant: Accurate and Scalable Large Language Model Quantization with Loss-error-aware Grid",
        "link_suffix": "/forum?id=ISqx8giekS",
        "link": "https://openreview.net/forum?id=ISqx8giekS",
        "pdf_link": "https://openreview.net/pdf?id=ISqx8giekS",
        "keywords": "large language model, quantization",
        "abstract": "Large language models (LLMs) have shown immense potential across various domains, but their high memory requirements and inference costs remain critical challenges for deployment. Post-training quantization (PTQ) has emerged as a promising technique to reduce memory requirements and decoding latency. However, recent accurate quantization methods often depend on specialized computations or custom data formats to achieve better model quality, which limits their compatibility with popular frameworks, as they require dedicated inference kernels tailored to specific hardware and software platforms, hindering wider adoption. Furthermore, many competitive methods have high resource requirements and computational overhead, making it challenging to scale them to hundreds of billions of parameters. In response to these challenges, we propose LeanQuant (Loss-error-aware Network Quantization), a novel quantization method that is accurate, versatile, and scalable. In the existing popular iterative loss-error-based quantization framework, we identify a critical limitation in prior methods: the min-max affine quantization grid fails to preserve model quality due to outliers in inverse Hessian diagonals. To overcome this fundamental issue, we propose learning loss-error-aware grids, instead of using non-adaptive min-max affine grids. Our approach not only produces quantized models that are more accurate but also generalizes to a wider range of quantization types, including affine and non-uniform quantization, enhancing compatibility with more frameworks. Extensive empirical evaluations on recent LLMs demonstrate that LeanQuant is highly accurate, comparing favorably against recent competitive baselines in model quality, and scalable, achieving very accurate quantization of Llama-3.1 405B, one of the largest open-source LLMs to date, using two Quadro RTX 8000-48GB GPUs in 21 hours."
    },
    {
        "title": "Novel View Acoustic Parameter Estimation",
        "link_suffix": "/forum?id=o5wGjBEgH8",
        "link": "https://openreview.net/forum?id=o5wGjBEgH8",
        "pdf_link": "https://openreview.net/pdf?id=o5wGjBEgH8",
        "keywords": "acoustic parameter estimation, room impulse response, reverberation, spatial audio, room acoustics, novel view acoustic synthesis",
        "abstract": "The task of Novel View Acoustic Synthesis (NVAS) -- generating Room Impulse Responses (RIRs) for unseen source and receiver positions in a scene -- has recently gained traction, especially given its relevance to Augmented Reality (AR) and Virtual Reality (VR) development.  However, many of these efforts suffer from similar limitations: they infer RIRs in the time domain, which prove challenging to optimize; they focus on scenes with simple, single-room geometries; they infer only single-channel, directionally-independent acoustic characteristics; and they require inputs, such as 3D geometry meshes with material properties, that may be impractical to obtain for on-device applications.  On the other hand, research suggests that sample-wise accuracy of RIRs is not required for perceptual plausibility in AR and VR.  Standard acoustic parameters like Clarity Index (C50) or Reverberation Time (T60) have been shown to capably describe pertinent characteristics of the RIRs, especially for late reverberation.  To address these gaps, this paper introduces a new task centered on estimating spatially distributed acoustic parameters that can be then used to condition a simple reverberator for arbitrary source and receiver positions. The approach is modelled as an image-to-image translation task, which translates 2D floormaps of a scene into 2D heatmaps of acoustic parameters. We introduce a new, large-scale dataset of 1000 rooms consisting of complex, multi-room apartment conditions, and show that our method outperforms statistical baselines significantly.  Moreover, we show that the method also works for directionally-dependent  (i.e. beamformed) parameter prediction.  Finally, the proposed method operates on very limited information, requiring only a broad outline of the scene and a single RIR at inference time."
    },
    {
        "title": "Harmonic Machine Learning Models are Robust",
        "link_suffix": "/forum?id=UcFjiiObbM",
        "link": "https://openreview.net/forum?id=UcFjiiObbM",
        "pdf_link": "https://openreview.net/pdf?id=UcFjiiObbM",
        "keywords": "machine learning; robustness; explainable AI; adversarial attacks",
        "abstract": "We introduce Harmonic Robustness, a powerful and intuitive method to test the robustness of any machine-learning model either during training or in black-box real-time inference monitoring without ground-truth labels. It is based on functional deviation from the harmonic mean-value property, indicating instability and lack of explainability. We show implementation examples in low-dimensional trees and feedforward NNs, where the method reliably identifies overfitting, as well as in more complex high-dimensional models such as ResNet-50 and Vision Transformer where it efficiently measures adversarial vulnerability across image classes."
    },
    {
        "title": "Finding Symmetry in Neural Network Parameter Spaces",
        "link_suffix": "/forum?id=oMfZUSbVwf",
        "link": "https://openreview.net/forum?id=oMfZUSbVwf",
        "pdf_link": "https://openreview.net/pdf?id=oMfZUSbVwf",
        "keywords": "Parameter space symmetry",
        "abstract": "Parameter space symmetries, or loss-invariant transformations, are important for understanding neural networks' loss landscape, training dynamics, and generalization. \nHowever, identifying the full set of these symmetries remains a challenge. \nIn this paper, we formalize data-dependent parameter symmetries and derive their infinitesimal form, which enables an automated approach to discover symmetry across different architectures. \nOur framework systematically uncovers parameter symmetries, including previously unknown ones. \nWe also prove that symmetries in smaller subnetworks can extend to larger networks, allowing the discovery of symmetries in small architectures to generalize to more complex models."
    },
    {
        "title": "Who Should Join the Decision-Making Table? Targeted Expert Selection for Enhanced Human-AI Collaboration",
        "link_suffix": "/forum?id=gLGp77MxFo",
        "link": "https://openreview.net/forum?id=gLGp77MxFo",
        "pdf_link": "https://openreview.net/pdf?id=gLGp77MxFo",
        "keywords": "human-ai complementarity, calibration, rule learning",
        "abstract": "Integrating AI and human expertise can significantly enhance decision-making across various scenarios. This paper introduces a novel approach that leverages the Product of Experts (PoE) model to optimize decision-making by strategically combining AI with human inputs. While human experts bring diverse perspectives, their decisions may be constrained by biases or knowledge gaps. To address these limitations, we propose an AI agent that provides probabilistic, rule-based insights, complementing and filling human experts' knowledge gaps. A key feature of our approach is the strategic selection of human experts based on how well their knowledge complements or enhances the AI’s recommendations. By dynamically adapting the expert selection process, we ensure that decisions benefit from the most impactful and complementary inputs. Our PoE model calibrates inputs from both AI and human experts, leveraging their combined strengths to improve decision outcomes. Furthermore, operating in an online setting, our framework can also continuously update the AI’s knowledge and refine expert selection criteria, ensuring adaptability to evolving environments. Experiments in simulation environments demonstrate that our model effectively integrates logic rule-informed AI with human expertise, enhancing collaborative decision-making."
    },
    {
        "title": "Show, Don't Tell: Uncovering Implicit Character Portrayal using LLMs",
        "link_suffix": "/forum?id=AN3VTbqM1N",
        "link": "https://openreview.net/forum?id=AN3VTbqM1N",
        "pdf_link": "https://openreview.net/pdf?id=AN3VTbqM1N",
        "keywords": "LLMs, NLP, societal applications, fairness, bias, prompting, machine learning, deep learning, commonsense reasoning",
        "abstract": "Tools for analyzing character portrayal in fiction are valuable for writers and literary scholars in developing and interpreting compelling stories. Existing tools, such as visualization tools for analyzing fictional characters, primarily rely on explicit textual indicators of character attributes. However, portrayal is often implicit, revealed through actions and behaviors rather than explicit statements. We address this gap by leveraging large language models (LLMs) to uncover implicit character portrayals. We start by generating a dataset for this task with greater cross-topic similarity, lexical diversity, and narrative lengths than existing narrative text corpora such as TinyStories and WritingPrompts. We then introduce LIIPA (LLMs for Inferring Implicit Portrayal for Character Analysis), a framework for prompting LLMs to uncover character portrayals. LIIPA can be configured to use various types of intermediate computation (character attribute word lists, chain-of-thought) to infer how fictional characters are portrayed in the source text. We find that LIIPA outperforms existing approaches, and is more robust to increasing character counts (number of unique persons depicted) due to its ability to utilize full narrative context. Lastly, we investigate the sensitivity of portrayal estimates to character demographics, identifying a fairness-accuracy tradeoff among methods in our LIIPA framework -- a phenomenon familiar within the algorithmic fairness literature. Despite this tradeoff, all LIIPA variants consistently outperform non-LLM baselines in both fairness and accuracy. Our work demonstrates the potential benefits of using LLMs to analyze complex characters and to better understand how implicit portrayal biases may manifest in narrative texts."
    },
    {
        "title": "Understanding Mode Connectivity via Parameter Space Symmetry",
        "link_suffix": "/forum?id=E5YnuidZ9W",
        "link": "https://openreview.net/forum?id=E5YnuidZ9W",
        "pdf_link": "https://openreview.net/pdf?id=E5YnuidZ9W",
        "keywords": "symmetry, mode connectivity",
        "abstract": "Neural network minima have been observed to be connected by curves along which train and test loss remain nearly constant, a phenomenon known as mode connectivity. \nWhile this has enabled applications such as model merging and fine-tuning, its theoretical explanation remains unclear. \nWe propose a new approach to exploring the connectedness of minima using parameter space symmetry.\nBy linking the topology of symmetry groups to that of the minima, we derive the number of connected components of the minima of linear networks and show that skip connections reduce this number. \nWe then examine when mode connectivity and linear mode connectivity hold or fail, using parameter symmetries which account for a significant part of the minimum.\nFinally, we provide explicit expressions for connecting curves in the minima induced by symmetry. \nUsing the curvature of these curves, we derive conditions under which linear mode connectivity approximately holds.\nOur analysis highlights the role of continuous symmetries in understanding the neural network loss landscape."
    },
    {
        "title": "Strategist: Self-improvement of LLM Decision Making via Bi-Level Tree Search",
        "link_suffix": "/forum?id=gfI9v7AbFg",
        "link": "https://openreview.net/forum?id=gfI9v7AbFg",
        "pdf_link": "https://openreview.net/pdf?id=gfI9v7AbFg",
        "keywords": "LLMs, games, search, self-improvement, self-play, RL, agent, multi-agent",
        "abstract": "Traditional reinforcement learning (RL) typically requires vast amounts of training\ndata to develop effective policies. In contrast, large language models (LLMs)\nexhibit strong generalization and zero-shot capabilities, but struggle with plan-\nning and understanding complex action policies. In this work, we introduce\nSTRATEGIST, a novel approach that integrates the strengths of both methods. Our\napproach leverages LLMs to learn high-level strategic abstractions, which are\nthen refined and executed by a low-level mechanism, such as Monte Carlo Tree\nSearch (MCTS). STRATEGIST is a generalizable framework that can be trained\nthrough population-based self-play simulations and self-improvement, without the\nneed for prior training data. We demonstrate the effectiveness of STRATEGIST in\nlearning optimal policies for competitive, multi-turn games with partial informa-\ntion, including Game of Pure Strategy (GOPS) and multi-agent, hidden-identity\ndiscussion games like The Resistance: Avalon. Our results show that agents trained\nwith STRATEGIST outperform those trained with traditional RL methods, other\nLLM-based skill acquisition techniques, and pre-existing LLM agents across both\ngame environments."
    },
    {
        "title": "Diverging Preferences: When do Annotators Disagree and do Models Know?",
        "link_suffix": "/forum?id=1lB5ErmIY0",
        "link": "https://openreview.net/forum?id=1lB5ErmIY0",
        "pdf_link": "https://openreview.net/pdf?id=1lB5ErmIY0",
        "keywords": "RLHF, Pluralistic Alignment",
        "abstract": "We examine diverging preferences in human-labeled preference datasets. We develop a taxonomy of disagreement sources spanning 10 categories across four high-level classes---task underspecification, response style, refusals, and annotation errors. We find that the majority of disagreements  are in opposition with standard reward modeling approaches, which are designed with the assumption that annotator disagreement is noise. We then explore how these findings impact two areas of LLM development: reward modeling and evaluation. In our experiments, we demonstrate how standard reward modeling methods, like the Bradley-Terry model, fail to differentiate whether a given preference judgment is the result of unanimous agreement among annotators or the majority opinion among diverging user preferences. We also find that these tendencies are also echoed by popular LM-as-Judge evaluation methods, which consistently identify a winning response in cases of diverging preferences. These findings highlight remaining challenges in LLM evaluations, which are greatly influenced by divisive features like response style, and in developing pluralistically aligned LLMs. To address these issues, we develop methods for identifying diverging preferences to mitigate their influence in evaluations and during LLM training."
    },
    {
        "title": "DataMan: Data Manager for Pre-training Large Language Models",
        "link_suffix": "/forum?id=eNbA8Fqir4",
        "link": "https://openreview.net/forum?id=eNbA8Fqir4",
        "pdf_link": "https://openreview.net/pdf?id=eNbA8Fqir4",
        "keywords": "Large Language Models, Data Selection, Pre-Training",
        "abstract": "As the performance of large language models (LLMs) emerges via data scaling, the significance of pre-training data become increasingly evident. Although methods such as deduplication and high-quality sampling have explored data selection, comprehensive criteria for text quality remains underdeveloped, hindering efficient pre-training data selection and composition.\nThis paper establishes guidelines for data selection, fosters consensus on data quality, and introduces a management tool to evaluate data quality and domain types. \nWe believe that robust quality criteria should be applicable across diverse texts, showcasing semantic content understanding, and mutual complement.\nPrevious work mainly relies on intuition and lacks generalizability. To tackle this, we employ reverse thinking—\\emph{prompting LLMs to self-identify the causes of anomalous perplexity (PPL)} in training data—and outline 13 quality criteria linked to LLM performance, collectively derive a comprehensive\nmetric as \\emph{Overall Score}. \nWe developed a complete Prompt that integrates quality criteria and common LLM domains.\nWe uses LLM's pointwise ratings and compares the computational complexities of pointwise and pairwise ratings ((O(N)) V.S. (O(N^{2}))), showing that pointwise ratings are more feasible for vasrtdatasets, with over 95% agreement with human assessments.\nBy annotating 356K documents using GPT-4-turbo and fine-tuning a Qwen2-1.5B model, we created the \\textbf{Data} \\textbf{Man}ager (\\textbf{DataMan}), with an average sft accuracy across all criteria approaching 80% and 81.6% for Overall Score.\nWe annotated 447B tokens from the slimpajama corpus by DataMan, and selected a 30B token subset to maximize quality representation while ensuring diversity to train 1.3B-parameter LLM.\nResults show that models trained on DataMan-sampled data exceed state-of-the-art benchmarks in in-context learning (ICL) gain by 0.4% to 4.3% and in instrcut following winrate by 34.2% to 57%.\nThe strongest model ``Overall Score l=5'', significantly suparss models trained on uniform sampling with 50% more data.\nContinued pre-training on high-rated domain-specific data further boosts ICL performance, validating DataMan's effectiveness in domain mixing. \nWe reveals that PPL and ICL results do not strictly align, underscoring the distinction between understanding and generalization abilities.\nOur contributions include: i)-developing a data quality criteria system based on LLM PPL features; ii)-creating DataMan for data quality rating and domain identification; and iii)-releasing our code, models, and annotated datasets to facilitate research on the relationship between data and LLMs."
    },
    {
        "title": "Annotation-Efficient Language Model Alignment via Diverse and Representative Response Texts",
        "link_suffix": "/forum?id=JFk8F7w8Iz",
        "link": "https://openreview.net/forum?id=JFk8F7w8Iz",
        "pdf_link": "https://openreview.net/pdf?id=JFk8F7w8Iz",
        "keywords": "Language model alignment, Direct preference optimization",
        "abstract": "Preference optimization is a standard approach to fine-tuning large language models to align with human preferences.\nThe quantity, diversity, and representativeness of the preference dataset are critical to the effectiveness of preference optimization.\nHowever, obtaining a large amount of preference annotations is difficult in many applications.\nThis raises the question of how to use the limited annotation budget to create an effective preference dataset.\nTo this end, we propose Annotation-Efficient Preference Optimization (AEPO). \nInstead of exhaustively annotating preference over all available response texts, AEPO selects a subset of responses that maximizes diversity and representativeness from the available responses and then annotates preference over the selected ones.\nIn this way, AEPO focuses the annotation budget on labeling preference over a smaller subset of responses.\nWe evaluate the performance of Direct Preference Optimization (DPO) using AEPO and show that it outperforms models trained using a standard DPO with the same annotation budget."
    },
    {
        "title": "Learning Ante-hoc Explanations for Molecular Graphs",
        "link_suffix": "/forum?id=70xsq3EO2M",
        "link": "https://openreview.net/forum?id=70xsq3EO2M",
        "pdf_link": "https://openreview.net/pdf?id=70xsq3EO2M",
        "keywords": "graph neural network, explainer, molecular graph, ante-hoc",
        "abstract": "Explaining the decisions made by machine learning models for high-stakes applications is critical for transparency. This is particularly true in the case of models for graphs, where decisions depend on complex patterns combining structural and attribute data. We propose EAGER (Effective Ante-hoc Graph Explainer), a novel and flexible ante-hoc explainer designed to discover explanations for graph neural networks, with a focus on the chemical domain. As an ante-hoc model, EAGER inductively learn a graph predictive model and the associating explainer together. We employ a novel bilevel iterative training process based on optimizing the Information Bottleneck principle, effectively distilling the most useful substructures while discarding irrelevant details. As a result, EAGER can identify molecular substructures that contain the necessary and precise information needed for prediction. Our experiments on various molecular classification tasks show that EAGER explanations are better than existing post-hoc and ante-hoc approaches."
    },
    {
        "title": "EditRoom: LLM-parameterized Graph Diffusion for Composable 3D Room Layout Editing",
        "link_suffix": "/forum?id=Y2Dh8rWwlb",
        "link": "https://openreview.net/forum?id=Y2Dh8rWwlb",
        "pdf_link": "https://openreview.net/pdf?id=Y2Dh8rWwlb",
        "keywords": "3D Scene Editing, Large Language Model, Diffusion-based Models",
        "abstract": "Given the steep learning curve of professional 3D software and the time-\nconsuming process of managing large 3D assets, language-guided 3D scene editing has significant potential in fields such as virtual reality, augmented reality, and\ngaming. However, recent approaches to language-guided 3D scene editing either\nrequire manual interventions or focus only on appearance modifications without\nsupporting comprehensive scene layout changes. In response, we propose EditRoom, a unified framework capable of executing a variety of layout edits through\nnatural language commands, without requiring manual intervention. Specifically,\nEditRoom leverages Large Language Models (LLMs) for command planning and\ngenerates target scenes using a diffusion-based method, enabling six types of edits: rotate, translate, scale, replace, add, and remove. To address\nthe lack of data for language-guided 3D scene editing, we have developed an automatic pipeline to augment existing 3D scene synthesis datasets and introduced\nEditRoom-DB, a large-scale dataset with 83k editing pairs, for training and evaluation. Our experiments demonstrate that our approach consistently outperforms\nother baselines across all metrics, indicating higher accuracy and coherence in\nlanguage-guided scene layout editing."
    },
    {
        "title": "Personalized Language Modeling from Personalized Human Feedback",
        "link_suffix": "/forum?id=bqUsdBeRjQ",
        "link": "https://openreview.net/forum?id=bqUsdBeRjQ",
        "pdf_link": "https://openreview.net/pdf?id=bqUsdBeRjQ",
        "keywords": "Large Language Models, RLHF, Personalization",
        "abstract": "Personalized large language models (LLMs)  are designed to tailor responses to individual user preferences. While Reinforcement Learning from Human Feedback (RLHF) is a commonly used framework for aligning LLMs with human preferences, vanilla RLHF assumes that all human preferences share the same distribution, preventing fine-tuned LLMs from generating personalized content when user preferences are diverse. In this work, we propose Personalized-RLHF (P-RLHF), an efficient framework that utilizes a lightweight user model to capture individual user preferences and jointly learns the user model and the personalized LLM from human feedback. P-RLHF exhibits the following three characteristics: (1) It enables an LLM to generate personalized content and scale efficiently with growing number of users. (2) It handles both explicit user preferences described as textual input and implicit user preferences encoded in the feedback data. (3) It eliminates the need for users to fully articulate their preferences, which are normally needed for prompting LLMs to generate personalized content yet are often impractical to obtain in real-world scenarios. Our experimental results show that personalized LLMs trained using P-RLHF generate responses that are more closely aligned with individual user preferences, outperforming vanilla, non-personalized RLHF and prompting-based personalization approaches across different tasks."
    },
    {
        "title": "DiLQR: Differentiable Iterative Linear Quadratic Regulator",
        "link_suffix": "/forum?id=Mpp6SakVzl",
        "link": "https://openreview.net/forum?id=Mpp6SakVzl",
        "pdf_link": "https://openreview.net/pdf?id=Mpp6SakVzl",
        "keywords": "Differentiable control;iLQR;IL",
        "abstract": "Differentiable control promises end-to-end differentiability and adaptability, effectively combining the advantages of both model-free and model-based control\napproaches. However, the iterative Linear Quadratic Regulator (iLQR), despite\nbeing a powerful nonlinear controller, still lacks differentiable capabilities. The\nscalability of differentiating through extended iterations and horizons poses signifi\ncant challenges, hindering iLQR from being an effective differentiable controller.\nThis paper introduces a framework that facilitates differentiation through iLQR,\nallowing it to serve as a trainable and differentiable module, either as or within\na neural network. for control purposes. A novel aspect of this framework is the\nanalytical solution that it provides for the gradient of an iLQR controller through\nimplicit differentiation, which ensures a constant backward cost regardless of iteration, while producing an accurate gradient. We evaluate our framework on\nimitation tasks on famous control benchmarks. Our analytical method demonstrates superior computational performance, achieving up to $\\mathbf{128x}$ speedup and\na minimum of $\\mathbf{21x}$ speedup compared to automatic differentiation. Our method\nalso demonstrates superior learning performance ($\\mathbf{10^6}$x) compared to traditional\nneural network policies and better model loss with differentiable controllers that\nlack exact analytical gradients. Furthermore, we integrate our module into a\nlarger network with visual inputs to demonstrate the capacity of our method for\nhigh-dimensional, fully end-to-end tasks. Codes can be found on the project\nhomepagehttps://sites.google.com/view/dilqr/."
    },
    {
        "title": "On the Global Convergence of RLHF Based Alignment With Neural Parametrization",
        "link_suffix": "/forum?id=GCzpUJO5rx",
        "link": "https://openreview.net/forum?id=GCzpUJO5rx",
        "pdf_link": "https://openreview.net/pdf?id=GCzpUJO5rx",
        "keywords": "Reinforcement Learning from Human Feedback (RLHF)",
        "abstract": "The importance of Reinforcement Learning from Human Feedback (RLHF) in aligning large language models (LLMs) with human values cannot be overstated. RLHF is a three-stage process that includes supervised fine-tuning (SFT), reward learning, and policy learning. Although there are several offline and online approaches to aligning LLMs, they often suffer from distribution shift issues. These issues arise from the inability to accurately capture the distributional interdependence between the reward learning and policy learning stages. Consequently, this has led to various approximated approaches, but the theoretical insights and motivations remain largely limited to tabular settings, which do not hold in practice.\nThis gap between theoretical insights and practical implementations is critical. It is challenging to address this gap as it requires analyzing the performance of AI alignment algorithms in neural network-parameterized settings. Although bi-level formulations have shown promise in addressing distribution shift issues, they suffer from the hyper-gradient problem, and current approaches lack efficient algorithms to solve this. \nIn this work, we tackle these challenges employing the bi-level formulation laid out in Kwon et al. (2024) along with the assumptionWeak Gradient Dominationto demonstrate convergence in an RLHF setup, obtaining a sample complexity of  $\\epsilon^{-\\frac{7}{2}}$ . Our key contributions are twofold: (i) We propose a bi-level formulation for AI alignment in parameterized settings and introduce a first-order approach to solve this problem. (ii) We analyze the theoretical convergence rates of the proposed algorithm and derive state-of-the-art bounds. To the best of our knowledge, this is the first work to establish convergence rate bounds and global optimality for the RLHF framework in neural network-parameterized settings. Our contributions are primarily theoretical, providing crucial insights for future practical implementations."
    },
    {
        "title": "Exploration in the Face of Strategic Responses: Provable Learning of Online Stackelberg Games",
        "link_suffix": "/forum?id=Nw7i9Gd1WU",
        "link": "https://openreview.net/forum?id=Nw7i9Gd1WU",
        "pdf_link": "https://openreview.net/pdf?id=Nw7i9Gd1WU",
        "keywords": "Stackelberg game, RL, exploration",
        "abstract": "We study online leader-follower games where the leader interacts with a myopic follower using a quantal response policy. The leader's objective is to design an algorithm without prior knowledge of her reward function or the state transition dynamics. \nCrucially, the leader also lacks insight into the follower's reward function and realized rewards, posing a significant challenge. \nTo address this, the leader must learn the follower's quantal response mapping solely through strategic interactions --- announcing policies and observing responses. \nWe introduce a unified algorithm, Planning after Estimation, which updates the leader's policies in a two-step approach.In particular, we first jointly estimate the leader's value function and the follower's response mapping by maximizing a sum of the Bellman error of the value function, the likelihood of the quantal response model, and a regularization term that encourages exploration. The leader's policy is then updated through a greedy planning step based on these estimates. Our algorithm achieves a $\\sqrt{T}$-regret in the context of  general function approximation. \nMoroever, this algorithm avoids the intractable optimistic planning and thus enhances implementation simplicity."
    },
    {
        "title": "LocoVR: Multiuser Indoor Locomotion Dataset in Virtual Reality",
        "link_suffix": "/forum?id=9mBodivRIo",
        "link": "https://openreview.net/forum?id=9mBodivRIo",
        "pdf_link": "https://openreview.net/pdf?id=9mBodivRIo",
        "keywords": "Dataset, Human trajectory, Indoor locomotion, Virtual reality, Social motion behavior",
        "abstract": "Understanding human locomotion is crucial for AI agents such as robots, particularly in complex indoor home environments. Modeling human trajectories in these spaces requires insight into how individuals maneuver around physical obstacles and manage social navigation dynamics. These dynamics include subtle behaviors influenced by proxemics - the social use of space, such as stepping aside to allow others to pass or choosing longer routes to avoid collisions. Previous research has developed datasets of human motion in indoor scenes, but these are often limited in scale and lack the nuanced social navigation dynamics common in home environments. \nTo address this, we present LocoVR, a dataset of 7000+ two-person trajectories captured in virtual reality from over 130 different indoor home environments. LocoVR provides full body pose data and precise spatial information, along with rich examples of socially-motivated movement behaviors. \nFor example, the dataset captures instances of individuals navigating around each other in narrow spaces, adjusting paths to respect personal boundaries in living areas, and coordinating movements in high-traffic zones like entryways and kitchens. Our evaluation shows that LocoVR significantly enhances model performance in three practical indoor tasks utilizing human trajectories, and demonstrates predicting socially-aware navigation patterns in home environments."
    },
    {
        "title": "CrossModalNet: Multimodal Medical Segmentation with Guaranteed Cross-Modal Flow and Domain Adaptability",
        "link_suffix": "/forum?id=ZZVOrId3yN",
        "link": "https://openreview.net/forum?id=ZZVOrId3yN",
        "pdf_link": "https://openreview.net/pdf?id=ZZVOrId3yN",
        "keywords": "biomedical imaging, transfer learning",
        "abstract": "The fusion of multimodal data in medical image segmentation has emerged as a critical frontier in biomedical research, promising unprecedented diagnostic precision and insights. However, the intricate challenge of effectively integrating diverse data streams while preserving their unique characteristics has persistently eluded comprehensive solutions. This study introduces CrossModalNet, a groundbreaking architecture that revolutionizes multimodal medical image segmentation through advanced mathematical frameworks and innovative domain adaptation techniques. We present a rigorous mathematical analysis of CrossModalNet, proving its universal approximation capabilities and deriving tight generalization bounds. Furthermore, we introduce the Cross-Modal Information Flow (CMIF) metric, providing theoretical justification for the progressive integration of multimodal information through the network layers. Our Joint Adversarial Domain Adaptation (JADA) framework addresses the critical issue of domain shift, simultaneously aligning marginal and conditional distributions while preserving topological structures. Extensive experiments on the MM-WHS dataset demonstrate CrossModalNet's superior performance. This work not only advances the field of medical image segmentation but also provides a robust theoretical foundation for future research in multimodal learning and domain adaptation across various biomedical applications."
    },
    {
        "title": "Policy optimization can be memory-efficient: LLM Alignment Through Successive Policy Re-weighting (SPR)",
        "link_suffix": "/forum?id=LglOy15bqe",
        "link": "https://openreview.net/forum?id=LglOy15bqe",
        "pdf_link": "https://openreview.net/pdf?id=LglOy15bqe",
        "keywords": "Alignment, Large Language Models, Reinforcement Learning with Human Feedback, Policy Optimization, Reweighting",
        "abstract": "Reinforcement learning (RL) is serving as the cornerstone of aligning large language models (LLMs) to human behavior, by providing an appealing formulation and a suite of effective algorithms for learning behavior strategies through interacting with the underlying environment. Current paradigm of RL-based methods for LLM alignment, such as reinforcement learning with human feedback (RLHF) involves utilizing a reward function learned from extensive offline datasets to expediate the online training of reinforcement learning. The reward function learned is then used for policy optimization to obtain an improved policy (i.e. the LLM). Despite the success of RL approaches in aligning LLM with offline datasets, there are significant computational/limit of resources concern on applying RL-based methods for LLMs. For example, standard RLHF requires simultaneous loading of four models to the computing unit. In this paper, we develop a novel policy optimization algorithm named Successive Policy Re-weighting (SPR), matching the peak memory consumption of standard supervised fine-tune (SFT). Further, SPR can leverage both offline and online datasets to expediate online training and improve the sample efficiency. Specifically, SPR leverages a supervised learning subroutine to achieve policy improvement through re-weighting the policy according to the importance/performance of executed actions. Such simple and effective method is computationally inexpensive, requiring loading only one model at each update step, matching the computational cost of standard supervised fine-tuning procedure. Experimental results show that the proposed method can significantly outperform benchmark algorithms and accelerate the online training with available offline dataset."
    },
    {
        "title": "A Generic Class-agnostic Object Counting Network with Adaptive Offset Deformable Convolution",
        "link_suffix": "/forum?id=loihphEsnB",
        "link": "https://openreview.net/forum?id=loihphEsnB",
        "pdf_link": "https://openreview.net/pdf?id=loihphEsnB",
        "keywords": "Class-agnostic Object Counting, Adaptive Offset, 4D Convolution, Deformable Convolution",
        "abstract": "Class-agnostic object counting (CAC) aims at counting the number of objects in the unseen category in an image. In this paper, we design a generic class-agnostic object counting network with Adaptive Offset Deformable Convolution (AODC), which initially focus on the reference-less class-agnostic object counting task without any exemplar. Our method calculates the self-similarity maps of the image features and performing a 4D convolution on these maps, obtaining the adaptive offsets for the deformable convolution, so that the model can obtain complete information about the object at that location. Through this process, AODC is able to recognize objects of different scales in a same sample. In addition to this, we adopt our approach to both zero-shot setting and few-shot setting, the former with semantic text and the latter with visual exemplars as references. We conduct experiments on the few-shot object counting dataset FSC-147, as well as other large-scale datasets, and show that our method significantly outperforms state-of-the-art approaches on all the three settings."
    },
    {
        "title": "Prompting Fairness: Integrating Causality to Debias Large Language Models",
        "link_suffix": "/forum?id=7GKbQ1WT1C",
        "link": "https://openreview.net/forum?id=7GKbQ1WT1C",
        "pdf_link": "https://openreview.net/pdf?id=7GKbQ1WT1C",
        "keywords": "large language models, prompting, social biases, causality, debias, selection mechanisms",
        "abstract": "Large language models (LLMs), despite their remarkable capabilities, are susceptible to generating biased and discriminatory responses. As LLMs increasingly influence high-stakes decision-making (e.g., hiring and healthcare), mitigating these biases becomes critical. In this work, we propose a causality-guided debiasing framework to tackle social biases, aiming to reduce the harmful dependence between LLMs' decisions and the social information in the input. Our framework introduces a novel perspective to identify how social information can affect an LLM's decision through different causal pathways. Leveraging these causal insights, we outline principled prompting strategies that regulate these pathways through selection mechanisms. This framework not only unifies existing prompting-based debiasing techniques but also opens up new directions for reducing bias by encouraging the model to prioritize fact-based reasoning over reliance on biased social cues. We validate our framework through extensive experiments on real-world datasets across multiple domains, demonstrating its effectiveness in debiasing LLM decisions, even with only black-box access to the model."
    }
]