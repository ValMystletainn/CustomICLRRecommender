[{"title": "Learnability of Discrete Dynamical Systems under High Classification Noise", "link_suffix": "/forum?id=83iej2ANig", "link": "https://openreview.net/forum?id=83iej2ANig", "pdf_link": "https://openreview.net/pdf?id=83iej2ANig", "keywords": "Efficient learning under noise, Dynamical systems, PAC model, Sample complexity", "abstract": "Due to the important role of discrete dynamical systems in modeling real-world cascading phenomena on networks, problems for learning such systems have garnered considerable attention in ML. However, existing studies on this topic typically assume that the training data is noise-free, an assumption that is often impractical. In this work, we address this gap by investigating a more realistic and challenging setting: learning discrete dynamical systems from data contaminated with noise. Towards this end, we present efficient noise-tolerant learning algorithms that provide provable performance guarantees under the PAC model, and establish tight bounds on sample complexity. We show that, even in the presence of noise, the proposed learner only needs a small training set to infer a system. Notably, the number of training samples required by the algorithm in the noisy setting is the same (to within a constant factor) as the information-theoretic upper bound in the noise-free scenario. Further, the number of noisy training samples used by the algorithm is only a logarithmic factor higher than the best-known lower bound. Through experimental studies, we evaluate the empirical performance of the algorithms on both synthetic and real-world networks.", "title_embedding_index": 10900, "title_abs_embedding_index": 10925}, {"title": "Navigating the Labyrinth: Evaluating and Enhancing LLMs\u2019 Ability to Reason About Search Problems", "link_suffix": "/forum?id=DZBFchnM3b", "link": "https://openreview.net/forum?id=DZBFchnM3b", "pdf_link": "https://openreview.net/pdf?id=DZBFchnM3b", "keywords": "Mathematical & reasoning benchmark, Search & Combinatorial problems, A* algorithm", "abstract": "Recently, Large Language Models (LLMs) attained impressive performance in math and reasoning benchmarks. However, they still often struggle with multi-step reasoning which is relatively easy for humans. To further investigate this, we introduce a new benchmark, SearchBench, containing 11 unique combinatorial problems that avoid training contamination (each equipped with automated pipelines to generate an arbitrary number of instances) and analyze the feasibility, correctness, and optimality of LLM-generated solutions. We show that even the most advanced LLMs fail to solve these problems end-to-end in text, e.g., GPT4 and o1-preview respectively solve only 1.4% and 18.6% correctly. SearchBench problems require considering multiple pathways to the solution and backtracking, posing a significant challenge to auto-regressive models. Instructing LLMs to generate code that solves the problem helps only slightly. We next introduce an in-context learning approach that prompts the model to implement A*, an informed search algorithm, to comprehensively traverse the problem state space, improving the performance of models. We further extend this approach and propose the Multi-Stage-Multi-Try inference method which breaks down the A* algorithm implementation into two stages and auto-verifies the first stage against unit tests, raising GPT-4's performance above 57%.", "title_embedding_index": 10901, "title_abs_embedding_index": 10926}, {"title": "Neighbor-aware Geodesic Transportation for Neighborhood Refinery", "link_suffix": "/forum?id=DWI1xx2sX5", "link": "https://openreview.net/forum?id=DWI1xx2sX5", "pdf_link": "https://openreview.net/pdf?id=DWI1xx2sX5", "keywords": "image retrieval, reranking, deep clustering, self-supervised learning", "abstract": "Neighborhood refinery aims to enhance the neighbor relationships by refining the original distance matrix to ensure pairwise consistency.\nTraditional context-based methods, which encode instances alongside their local neighbors in a contextual affinity space, are limited in capturing global relationships and are vulnerable to the negative impacts of outliers in the neighborhood. To overcome these limitations, we propose a novel Neighbor-aware Geodesic Transportation (NGT) for the neighborhood refinery. NGT first constructs a global-aware distribution for each instance, capturing the intrinsic manifold relationships among all instances. This is followed by an optimization transportation process that utilizes the global-aware distribution within the underlying manifold, incorporating global geometric spatial information to generate a refined distance. NGT first involves Manifold-aware Neighbor Encoding (MNE) to project each instance into a global-aware distribution by constraining pairwise similarity with the corresponding affinity graph to capture global relationships. Subsequently, a Regularized Barycenter Refinery (RBR) module is proposed to integrate local neighbors into a barycenter, employing a Wasserstein term to reduce the influence of outliers. Lastly, Geodesic Transportation (GT) leverages geometric and global context information to transport the barycenter distribution along the geodesic paths within the affinity graph. Extensive evaluations on several tasks, such as re-ranking and deep clustering, demonstrate the superiority of our proposed NGT.", "title_embedding_index": 10902, "title_abs_embedding_index": 10927}, {"title": "Sailing in high-dimensional spaces: Low-dimensional embeddings through angle preservation", "link_suffix": "/forum?id=Revyper1Mi", "link": "https://openreview.net/forum?id=Revyper1Mi", "pdf_link": "https://openreview.net/pdf?id=Revyper1Mi", "keywords": "data visualization, low-dimensional embeddings, dimensionality reduction", "abstract": "Low-dimensional embeddings (LDEs) of high-dimensional data are ubiquitous in science and engineering. They allow us to quickly understand the main properties of the data, identify outliers and processing errors, and inform the next steps of data analysis. As such, LDEs have to be faithful to the original high-dimensional data, i.e., they should represent the relationships that are encoded in the data, both at a local as well as global scale. The current generation of LDE approaches focus on reconstructing local distances between pair of samples correctly, often outperforming traditional approaches aiming at all distances. For these approaches, global relationships are, however, usually strongly distorted, often argued to be an inherent trade-off between local and global structure learning for embeddings. We suggest a new perspective on LDE learning, reconstructing angles between data points. We show that our approach, MERCAT, yields good reconstruction across a diverse set of experiments and metrics, and preserve structures well across all scales. Compared to existing work, our approach also has a simple formulation, facilitating future theoretical analysis and algorithmic improvements.", "title_embedding_index": 10903, "title_abs_embedding_index": 10928}, {"title": "Improving Discrete Optimisation Via Decoupled Straight-Through Gumbel-Softmax", "link_suffix": "/forum?id=5pFV1FxG9d", "link": "https://openreview.net/forum?id=5pFV1FxG9d", "pdf_link": "https://openreview.net/pdf?id=5pFV1FxG9d", "keywords": "Gumbel-Max Trick, Gradient Estimation, Discretisation, Straight-Through Gumbel Softmax, Discrete Optimisation", "abstract": "Discrete representations play a crucial role in many deep learning architectures, yet their non-differentiable nature poses significant challenges for gradient-based optimization. To address this issue, various gradient estimators have been developed, including the Straight-Through Gumbel-Softmax (ST-GS) estimator, which combines the Straight-Through Estimator (STE) and the Gumbel-based reparameterization trick. However, the performance of ST-GS is highly sensitive to temperature, with its selection often compromising gradient fidelity. In this work, we propose a simple yet effective extension to ST-GS by employing decoupled temperatures for forward and backward passes, which we refer to as \"Decoupled ST-GS\". We show that our approach significantly enhances the original ST-GS through extensive experiments across multiple tasks and datasets. We further investigate the impact of our method on gradient fidelity from multiple perspectives, including the gradient gap and the bias-variance trade-off of estimated gradients. Our findings contribute to the ongoing effort to improve discrete optimization in deep learning, offering a practical solution that balances simplicity and effectiveness.", "title_embedding_index": 10904, "title_abs_embedding_index": 10929}, {"title": "Sanitizing LLMs: Retrospective Learning for Self-Correction of Inconsistent Samples via User Preferences", "link_suffix": "/forum?id=kFALGqLp46", "link": "https://openreview.net/forum?id=kFALGqLp46", "pdf_link": "https://openreview.net/pdf?id=kFALGqLp46", "keywords": "Large language Model, Prompt Based Task, New Downstream Task with Unsupervised Data, Unsupervised Data annotation with User Preference", "abstract": "With the advent of large language models (LLMs), using LLMs in conjunction with prompt-based tasks has demonstrated the ability to reduce the high cost and inefficiency of human annotations. Nonetheless, in unsupervised new downstream tasks that require user preferences to align data annotations with expectations, existing evaluation methods for prompt-based tasks become ineffective, especially when ground truth annotations are insufficient or missing. To fill this gap, we propose the novel Consistent and Inconsistent (CAI) Ratio, inspired by our experimental observation that LLMs underperform when the number of inconsistent samples\u2014those with inconsistent predictions across LLMs and the student model\u2014exceeds the number of consistent samples. By estimating the CAI ratio and identifying consistent and inconsistent samples with our proposed CAI identification approach, we aim to minimize inconsistency and enhance the accuracy of LLM-generated annotations for unsupervised data. To achieve this, we introduce Retrospective Learning (RL) with user preference, a data-centric approach that collaborates with the student model and LLMs, using a small number of human annotations as user preferences to resolve inconsistencies in the identified samples. Applied to five domain-specific NLP datasets, our Retrospective Learning approach, leveraging CAI identification, significantly improved the accuracy of LLM-generated responses, with the CAI ratio increasing as the accuracy improved.", "title_embedding_index": 10905, "title_abs_embedding_index": 10930}, {"title": "Neural Functions for Learning Periodic Signal", "link_suffix": "/forum?id=GCH5leffZp", "link": "https://openreview.net/forum?id=GCH5leffZp", "pdf_link": "https://openreview.net/pdf?id=GCH5leffZp", "keywords": "Periodicity, Neural function", "abstract": "As function approximators, deep neural networks have served as an effective tool to represent various signal types. Recent approaches utilize multi-layer perceptrons (MLPs) to learn a nonlinear mapping from a coordinate to its corresponding signal, facilitating the learning of continuous neural representations from discrete data points. Despite notable successes in learning diverse signal types, coordinate-based MLPs often face issues of overfitting and limited generalizability beyond the training region, resulting in subpar extrapolation performance. This study addresses scenarios where the underlying true signals exhibit periodic properties, either spatially or temporally. We propose a novel network architecture, which extracts periodic patterns from measurements and leverages this information to represent the signal, thereby enhancing generalization and improving extrapolation performance. We demonstrate the efficacy of the proposed method through comprehensive experiments, including the learning of the periodic solutions for differential equations, and time series imputation (interpolation) and forecasting (extrapolation) on real-world datasets.", "title_embedding_index": 10906, "title_abs_embedding_index": 10931}, {"title": "Language Guided Skill Discovery", "link_suffix": "/forum?id=i3e92uSZCp", "link": "https://openreview.net/forum?id=i3e92uSZCp", "pdf_link": "https://openreview.net/pdf?id=i3e92uSZCp", "keywords": "Unsupervised Skill Discovery, Guided Skill Discovery, Reinforcement Learning, Language Guided RL", "abstract": "Skill discovery methods enable agents to learn diverse emergent behaviors without explicit rewards. To make learned skills useful for downstream tasks, obtaining a semantically diverse repertoire of skills is crucial. While some approaches use discriminators to acquire distinguishable skills and others focus on increasing state coverage, the direct pursuit of \u2018semantic diversity\u2019 in skills remains underexplored. We hypothesize that leveraging the semantic knowledge of large language models (LLM) can lead us to improve semantic diversity of resulting behaviors. In this sense, we introduce Language Guided Skill Discovery (LGSD), a skill discovery framework that aims to directly maximize the semantic diversity between skills. LGSD takes user prompts as input and outputs a set of semantically distinctive skills. The prompts serve as a means to constrain the search space into a semantically desired subspace, and the generated LLM outputs guide the agent to visit semantically diverse states within the subspace. We demonstrate that LGSD enables legged robots to visit different user-intended areas on a plane by simply changing the prompt. Furthermore, we show that language guidance aids in discovering more diverse skills compared to five existing skill discovery methods in robot-arm manipulation environments. Lastly, LGSD provides a simple way of utilizing learned skills via natural language.", "title_embedding_index": 10907, "title_abs_embedding_index": 10932}, {"title": "Controlling Language and Diffusion Models by Transporting Activations", "link_suffix": "/forum?id=l2zFn6TIQi", "link": "https://openreview.net/forum?id=l2zFn6TIQi", "pdf_link": "https://openreview.net/pdf?id=l2zFn6TIQi", "keywords": "controllability, generative models, toxicity, diffusion", "abstract": "The increasing capabilities of large generative models and their ever more widespread deployment have raised concerns about their reliability, safety, and potential misuse. To address these issues, recent works have proposed to control model generation by steering model activations in order to effectively induce or prevent the emergence of concepts or behaviours in the generated output. In this paper we introduce Activation Transport (AcT), a general framework to steer activations guided by optimal transport theory that generalizes many previous activation-steering works. AcT is modality-agnostic and provides fine-grained control over the model behaviour with negligible computational overhead, while minimally impacting model abilities. We experimentally show the effectiveness and versatility of our approach by addressing key challenges in large language models (LLMs) and text-to-image diffusion models (T2Is). For LLMs, we show that AcT can effectively mitigate toxicity, induce arbitrary concepts, and increase their truthfulness. In T2Is, we show how AcT enables fine-grained style control and concept negation.", "title_embedding_index": 10908, "title_abs_embedding_index": 10933}, {"title": "Memory retaining finetuning via distillation", "link_suffix": "/forum?id=5T3gpfUam7", "link": "https://openreview.net/forum?id=5T3gpfUam7", "pdf_link": "https://openreview.net/pdf?id=5T3gpfUam7", "keywords": "finetuning, alignment, forgetting, distillation", "abstract": "Large language models (LLMs) pretrained on large corpora of internet text possess much of the world knowledge.\nFollowing pretraining, one often needs to conduct continued pretraining on certain capabilities such as math and coding, or \"posttraining\" (a.k.a., alignment) techniques to make the models follow users' instructions and align them with human preferences.\nOne challenge during these finetuning stages is that the model can lose the pretraining knowledge or forget certain capabilities (e.g., in-context learning ability).\nMoreover, although there exist strong open-weight LLMs such as Llama 3, both their pretraining and posttraining data are not open to the public, making it difficult to mix the finetuning data with the models' own pretraining data as a solution for mitigating forgetting.\nWe propose label annealing, a method that mitigates forgetting during finetuning without requiring access to the original pretraining data.\nLabel annealing distills pretraining knowledge during finetuing by adding a KL divergence term in the loss function, regularizing the divergence between the finetuned model's predictions to those of the initial pretrained model.\nIn mathematics and code finetuning, label annealing improves the model's performance in target domains without sacrificing other capabilities of the pretrained model.\nIn alignment finetuning, our method introduces a smooth tradeoff between the instruction-following capability and the pretraining knowledge.\nWe complement our empirical investigation with a mathematical model with overparameterized linear regression that provides geometric intuition why label annealing would help.", "title_embedding_index": 10909, "title_abs_embedding_index": 10934}, {"title": "Too Big to Fool: Resisting Deception in Language Models", "link_suffix": "/forum?id=tet8yGrbcf", "link": "https://openreview.net/forum?id=tet8yGrbcf", "pdf_link": "https://openreview.net/pdf?id=tet8yGrbcf", "keywords": "Large Language Models, Evaluation, Misinformation, In-Context Learning, World Models, Reasoning", "abstract": "Large language models must balance their weight-encoded knowledge with in-context information from prompts to generate accurate responses. This paper investigates this interplay by analyzing how models of varying capacities within the same family handle intentionally misleading in-context information. Our experiments demonstrate that larger models exhibit higher resilience to deceptive prompts, showcasing an advanced ability to interpret and integrate prompt information with their internal knowledge. Furthermore, we find that larger models outperform smaller ones in following legitimate instructions, indicating that their resilience is not due to disregarding in-context information. We also show that this phenomenon is likely not a result of memorization but stems from the models' ability to better leverage implicit task-relevant information from the prompt alongside their internally stored knowledge.", "title_embedding_index": 10910, "title_abs_embedding_index": 10935}, {"title": "Towards Better Benchmark Datasets for Inductive Knowledge Graph Completion", "link_suffix": "/forum?id=npBAHV5BJI", "link": "https://openreview.net/forum?id=npBAHV5BJI", "pdf_link": "https://openreview.net/pdf?id=npBAHV5BJI", "keywords": "Knowledge graphs, graphs, link prediction", "abstract": "Knowledge Graph Completion (KGC) attempts to predict missing facts in a Knowledge Graph (KG). Recently, there's been an increased focus on designing KGC methods that can excel in theinductive setting, where a portion or all of the entities and relations seen in inference are unobserved during training. Numerous benchmark datasets have been proposed for inductive KGC, all of which are subsets of existing KGs used for transductive KGC. However, we find that the current procedure for constructing inductive KGC datasets inadvertently creates a shortcut that can be exploited even while disregarding the relational information. Specifically, we observe that the Personalized PageRank (PPR) score can achieve strong or near SOTA performance on most inductive datasets. In this paper, we study the root cause of this problem. Using these insights, we propose an alternative strategy for constructing inductive KGC datasets that helps mitigate the PPR shortcut. We then benchmark multiple popular methods using the newly constructed datasets and analyze their performance. The new benchmark datasets help promote a better understanding of the capabilities and challenges of inductive KGC by removing any shortcuts that obfuscate performance.", "title_embedding_index": 10911, "title_abs_embedding_index": 10936}, {"title": "A Differentiable Rank-Based Objective for Better Feature Learning", "link_suffix": "/forum?id=KiN7g8mf9N", "link": "https://openreview.net/forum?id=KiN7g8mf9N", "pdf_link": "https://openreview.net/pdf?id=KiN7g8mf9N", "keywords": "ranks, statistics, machine learning, feature learning, rank ordering correlation, fairness", "abstract": "In this paper, we leverage existing statistical methods to better understand feature learning from data. We tackle this by modifying the model-free variable selection method, Feature Ordering by Conditional Independence (FOCI), which is introduced in Azadkia & Chatterjee (2021). While FOCI is based on a non-parametric coefficient of conditional dependence, we introduce its parametric, differentiable approximation. With this approximate coefficient of correlation, we present a new algorithm called difFOCI, which is applicable to a wider range of machine learning problems thanks to its differentiable nature and learnable parameters. We present difFOCI in three contexts: (1) as a variable selection method with baseline comparisons to FOCI, (2) as a trainable model parametrized with a neural network, and (3) as a generic, widely applicable neural network regularizer, one that improves feature learning with better management of spurious correlations. We evaluate difFOCI on increasingly complex problems ranging from basic variable selection in toy examples to saliency map comparisons in convolutional networks. We then show how difFOCI can be incorporated in the context of fairness to facilitate classifications without relying on sensitive data.", "title_embedding_index": 10912, "title_abs_embedding_index": 10937}, {"title": "SALMONN-omni: A Speech Understanding and Generation LLM in a Codec-free Full-duplex Framework", "link_suffix": "/forum?id=eJpI20hzWf", "link": "https://openreview.net/forum?id=eJpI20hzWf", "pdf_link": "https://openreview.net/pdf?id=eJpI20hzWf", "keywords": "Large speech text model, Full-duplex model", "abstract": "Speech large language models (LLMs) offer a unified approach to handling various speech-processing tasks using a single autoregressive model built on discrete speech and audio codecs. Unlike traditional pipeline-based systems, which involve separate components for speech recognition, understanding, and generation, end-to-end speech LLMs can capture both verbal and non-verbal information, such as paralinguistic and speaker characteristics. This enables full-duplex capabilities, allowing the system to listen and speak simultaneously with low latency, making it ideal for conversational AI. In this paper, we introduce a novel codec-free, full-duplex framework for speech understanding and generation, and present {SALMONN-omni}, an instance of this speech LLM. SALMONN-omni can listen to its own generated speech and background sounds while speaking. To align the frame rate gap between text and audio, we propose a novel \\textit{thinking} step, ensuring high performance on pre-trained tasks. Using a two-stage \\textit{understand then generate} training approach, SALMONN-omni effectively addresses a variety of streaming speech tasks, including speech recognition, synthesis, enhancement, dereverberation, target speaker extraction, and spoken question answering.", "title_embedding_index": 10913, "title_abs_embedding_index": 10938}, {"title": "Exploration by Running Away from the Past", "link_suffix": "/forum?id=zyGrziIVdE", "link": "https://openreview.net/forum?id=zyGrziIVdE", "pdf_link": "https://openreview.net/pdf?id=zyGrziIVdE", "keywords": "Reinforcement Learning, Exploration, Deep Learning", "abstract": "The ability to explore efficiently and effectively is a central challenge of reinforcement learning.\nIn this work, we consider exploration through the lens of information theory.\nSpecifically, we cast exploration as a problem of maximizing the Shannon entropy of the state occupation measure.\nThis is done by maximizing a sequence of divergences between distributions representing an agent's past behavior and its current behavior.\nIntuitively, this encourages the agent to explore new behaviors that are distinct from past behaviors.\nHence, we call our method RAMP, for ``$\\textbf{R}$unning $\\textbf{A}$way fro$\\textbf{m}$ the $\\textbf{P}$ast.''\nA fundamental question of this method is the quantification of the distribution change over time.\nWe consider both the Kullback-Leibler divergence and the Wasserstein distance to quantify divergence between successive state occupation measures, and explain why the former might lead to undesirable exploratory behaviors in some tasks. \nWe demonstrate that by encouraging the agent to explore by actively distancing itself from past experiences, it can effectively explore mazes and a wide range of behaviors on robotic manipulation and locomotion tasks.", "title_embedding_index": 10914, "title_abs_embedding_index": 10939}, {"title": "LOL-EVE: Predicting Promoter Variant  Effects from Evolutionary Sequences", "link_suffix": "/forum?id=LQRglYZ2Ri", "link": "https://openreview.net/forum?id=LQRglYZ2Ri", "pdf_link": "https://openreview.net/pdf?id=LQRglYZ2Ri", "keywords": "Genomics, LLM, bioinformatics, compuational biology", "abstract": "Genetic studies reveal extensive disease-associated variation across the human genome, predominantly in noncoding regions, such as promoters. Quantifying the impact of these variants on disease risk is crucial to our understanding of the underlying disease mechanisms and advancing personalized medicine. However, current computational methods struggle to capture variant effects, particularly those of insertions and deletions (indels), which can significantly disrupt gene expression. To address this challenge, we present LOL-EVE (Language Of Life across EVolutionary Effects), a conditional autoregressive transformer model trained on 14.6 million diverse mammalian promoter sequences. Leveraging evolutionary information and proximal genetic context, LOL-EVE predicts indel variant effects in human promoter regions. We introduce three new benchmarks for indel variant effect prediction in promoter regions, comprising the identification of causal eQTLs, prioritization of rare variants in the human population, and understanding disruptions of transcription factor binding sites. We find that LOL-EVE achieves state-of-the-art performance on these tasks, demonstrating the potential of region-specific large genomic language models and offering a powerful tool for prioritizing potentially causal non-coding variants in disease studies.", "title_embedding_index": 10915, "title_abs_embedding_index": 10940}, {"title": "Takin-VC: Zero-shot Voice Conversion via Jointly Hybrid Content and Memory-Augmented Context-Aware Timbre Modeling", "link_suffix": "/forum?id=F0TrRRKkQT", "link": "https://openreview.net/forum?id=F0TrRRKkQT", "pdf_link": "https://openreview.net/pdf?id=F0TrRRKkQT", "keywords": "zero-shot voice conversion, hybrid content encoder, memory-augmented context-aware timbre modeling, conditional flow matching", "abstract": "Zero-shot voice conversion (VC) aims to transform the source speaker timbre into an arbitrary unseen one without altering the original speech content. While recent advancements in zero-shot VC methods have shown remarkable progress, there still remains considerable potential for improvement in terms of improving speaker similarity and speech naturalness. In this paper, we propose Takin-VC, a novel zero-shot VC framework based on jointly hybrid content and memory-augmented context-aware timbre modeling to tackle this challenge. Specifically, an effective hybrid content encoder, guided by neural codec training, that leverages quantized features from pre-trained WavLM and HybridFormer is first presented to extract the linguistic content of the source speech. Subsequently, we introduce an advanced cross-attention-based context-aware timbre modeling approach that learns the fine-grained, semantically associated target timbre features. To further enhance both speaker similarity and real-time performance, we utilize a conditional flow matching model to reconstruct the Mel-spectrogram of the source speech. Additionally, we advocate an efficient memory-augmented module designed to generate high-quality conditional target inputs for the flow matching process, thereby improving the overall performance of the proposed system. Experimental results demonstrate that the proposed Takin-VC method surpasses state-of-the-art zero-shot VC systems, delivering superior performance in terms of both speech naturalness and speaker similarity.", "title_embedding_index": 10916, "title_abs_embedding_index": 10941}, {"title": "Robust Deep Equivariant Structure from Motion", "link_suffix": "/forum?id=wldwEhQ7cl", "link": "https://openreview.net/forum?id=wldwEhQ7cl", "pdf_link": "https://openreview.net/pdf?id=wldwEhQ7cl", "keywords": "3D Reconstruction, Outlier Removal, Structure from Motion", "abstract": "Multiview Structure from Motion is a fundamental and challenging computer vision problem. A recent deep-based approach utilized matrix equivariant architectures for simultaneous recovery of camera pose and 3D scene structure from large image collections. That work, however, made the unrealistic assumption that the point tracks given as input are almost clean of outliers. Here, we propose an architecture suited to dealing with outliers by adding a multiview inlier/outlier classification module that respects the model equivariance and by utilizing a robust bundle adjustment step. Experiments demonstrate that our method can be applied successfully in realistic settings that include large image collections and point tracks extracted with common heuristics that include many outliers, achieving state-of-the-art accuracies in almost all runs, superior to existing deep-based methods and on-par with leading classical (non-deep) sequential and global methods.", "title_embedding_index": 10917, "title_abs_embedding_index": 10942}, {"title": "TweedieMix: Improving Multi-Concept Fusion for Diffusion-based Image/Video Generation", "link_suffix": "/forum?id=ee2c4MEx9l", "link": "https://openreview.net/forum?id=ee2c4MEx9l", "pdf_link": "https://openreview.net/pdf?id=ee2c4MEx9l", "keywords": "Diffusion Models, Custom Concepts, Text-to-Image, Video", "abstract": "Despite significant advancements in customizing text-to-image and video generation models, generating images and videos that effectively integrate multiple personalized concepts remains a challenging task. To address this, we present TweedieMix, a novel method for composing customized diffusion models during the inference phase. By analyzing the properties of reverse diffusion sampling, our approach divides the sampling process into two stages. During the initial steps, we apply a multiple object-aware sampling technique to ensure the inclusion of the desired target objects. In the later steps, we blend the appearances of the custom concepts in the de-noised image space using Tweedie's formula. Our results demonstrate that TweedieMix can generate multiple personalized concepts with higher fidelity than existing methods. Moreover, our framework can be effortlessly extended to image-to-video diffusion models, enabling the generation of videos that feature multiple personalized concepts.", "title_embedding_index": 10918, "title_abs_embedding_index": 10943}, {"title": "Filtered not Mixed: Filtering-Based Online Gating for Mixture of Large Language Models", "link_suffix": "/forum?id=ecIvumCyAj", "link": "https://openreview.net/forum?id=ecIvumCyAj", "pdf_link": "https://openreview.net/pdf?id=ecIvumCyAj", "keywords": "Optimal filtering, LLMs, mixture-of-experts, time-series-forecasting, financial-market-movement", "abstract": "We propose MoE-F \u2014 a formalized mechanism for combining N pre-trained expert Large Language Models (LLMs) in online time-series prediction tasks by adaptively forecasting the best weighting of LLM predictions at every time step. Our mechanism leverages the conditional information in each expert's running performance to forecast the best combination of LLMs for predicting the time series in its next step. Diverging from static (learned) Mixture of Experts (MoE) methods, our approach employs time-adaptive stochastic filtering techniques to combine experts. By framing the expert selection problem as a finite state-space, continuous-time Hidden Markov model (HMM), we can leverage the Wohman-Shiryaev filter. Our approach first constructs N parallel filters corresponding to each of the N individual LLMs. Each filter proposes its best combination of LLMs, given the information that they have access to. Subsequently, the N filter outputs are optimally aggregated to maximize their robust predictive power, and this update is computed efficiently via a closed-form expression, thus generating our ensemble predictor.Our contributions are:(I)the MoE-F algorithm \u2014 deployable as a plug-and-play filtering harness,(II)theoretical optimality guarantees of the proposed filtering-based gating algorithm (via optimality guarantees for its parallel Bayesian filtering and its robust aggregation steps), and(III)empirical evaluation and ablative results using state-of-the-art foundational and MoE LLMs on a real-worldFinancial Market Movementtask where MoE-F attains a remarkable 17% absolute and 48.5% relative F1 measure improvement over the next best performing individual LLM expert predicting short-horizon market movement based on streaming news. Further, we provide empirical evidence of substantial performance gains in applying MoE-F over specialized models in thelong-horizon time-series forecastingdomain.", "title_embedding_index": 10919, "title_abs_embedding_index": 10944}, {"title": "Sources of Gain: Decomposing Performance in Conditional Average Dose Response Estimation", "link_suffix": "/forum?id=p1b96KC6rj", "link": "https://openreview.net/forum?id=p1b96KC6rj", "pdf_link": "https://openreview.net/pdf?id=p1b96KC6rj", "keywords": "Dose response estimation, Causal Machine Learning, Performance Decomposition, Datasets", "abstract": "Estimating conditional average dose responses (CADR) is an important but challenging problem. Estimators must correctly model the potentially complex relationships between covariates, interventions, doses, and outcomes. In recent years, the machine learning community has shown great interest in developing tailored CADR estimators that target specific challenges. Their performance is typically evaluated against other methods on (semi-) synthetic benchmark datasets. Our paper analyses this practice and shows that using popular benchmark datasets without further analysis is insufficient to judge model performance. Established benchmarks entail multiple challenges, whose impacts must be disentangled. Therefore, we propose a novel decomposition scheme that allows the evaluation of the impact of five distinct components contributing to CADR estimator performance. We apply this scheme to eight popular CADR estimators on four widely-used benchmark datasets, running nearly 1,500 individual experiments. Our results reveal that most established benchmarks are challenging for reasons different from their creators' claims. Notably, we find that confounding - the key challenge that motivated recent methods - does not significantly affect CADR estimation performance for the considered datasets. We discuss the major implications of our findings and present directions for future research.", "title_embedding_index": 10920, "title_abs_embedding_index": 10945}, {"title": "LiNeS: Post-training Layer Scaling Prevents Forgetting and Enhances Model Merging", "link_suffix": "/forum?id=J5sUOvlLbQ", "link": "https://openreview.net/forum?id=J5sUOvlLbQ", "pdf_link": "https://openreview.net/pdf?id=J5sUOvlLbQ", "keywords": "Model merging; Model editing; Catastrophic forgetting; Multi-task learning;  OOD generalization", "abstract": "Large pre-trained models exhibit impressive zero-shot performance across diverse tasks, but fine-tuning often leads to catastrophic forgetting, where improvements on a target domain degrade generalization on other tasks. To address this challenge, we introduce LiNeS, Layer-Increasing Network Scaling, a post-training editing technique designed to preserve pre-trained generalization while enhancing fine-tuned task performance. LiNeS scales parameter updates linearly based on their layer depth within the network, maintaining shallow layers close to their pre-trained values to preserve general features, while allowing deeper layers to retain task-specific representations.\nWe further extend this approach to multi-task model merging scenarios, where layer-wise scaling of merged parameters reduces negative task interference. LiNeS demonstrates significant improvements in both single-task and multi-task settings across various benchmarks in vision and natural language processing. It enhances out-of-distribution generalization, integrates seamlessly with existing multi-task model merging baselines enchancing their performance across bcenchmarks and model sizes, and can boost generalization when merging LLM policies aligned with different rewards via RLHF. Importantly, our method is simple to implement and complementary to many existing techniques.", "title_embedding_index": 10921, "title_abs_embedding_index": 10946}, {"title": "PASRL: Stabilising Reinforcement Learning with Past Action-State Representation Learning", "link_suffix": "/forum?id=IcHHjgdb0o", "link": "https://openreview.net/forum?id=IcHHjgdb0o", "pdf_link": "https://openreview.net/pdf?id=IcHHjgdb0o", "keywords": "Reinforcement learning, action smoothness, recurrent neural networks", "abstract": "Although deep reinforcement learning (DRL) deals with sequential decision making problems, temporal information representation is absent from state of the art actor-critic algorithms. The reliance on only the current time step information and densely connected neural networks causes instability and oscillations in the smoothness of concurrent actions. Therefore many applied DRL robotics control methods employ various reward shaping, low-pass filter and traditional controller based methods to mitigate this effect. However the interactions of these different parts hinders the performance of the original goal for the RL algorithm. In this paper we present a reinforcement learning algorithm extended with past action-state representation learning, which allows for the end-to-end training of RL based control methods without the need for common heuristics. Our end-to-end training approach produces the smoothest actions while achieving performance scores comparable to the top mixed traditional control and reinforcement learning algorithms.", "title_embedding_index": 10922, "title_abs_embedding_index": 10947}, {"title": "Branches: A Fast Dynamic Programming and Branch & Bound Algorithm for Optimal Decision Trees", "link_suffix": "/forum?id=Mw16Akb1CR", "link": "https://openreview.net/forum?id=Mw16Akb1CR", "pdf_link": "https://openreview.net/pdf?id=Mw16Akb1CR", "keywords": "Optimal Decision Trees, Dynamic Programming, Branch & Bound, Markov Decision Process", "abstract": "Decision Tree (DT) Learning is a fundamental problem in Interpretable Machine Learning, yet it poses a formidable optimisation challenge. Despite numerous efforts dating back to the early 1990's, practical algorithms have only recently emerged, primarily leveraging Dynamic Programming (DP) and Branch & Bound (B&B) techniques. These methods fall into two categories: algorithms like DL8.5, MurTree and STreeD utilise an efficient DP strategy but lack effective bounds for pruning the search space; while algorithms like OSDT and GOSDT employ more efficient pruning bounds but at the expense of a less refined DP strategy. We introduce Branches, a new algorithm that combines the strengths of both approaches. Using DP and B&B with a novel analytical bound for efficient pruning, Branches offers both speed and sparsity optimisation. Unlike other methods, it also handles non-binary features. Theoretical analysis shows its lower complexity compared to existing methods, and empirical results confirm that Branches outperforms the state-of-the-art in speed, iterations, and optimality.", "title_embedding_index": 10923, "title_abs_embedding_index": 10948}, {"title": "RankMatch: A Novel Approach to Semi-Supervised Label Distribution Learning Leveraging Inter-label Correlations", "link_suffix": "/forum?id=G2BiEoB77Z", "link": "https://openreview.net/forum?id=G2BiEoB77Z", "pdf_link": "https://openreview.net/pdf?id=G2BiEoB77Z", "keywords": "multi-label learning; semi-supervised learning; label distributiob learning", "abstract": "This paper introduces RankMatch, an innovative approach for Semi-Supervised Label Distribution Learning (SSLDL). Addressing the challenge of limited labeled data, RankMatch effectively utilizes a small number of labeled examples in conjunction with a larger quantity of unlabeled data, reducing the need for extensive manual labeling in Deep Neural Network (DNN) applications. Specifically, RankMatch introduces an ensemble learning-inspired averaging strategy that creates a pseudo-label distribution from multiple weakly augmented images. This not only stabilizes predictions but also enhances the model's robustness. Beyond this, RankMatch integrates a pairwise relevance ranking (PRR) loss, capturing the complex inter-label correlations and ensuring that the predicted label distributions align with the ground truth. We establish a theoretical generalization bound for RankMatch, and through extensive experiments, demonstrate its superiority in performance against existing SSLDL methods.", "title_embedding_index": 10924, "title_abs_embedding_index": 10949}]