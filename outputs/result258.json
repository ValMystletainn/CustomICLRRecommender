[{"title": "Reverse the auditory processing pathway: Coarse-to-fine audio reconstruction from fMRI", "link_suffix": "/forum?id=3JoLo0mmHH", "link": "https://openreview.net/forum?id=3JoLo0mmHH", "pdf_link": "https://openreview.net/pdf?id=3JoLo0mmHH", "keywords": "Brain-to-audio reconstruction, Coarse-to-fine, fMRI, Auditory processing pathway", "abstract": "Drawing inspiration from the hierarchical processing of the human auditory system, which transforms sound from low-level acoustic features to high-level semantic understanding, we introduce a novel coarse-to-fine audio reconstruction method. Leveraging non-invasive functional Magnetic Resonance Imaging (fMRI) data, our approach mimics the inverse pathway of auditory processing. Initially, we utilize CLAP to decode fMRI data coarsely into a low-dimensional semantic space, followed by a fine-grained decoding into the high-dimensional AudioMAE latent space guided by semantic features. These fine-grained neural features serve as conditions for audio reconstruction through a Latent Diffusion Model (LDM). Validation on three public fMRI datasets\u2014Brain2Sound, Brain2Music, and Brain2Speech\u2014underscores the superiority of our coarse-to-fine decoding method over stand-alone fine-grained approaches, showcasing state-of-the-art performance in metrics like FD, FAD, and KL. Moreover, by employing semantic prompts during decoding, we enhance the quality of reconstructed audio when semantic features are suboptimal. The demonstrated versatility of our model across diverse stimuli highlights its potential as a universal brain-to-audio framework. This research contributes to the comprehension of the human auditory system, pushing boundaries in neural decoding and audio reconstruction methodologies.", "title_embedding_index": 12850, "title_abs_embedding_index": 12875}, {"title": "Understanding Constraint Inference in Safety-Critical Inverse Reinforcement Learning", "link_suffix": "/forum?id=B2RXwASSpy", "link": "https://openreview.net/forum?id=B2RXwASSpy", "pdf_link": "https://openreview.net/pdf?id=B2RXwASSpy", "keywords": "Constraint Inference, Training Efficiency, Cross-environment Transferability", "abstract": "In practical applications, the underlying constraint knowledge is often unknown and difficult to specify. To address this issue, recent advances in Inverse Constrained Reinforcement Learning (ICRL) have focused on inferring these constraints from expert demonstrations. However, the ICRL approach typically characterizes constraint learning as a tri-level optimization problem, which is inherently complex due to its interdependent variables and multiple layers of optimization.\nConsidering these challenges, a critical question arises:Can we implicitly embed constraint signals into reward functions and effectively solve this problem using a classic reward inference algorithm?The resulting method, known as Inverse Reward Correction (IRC), merits investigation. In this work, we conduct a theoretical analysis comparing the sample complexities of both solvers. Our findings confirm that the IRC solver achieves lower sample complexity than its ICRL counterpart.\nNevertheless, this reduction in complexity comes at the expense of generalizability. Specifically, in the target environment, the reward correction terms may fail to guarantee the safety of the resulting policy, whereas this issue can be effectively mitigated by transferring the constraints via the ICRL solver. \nAdvancing our inquiry, we investigate conditions under which the ICRL solver ensures $\\epsilon$-optimality when transferring to new environments. Empirical results across various environments validate our theoretical findings, underscoring the nuanced trade-offs between complexity reduction and generalizability in safety-critical applications.", "title_embedding_index": 12851, "title_abs_embedding_index": 12876}, {"title": "LVP: Language-guide Visual Projector for Efficient Multimodal LLM", "link_suffix": "/forum?id=PxBzxO02Ef", "link": "https://openreview.net/forum?id=PxBzxO02Ef", "pdf_link": "https://openreview.net/pdf?id=PxBzxO02Ef", "keywords": "Multimodal large language model, Visual projector, Language-guide visual token selection.", "abstract": "Visual projector plays a crucial role in bridging the visual model and the large language model (LLM) in modern multimodal LLM. \nTypically, MLLMs utilize a simple MLP to preserve all visual tokens, causing a heavy computational burden and redundant visual tokens.\nSome recent works adopt either a resampler or an adaptive pooling to reduce the visual tokens. However, they only reduce the visual tokens based on the image feature,\nleading to the feature misalignment between visual tokens and text tokens. In this paper, we present a novel Language-guidance Visual Projector (LVP), where the text \nfeature serves as a guide to selecting the important visual tokens. Specially, we first adopt a lightweight text encoder to extract the text feature. Then, a lightweight\ncross-modal feature enhancement module is proposed to enhance the cross-modal feature alignment. Finally, we select the important visual tokens according to the feature similarity between visual tokens and text tokens and apply\na deformable attention module to integrate the visual features from the visual encoder into the selected visual tokens. We further propose a multi-level language-guidance visual projector, which selects the visual tokens from different stages of the visual encoder.\nExtensive experiments demonstrate that our LVP compresses the visual tokens by 75%~95% while achieving competitive even better performance across diverse benchmarks with a significant efficiency advantage. For instance, LLaVA1.5-LVP with Vicuna-13B\nobtains 71.8% accuracy on VQA$^\\text{T}$, realizing the state-of-the-art result.", "title_embedding_index": 12852, "title_abs_embedding_index": 12877}, {"title": "Few for Many: Tchebycheff Set Scalarization for Many-Objective Optimization", "link_suffix": "/forum?id=O4N9kWwV6R", "link": "https://openreview.net/forum?id=O4N9kWwV6R", "pdf_link": "https://openreview.net/pdf?id=O4N9kWwV6R", "keywords": "multi-objective optimization, many-objective optimization, Tchebycheff scalarization", "abstract": "Multi-objective optimization can be found in many real-world applications where some conflicting objectives can not be optimized by a single solution. Existing optimization methods often focus on finding a set of Pareto solutions with different optimal trade-offs among the objectives. However, the required number of solutions to well approximate the whole Pareto optimal set could be exponentially large with respect to the number of objectives, which makes these methods unsuitable for handling many optimization objectives. In this work, instead of finding a dense set of Pareto solutions, we propose a novel Tchebycheff set scalarization method to find a few representative solutions (e.g., 5) to cover a large number of objectives (e.g., $>100$) in a collaborative and complementary manner. In this way, each objective can be well addressed by at least one solution in the small solution set. In addition, we further develop a smooth Tchebycheff set scalarization approach for efficient optimization with good theoretical guarantees. Experimental studies on different problems with many optimization objectives demonstrate the effectiveness of our proposed method.", "title_embedding_index": 12853, "title_abs_embedding_index": 12878}, {"title": "OMCAT: Omni Context Aware Transformer", "link_suffix": "/forum?id=oCIEUHJjNj", "link": "https://openreview.net/forum?id=oCIEUHJjNj", "pdf_link": "https://openreview.net/pdf?id=oCIEUHJjNj", "keywords": "Multimodal LLMs, Audio-Video Understanding, Cross-modal", "abstract": "Large Language Models (LLMs) have made significant strides in text generation and comprehension, with recent advancements extending into multimodal LLMs that integrate visual and audio inputs. However, these models continue to struggle with fine-grained, cross-modal temporal understanding, particularly when correlating events across audio and video streams. We address these challenges with two key contributions: a new dataset and model, called OCTAV and OMCAT respectively. OCTAV (Omni Context and Temporal Audio Video) is a novel dataset designed to capture event transitions across audio and video. Second, OMCAT (Omni Context Aware Transformer) is a powerful model that leverages RoTE (Rotary Time Embeddings), an innovative extension of RoPE, to enhance temporal grounding and computational efficiency in time-anchored tasks. Through a robust three-stage training pipeline\u2014feature alignment, instruction tuning, and OCTAV-specific training\u2014OMCAT excels in cross-modal temporal understanding. Our model demonstrates state-of-the-art performance on Audio-Visual Question Answering (AVQA) tasks and the OCTAV benchmark, showcasing significant gains in temporal reasoning and cross-modal alignment, as validated through comprehensive experiments and ablation studies. Our dataset and code will be made publicly available. The link to our demo page ishttps://om-cat.github.io.", "title_embedding_index": 12854, "title_abs_embedding_index": 12879}, {"title": "Forget the Data and Fine-Tuning! Just Fold the Network to Compress", "link_suffix": "/forum?id=W2Wkp9MQsF", "link": "https://openreview.net/forum?id=W2Wkp9MQsF", "pdf_link": "https://openreview.net/pdf?id=W2Wkp9MQsF", "keywords": "Model compression, model folding, model merging", "abstract": "We introduce model folding, a novel data-free model compression technique that merges structurally similar neurons across layers, significantly reducing the model size without the need for fine-tuning or access to training data. Unlike existing methods, model folding preserves data statistics during compression by leveraging $k$-means clustering, and using novel data-free techniques to prevent variance collapse or explosion. Our theoretical framework and experiments across standard benchmarks, including ResNet18 and LLaMA-7B, demonstrate that model folding achieves comparable performance to data-driven compression techniques and outperforms recently proposed data-free methods, especially at high sparsity levels. This approach is particularly effective for compressing large-scale models, making it suitable for deployment in resource-constrained environments.", "title_embedding_index": 12855, "title_abs_embedding_index": 12880}, {"title": "JustLogic: A benchmark for natural language deductive reasoning", "link_suffix": "/forum?id=THSm9HyCKo", "link": "https://openreview.net/forum?id=THSm9HyCKo", "pdf_link": "https://openreview.net/pdf?id=THSm9HyCKo", "keywords": "benchmark, logical reasoning, LLM, natural language processing (NLP), propositional logic", "abstract": "Logical reasoning is a critical component of Large Language Models (LLMs), and substantial research efforts in recent years have aimed to enhance their deductive capabilities. However, existing deductive reasoning benchmarks, which are crucial for evaluating and advancing LLMs, are inadequate due to their lack of task complexity, presence of prior knowledge as a confounder, and superficial error analysis. To address these deficiencies, we introduce JustLogic, a synthetically generated deductive reasoning benchmark designed for rigorous evaluation of LLMs. JustLogic is (i) highly complex, capable of generating a diverse range of linguistic patterns, vocabulary, and argument structures; (ii) context-independent, eliminating the advantage of models possessing prior knowledge and ensuring that only deductive reasoning is used to answer questions; and (iii) capable of in-depth error analysis on the heterogeneous effects of reasoning depth and argument form on model accuracy. Our experimental results on JustLogic reveal that the performance of most state-of-the-art (SOTA) LLMs, specifically Llama3-8B (57.8%), Llama3-70B (64.6%), and GPT-4o (65.6%), is significantly worse than the average human performance (73.0%). A recently released reasoning model, OpenAI o1-preview, performed substantially better, with an accuracy of 81.0%. However, it still lags behind the human ceiling of 100.0%. These results demonstrate that the JustLogic benchmark is realistic and achievable for both humans and models and that there is still substantial room for improvement in the deductive reasoning capabilities of LLMs. We posit that the use of context-dependent and relatively simplistic benchmarks has misrepresented the reasoning abilities of many SOTA models. We release our open-source dataset to provide accurate evaluations of model performance in deductive reasoning and to facilitate LLM advancement through in-depth error analysis.", "title_embedding_index": 12856, "title_abs_embedding_index": 12881}, {"title": "HeadMap: Locating and Enhancing Knowledge Circuits in LLMs", "link_suffix": "/forum?id=jUsrbOuQ5e", "link": "https://openreview.net/forum?id=jUsrbOuQ5e", "pdf_link": "https://openreview.net/pdf?id=jUsrbOuQ5e", "keywords": "Large Language Model, Parameter-Efficient Fine-Tuning, Interpretability", "abstract": "Large language models (LLMs), through pretraining on extensive corpora, encompass rich semantic knowledge and exhibit the potential for efficient adaptation to diverse downstream tasks. However, the intrinsic mechanisms underlying LLMs remain unexplored, limiting the efficacy of applying these models to downstream tasks. In this paper, we explore the intrinsic mechanisms of LLMs from the perspective of knowledge circuits. Specifically, considering layer dependencies, we propose a layer-conditioned locating algorithm to identify a series of attention heads, which is a knowledge circuit of some tasks. Experiments demonstrate that simply masking a small portion of attention heads in the knowledge circuit can significantly reduce the model's ability to make correct predictions. This suggests that the knowledge flow within the knowledge circuit plays a critical role when the model makes a correct prediction. Inspired by this observation, we propose a novel parameter-efficient fine-tuning method called HeadMap, which maps the activations of these critical heads in the located knowledge circuit to the residual stream by two linear layers, thus enhancing knowledge flow from the knowledge circuit in the residual stream. Extensive experiments conducted on diverse datasets demonstrate the efficiency and efficacy of the proposed method.", "title_embedding_index": 12857, "title_abs_embedding_index": 12882}, {"title": "X-PlugVid: Versatile Adaptation of Image Plugins for Controllable Video Generation", "link_suffix": "/forum?id=TTWxMAwS6n", "link": "https://openreview.net/forum?id=TTWxMAwS6n", "pdf_link": "https://openreview.net/pdf?id=TTWxMAwS6n", "keywords": "video generation, diffusion model, efficiency", "abstract": "We introduce X-PlugVid, a unified framework designed to seamlessly adapt pretrained image-based plug-and-play modules for video diffusion models, facilitating controllable video generation without the need for retraining. This framework leverages a spatial-temporal adapter to effectively bridge the gap between image and video diffusion models. Specifically, we adopt a frozen copy of a large-scale\npretrained image diffusion model (e.g. Stable Diffusion v1.5) as spatial prior. Then we train a spatial-temporal adapter to convert the prior into temporally consistent guidance for video diffusion models (e.g. SVD). To further enhance the effectiveness of image plugins in guiding video models, we introduce a timestep remapping strategy. Recognizing that denoising is an entropic reduction process, this strategy selects priors from later timesteps of the image model, which contain richer information, to be injected into the video models, optimizing the quality and consistency of the generated videos. Comprehensive experimental evaluations of X-PlugVid demonstrate its broad compatibility with diverse operational conditions and different plugins, confirming that leveraging priors from a pretrained diffusion model can minimize redundant training and enable versatile controllable video generation.", "title_embedding_index": 12858, "title_abs_embedding_index": 12883}, {"title": "Genetic-evolutionary Graph Nerual Networks: A Paradigm for Improved Graph Representation Learning", "link_suffix": "/forum?id=bOjmeZkmxI", "link": "https://openreview.net/forum?id=bOjmeZkmxI", "pdf_link": "https://openreview.net/pdf?id=bOjmeZkmxI", "keywords": "graph neural networks, graph representation learning, genetic evolution", "abstract": "Message-passing graph neural networks have become the dominant framework for learning over graphs. However, empirical studies continually show that message-passing graph neural networks tend to generate over-smoothed representations for nodes after iteratively applying message passing. This over-smoothing problem is a core issue that limits the representational capacity of message-passing graph neural networks. We argue that the fundamental problem with over-smoothing is a lack of diversity in the generated embeddings, and the problem could be reduced by preserving the embedding diversity in their generation process. To this end, we propose genetic-evolutionary graph neural networks, a new paradigm for graph representation learning inspired by genetic algorithms. We model each layer of a graph neural network as an evolutionary process and develop operations based on crossover and mutation to prevent embeddings from becoming similar to one another, thus enabling the model to generate improved graph representations. The proposed framework is interpretable, as it directly draws inspiration from genetic algorithms for preserving population diversity. We experimentally validate the proposed framework on six benchmark datasets on different tasks. The results show that our method significant advances the performance current graph neural networks, resulting in new state-of-the-art results for graph representation learning on the datasets.", "title_embedding_index": 12859, "title_abs_embedding_index": 12884}, {"title": "Shapley Image Explanations With Data-Aware Binary Partition Trees", "link_suffix": "/forum?id=dUiMNQHYXG", "link": "https://openreview.net/forum?id=dUiMNQHYXG", "pdf_link": "https://openreview.net/pdf?id=dUiMNQHYXG", "keywords": "Shapley coefficients, Owen approximation, Binary Partition Tree, eXplainable AI", "abstract": "Extracting a visual interpretation of a learned representation of a machine learning model applied to image data is a relevant task in eXplainable AI (XAI). \nPixel-level feature attributions serve as a valuable tool in this context, as they identify the regions within an image responsible for the classification outcome.\nThe hierarchical Owen approximation of the Shapley values has proved to be an effective strategy for this task. \nHowever, existing approaches lack data-awareness, leading to poor alignment between the pixel-level attributions and the actual morphological features of the classified image.This paper introduces ShapBPT, a novel XAI method that computes the Owen approximation of the Shapley coefficients following a data-aware binary hierarchical coalition structure derived from the Binary Partition Tree computer vision algorithm. \nBy aligning with the morphological features of the image, the proposed method significantly enhances the identification of relevant image regions.\nExperimental results confirm the effectiveness of the proposed method.", "title_embedding_index": 12860, "title_abs_embedding_index": 12885}, {"title": "Towards Formally Verifying LLMs: Taming the Nonlinearity of the Transformer", "link_suffix": "/forum?id=evDSvZBFRP", "link": "https://openreview.net/forum?id=evDSvZBFRP", "pdf_link": "https://openreview.net/pdf?id=evDSvZBFRP", "keywords": "large language models, formal verification, set-based computing, matrix polynomial zonotopes, neural networks", "abstract": "Large language models are increasingly used across various domains,\n    which raises important safety concerns, particularly regarding adversarial attacks.\n    While recent advancements in formal neural network verification have shown promising results, \n    the complexity of transformers, the backbone of large language models, poses unique challenges for formal robustness verification.\n    Traditional convex relaxation methods often result in large approximation errors due to the transformer's parallel, nonlinear attention heads.\n    In this work, we address these limitations by introducing a novel approach based on non-convex, set-based computing to preserve the nonlinear dependencies through a transformer.\n    Our approach generalizes previous methods on robustness verification of transformers,\n    and the desired precision is tunable at the cost of additional computation time with a single parameter.", "title_embedding_index": 12861, "title_abs_embedding_index": 12886}, {"title": "KD-HGRL: Knowledge Distillation for Multi-Task Heterogeneous Graph Representation Learning", "link_suffix": "/forum?id=OMN1UtDbRf", "link": "https://openreview.net/forum?id=OMN1UtDbRf", "pdf_link": "https://openreview.net/pdf?id=OMN1UtDbRf", "keywords": "Knowledge Distillation, Graph Neural Network, Embedding Transfer, Heterogeneous Graph, Contrastive Learning, Self-Supervised Learning.", "abstract": "Heterogeneous graphs capture complex relationships in real-world systems like social networks and knowledge graphs. Heterogeneous Graph Neural Networks (HGNNs) have been proposed for processing these graphs. Still, they often require extensive labeled data, especially for training on large networks, which is impractical or even impossible to obtain. The key question is whether we can train heterogeneous graphs using self-supervised learning and then transfer this knowledge to downstream tasks like node classification and link prediction. To address this, we propose a novel framework, KD-HGRL, which leverages \\textbf{K}nowledge \\textbf{D}istillation for multi-task \\textbf{H}eterogenous \\textbf{G}raph \\textbf{R}epresentation \\textbf{L}earning. Our approach consists of two main phases: teacher and student. The teacher phase generates rich node embeddings from two views\u2014the semantic view and the topological view. A self-supervised learning strategy trains the teacher model without needing labeled data. The student model uses a light graph neural network with a multi-layer perception to predict the task output. A novel knowledge distillation strategy transfers the wealthy, learned knowledge, which reflects the graph's topological structure and semantic information, from the pre-trained teacher to the student model. This allows the student model to understand the downstream tasks graph better. The results from several experiments performed on real-world benchmarks demonstrate the superiority of the proposed method over state-of-the-art approaches.", "title_embedding_index": 12862, "title_abs_embedding_index": 12887}, {"title": "A Kernel Distribution Closeness Testing", "link_suffix": "/forum?id=fgaNjiijS1", "link": "https://openreview.net/forum?id=fgaNjiijS1", "pdf_link": "https://openreview.net/pdf?id=fgaNjiijS1", "keywords": "hypothesis testing, Maximum Mean Discrepancy, distribution closeness testing, two samples testing", "abstract": "The \\emph{distribution closeness testing} (DCT) assesses whether two distributions are at least $\\epsilon$-far, including two-sample testing as a specific case with $\\epsilon=0$. However, existing DCT methods are mainly based on discrepancies between two distributions defined on discrete one-dimensional spaces (e.g., total variation on a discrete one-dimensional space), which limits the DCT to be used on complex data (e.g., images). To make DCT applicable on complex data, a natural idea is to introduce the \\emph{maximum mean discrepancy} (MMD), a powerful measurement to see the difference between a pair of two complex distributions, to DCT scenarios. Nonetheless, in this paper, we find that MMD value is less informative when measuring the closeness between two distributions, i.e., MMD value can be the same for many pairs of distributions that have different norms in the \\emph{reproducing kernel Hilbert space} (RKHS). To mitigate the above drawback of MMD, we propose a new kernel DCT with the \\emph{norm-adaptive MMD} (NAMMD), by scaling MMD with the norms of distributions in the RKHS. Theoretically, we prove that our NAMMD test achieves higher test power compared to the MMD test, along with asymptotic distribution analysis. We also present upper bounds on the sample complexity of our NAMMD test and prove that Type-I error is controlled.  We finally conduct experiments to validate the effectiveness of our NAMMD test. Both theoretical and empirical results show that, benefiting from RKHS, we can successfully test the closeness of two distributions defined on a complex space.", "title_embedding_index": 12863, "title_abs_embedding_index": 12888}, {"title": "Multi-Session Budget Optimization for Forward Auction-based Federated Learning", "link_suffix": "/forum?id=rpEATZvmjr", "link": "https://openreview.net/forum?id=rpEATZvmjr", "pdf_link": "https://openreview.net/pdf?id=rpEATZvmjr", "keywords": "Federated Learning, Auction-based Federated Learning, Bidding Strategy", "abstract": "Auction-based Federated Learning (AFL) has emerged as an important research field in recent years. The prevailing strategies for FL data consumers (DCs) assume that the entire team of the required data owners (DOs) for an FL task must be assembled before training can commence. In practice, a DC can trigger the FL training process multiple times. DOs can thus be gradually recruited over multiple FL model training sessions. Existing bidding strategies for AFL DCs are not designed to handle such scenarios. Therefore, the problem of multi-session AFL remains open. To address this problem, we propose the Multi-session Budget Optimization Strategy for forward Auction-based Federated Learning (MultiBOS-AFL). Based on hierarchical reinforcement learning, MultiBOS-AFL jointly optimizes inter-session budget pacing and intra-session bidding for AFL DCs, with the objective of maximizing the total utility. Extensive experiments on six benchmark datasets show that it significantly outperforms seven state-of-the-art approaches. On average, MultiBOS-AFL achieves 12.28% higher utility, 14.52% more data acquired through auctions for a given budget, and 1.23% higher test accuracy achieved by the resulting FL model compared to the best baseline. To the best of our knowledge, it is the first budget optimization decision support method with budget pacing capability designed for DCs in multi-session forward auction-based FL.", "title_embedding_index": 12864, "title_abs_embedding_index": 12889}, {"title": "Matrix Sketching in Bandits: Current Pitfalls and New Framework", "link_suffix": "/forum?id=X75isqETqR", "link": "https://openreview.net/forum?id=X75isqETqR", "pdf_link": "https://openreview.net/pdf?id=X75isqETqR", "keywords": "Linear Bandits, Matrix Sketching, Regret Analysis", "abstract": "The utilization of sketching techniques has progressively emerged as a pivotal method for enhancing the efficiency of online learning. \n    In linear bandit settings, current sketch-based approaches leverage matrix sketching to reduce the per-round time complexity from $\\Omega\\left(d^2\\right)$ to $O(d)$, where $d$ is the input dimension. Despite this improved efficiency, these approaches encounter critical pitfalls: if the spectral tail of the covariance matrix does not decrease rapidly, it can lead to linear regret.\n    In this paper, we revisit the regret analysis and algorithm design concerning approximating the covariance matrix using matrix sketching in linear bandits. \n    We illustrate how inappropriate sketch sizes can result in unbounded spectral loss, thereby causing linear regret. \n    To prevent this issue, we propose Dyadic Block Sketching, an innovative streaming matrix sketching approach that adaptively manages sketch size to constrain global spectral loss. \n    This approach effectively tracks the best rank-$k$ approximation in an online manner, ensuring efficiency when the geometry of the covariance matrix is favorable. \n    Then, we apply the proposed Dyadic Block Sketching to linear bandits and demonstrate that the resulting bandit algorithm can achieve sublinear regret without prior knowledge of the covariance matrix, even under the worst case. \n    Our method is a general framework for efficient sketch-based linear bandits, applicable to all existing sketch-based approaches, and offers improved regret bounds accordingly.\n    Additionally, we conduct comprehensive empirical studies using both synthetic and real-world data to validate the accuracy of our theoretical findings and to highlight the effectiveness of our algorithm.", "title_embedding_index": 12865, "title_abs_embedding_index": 12890}, {"title": "Consistency Diffusion Models for Singel-Image 3D Reconstruction with priors", "link_suffix": "/forum?id=385gQZuuuR", "link": "https://openreview.net/forum?id=385gQZuuuR", "pdf_link": "https://openreview.net/pdf?id=385gQZuuuR", "keywords": "Bound, Variational Bayesian, 3D Point Cloud, Single-Image, Reconstruction", "abstract": "This paper delves into the study of 3D point cloud reconstruction from a single image. Our objective is to develop the Consistency Diffusion Model, exploring synergistic 2D and 3D priors in the Bayesian framework to ensure superior consistency in the reconstruction process, a challenging yet critical requirement in this field. Specifically, we introduce a pioneering training framework under diffusion models that brings two key innovations. First, we convert 3D structural priors derived from the initial 3D point cloud as a bound term to increase evidence in the variational Bayesian framework, leveraging these robust intrinsic priors to tightly govern the diffusion training process and bolster consistency in reconstruction. Second, we extract and incorporate 2D priors from the single input image, projecting them onto the 3D point cloud to enrich the guidance for diffusion training. Our framework not only sidesteps potential model learning shifts that may arise from directly imposing additional constraints during training but also precisely transposes the 2D priors into the 3D domain. Extensive experimental evaluations reveal that our approach sets new benchmarks in both synthetic and real-world datasets. The code will be released.", "title_embedding_index": 12866, "title_abs_embedding_index": 12891}, {"title": "Harnessing Task Overload for Scalable Jailbreak Attacks on Large Language Models", "link_suffix": "/forum?id=qPZaTqLee4", "link": "https://openreview.net/forum?id=qPZaTqLee4", "pdf_link": "https://openreview.net/pdf?id=qPZaTqLee4", "keywords": "Jailbreak Attack, Large Language Model, Adversarial Attack", "abstract": "Large Language Models (LLMs) remain vulnerable to jailbreak attacks that bypass their safety mechanisms. Existing attack methods are fixed or specifically tailored for certain models and cannot flexibly adjust attack strength, which is critical for generalization when attacking models of various sizes. We introduce a novel scalable jailbreak attack that preempts the activation of an LLM's safety policies by occupying its computational resources. Our method involves engaging the LLM in a resource-intensive preliminary task\u2014a Character Map lookup and decoding process\u2014before presenting the target instruction. \nBy saturating the model's processing capacity, we prevent the activation of safety protocols when processing the subsequent instruction. \nExtensive experiments on state-of-the-art LLMs demonstrate that our method achieves a high success rate in bypassing safety measures without requiring gradient access, manual prompt engineering.  We verified our approach offers a scalable attack that quantifies attack strength and adapts to different model scales at the optimal strength. We shows safety policies of LLMs might be more susceptible to resource constraints.\nOur findings reveal a critical vulnerability in current LLM safety designs, highlighting the need for more robust defense strategies that account for resource-intense condition.", "title_embedding_index": 12867, "title_abs_embedding_index": 12892}, {"title": "Strategic Exploration for Inverse Constraint Inference with Efficiency Guarantee", "link_suffix": "/forum?id=2jzhImk4br", "link": "https://openreview.net/forum?id=2jzhImk4br", "pdf_link": "https://openreview.net/pdf?id=2jzhImk4br", "keywords": "Inverse Constrained Reinforcement Learning, Exploration Algorithm, Sample Efficiency", "abstract": "In many realistic applications, the constraint is not readily available, and we need to infer the constraints respected by the expert agents from their behaviors. The problem is known as Inverse Constraint Inference (ICI). A common solver, Inverse Constrained Reinforcement Learning (ICRL) seeks to recover the optimal constraints in complex environments in a data-driven manner. Existing ICRL algorithms collect training samples from an interactive environment. However, the efficacy and efficiency of these sampling strategies remain unknown. To bridge this gap, we introduce a strategic exploration framework with guaranteed efficiency. Specifically, we define a feasible constraint set for ICRL problems and investigate how expert policy and environmental dynamics influence the optimality of constraints. Motivated by our findings, we propose two exploratory algorithms to achieve efficient constraint inference via 1) dynamically reducing the bounded aggregate error of cost estimation and 2) strategically constraining the exploration policy. Both algorithms are theoretically grounded with tractable sample complexity. We empirically demonstrate the performance of our algorithms under various environments.", "title_embedding_index": 12868, "title_abs_embedding_index": 12893}, {"title": "INDIRECT ATTENTION: IA-DETR FOR ONE SHOT OBJECT DETECTION", "link_suffix": "/forum?id=DcJuTtfYss", "link": "https://openreview.net/forum?id=DcJuTtfYss", "pdf_link": "https://openreview.net/pdf?id=DcJuTtfYss", "keywords": "One shot object detection, DETR, cross-attention", "abstract": "One-shot object detection presents a significant challenge, requiring the identification of objects within a target image using only a single sample image of the object class as query image. Attention-based methodologies have garnered considerable attention in the field of object detection. Specifically, the cross-attention module, as seen in DETR, plays a pivotal role in exploiting the relationships be-\ntween object queries and image features. However, in the context of DETR networks for one-shot object detection, the intricate interplay among target image features, query image features, and object queries must be carefully considered.\nIn this study, we propose a novel module termed \u201dindirect attention.\u201d We illustrate that relationships among target image features, query image features, and object queries can be effectively captured in a more concise manner compared to\ncross-attention. Furthermore, we introduce a pre-training pipeline tailored specifically for one-shot object detection, addressing three primary objectives: identifying objects of interest, class differentiation, and object detection based on a given\nquery image. Our experimental findings demonstrate that the proposed IA-DETR (Indirect-Attention DETR) significantly outperforms state-of-the-art one-shot object detection methods on both the Pascal VOC and COCO benchmarks.", "title_embedding_index": 12869, "title_abs_embedding_index": 12894}, {"title": "Optimization by Parallel Quasi-Quantum Annealing with Gradient-Based Sampling", "link_suffix": "/forum?id=9EfBeXaXf0", "link": "https://openreview.net/forum?id=9EfBeXaXf0", "pdf_link": "https://openreview.net/pdf?id=9EfBeXaXf0", "keywords": "Combinatorial Optimization, Discrete Optimization, Learning for Combinatorial Optimization, Unsupervised Learning for Combinatorial Optimization, Learning for Combinatorial Optimization", "abstract": "Learning-based methods have gained attention as general-purpose solvers due to their ability to automatically learn problem-specific heuristics, reducing the need for manually crafted heuristics. However, these methods often face scalability challenges. To address these issues, the improved Sampling algorithm for Combinatorial Optimization (iSCO), using discrete Langevin dynamics, has been proposed, demonstrating better performance than several learning-based solvers. This study proposes a different approach that integrates gradient-based update through continuous relaxation, combined with Quasi-Quantum Annealing (QQA). QQA smoothly transitions the objective function, starting from a simple convex function, minimized at half-integral values, to the original objective function, where the relaxed variables are minimized only in the discrete space. Furthermore, we incorporate parallel run communication leveraging GPUs to enhance exploration capabilities and accelerate convergence. Numerical experiments demonstrate that our method is a competitive general-purpose solver, achieving performance comparable to iSCO and learning-based solvers across various benchmark problems. Notably, our method exhibits superior speed-quality trade-offs for large-scale instances compared to iSCO, learning-based solvers, commercial solvers, and specialized algorithms.", "title_embedding_index": 12870, "title_abs_embedding_index": 12895}, {"title": "Debiased Deep Evidential Regression for Video Temporal Grounding", "link_suffix": "/forum?id=1V28zvLJMg", "link": "https://openreview.net/forum?id=1V28zvLJMg", "pdf_link": "https://openreview.net/pdf?id=1V28zvLJMg", "keywords": "Video Temporal Grounding, Uncertainty Quantification, Multi-Modal Fusion, Deep evidential regression, Evidential deep learning", "abstract": "Existing Video Temporal Grounding (VTG) models perform well in accuracy but often fail to address open-world challenges posed by open-vocabulary queries and out-of-distribution (OOD) videos, which can lead to unreliable predictions. To address uncertainty, particularly with OOD data, we build a VTG baseline using Deep Evidential Regression (DER), which excels in capturing both aleatoric and epistemic uncertainty. Despite promising results, our baseline faces two key biases in multimodal tasks: (1) Modality imbalance, where uncertainty estimation is more sensitive to the visual modality than the text modality; (2) Counterintuitive uncertainty, resulting from excessive evidence suppression in regularization and uneven sample error distribution in conventional DER. To address these, we propose an RFF block for progressive modality alignment and a query reconstruction task to enhance sensitivity to text queries. Additionally, we introduce a Geom-regularizer to debias and calibrate uncertainty estimation. This marks the first extension of DER in VTG tasks. Extensive experiments demonstrate the effectiveness and robustness of our approach. Our code will be released soon.", "title_embedding_index": 12871, "title_abs_embedding_index": 12896}, {"title": "Guaranteed Generation from Large Language Models", "link_suffix": "/forum?id=8roRgrjbjv", "link": "https://openreview.net/forum?id=8roRgrjbjv", "pdf_link": "https://openreview.net/pdf?id=8roRgrjbjv", "keywords": "Guaranteed Generation, Controlled Text Generation, LLM Alignment, Limitations of Autoregressive Models, Rejection Sampling", "abstract": "As large language models (LLMs) are increasingly used across various applications, there is a growing need to control text generation to satisfy specific constraints or requirements. This raises a crucial question: Is it possible to guarantee strict constraint satisfaction in generated outputs while preserving the distribution of the original model as much as possible? We first define the ideal distribution \u2014 the one closest to the original model, which also always satisfies the expressed constraint \u2014 as the ultimate goal of guaranteed generation. We then state a fundamental limitation, namely that it is impossible to reach that goal through autoregressive training alone. This motivates the necessity of combining training-time and inference-time methods to enforce such guarantees. Based on this insight, we propose GUARD, a simple yet effective approach that combines an autoregressive proposal distribution with rejection sampling. Through GUARD\u2019s theoretical properties, we show how controlling the KL divergence between a specific proposal and the target ideal distribution simultaneously optimizes inference speed and distributional closeness. To validate these theoretical concepts, we conduct extensive experiments on two text generation settings with hard-to-satisfy constraints: a lexical constraint scenario and a sentiment reversal scenario. These experiments show that GUARD achieves perfect constraint satisfaction while almost preserving the ideal distribution with highly improved inference efficiency. GUARD provides a principled approach to enforcing strict guarantees for LLMs without compromising their generative capabilities.", "title_embedding_index": 12872, "title_abs_embedding_index": 12897}, {"title": "LLaMoCo: Instruction Tuning of Large Language Models for Optimization Code Generation", "link_suffix": "/forum?id=EKCubxFdOs", "link": "https://openreview.net/forum?id=EKCubxFdOs", "pdf_link": "https://openreview.net/pdf?id=EKCubxFdOs", "keywords": "Large Language Models, Instruction Tuning, Optimization Code Generation", "abstract": "Recent research on optimization using large language models (LLMs) typically involves either iterative next-step solution seeking or directly prompting LLMs to generate critical optimization codes. However, these methods often suffer from low computational efficiency, high sensitivity to prompt design, and a lack of domain-specific knowledge. We introduce LLaMoCo, the first instruction-tuning framework designed to adapt LLMs for solving optimization problems in a code-to-code manner. LLaMoCo features a comprehensive instruction set that includes code-style problem descriptions as input prompts and robust optimization codes from expert optimizers as target outputs. We then develop a novel two-phase learning strategy with a contrastive learning-based warm-up to enhance convergence during instruction tuning. Extensive experiments demonstrate that a CodeGen (350M) model tuned by our LLaMoCo yields a powerful domain-specific model for generating expert-level optimizers, achieving superior performance compared to GPT-4 Turbo and other competitors on both synthetic and realistic problem sets. The trained model and the usage instructions are available online.", "title_embedding_index": 12873, "title_abs_embedding_index": 12898}, {"title": "Learning Nash Equilibria in Normal-Form Games via Approximating Stationary Points", "link_suffix": "/forum?id=q2CcNvzgb7", "link": "https://openreview.net/forum?id=q2CcNvzgb7", "pdf_link": "https://openreview.net/pdf?id=q2CcNvzgb7", "keywords": "Deep Learning, Nash Equilibrium, Normal-Form Games", "abstract": "Nash equilibrium (NE) plays an important role in game theory. However, learning an NE in normal-form games (NFGs) is a complex, non-convex optimization problem. Deep Learning (DL), the cornerstone of modern artificial intelligence, has demonstrated remarkable empirical performance across various applications involving non-convex optimization. However, applying DL to learn an NE poses significant difficulties since most existing loss functions for using DL to learn an NE introduce bias under sampled play. A recent work proposed an unbiased loss function. Unfortunately, it suffers from high variance, which degrades the convergence rate. Moreover, learning an NE through this unbiased loss function entails finding a global minimum in a non-convex optimization problem, which is inherently difficult. To mitigate the high variance and reduce the complexity of learning an NE associated with the existing unbiased loss function, we propose a novel loss function, named Nash Advantage Loss (NAL). NAL is unbiased and exhibits significantly lower variance than the existing unbiased loss function. Furthermore, learning an NE by minimizing NAL is more tractable, as an NE is a stationary point of NAL rather than having to be a global minimum. Experimental results demonstrate that the algorithm minimizing NAL achieves significantly faster empirical convergence rates compared to previous algorithms, while also reducing the variance of estimated loss value by several orders of magnitude.", "title_embedding_index": 12874, "title_abs_embedding_index": 12899}]